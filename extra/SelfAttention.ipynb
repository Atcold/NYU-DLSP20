{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SelfAttention.ipynb","provenance":[],"authorship_tag":"ABX9TyMf+ru+H66ry2aTHPTIRzWf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1_uyTmZOslbl","colab_type":"text"},"source":["## Self Attention - Pytorch\n","\n","- A self-attention module takes in n inputs, and returns n outputs. \n","- In layman’s terms, the self-attention mechanism allows the inputs to interact with each other and find out who they should pay more attention to. \n","- The outputs are aggregates of these interactions and attention scores."]},{"cell_type":"markdown","metadata":{"id":"AP21aiBtY8O0","colab_type":"text"},"source":["## Why we need self attention?\n","\n","![alt text](https://miro.medium.com/max/1200/0*hJfCjjx0r0slacNm.png)\n","\n","- Attention Mechanism are common in NLP tasks based on neural networks such as RNN/CNN.\n","\n","- Self-Attention is capable of learning the distant dependencies within the phrase.\n","\n","-  Self-Attention is unique as it ignores the distance between words and directly computes dependency relationships, making it capable of learning the internal structure of a sentence and more merely calculating in parallel.\n","\n","- Below are the mathematical steps to calculate self attention."]},{"cell_type":"markdown","metadata":{"id":"uL19i-WcQJ_s","colab_type":"text"},"source":["![alt text](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"]},{"cell_type":"code","metadata":{"id":"u1UxPJlHBVmS","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ridNEUL-K5hH","colab_type":"text"},"source":["## 1. Prepare inputs - All inputs are vectorised:"]},{"cell_type":"markdown","metadata":{"id":"x0rxKVkYLpgC","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"dE8zvglOV3-H","colab_type":"code","outputId":"c3e02990-be56-4f58-b408-9dae2d617cd8","executionInfo":{"status":"ok","timestamp":1584476898682,"user_tz":240,"elapsed":2466,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["\n","\n","input = [\n","  [1, 0, 1, 0], # Input 1\n","  [0, 2, 0, 2], # Input 2\n","  [1, 1, 1, 1],  # Input 3\n","  [3, 1, 1, 1]  # Input 4\n"," ]\n","input = torch.tensor(input, dtype=torch.float32)\n","input"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 1., 0.],\n","        [0., 2., 0., 2.],\n","        [1., 1., 1., 1.],\n","        [3., 1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"6tnsUy7936sm","colab_type":"text"},"source":["## 2. Initialize weights: \n","- Every input must have three representations. \n","- These representations are called key, query, and value.\n","- Lets say each of this representation to have a size of 5.\n","- Because every input has a dimension of 4, this means each set of the weights must have a shape of 4×5."]},{"cell_type":"code","metadata":{"id":"z3GEsieaxavI","colab_type":"code","outputId":"00d2c0ad-5776-47c6-a609-49b11264a062","executionInfo":{"status":"ok","timestamp":1584476898683,"user_tz":240,"elapsed":2460,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["w_key = [\n","  [0, 0, 1, 1, 1],\n","  [1, 1, 0, 0, 1],\n","  [0, 1, 0, 1, 1],\n","  [1, 1, 0, 0, 0]\n","]\n","w_query = [\n","  [1, 0, 1, 0, 1],\n","  [1, 0, 0, 1, 1],\n","  [0, 0, 1, 0, 0],\n","  [0, 1, 1, 1, 0]\n","]\n","w_value = [\n","  [0, 2, 0, 3, 1],\n","  [0, 3, 0, 1, 3],\n","  [1, 0, 3, 1, 1],\n","  [1, 1, 0, 2, 2]\n","]\n","w_key = torch.tensor(w_key, dtype=torch.float32)\n","w_query = torch.tensor(w_query, dtype=torch.float32)\n","w_value = torch.tensor(w_value, dtype=torch.float32)\n","\n","print(\"Weights for key: \\n\", w_key)\n","print(\"Weights for query: \\n\", w_query)\n","print(\"Weights for value: \\n\", w_value)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Weights for key: \n"," tensor([[0., 0., 1., 1., 1.],\n","        [1., 1., 0., 0., 1.],\n","        [0., 1., 0., 1., 1.],\n","        [1., 1., 0., 0., 0.]])\n","Weights for query: \n"," tensor([[1., 0., 1., 0., 1.],\n","        [1., 0., 0., 1., 1.],\n","        [0., 0., 1., 0., 0.],\n","        [0., 1., 1., 1., 0.]])\n","Weights for value: \n"," tensor([[0., 2., 0., 3., 1.],\n","        [0., 3., 0., 1., 3.],\n","        [1., 0., 3., 1., 1.],\n","        [1., 1., 0., 2., 2.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gpVylNvPLfHh","colab_type":"text"},"source":["## 3.Derive keys, querys and values"]},{"cell_type":"code","metadata":{"id":"w5NaRUhNxM22","colab_type":"code","outputId":"f6076890-1490-4825-c779-e82aa51145bb","executionInfo":{"status":"ok","timestamp":1584476898864,"user_tz":240,"elapsed":2634,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["queries = torch.matmul(input, w_query)\n","queries"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 2., 0., 1.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 1., 3., 2., 2.],\n","        [4., 1., 5., 2., 4.]])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"d_SIb5-dIIMY","colab_type":"code","outputId":"c0e35b53-2910-4fc8-c4e4-adc7395caee0","executionInfo":{"status":"ok","timestamp":1584476898865,"user_tz":240,"elapsed":2628,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# The above could be written also as:\n","input @ w_query"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 2., 0., 1.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 1., 3., 2., 2.],\n","        [4., 1., 5., 2., 4.]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"CrWLPYtNEUNI","colab_type":"code","outputId":"608b2028-b27a-4a5e-96cf-980543a31283","executionInfo":{"status":"ok","timestamp":1584476898866,"user_tz":240,"elapsed":2623,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["values = torch.matmul(input, w_value)\n","values"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.,  2.,  3.,  4.,  2.],\n","        [ 2.,  8.,  0.,  6., 10.],\n","        [ 2.,  6.,  3.,  7.,  7.],\n","        [ 2., 10.,  3., 13.,  9.]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"mibkhfFdEUmo","colab_type":"code","outputId":"5f4b4628-55e7-490f-e3bc-996d95b109c4","executionInfo":{"status":"ok","timestamp":1584476898866,"user_tz":240,"elapsed":2617,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["input @ w_value"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.,  2.,  3.,  4.,  2.],\n","        [ 2.,  8.,  0.,  6., 10.],\n","        [ 2.,  6.,  3.,  7.,  7.],\n","        [ 2., 10.,  3., 13.,  9.]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"2R6-wNV5Jj3S","colab_type":"code","outputId":"ab850625-66a4-4559-9cdd-2dc11e81ef06","executionInfo":{"status":"ok","timestamp":1584476898867,"user_tz":240,"elapsed":2611,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["keys = torch.matmul(input, w_key)\n","keys"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 1., 1., 2., 2.],\n","        [4., 4., 0., 0., 2.],\n","        [2., 3., 1., 2., 3.],\n","        [2., 3., 3., 4., 5.]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"pYU8ZT2jJoNu","colab_type":"code","outputId":"a0b50c9b-93cf-44ae-fc8f-cb58b2d05faa","executionInfo":{"status":"ok","timestamp":1584476898867,"user_tz":240,"elapsed":2605,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["input @ w_key"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 1., 1., 2., 2.],\n","        [4., 4., 0., 0., 2.],\n","        [2., 3., 1., 2., 3.],\n","        [2., 3., 3., 4., 5.]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"y7IjBxkPNx-J","colab_type":"text"},"source":["## 4. Calculate Attention Scores\n","\n","- To obtain attention scores, we start off with taking a dot product between Input 1’s query with all keys , including itself. Since there are 4 key representations , we obtain 4 attention scores."]},{"cell_type":"code","metadata":{"id":"6AERh4u0NxPT","colab_type":"code","outputId":"276526c3-3b94-4501-d733-ab8083ef1497","executionInfo":{"status":"ok","timestamp":1584476898868,"user_tz":240,"elapsed":2600,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["attn_sc = torch.matmul(queries, keys.T)\n","attn_sc\n","\n","# attention scores from Query 1\n","# attention scores from Query 2\n","# attention scores from Query 3\n","# attention scores from Query 4"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4.,  6.,  7., 13.],\n","        [16., 20., 26., 42.],\n","        [12., 16., 20., 34.],\n","        [18., 28., 32., 54.]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"2xqygNjnRMAd","colab_type":"text"},"source":["## 5. Calculate Softmax\n"]},{"cell_type":"code","metadata":{"id":"70s8k1TuRMrU","colab_type":"code","outputId":"d1d8921a-087f-4493-dff2-f10419654c49","executionInfo":{"status":"ok","timestamp":1584476898868,"user_tz":240,"elapsed":2593,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["from torch.nn.functional import softmax\n","\n","attn_scores_softmax = softmax(attn_sc, dim=-1)\n","print(attn_scores_softmax)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[1.2298e-04, 9.0869e-04, 2.4701e-03, 9.9650e-01],\n","        [5.1091e-12, 2.7895e-10, 1.1254e-07, 1.0000e+00],\n","        [2.7895e-10, 1.5230e-08, 8.3153e-07, 1.0000e+00],\n","        [2.3195e-16, 5.1091e-12, 2.7895e-10, 1.0000e+00]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9pl5o-fHUq_t","colab_type":"text"},"source":["## 6. Multiply attention scores with values\n","\n","- The softmaxed attention scores for each input is multiplied with its corresponding value. This results in 4 alignment vectors. In this tutorial, we’ll refer to them as weighted values."]},{"cell_type":"code","metadata":{"id":"0sz_-eeVUp-K","colab_type":"code","outputId":"bf806331-18ad-4406-818d-7e71ee038eef","executionInfo":{"status":"ok","timestamp":1584476898869,"user_tz":240,"elapsed":2588,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]\n","print(weighted_values)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[[1.2298e-04, 2.4596e-04, 3.6893e-04, 4.9191e-04, 2.4596e-04],\n","         [5.1091e-12, 1.0218e-11, 1.5327e-11, 2.0436e-11, 1.0218e-11],\n","         [2.7895e-10, 5.5789e-10, 8.3684e-10, 1.1158e-09, 5.5789e-10],\n","         [2.3195e-16, 4.6390e-16, 6.9586e-16, 9.2781e-16, 4.6390e-16]],\n","\n","        [[1.8174e-03, 7.2695e-03, 0.0000e+00, 5.4521e-03, 9.0869e-03],\n","         [5.5789e-10, 2.2316e-09, 0.0000e+00, 1.6737e-09, 2.7895e-09],\n","         [3.0460e-08, 1.2184e-07, 0.0000e+00, 9.1380e-08, 1.5230e-07],\n","         [1.0218e-11, 4.0873e-11, 0.0000e+00, 3.0655e-11, 5.1091e-11]],\n","\n","        [[4.9401e-03, 1.4820e-02, 7.4102e-03, 1.7291e-02, 1.7291e-02],\n","         [2.2507e-07, 6.7521e-07, 3.3761e-07, 7.8775e-07, 7.8775e-07],\n","         [1.6631e-06, 4.9892e-06, 2.4946e-06, 5.8207e-06, 5.8207e-06],\n","         [5.5789e-10, 1.6737e-09, 8.3684e-10, 1.9526e-09, 1.9526e-09]],\n","\n","        [[1.9930e+00, 9.9650e+00, 2.9895e+00, 1.2954e+01, 8.9685e+00],\n","         [2.0000e+00, 1.0000e+01, 3.0000e+00, 1.3000e+01, 9.0000e+00],\n","         [2.0000e+00, 1.0000e+01, 3.0000e+00, 1.3000e+01, 9.0000e+00],\n","         [2.0000e+00, 1.0000e+01, 3.0000e+00, 1.3000e+01, 9.0000e+00]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bUcAqINfXBnl","colab_type":"text"},"source":["## 7. Get Sum weighted values.\n","\n","- Get the attention scores for each input vector."]},{"cell_type":"code","metadata":{"id":"RRzt0LVyXAju","colab_type":"code","outputId":"45e09a00-a1fd-45d0-ebab-4601015122f4","executionInfo":{"status":"ok","timestamp":1584476898869,"user_tz":240,"elapsed":2582,"user":{"displayName":"Sree gowri Addepalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjF4L2xWwBcN9IWYV5ruFiL95mA5XT1CTp4gbsBxA=s64","userId":"10025619348007766215"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["outputs = weighted_values.sum(dim=0)\n","print(outputs)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 1.9999,  9.9873,  2.9973, 12.9777,  8.9951],\n","        [ 2.0000, 10.0000,  3.0000, 13.0000,  9.0000],\n","        [ 2.0000, 10.0000,  3.0000, 13.0000,  9.0000],\n","        [ 2.0000, 10.0000,  3.0000, 13.0000,  9.0000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NxbNxg9-ZEhf","colab_type":"text"},"source":["## Conclusion\n","\n","- These attention values are then fed into feedforward neural networks that go into neural network architectures like transformers.\n","![alt text](http://jalammar.github.io/images/t/encoder_with_tensors.png)"]},{"cell_type":"markdown","metadata":{"id":"etRH7atxV4Z6","colab_type":"text"},"source":["## References\n","\n","- https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n","- http://jalammar.github.io/illustrated-transformer/\n","- https://medium.com/@Alibaba_Cloud/self-attention-mechanisms-in-natural-language-processing-9f28315ff905"]},{"cell_type":"markdown","metadata":{"id":"Pkc5hw9XFhGW","colab_type":"text"},"source":["\n","\n","- https://bastings.github.io/annotated_encoder_decoder/\n","- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","- https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53\n","- https://talbaumel.github.io/blog/attention/"]}]}
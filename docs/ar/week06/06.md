---
lang-ref: ch.06
lang: ar
title: الأسبوع 6
translation-date: 1 Sep 2020
translator: Ali elfilali
---


<!-- ## Lecture part A

We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment. -->


## **الجزء اﻷول من المحاضرة**

ناقشنا ثلاثة تطبيقات للشبكات العصبية الالتفافية. بدأنا بالتعرف على الأرقام وتطبيقها في تحديد رمز بريدي مكون من 5 أرقام. في موضوع الكشف عن العناصر (object detection) تحدثنا عن كيفية استخدام معمارية متعددة المقاييس في تطبيقات الكشف عن الوجوه. أخيرًا، رأينا كيف يتم استخدام الشبكات العصبية الالتفافية في مهام التجزئة الدلالية (semantic segmentation) مع أمثلة ملموسة في نظام الرؤية الروبوتية وتجزئة العناصر في بيئة مدنية.

<!-- ## Lecture part B

We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq.
 -->

## **الجزء الثاني من المحاضرة**

ندرس الشبكات العصبية المتكررة، مشاكلها والتقنيات الشائعة للتخفيف من هذه المشكلات. نراجع بعد ذلك مجموعة متنوعة من الوحدات التي تم تطويرها لحل مشكلات نموذج الشبكات العصبية المتكررة كالانتباه، و وحدات GRU (وحدة البوابات المتكررة)، و LSTM (الذاكرة قصيرة المدى المطولة)، و Seq2Seq.


<!-- ## Practicum
We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models. -->

## تطبيق عملي

ناقشنا معمارية نماذج Vanilla RNN و LSTM و قارننا الأداء بينهما. ترث LSTM مزايا RNN، مع تحسين نقاط ضعف RNN من خلال تضمين "خلية ذاكرة" لتخزين المعلومات في الذاكرة لفترات طويلة من الزمن. تتفوق نماذج LSTM بشكل كبير على نماذج RNN.

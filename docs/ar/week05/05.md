---
lang-ref: ch.05
title: الاسبوع 5
lang: ar
translation-date: 26 Aug 2020
translator: Ala Alsanabni
---
<div dir="rtl">
<!--## Lecture part A
-->

##  المحاضرة أ

<!--We begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on convergence.
-->

نبدأ بمقدمة عن الانحدار التدريجي. سنناقش الفكرة ونتحدث أيضًا عن كيف تلعب أحجام الخطوات دورًا مهمًا في الوصول إلى الحل. ثم ننتقل إلى النزول الاشتقاقي العشوائي (SGD - Stochastic Gradient Descent) وأدائها مقارنةً بـحزمة النزول الاشتقاقي الكامل (Full Batch GD). أخيرًا نتحدث عن تحديثات الزخم وبالتحديد قاعدتي التحديث، و الفكرة وراء الزخم، وتأثيره على التقارب.

<!--
## Lecture part B
-->

## المحاضرة ب

<!--We discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient.
-->

نناقش الطرق التكيفية للنزول الاشتقاقي العشوائي (SGD - Stochastic Gradient Descent) مثل طريقة انشار متوسط مربع  الجذر (RMSprop - Root Mean Square Propagation)  و وطريقة ادم (ADAM). نتحدث أيضًا عن طبقات التسوية وتأثيراتها على عملية تدريب الشبكة العصبية. أخيرًا ، نناقش مثالًا واقعيًا للشبكات العصبية المستخدمة في الصناعة لجعل عمليات التصوير بالرنين المغناطيسي أسرع وأكثر كفاءة.

<!--
## Practicum
-->

## التدريب العملي

<!--We briefly review the matrix-multiplications and then discuss the convolutions. Key point is we use kernels by stacking and shifting. We first understand the 1D convolution by hand, and then use PyTorch to learn the dimension of kernels and output width in 1D and 2D convolutions examples. Furthermore, we use PyTorch to learn about how automatic gradient works and custom-grads.
-->

نراجع بإيجاز مضاعفات المصفوفات ثم نناقش التلافيف. النقطة الأساسية هي أننا نستخدم الانوية (kernals) عن طريق التكديس والازاجة. نفهم أولاً الالتفاف ذو البعد الوحد (1D convolution) يدويًا ، ثم نستخدم PyTorch لمعرفة أبعاد النواة وعرض الإخراج في أمثلة التلافيف 1D و 2D. علاوة على ذلك ، نستخدم PyTorch للتعرف على كيفية عمل الانحدار التلقائي والتدرجات المخصصة.
</div>

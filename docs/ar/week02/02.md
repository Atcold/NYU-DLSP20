---
lang: ar
lang-ref: ch.02
title: الأسبوع 2
---

<!--
## Lecture part A

We start by understanding what parametrised models are and then discuss what a loss function is. We then look at Gradient-based methods and how it's used in the backpropagation algorithm in a traditional neural network. We conclude this section by learning how to implement a neural network in PyTorch followed by a discussion on a more generalized form of backpropagation.
-->

## الجزء الأول من المحاضرة

سنبدأ بفهم ماهية النماذج ذات معاملات ثم نناقش ماهية دالة الخسارة. كما سنلقي نظرة على الأساليب القائمة على التدرج وكيف يتم استخدامها في خوارزمية الانتشار الخلفي في شبكة عصبية تقليدية. سنختتم هذا القسم بتعلم كيفية تنفيذ شبكة عصبية في PyTorch متبوعة بمناقشة شكل آخر من الانتشار الخلفي ولكن أكثر عمومية.

<!--
## Lecture part B

We begin with a concrete example of backpropagation and discuss the dimensions of Jacobian matrices. We then look at various basic neural net modules and compute their gradients, followed by a brief discussion on softmax and logsoftmax. The other topic of discussion in this part is Practical Tricks for backpropagation.
-->

## الجزء الثاني من المحاضرة

سنبدأ بمثال ملموس للانتشار الخلفي ونناقش أبعاد المصفوفات الجاكوبية. ثم نلقي نظرة على وحدات مختلفة من الشبكة العصبية الأساسية وسنقوم بحساب تدرجاتها، تليها مناقشة موجزة حول softmax و logsoftmax. موضوع المناقشة الآخر في هذا الجزء هو الحيل العملية للانتشار الخلفي.

<!--
## Practicum

We give a brief introduction to supervised learning using artificial neural networks. We expound on the problem formulation and conventions of data used to train these networks. We also discuss how to train a neural network for multi class classification, and how to perform inference once the network is trained.
-->

## تطبيق عملي

سنعرض مقدمة موجزة للتعلم بالإشراف باستخدام الشبكات العصبية الاصطناعية. سنشرح صياغة المشكلة ومصطلحات البيانات المتعارف عليها لتدريب هذه الشبكات. سنناقش أيضًا كيفية تدريب شبكة عصبية على للتصنيف لعدة فئات، وكيفية استخدامها للتنبؤ بعدما يتم تدريبها.



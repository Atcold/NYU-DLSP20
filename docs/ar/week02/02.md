
---
lang-ref: ch.02
العنوان: الإسبوع الثاني
---


## الجزئ أ من المحاضرة

 سنبدأ بفهم ما هي `النماذج ذات العوامل` ثم سنناقش ما هي `دالة الفرق`. بعد ذلك سننظر في الخوارزميات القائمة علي `الإنحدار` وطريقة إستخدامها في خوارزمية `الانتشار الخلفي` علي `شبكة عصبية` تقليدية.
 ثم سننهي هذا الجزئ بتعلم طريقة تنفيذ شبكة عصبية من خلال __ ثم مناقشة .ضيغة اخري من `الانتشار الخلفي` اكثر شمولا من السابقة


## الجزئ ب من المحاضرة
سنبدأ بمثال واقعي ل `الانتشار الخلفي` ونناقش ابعاد `مصفوفات جاكوب`. ثم سننظر في عدة أشكال من الشبكات العصبية ونقوم بحساب الانحدار الخاص بهم. ونتابع بمناقشة حول دوال `سوفت-ماكس` و `لوج-ماكس` .  واخيرا نناقش طرق اكثر عملية لحساب `اللإنتشار الخلفي`


## التدريب العملي
سوف نعطي مقدمة موجزة عن `التعليم بالأمثلة` بإستخدام `الشبكات العصبية االإصطناعية` . سنقوم بشرح صياغة المشكلة وصيغ البيانات المتعارف عليها  لتدريب تلك الشبكات. سنناقش ايضا طريقة تدريب تلك الشبكات العصبية لإستخدامها في `التصنيف المتعدد`. وكيفية استخدام هذه الشبكات `للتنبؤ` بعدما يتم `تدريبها`

## مصطلحات
|Parametrised models| النماذج ذات العوامل |
|--|--|
| loss function | دالة الفرق |
| Gradient | الإنحدار |
|backpropagation|الانتشار الخلفي|
|neural network| شبكة عصبية |
|Jacobian matrices|مصفوفات جاكوب|
| softmax | سوفت-ماكس |
| logsoftmax |  لوج-ماكس|
|supervised learning|التعليم بالأمثلة|
| multi class classification | التصنيف المتعدد |
|inference|التنبؤ|
| training| التدريب |



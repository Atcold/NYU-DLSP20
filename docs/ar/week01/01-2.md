<p dir='rtl' align='right'>test</p>
---
lang-ref: ch.01-2
lang: ar
lecturer: Yann LeCun
title: تطور واستخدامات ال CNNs ولماذا التعلم العميق؟
authors: Marina Zavalina, Peeyush Jain, Adrian Pearl, Davida Kollmar
date: 27 Jan 2020
translation date: --
translator: Ahmed Shahin
---


## [تطور ال CNNs](https://www.youtube.com/watch?v=0bMe_vCZo30&t=2965s)

في مخ الحيوان, تتفاعل الخلايا العصبية مع النهايات الطرفية التي لها اتجاه معين. مجموعة الخلايا العصبية التي تتفاعل مع نفس الاتجاه يتم تكرارها في جميع أنحاء مجال الرؤية.

فوكوشيما (1982) قام بتصميم شبكة عصبية تعمل بنفس الطريقة التي يعمل بها المخ. اولاً, الخلايا العصبية يتم تكرارها في مجال الرؤية. ثانياً, هناك خلايا معقدة تجمع المعلومات من الخلايا البسيطة (التي تستخلص المعلومات بناء على الاتجاه فقط). نتيجة لذلك, تحريك الصورة سيؤدي إلى تغيير ناتج الخلايا البسيطة (التفعيل), لكن لن يؤثر على التفعيل النهائي للخلايا المعقدة (التجميع الالتفافي).

ليكون (1990) استخدم الانتشار الرجعي لتدريب شبكة عصبية التفافية (CNN, Convolutional Neural Network) للتعرف على الأرقام المكتوبة بخط اليد. يوجد عرض تجريبي من 1992 يظهر أن الخوارزمية قادرة على التعرف على الأرقام المكتوبة بأي أسلوب. امكانية التعرف على الأرقام باستخدام نموذج مدرب كلياً كان سبقاً جديداً في هذا الوقت. قبل ذلك, مستخرجات الخصائص كانت تستخدم اولاً ثم يتم اضافة نموذج على الخصائص المستخلصة بعد ذلك.

نظم الـ CNN الجديدة كان لها القدرة علي التعرف علي أنماط/حروف متعددة في الصورة في نفس الوقت. لفعل ذلك, كان الناس يستخدمون نافذة صغيرة من الصورة كدخل للـ CNN. هذه النافذة تقوم بمسح كل أجزاء الصورة. اذا تم تفعيل النافذة, هذا يعني ان هناك حرف/نمط ما موجود في هذا الجزء من الصورة.

لاحقاً, تم تطبيق هذه الفكرة علي التعرف علي الوجوه/الأشخاص وتنفيذ عمليات التجزئة الدلالية (تصنيف كل نقطة/بيكسل في الصورة على حدة). أمثلة على ذلك تتضمن هادسيل (2009) وفارابت (2012). التطبيقات علي ذلك أصبحت منتشرة في الصناعة, واستخدمت في تطبيقات القيادة الآلية للسيارات مثل تعقب خطوط الطريق.

انواع خاصة من المعدات/الهاردوير لتستخدم في تدريب الCNN كانت محل اهتمام الباحثين في الثمانينات, ولكن سرعان ما قل هذا الاهتمام, ثم عاد مرة أخري لاحقاً.

ثورة التعلم العميق (رغم أن هذا المصطلح لم يستخدم في ذلك الوقت) بدأت في الفترة من 2010 الى 2013. قام الباحثون بالتركيز على ابتكار خوارزميات قادرة على تدريب شبكات CNN ضخمة بصورة أسرع. كريزيفسكي (2012) قدم شبكة اليكس (ِAlexNet), التي كانت أضخم بكثير من كل الشبكات المستخدمة من قبل, وتم تدريبها علي قاعدة بيانات ImageNet التي تحتوي على 1.3 مليون صورة باستخدام ال GPUs. بعد التدريب لمدة أسبوعين, شبكة اليكس كانت قادرة علي هزيمة أفضل النظم والخوارزميات المنافسة بفارق كبير -- معدل خطأ على افضل خمس توقعات 16.4% مقابل 25.8%.

بعد رؤية النجاح الذي حققته شبكة اليكس, مجتمع الرؤية بالحاسب اصبح على اقتناع ان شبكات الCNN تعمل بصورة جيدة. في حين ان كل الاوراق البحثية في الفترة من 2011 الى 2012 التي كانت تستخدم ال CNNs كان يتم رفضها, بداية من 2016 غالبية الأوراق المقبولة في الرؤية بالحاسب كانت تستخدم الـ CNNs.

علي مر السنين, عدد الطبقات المستخدمة كان في ازدياد واضح, LeNet -- 7, AlexNet -- 12, VGG -- 19, ResNet -- 50. علي الرغم من ذلك, هناك دائما موائمة بين عدد العمليات المطلوبة للحصول علي النتيجة المطلوبة وحجم النموذج ودقته. وبناءً على ذلك, أحد الموضوعات البحثية المنتشرة حالياً هو كيفية ضغط وتقليل حجم الشبكات لتنفيذ العمليات بشكل أسرع.

## [التعلم العميق واستخلاص الخصائص](https://www.youtube.com/watch?v=0bMe_vCZo30&t=3955s)

الشبكات متعددة الطبقات أظهرت نجاحاً بسبب قدرتها علي استغلال الهيكل التركيبي للبيانات الطبيعية. كتركيبات هرمية, مجموعة الكائنات في طبقة معينة تكون كائن جديد في الطبقة التالية. إذا قمنا بمحاكاة هذا التسلسل الهرمي كمجموعة من الطبقات وتركنا للشبكة مهمة استخلاص وتعلم الخصائص المناسبة لذلك, نكون قد كوننا ما يعرف بنموذج التعلم العميق. ومن هنا يمكن القول أن شبكات التعلم العميق هي شبكات هرمية.

نماذج التعلم العميق كانت السبب في تقدم غير مسبوق في تطبيقات الرؤية بالحاسب بدايةً من التعرف على الكائنات في الصور وتحديد أماكنها بدقة, وصولاً إلى تحديد خصائص مكانية للكائن المراد التعرف عليه في الصورة. نماذج مثل Mask-RCNN وRetinaNet كان لها دور كبير في هذا التطور.

النماذج المبنية علي Mask-RCNN لها استخدامات في تجزئة الكائنات, بمعنى الحصول على غطاء لكل نوع من الكائنات في الصورة يوضح مكانه بدقة. في هذه النماذج, الدخل والخرج يكون صورة. النموذج يمكن أيضاً أن يستخدم لعمل تجزئة للمثيل, بمعنى الحصول على غطاء منفرد لكل كائن في الصورة حتى ولو كانوا من نفس النوع. ديتيكترون Detectron, أحد النظم التي تم تطويرها في فيسبوك للذكاء الاصطناعي, يوفر تصاميم مفتوحة المصدر لكل هذة النماذج الأحدث والأفضل من نوعها في مجال التعرف وتحديد مكان الكائنات في الصورة.

بعض التطبيقات العملية لل CNNs يتم توظيفها في القيادة الآلية ومعالجة وتحليل الصور الطبية.

علي الرغم من أن الخلفية العلمية والرياضية للتعلم العميق لم يتم فهمها بشكل كامل, إلا أنه مازال هناك بعض الاسئلة البحثية    الجديرة بالاهتمام التي تحتاج لمزيد من البحث والدراسة. هذه الاسئلة تتضمن: لماذا النماذج متعددة الطبقات تقدم أداءً افضل, مع العلم أنه يمكننا تقريب أي دالة رياضية بطبقتين فقط؟ لماذا تعمل شبكات الـ CNN بشكل جيد مع البيانات الطبيعية مثل الكلام, الصور, والنصوص؟ كيف يمكننا تحسين الدوال الغير محدبة non-convex بهذا الشكل الجيد؟ عملياً, وجدنا أنه حتى النماذج التي تحتوي عدد من المتغيرات اكثر من اللازم تعمل بشكل مقبول, لماذا؟

استخلاص الخصائص يتكون من توسيع/مد تمثيل هذه الخصائص وعرضها في أبعاد أعلى, حيث أن الخصائص الممتدة في الأبعاد الأعلى تزداد احتمالية القدرة على فصلها خطياً, بسبب زيادة عدد مستويات الفصل الممكنة في الأبعاد الأعلى.

ممارسو تعلم الآلة في العصور السابقة كانوا يستخلصون خصائص عالية الجودة ,بطرق يدوية/غير آلية. هذه الخصائص عادةً ما تكون مرتبطة بالمهمة المراد تنفيذها. بعد ذلك, يتم استخدام هذه الخصائص في نماذج الذكاء الاصطناعي. ولكن, ومع قدوم التعلم العميق, أصبحت النماذج قادرة على استخلاص خصائص عامة (غير مرتبطة بمهمة معينة) بصورة آلية. بعض الأساليب المشهورة في خوارزميات استخلاص الخصائص هي:

- تغطية فضاء المعلومات بأشكال مختلفة
- الاسقاطات العشوائية
- المصنفات متعددة الحدود
- دوال القواعد الشعاعية
- الآلات النووية

بسبب الطبيعة التركيبية للمعلومات, الخصائص المستخلصة لها تمثيلات هرمية ذات مستوى متصاعد من التجريد. علي سبيل المثال:

- الصور - علي اصغر وادق نطاق, يمكن التفكير في الصور باعتبارها مجموعة من النقاط. تجميع هذه النقاط مع بعضها البعض ينتج حواف, تجميع الحواف ينتج كائنات متعددة الحواف, تجميع هذه الكائنات ينتج أشكال غير منتظمة والتي بدورها تكون جزء من الصورة, تجميع مجموعة من هذه الأشكال ينتج الصورة كما نراها.
- النصوص - بنفس الطريقة, هناك هرمية متأصلة في المعلومات النصية. الحروف تكون كلمات, الكلمات تكون جمل, والجمل تكون النص الذي يحكي القصة المراد حكايتها.
- الكلام - الأصوات تكون نغمات ثم حروف ثم كلمات ثم جمل. ومن هنا نرى هرمية واضحة في تمثيل الكلام.

## [تعلم التمثيلات](https://www.youtube.com/watch?v=0bMe_vCZo30&t=4767s)

هناك بعض الناس الذين يرفضون التعلم العميق: اذا كنا نستطيع تمثيل أي دالة باستخدام طبقتين فقط, لم الحاجة إلى طبقات كثيرة؟

علي سبيل المثال, نماذج المتجهات المتوازية (SVMs, Support Vector Machines) تبحث عن مستوي فاصل للمعلومات, بمعنى أن النتائج سوف تعتمد على المقارنة مع المعلومات المستخدمة في عملية التدريب. SVMs شبكة عصبية بسيطة مكونة من طبقتين, الطبقة الأولى تقوم بتعريف القوالب, والثانية هي مصنِف خطي. المشكلة في مغالطة الطبقتين, هي أن حجم ومدى تعقيد الطبقة الوسطى ينمو كدالة أسية, وهو ما يعني أنه للحصول على أداء جيد في مهمة أكثر صعوبة, يستلزم الأمر عمل عدد قوالب كثيرة جداً. بينما اذا قمنا بزيادة عدد الطبقات يصبح التناسب خطياً بدلاً من التناسب الاسي. هناك دائماً موائمة بين الوقت والمساحة المطلوبة.

مثال مشابه لذلك يظهر في تصميم الدوائر الكهربية لحساب دالة منطقية بطبقتين من البوابات -- يمكننا حساب **أي دالة منطقية** بطبقتين فقط! ولكن, تعقيد وعدد البوابات المستخدمة في الطبقة الأولى سرعان ما يصبح غير منطقي للدوال المعقدة.

ما معنى "عميق"؟

- ال SVM غير عميق ﻷنه يحتوي فقط على طبقتين
- شجرة التصنيف غير عميقة ﻷن كل طبقة تعالج نفس مجموعة الخصائص الاصلية
- الشبكة العميقة لها طبقات عديدة وتستخدمهم لبناء **هرمية من الخصائص متزايدة التعقيد**

كيف يمكن للنماذج تعلم تمثيلات (خصائص جيدة)؟

فرضية المعلومات متعددة الخصائص: المعلومات الطبيعية لها خصائص متعددة ومتشعبة في فضاء منخفض الأبعاد. مجموعة الصور الممكنة تحتوي عدد لا نهائي من الصور, مجموعة الصور "الطبيعية" هي جزء صغير منها. علي سبيل المثال: صورة شخص, مجموعة الصور الممكنة يتغير بعدد عضلات الوجه التي يمكن لهذا الشخص تحريكها, ومن ثم تغيير الصورة (تقريباً 50). مستخلص الخصائص المثالي (والغير واقعي) يمثل كل عوامل التغيير الممكنة (عضلات الوجه, مستوي الاضاءة, الخ).

أسئلة وإجابات من نهاية المحاضرة:

- فيما يتعلق بمثال الوجه, هل يمكن لتقنيات تقليل الابعاد الاخرى *(علي سبيل المثال محلل الأبعاد الأساسية Principal Component Analysis, PCA)* استخلاص كل هذه الخصائص؟
- الاجابة: سوف يعمل فقط إذا كان مستوي الخصائص عالي الأبعاد, وهو ما لم يحدث هنا.

---
lang-ref: ch.04-1
title: Linear Algebra and Convolutions
authors: Yuchi Ge, Anshan He, Shuting Gu, and Weiyang Wen
date: 18 Feb 2020
---


## Linear Algebra Review

This part is a recap of basic linear algebra in the context of neural networks. We start with a simple hidden layer $\boldsymbol{h}$:

$$
\boldsymbol{h} = f(\boldsymbol{z})
$$

The output is a non-linear function $f$ applied to a vector $z$. Here $z$ is the output of an affine transformation $A \in\mathbb{R^{m\times n}}$ to the input vector $\boldsymbol{x} \in\mathbb{R^n}$:

$$
\boldsymbol{z} = A \boldsymbol{x}
$$

For simplicity bias are ignored. The linear equation can be expanded as:

$$
\begin {aligned}
\boldsymbol{z} &= A \boldsymbol{x} \ =\left[\begin{matrix}
a_{11} & a_{11} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \end{matrix} \right] \left[\begin{matrix}
x_1 \\ x_2\\\vdots \\x_n \end{matrix} \right]
=\left[\begin{matrix}
a^{(1)}\boldsymbol{x} \\a^{(2)}\boldsymbol{x}\\\vdots \\
a^{(m)}\boldsymbol{x}\end{matrix} \right] =\boldsymbol{z}
\end {aligned}
$$

where $\boldsymbol{a}^{(i)}$ is the $i$-th row of the matrix $\boldsymbol{A}$.

To understand the meaning of this transformation, let us analyse one component of $\boldsymbol{z}$ such as $a^{(1)}\boldsymbol{x}$. For simplicity, assume $n=2$ thus $\boldsymbol{a} = (a_1,a_2)$ and $\boldsymbol{x}  = (x_1,x_2)$. We can draw $\boldsymbol{a}$ and $\boldsymbol{x}$ as vectors in 2D coordinate axis. Denote the angle between $\boldsymbol{a}$ and $\hat{\boldsymbol{\imath}}$ as $\alpha$ and angle between $\boldsymbol{x}$ and $\hat{\boldsymbol{\imath}}$ as $\xi$. Then with trigonometric formulas $a^\top\boldsymbol{x}$ can be expanded as:

$$
\begin {aligned}
\boldsymbol{a}^\top\boldsymbol{x} &= a_1x_1+a_2x_2\\
&=\lVert \boldsymbol{a} \rVert \cos(\alpha)\lVert \boldsymbol{x} \rVert \cos(\xi) + \lVert \boldsymbol{a} \rVert \sin(\alpha)\lVert \boldsymbol{x} \rVert \sin(\xi)\\ &=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert (\cos(\alpha)\cos(\xi)+\sin(\alpha)\sin(\xi))\\ &=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert \cos(\xi-\alpha)
\end {aligned}
$$

The output measures the alignment of the input to specific row of the matrix $A$. To see this, let us look at the angle between the two vectors, $\xi-\alpha$. When $\xi = \alpha$, the two vectors are perfectly aligned and maximum is attained. If $\xi - \alpha = \pi$, then $\boldsymbol{a}^\top\boldsymbol{x}$ attains its minimum and the two vectors are pointing at opposite directions. In essence, the linear transformation allows one to see the projection of input to various orientations as defined by $A$. The intuition is expandable to higher dimensions.


Another way to understand the linear transformation, $\boldsymbol{z}$ can also be expanded as:

$$
a^\top\boldsymbol{x}=
\left[
\begin{matrix}
a_{11} \\
a_{21}\\
\vdots \\
a_{m1}
\end{matrix}
\right]x_1+\left[
\begin{matrix}
a_{12} \\
a_{22}\\
\vdots \\
a_{m2}
\end{matrix}
\right]x_2 + ...... +\left[
\begin{matrix}
a_{1n} \\
a_{2n}\\
\vdots \\
a_{mn}
\end{matrix}
\right]x_n
$$

The output is the weighted sum of the columns of the $A$ matrix. So the signal is nothing but a composition of the input.


## Extend Linear Algebra To Convolutions

In this part we extend the linear algebra to convolutions, by using the example of audio data analysis. To start with, It is easy to write a fully connected layer into a matrix multiplication form.

$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13}\\
w_{21} & w_{22} & w_{23}\\
w_{31} & w_{32} & w_{33}\\
w_{41} & w_{42} & w_{43}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

In this example, the weight matrix has a size $4 \times 3$, the input vector has a size $3 \times 1$ and the output vector has a size $4 \times 1$.

However, when we have audio data, the data is not only 3-sample long. Instead, the number of samples that the audio data has is much longer, which is equal to the duration of the audio (e.g. 3 seconds) times the sampling rate (e.g. 22.05 kHz). Like what is shown below, the input vector $\boldsymbol{x}$ will be long. Correspondingly, the weight matrix will become "fat".

$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13} & w_{14} & \cdots &w_{1k}& \cdots &w_{1n}\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

The above formulation will be difficult to train. Fortunately there are ways to simply.


### Property: Locality

Because of the property of locality (i.e. we do not care data points that are far away), $w_{1k}$, as an example in the weight matrix above, can be filled with 0 when $k$ is relatively large. Therefore, the first row of the matrix becomes a kernel of size 3. Let's denote this size-3 kernel as $\boldsymbol{a}^{(1)} = \begin{bmatrix} a_1^{(1)}  & a_2^{(1)}  & a_3^{(1)} \end{bmatrix}$.

$$
\begin{bmatrix}
a_1^{(1)}  & a_2^{(1)}  & a_3^{(1)}  & 0 & \cdots &0& \cdots &0\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$


### Property: stationarity

Another property that the natural data signals has is stationarity (i.e. the certain pattern/motif will repeat over and over again). So the kernel $\mathbf{a}^{(1)}$ that we defined previously can be reused. We use the size-3 kernel one step further each time (i.e. stride is 1). Thus, it becomes following:

$$
\begin{bmatrix}
a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0 & 0 & 0 & 0&\cdots  &0\\
0 & a_1^{(1)}  & a_2^{(1)} & a_3^{(1)}  & 0&0&0&\cdots &0\\
0 & 0 & a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0&0&\cdots &0\\
0 & 0 & 0& a_1^{(1)}  & a_2^{(1)}  &a_3^{(1)} &0&\cdots &0\\
0 & 0 & 0& 0 & a_1^{(1)}  &a_2^{(1)} &a_3^{(1)} &\cdots &0\\
\vdots&&\vdots&&\vdots&&\vdots&&\vdots
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix}
$$

Both the upper right part and lower left part of the matrix are filled with 0 due to the property of locality. Here, putting zeros around a matrix is called sparsity. And the replication of the certain kernel again and again is called weight sharing.


### Multiple layers of Toeplitz matrix

After these changes, the number of parameters we have is 3 (i.e. $a_1,a_2,a_3$). Compared with the previous weight matrix, whose number of parameter is 12 (i.e. $w_{11},w_{12},\cdots,w_{43}$), the current parameters are too restrictive. So more parameters are needed.

We can regard the previous matrix as one layer (i.e. a convolutional layer) with the kernel $\boldsymbol{a}^{(1)}$ and construct multiple layers with different kernels $\boldsymbol{a}^{(2)}$, $\boldsymbol{a}^{(3)}$, etc. Thus, the number of parameters increases.

Each layer has a matrix containing just one kernel that replicates multiple times. This kind of matrix is called Toeplitz matrix. In every Toeplitz matrix, each descending diagonal from left to right is constant. As for the Toeplitz matrices that we use here, they are also sparse matrices.

Given the first kernel $\boldsymbol{a}^{(1)}$ and the input vector $\boldsymbol{x}$, the first entry in the output given by this layer is $a_1^{(1)} x_1 + a_2^{(1)} x_2 + a_3^{(1)}x_3$. Therefore, the whole output vector is like

$$
\begin{bmatrix}
\mathbf{a}^{(1)}x[1:3]\\
\mathbf{a}^{(1)}x[2:4]\\
\mathbf{a}^{(1)}x[3:5]\\
\vdots
\end{bmatrix}
$$

For other kernels (e.g. $\boldsymbol{a}^{(2)}$ and $\boldsymbol{a}^{(3)}$) in following convolutional layers, we can apply the same matrix multiplication method and get the similar results.


## Listening to Convolutions - Jupyter Notebook

The Jupyter Notebook can be found [here](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/07-listening_to_kernels.ipynb).

In this notebook, we are going to explore Convolution as 'running scalar product'.

Importing library `librosa` and we can load data $\boldsymbol{x}$ and sampling rate. In this case, there are 70641 samples, learning rate is 22.05kHz and total time is 3.2s. The imported audio signal is wavy (refer to Fig 1) and we can guess what it sounds like from the amplitude of y axis. Listening to x(t), the audio signal is actually the sound played when turning off the Windows system (refer to Fig 2).

<center>
<img src="audioSignal.png" width="500px" /><br>
<b>Fig. 1</b>: A visualization of the audio Signal. <br>
</center>

<center>
<img src="notes.png" width="500px" /><br>
<b>Fig. 2</b>: Notes for the above audio.<br>
</center>


To get notes from wave from, Fourier transform (FT) is a good guess, however, if FT is played on the whole signal, notes would come out together, the is hard to figure out the exact time and location of each pitch. Localized FT is needed, which is called spectroscope. As is shown in the spectrogram (refer to Fig 3), different pitched peak at different values (e.g. first pitch peaks at 1600). Concatenate the four pitches based on their frequencies, we get the reconstruction.

<center>
<img src="spectrogram.png" width="500px" /><br>
<b>Fig. 3</b>: Audio signal and its spectrum.<br>
</center>

Convolution of signal with different pitches can help to extract all notes from a given piece, audio matches specific kernels. The spectrograms of origin signal and the concatenation (refer to Fig 4) is shown, as well as the frequencies of real signal and four pitches (refer to Fig 5). The plot and sound of convolution of four kernels indicate convolution performance is pretty good (refer to Fig 6).


<center>
<img src="fig4.png" width="500px" /><br>
<b>Fig. 4</b>: Spectrogram of real signal and concatenation.<br>
</center>

<center>
<img src="fig5.png" width="500px" /><br>
<b>Fig. 5</b>: First melody's note.<br>
</center>

<center>
<img src="fig6.png" width="500px" /><br>
<b>Fig. 6</b>: Convolution of four kernels.<br>
</center>


## Dimensionality of different datasets

The last part is a short digression that give some examples of dimensionality of different datasets. Here we consider input set $X$ is made of  functions mapping from domains $\Omega$ to channels $c$.


### Examples

* Audio data: domain is one-dimension discrete signal indexed by time; number of channels can range from 1 (mono), 2 (stereo), 5+1 (Dolby 5.1), etc.
* Image data: domain is two-dimensional (pixels); channel $c$ can range from 1(greyscale), 3(colour), 20(hyperspectral), *etc.*
* Special relativity: domain is $\mathbb{R^4} \times \mathbb{R^4}$ (space-time $\times$ four-momentum); channel $c = 1$ (*e.g.* Hamiltonian).

<center>
<img src="fig7.png" width="600px" /><br>
<b>Fig. 7</b>: Different dimension of different signals.<br>
</center>

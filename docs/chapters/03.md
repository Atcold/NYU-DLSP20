---
title: Week 3
---


## Lecture part A

We discuss an intuitive way to understand how backpropagation algorithm works. We start with an introduction to parametrised models and their representations using computation graphs. We then discuss what a loss function is followed by a discussion on Gradient Based methods which is the backbone of the backpropagation algorithm. After this, we discuss how the Backpropagation algorithm works in a traditional neural network and learnt how to implement a neural network in PyTorch. We conclude this section with a discussion on a more generalized form of Backpropagation.

## Lecture part B

We give an introduction on ConvNet evolutions. We discuss in detail on architectures of ConvNet with modern implementation of LeNet5, examplified by the task of digit recognition on MNIST. Based on its design principles, we expand on the advantages of ConvNet which fully explores compositionality, stationality, and locality features of natural images.


## Lab

We give a brief introduction to supervised learning using artificial neural networks. We expound on the problem formulation and conventions of data used to train these networks. We also discuss how to train a neural network for multi class classification, and how to perform inference once the network is trained.

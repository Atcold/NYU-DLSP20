---
layout: default
title: Week 1
---


## Lecture part A

We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.

## Lecture part B

In this section we first discuss the evolution of CNNs, from Fukushima to LeCun to Alexnet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.

## Lab

We discuss the motivation for applying transformations to data points as visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to better understand the function and effects of these transformations. We then walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by Neural Networks.

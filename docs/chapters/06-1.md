---
lang-ref: ch.06-1
title: 
authors: Shiqing Li, Chenqin Yang, Yakun Wang, Jimin Tan
date: 4 Mar 2020
---

## Zip Code Recognition

In the previous lecture, we have shown that a convolutional network can recognize digits, still, the question remains on how model picks on each digits and avoids perturbance on neighboring digits. Extending on this topic is to detect non/overlapping objects, and general approach by Non-Maximum Suppression (NMS). Now, given the assumption that the input is a series of non-overlapping digits, the strategy is to train several convolutional networks and use either majority vote or pick the digits corresponding to the highest score generated by the convolutional network. 

### Recognition with ConvNets

Here we present the task of recognizing 5 non-overlapping zip codes. The system has not given instructions on how to separate each digit but knowing there are 5 digits to predict. The system (Figure 1) consists of 4 different sized convolutional networks, each producing one set of outputs. The output is represented in matrices. The four output matrices are from models with different kernel width in the last layer. In each output, there are 10 rows, representing 10 categories from 0 to 9. The larger white square represents a higher score in that category. In these four output blocks, the horizontal sizes of the last kernel layers are 5, 4, 3 and 2 respectively. The size of the kernel decides the width of the model's viewing window on the input, therefore each model is predicting digits based on different window sizes. The model then takes the majority vote and selects the category corresponds to the highest score in that window. To extract useful information, one should keep in mind that not all combinations of characters are possible, therefore error correction leveraging input restrictions is useful to ensure the outputs are true zip codes. 

<center>
<img src="https://i.imgur.com/O1IN3JD.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<b>Figure 1:</b> Multiple classifiers on zip code recognition 
</center>

Then to impose the order of the characters, the trick is to utilize the shortest path algorithm. Since we are given ranges of possible characters and the total number of digits to predict, We can approach this problem by computing the minimum cost of producing digits and transitions between digit. The path has to be continuous from lower left cell to upper right cell on the graph, and the path is restricted from left to right, bottom to top. Note that if two repetitive numbers are next to each other, the algorithm should be able to distinguish there are two repetitive numbers instead of predicting one single digit. 


## Face detection
The convolutional neural network performs well on detection tasks and face detection is no exception. To perform a face detection task we collect a dataset of images with faces and without faces, on which we train a convolutional net with window size such as 30 $\times$ 30 pixels to tell whether there is a face or not. Then we can apply the trained model on an image and if there are faces of roughly 30 $\times$ 30 pixels. The convolutional net will light up the output at corresponding locations. However, two problems exist.


- **False Positives**: There are many ways that a patch of an image is not a face. During the training stage, the model may not see all of them, i.e., representative sets of them. Therefore, the model may suffer from a lot of false positives at test time. 
- **Different Face Size:** Not all faces are 30 $\times$ 30 pixels. One way to handle this issue is to have a multi-scale version of the same image. The original detector will detect faces around 30 $\times$ 30 pixels. If applying a scale on the image of factor $\sqrt 2$, the model will detect faces that were larger in the original image since what was 30 $\times$ 30 is now 20 $\times$ 20 pixels roughly. With bigger faces, we can rescale again to shrink the image down even further. This process is not expensive as half of the expense comes from processing the original non-scaled image. The sum of the expenses of all other networks combined is about the same as processing the original non-scaled image. The size of the network is the square of the size of the image on one side, so if you scale down the image by $\sqrt 2$, the network you need to run is smaller by a factor of 2. So the overall cost is $1+1/2+1/4+1/8+1/16â€¦$, which is 2. Performing a multi-scale model only doubles the computational cost.

### A multi-scale face detection system

<center>
<img src="https://i.imgur.com/8R3v0Dj.png" style="zoom: 30%; background-color:#DCDCDC;"/><br>
<b>Figure 2:</b> Face detection system
</center>

The maps shown in (Figure 3) are the ones indicating scores of face detectors. This face detector recognizes faces that are 20 $\times$ 20 pixels in size. In fine-scale (Scale 3) there are many high scores but are not very definitive. When the scaling factor goes up (Scale 6), we see more clustered white regions. Those white regions represent detected faces. Then we apply non-maximum suppression to get final face location.

<center>
<img src="https://imgur.com/CQ8T00O.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<b>Figure 3:</b> Face detector scores for various scaling factors
</center>

### Non-maximum suppression

For each high-scoring region, there's probably a face underneath. If there is another detected face nearby, it means one of those two is wrong. With non-maximum suppression, we take the higher scoring one and suppress all the others at all scales. The result will be a single bounding box at the optimum location.

### Negative mining

In the last section, we talk about the model will run into a large number of false positives at test time as there are many ways to be different from the face. It is very likely that the training set does not include all the non-face objects that look like faces. We can alleviate this problem through negative mining. In negative mining, we create a negative dataset of non-face patches that are detected as faces. The data is collected by running the model on inputs with no faces. Then we retrain the detector using the negative dataset. We can repeat this process to increase the robustness of our model against false positives. 

## Semantic segmentation

Semantic segmentation is the problem of assigning a category to every pixel in an image and every pixel will be labeled with a category of the object belongs to.

### ConvNet for Long Range Adaptive Robot Vision

In this project, the goal is to label regions from input images so that a robot can distinguish between roads and obstacles. In the figure, the green regions are areas the robot can drive on and the red regions are obstacles like tall grass. To train a network for this task, we take a patch from the image and we label it traversable or not (green or red). We train the convolutional network on the patch by predicting the color. Once the system is trained, it is applied to the entire image, labelling regions as green or red depending on where they are. 

<center>
<img src="https://imgur.com/5mM7dTT.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<b>Figure 4:</b> ConvNet for Long Range Adaptive Robot Vision (DARPA LAGR program 2005-2008)
</center>

There were five categories for prediction: 1) super green, 2) green, 3) purple: obstacle footline, 4) red obstacle  5) super red: definitely an obstacle.



**Stereo Labels** (Figure 4, Column 2)

Collected by robots automatically through their stereo vision and 3D reconstruction. Images are captured by 4 cameras, forming 2 stereo vision pairs. The positions of every pixel in 3D would then be estimated. With the position information ready, a plane would be fitted to the ground, and pixels are labeled as green indicates they are near the ground and red indicates they are above it.

* **Limitations & Motivation for ConvNet**: The stereo vision only works up to 10 meters and driving a robot requires long-range vision.
    
<center>
<img src="https://i.imgur.com/rcxY4Lb.png" style="zoom: 100%; background-color:#DCDCDC;"/><br>
<b>Figure 5:</b> Scale-invariant Pyramid of Distance-normalized Images
</center>
 
* **Served as Model Inputs**: Important pre-processing includes building a scale-invariant pyramid of distance-normalized images (Figure 5). It is similar to what we have done earlier of this lecture when we tried to detect faces of multiple scales.

**Model Outputs** (Figure 4, Column 3)

The model outputs a label for every pixel in the image **up to the horizon**. These are classifier outputs of a multi-scale convolutional network.

* **How the Model Becomes Adaptive**: The robots have access to stereo labels constantly, so it can always re-train the neural network to adapt to the new environment it's in. Note that only the last layer of the network would be re-trained. Others are trained in the lab and fixed.

**System Performance**

When trying to get to a GPS coordinate behind a barrier, the robot "saw" the barrier far away from it and directly went aside. The ConvNet outputs work for up to 50-100m.

**Limitation**

Back in the 2000s, the computation resources were restricted. The robot processes around 1 frame per second, which means it would ignore a person that works in its way for 1 second before making any reaction. The solution for this limitation is a **Low-Cost Visual Odometry** model. It is not based on neural networks, has a vision of ~2.5m and reacts fast.




### Scene Parsing and Labeling

In this task, the model label object category (buildings, cars, sky, etc.) for every pixel. The architecture is also multi-scale (Figure 6).

<center>
<img src="https://i.imgur.com/VpVbkl5.jpg" style="zoom: 30%; background-color:#DCDCDC;"/><br>
<b>Figure 6:</b> Multi-scale ConvNet for scene parsing
</center>

Notice that if we back project one output of the ConvNet onto the input, it corresponds to an input window of size $46\times46$ on the original image at the bottom of the Laplacian Pyramid. It means we are **using the context of $46\times46$ pixels to decide the category of the central pixel**. 

However, sometimes this context size is not enough to determine the category for larger objects.

**The multiscale approach enables a wider vision by providing extra rescaled images as  inputs.** The steps are as follows:
1. Take the same image, reduce it by the factor of 2 and the factor of 4, separately. 
2. These two extra rescaled images are fed to **the same ConvNet** (same weights, same kernels) and we get another two sets of Level 2 Features.
3. **Upsample** these features so that they have the same size as the Level 2 Features of the original image. 
4. **Stack** the three sets of (upsampled) features together and feed them to a classifier.
    
Now the largest effective size of content, which is from the 1/4 resized image, is $184\times 184\, (46\times 4=184)$. 

**Performance**: With no post-processing and running frame-by-frame, the model runs really fast even on standard hardware. It has a rather small size of training data (2k~3k), but the results are still record-breaking. 

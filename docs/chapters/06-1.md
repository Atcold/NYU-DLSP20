---
lang-ref: ch.06-1
title: 
authors: Shiqing Li, Chenqin Yang, Yakun Wang, Jimin Tan
date: 4 Mar 2020
---

<!--
Feel free to change the subtitles
-->

## Part 1 Shiqing Li 
0:06:25 -> 0:22:10
## Outline: 
1. Convolutional network applications
2. Introduction on recently developed architectures

### Back in the 90's: Word level training with weak supervision
In the previous lecture, we have shown that a convolutional network can recognize digits, still, the question remains on how model pick on each digits and avoid perturbance on neighboring digits. Extending on this topic is to detect objects or overlapping objects, and general approach by Non-Maximum Suppression (NMS). Now, given the assumption that the input is a series of non-overlapping digits, the strategy is to train several convolutional networks and use majority vote or pick the digits corresponding to the highest score generated by the convolutional network. 

### Demonstration
Here we present an example of recognizing 5 non-overlapping zipcode task. The system has not given instructions on how to separate each digit but given there are 5 digits to predict. The system ([Figure 1]()) consists of 4 different sized convlutional nets, each producing one set of output. The output is represented in matrices. The four output matrices are from models with different kernel width in the last layer. In each outputs, there are 10 rows, representing 10 categories from 0 to 9. Larger white square represents higher score in that category. In these four output blocks, the horizontal sizes of the last kernel layers are 5, 4, 2 and 2 respectively. The size of the kernel decides the width of the model's viewing window on the input, therefore each model is predicting digits based on different window sizes. The model then takes majority vote / select category corresponds to the highest score in that window. To extract useful information, one should keep in mind that not all combination of characters are possible, therefore error correction leveraging input restrictions is useful to ensure the outputs are true zipcodes. 

<center>
<img src="https://i.imgur.com/O1IN3JD.png"/><br>
<b>Figure 1:</b> Multiple classifiers on digits recognition 
</center>



Then to impose the order of the characters, the trick is to utilize shortest path algorithm. Since we are given ranges of possible characters and total number of digits to predict, We can approach this problem by computing the minimum cost of producing digits and transitions between digit. The path has to be continuous from lower left cell to upper right cell on the graph, and the path is restricted on left to right, bottom to top. Note that if two repetitive numbers are next to each other, the alrogithm should be able to distinguish there is two repetitive numbers instead of predicitng one single digit. 
## Part 2 Chenqin Yang

0:22:10	-> 0:37:35


### Face detection
The convolutional neural network works pretty well for face detection. If you want to do face detection, you can first collect dataset of images with faces and without faces, on which you train a convolutional net with window size such as 20\*20 or 30\*30 pixels to tell whether there is a face or not. Then you can apply the trained model on an image and if there is a face that happens to be roughly 30\*30 pixels, the convolutional net will light up at the corresponding outputs. However, two problems exist.
- There are many ways that a patch of an image is not a face. During the training stage, you may not see all of them, i.e., representative sets of them. Therefore, you may suffer from a lot of false positives. 
- In a picture, not all faces are 30\*30 pixels. How do you handle size variation? One way to handle this issue is multi-scale approach. If you take the original image and run the detector on it, the model will detect faces that are small. And then you can reduce your image by some scale. In this case, the scaling factor can be $\sqrt 2$. You apply the convolutional net again on that image. It will detect faces that are larger in the original image since what was 30\*30 is now 20\*20 pixels roughly. But there maybe bigger faces. So you can apply the scaling factor again so that the image is half the size of the original one. And you run the convolutional net again, it will detect faces that were originally 60\*60 pixels. You may think the process is expensive but is not. Half of the expense comes from the fine scale. The sum of the expenses of all other networks combined is about the same as the fine scale. The size of the network is the square of the size of the image on one side, so if you scale down the image by $\sqrt 2$, the network you need to run is smaller by a factor of 2. So the overall cost is $1+1/2+1/4+1/8+1/16…$, which is 2. You waste a factor of 2 by doing multi-scale. 

So this is a completely ancient face detection system from the early 90s

The maps shown in Figure 2 are the ones indicating scores of face detectors. The face detector here is 20\*20 which is low resolution. It is a big mess at the fine scale where you see many high scores but are not very definitive. But you see more definitive things (white blobs) when scaling factor goes up.(Figure 2). Those white blobs represent detected faces. That's when you need to do non-maximum suppression to get the wining categories/locations where you have a face. 

<center>
<img src="https://imgur.com/CQ8T00O.png"/><br>
<b>Figure 2:</b> Face detector scores for various scaling factor
</center>

Non-maximum suppression here means you may have a high scoring white blob here. That means there's probably a face underneath which is roughly 20 by 20. If there is another face within a window of 20 by 20, it means one of those two is wrong. So you may take the higher scoring one within the window of 20 by 20 and suppress all the others. You will suppress all the others at that location at that scale but also at other scales. You pick the highest scoring blob for every location every scale and whenever you pick one you suppress the other ones that could be conflicting with it either because they are a different scale the same place or at the same scale but nearby.

And the second problem is the fact that as there's many ways to be different from the face and most likely your training set doesn't have all the non-faces things that look like faces. So the way people deal with this is that they do what's called negative mining. You go through a large collection of images when you know for a fact that there is no face and run your detector. At the same time,  you will keep all the patches where your detector fires. Then you verify that there is no face in them and if there is no face you add them to your negative set. Then you retrain your detector and use your retrained detector to do the same. Go again through a large dataset of images where you know there is no face and store the images as negative sample whenever you detector fires. You do this four or five times and finally you will have a very robust face detector that does not fall victim to negative samples since there are a lot of things that look like faces in natural images that are actually not faces.

### Intro to semantic segmentation

Here's another interesting use of convolutional nets — semantic segmentation. 

Semantic segmentation is the problem of assigning a category to every pixel in an image and every pixel will be labeled with a category of the object belongs to. It would be very useful if you want to drive a robot in nature. 

In Figure3, this is a robotics project that Prof. Yann worked on with his students long time ago. The goal is to label the image so that regions that the robot can drive on and obstacles that it shouldn't drive on are indicated. In the figure, you can observe the green areas are the things the robot can drive on and the red areas are obstacles like tall grass. The way you train a convolutional net to do this kind of semantic segmentation is very similar to what has just been described above. You take a patch from the image. In this case the patches were 20 by 40 for which you know what the central pixel is — whether it's traversable or not, whether it's green or red. The label is either being manually labeled or obtained in some way. You may run the convolutional net on this patch and you train it to tell if it's green or red (or to tell if it's drivable area or not). Once the system is trained, you apply it on the entire image and you know it will put green or red depending on where it is. 

<center>
<img src="https://imgur.com/5mM7dTT.png"/><br>
<b>Figure 3:</b> ConvNet for Long Range Adaptive Robot Vision (DARPA LAGR program 2005-2008)
</center>

In this particular case, there were five categories: 1) super green, 2) green, 3) purple which is a photo of an object, 4) red which is an obstacle that you know too often, 5) super red which is like a definite obstacle. Here we're only showing three colors.

## Part 3 Yakun Wang

0:37:35	-> 0:52:12

### ConvNet for Long Range Adaptive Robot Vision: Project Details 

* **Stereo Labels** (Figure 3, Column 2): Collected by robots automatically through their stereo vision and 3D reconstruction. Images are captured by 4 cameras, forming 2 stero vision pairs. The positions of every pixel in 3D would then be estimated. With the position information ready, a plane would be fitted to the gound, and pixels are labeled as green indicates they are near the ground and red indicates they are up it.

    **Problem & Why ConvNet**: The stereo vision works up to ~10m. Driving a robot with only ~10m of vision is clearly not efficient.
    
    **Served as Inputs of ConvNet**: Important pre-processing includes building scale invariant pyramid of distance-normalized images (Figure 4). It is similar to what we have done earlier of this lecture when we tried to detect faces of multiple scales.  
    
<center>
<img src="https://i.imgur.com/rcxY4Lb.png"/><br>
<b>Figure 4:</b> Scale Invariant Pyramid of Distance-normalized Images
</center>
    
* **Convolution Labels** (Figure 3, Column 3): Label every pixel in the image **up to the horizon**. These are classifer outputs of a multi-scale convlutional network.

* **How It be Adaptive**: The robots have access to stereo labels constantly, so it can always re-train the neural network to adapt to the new environment it's in. Note that only the last layer of the network would be re-trained. Others are trained in the lab and fixed.

* **Performance**: When trying to get to a GPS coordinate behind a barrier, the robot "saw" the barrier far away from it and directly went aside. The ConvNet outputs work for up to 50-100m.

* **Limitation**: Back to 2000s, the computation resources were restricted. The robot processes around 1 frame per second, which means it would ignore a person that works in its way for 1 second before making any reaction. The model for this situation is called **Low Cost Visual Odometry**. It is not based on neural networks, has a vision of ~2.5m and reacts fast.


---


### Scene Parsing/Labeling
==Another useage of the multiscale approach!==

**Purpose**: Label object category (buildings, cars, sky, etc.) for every pixels.

**Multiscale ConvNet Architecture** (Figure 5):

<center>
<img src="https://i.imgur.com/VpVbkl5.jpg"/><br>
<b>Figure 5:</b> Illustration of the Network
</center>


* As at the bottom of the Laplacian Pyramid where the original images are used, if we back project one output of the ConvNet onto the input, it corresponds to an input window of size $46\times46$. It means we are **using the context of $46\times46$ pixels to decide the category of the central pixel**. 
* Not enough when we need wider contexts to make decisions for the greater objects.
* **The multiscale approach enables wider vision by providing extra rescaled images as the inputs.**
    1. Take the same image, reduce it by the factor of 2 and the factor of 4, seperately. 
    2. These two extra rescaled images are fed to **the same ConvNet** (same weights, same kernels) and we get another two sets of Level 2 Features.
    3. **Upsample** these features so that they have the same size as the Level 2 Features of the original image. 
    4. **Stack** the three sets of (upsampled) features together and feed them to a classifier.
* Now the largest effective size of content, which is from the 1/4 resized image, is $184\times 184\, (46\times 4=184)$. 

**Performance**: With no post-processing and running frame-by-frame, the model runs really fast even on standard hardwares. It has a rather small size of training data (2k~3k), but the results are still record-breaking. 

---
layout: default
title: Week 2
---


## Lecture part A

We discuss an intuitive way to understand how backpropagation algorithm works. We start with an introduction to parameterized models and their representations using computation graphs. We then discuss what a loss function is followed by a discussion on Gradient Based methods which is the backbone of the backpropagation algorithm. After this, we discuss how the Backpropagation algorithm works in a traditional neural network and learnt how to implement a neural network in PyTorch. We conclude this section with a discussion on a more generalized form of Backpropagation.

## Lecture part B

We begin with a concrete example of backpropagation and discuss the dimensions of Jacobian matrices. We then look at various basic neural net modules and compute their gradients, followed by a brief discussion on softmax and logsoftmax. The other topic of discussion in this part is Practical Tricks for Backpropagation.

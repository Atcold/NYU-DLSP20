---
lang-ref: ch.08
lang: it
title: Settimana 8
translator: Marco Zullich
translation-date: 09 May 2020
---

## Lezione parte A

<!-- ## Lecture part A -->

In questa sezione ci focalizziamo sull'introduzione dei metodi contrastivi in vari aspetti dei modelli ad energia (*Energy Based Models, EBM*). Innanzitutto, discutiamo del vantaggio dato dall'applicazione dei metodi contrastivi nell'apprendimento auto-supervisionato (*self-supervised learning, SSL*). In seconda battuta, parliamo dell'architettura degli *autoencoder* per la riduzione del rumore (*denoising autoencoder*) e delle loro debolezze nei compiti di ricostruzione delle immagini. Discutiamo anche di altri metodi contrastivi, come la divergenza contrastiva (*contrastive divergence, CD*) e divergenza contrastiva persistente.

<!-- In this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence. -->

## Lezione parte B

<!-- ## Lecture part B -->

In questa sezione discutiamo dettagliatamente sugli EBM, coprendone concetti delle versioni condizionate e non-condizionate. Parliamo quindi degli algoritmi ISTA, FISTA e LISTA e vediamo degli esempi di codificazione sparsa (*sparse coding*) e dei filtri appresi da codificatori convoluzionali sparsi. Infine, parliamo degli *autoencoder* variazionali (*Variational Autoencoders, VAEs*) e dei concetti ad essi collegati.

<!-- In this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved. -->

## Pratica

<!-- ## Practicum -->

In questa sezione, discutiamo di una specifica tipologia di modelli generativi chiamati *autoencoder* variazionali. Esploriamo nel dettaglio la funzione obiettivo dei VAE, comprendendo come imponga una determinata struttura nello spazio latente. Infine, implementiamo e addestriamo un VAE sul dataset di MNIST e lo usiamo per generare nuove osservazioni.

<!-- In this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples. -->

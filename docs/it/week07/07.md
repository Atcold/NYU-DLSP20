---
lang: it
lang-ref: ch.07
title: Settimana 7
translation-date: 1 Apr 2020
translator: Michela Paganini
---

## Lezione parte A

Vengono introdotti il concetto di modelli ad energia (EBM, Energy-Based Models) e la necessità di approci differenti dalle reti *feed-forward*. Per arginare le difficoltà nella fase di inferenza negli EBM, vengono utilizzate variabli latenti per fornire informazioni ausiliari ed ammettere più di una soluzione. Infine, gli EBM possono esser generalizzati come modelli probabilistici con una funzione di punteggio più flessibile.

<!-- We introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions. -->

## Lezione parte B

Vengono introdotti l'apprendimento auto-supervisionato, metodi per l'apprendimento degli EBM, e gli EBM a variabili latenti, nello specifico, attraverso un esempio sull'algoritmo *K-means*. Vengono inoltre presentati i metodi di apprendimento a contrasto (*contrastive methods*), gli auto-codificatori per la riduzione del rumore (*denoising auto-encoders*) con mappa topografica, il loro processo di apprendimento e possibile utilizzo, per poi finire con una presentazione del modello BERT (*Bidirectional Encoder Representations from Transformers*). Viene infine presentata la divergenza contrastiva, sempre attraverso l'utilizzo di una mappa topografica.

<!-- We discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map. -->

## Pratica

Vengono esaminate varie applicazioni degli auto-codificatori e le ragioni del loro utilizzo. Vengono poi discusse le varie architetture degli auto-codificatori (a strati intermedi sottocompleti o sovracompleti), come evitare problemi relativi al sovradattamento (overfitting), e quale funzione di perdita utilizzare. Infine, vengono implementati un auto-codificatore standard e un auto-codificatore per la riduzione del rumore.

<!-- We discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder. -->

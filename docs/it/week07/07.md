---
lang: it
lang-ref: ch.07
title: Settimana 7
translation-date: 1 Apr 2020
translator: Michela Paganini
---

## Lezione parte A

Vengono introdotti il concetto di modelli ad energia (EBM, Energy-Based Models) e la necessità di approci differenti dalle reti feed-forward. Per arginare le difficoltà nella fase di inferenza negli EBM, variabli latenti vengono utilizzate per fornire informazioni ausiliari ed ammettere più di una soluzione. Infine, gli EBM possono esser generalizzati come modelli probabilistici con una funzione di punteggio più flessibile.

## Lezione parte B

Vengono introdotti l'apprendimento auto-supervisionato, metodi per l'apprendimento degli EBM, e gli EBM a variabili latenti, nello specifico, attraverso un esempio sull'algoritmo K-means. Vengono inoltre presentati i metodi di apprendimento a contrasto (contrastive methods), gli auto-codificatori per la riduzione del rumore (denoising auto-encoders) con mappa topografica, il loro processo di apprendimento e possibile utilizzo, per poi finire con una presentazione del modello BERT (Bidirectional Encoder Representations from Transformers). Viene infine presentata la divergenza contrastiva, sempre attraverso l'utilizzo di una mappa topografica.


## Pratica

Vengono esaminate varie applicazioni degli auto-codificatori e le ragioni del loro utilizzo. Vengono poi discusse le varie architetture degli auto-codificatori (a strati intermedi incompleti o overcompleti), come evitare problemi relativi al sovradattamento (overfitting), e quale funzione di perdita utilizzare. Infine, vengono implementati un auto-codificatore standard e un auto-codificatore per la riduzione del rumore.
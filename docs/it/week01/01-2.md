---
lang-ref: ch.01-2
lang: it
title: Evoluzione ed uso delle CNN e "perché l'apprendimendo profondo"?
authors: Marina Zavalina, Peeyush Jain, Adrian Pearl, Davida Kollmar
date: 27 Jan 2020
translation-date: 27 Mar 2020
translator: Stefano Pirra
---


## [Evoluzione delle CNN](https://www.youtube.com/watch?v=0bMe_vCZo30&t=2965s)

Nel cervello animale i neuroni reagiscono alle linee che hanno un particolare orientamento. Gruppi di neuroni che reagiscono allo stesso orientamento sono replicati su tutto il campo visivo.

Fukushima (1982) costruì una rete neurale (NN, neural net) che funzionava alla stessa maniera del cervello, basata su due concetti.
Primo, i neuroni sono replicati attraverso il campo visivo. Secondo, esistono cellule complesse che raccolgono l'informazione da cellule più semplici (unità selettive di orientamento). Come risultato, lo spostamento dell'immagine cambierà l'attivazione di cellule semplici ma non influenzerà l'attivazione integrata della cellula complessa (raggruppamento convoluzionale, convolutional pooling).

LeCun (1990) utilizzò la retropropagazione (backprop) per addestrare una CNN nel riconoscere dei numeri scritti a mano. Esiste una dimostrazione del 1992 in cui l'algoritmo riconosce i caratteri in diversi stili. L'utilizzo di un modello completamente addestrato per fare riconoscimento di pattern/caratteri fu una novità a quell'epoca. In passato le persone utilizzavano sistemi di estrazione di caratteristiche con, in cima, un modello supervisionato.

Questi nuovi sistemi CNN potevano riconoscere diversi caratteri in una sola immagine contemporaneamente. Per farlo le persone costruivano una piccola finestra per una sola CNN e la facevano scorrere lungo l'intera immagine. Se questa si attivava, allora un particolare carattere era presente.

In seguito questa idea fu applicata al riconoscimento di volti/persone e nella segmentazione semantica (classificazione di ogni pixel, pixel-wise classification). Esempi includono Hadsell (2009) e Farabet (2012). Questa tecnica divenne infine popolare nell'industria, applicata nei veicoli autonomi come ad esempio nell'identificazione delle corsie di marcia.

Tipi di hardware speciale per addestrare CNN divennero un tema caldo negli anni 1980 poi l'interesse diminuì ed oggi è di nuovo popolare.

La rivoluzione dell'apprendimento profondo (deep learning, sebbene il termine non venne ancora usato all'epoca) iniziò nel 2010-2013. I ricercatori si focalizzarono nell'inventare algoritmi che potessero addestrare grosse CNN più velocemente. Krizhevsky (2012) fece uscire AlexNet, che era una CNN molto più grande di quelle usate fino ad allora e la addestrò su di Imagenet (1.3 milioni di campioni), utilizzando le GPU. Dopo un paio di settimane di esecuzione, AlexNet fu in grado di battere in performance i migliori concorrenti, con un largo margine -- un tasso di errore del 25.8% contro il 16.4 dei migliori 5.

Dopo aver realizzato il successo di AlexNet, la comunità della visione artificiale (CV, computer vision) si convinse che le CNN funzionavano. Mentre tutte le pubblicazioni dal 2011-2012 che menzionavano le CNN vennero rifiutate, a partire dal 2016 la maggior parte delle pubblicazioni accettate sulla visione artificiale utilizza le CNN.

Nel corso degli anni il numero di strati usati è incrementato: LeNet -- 7, AlexNet -- 12, VGG -- 19, ResNEt -- 50. Tuttavia c'è un compromesso tra il numero di operazioni necessarie per calcolare l'output, la dimensione del modello e la sua accuratezza. Perciò un argomento popolare oggi è come comprimere le reti per rendere la computazione più veloce.


## [Apprendimento Profondo ed Estrazione di Caratteristiche](https://www.youtube.com/watch?v=0bMe_vCZo30&t=3955s)

Le reti multi-strato hanno successo perchè sfruttano la struttura compositiva dei dati naturali. 
Nella gerarchia compositiva, le combinazioni di oggetti su di un livello formano gli oggetti sul livello successivo. Se noi mimiamo questa gerarchia con strati multipli e lasciamo alla rete il compito di apprendere un'appropriata combinazione di caratteristiche, otteniamo quello che viene chiamato un'architettura ad apprendimento profondo. Perciò le reti di apprendimento profondo sono di natura gerarchica.

Le architetture di apprendimento profondo hanno portato un progresso incredibile in compiti di visione artificiale, dall'identificare e generare maschere accurate intorno ad oggetti fino all'identificazione di proprietà spaziali di un oggetto. Le architetture Mask-RCNN e RetinaNet sono quelle maggiormente responsabili di questo miglioramento.

Le Mask RCNN hanno trovato il loro utilizzo nella segmentazione di oggetti individuali, ad es. nel creare maschere per ogni oggetto in un'immagine. L'input e l'output sono immagini. L'architettura può anche essere utilizzata per fare segmentazione di istanze, ad es. nell'identificare oggetti differenti dello stesso tipo in un'immagine. Detectron, un sistema software del Facebook AI Research (FAIR) implementa tutti questi algoritmi all'avanguardia nell'identificazione di oggetti ed è software libero (open source).

Alcune delle applicazioni pratiche delle CNN sono nella guida autonoma di veicoli e nell'analisi di immagini mediche.

Sebbene la scienza e la matematica che si cela dietro all'apprendimento automatico sia abbastanza chiara, ci sono ancora diverse domande interessanti che richiedono ulteriore ricerca. Queste domande includono: Perchè le architetture a multi strato funzionano meglio, visto che possiamo approssimare qualsiasi funzione con due strati? Perchè le CNN funzionano bene con dati naturali come il parlato, le immagini ed il testo? Come possiamo ottimizzare funzioni non convesse così bene? Perchè le architetture funzionano anche se troppo parametrizzate?

L'estrazione di caratteristiche consiste nell'espansione della dimensione rappresentativa in modo che le caratteristiche così espanse siano più probabilmente linearmente separabili; i punti dei dati in uno spazio dimensionalmente maggiore sono più probabilmente separabili a causa dell'incremento dei possibili piani di separazione. 

I primi professionisti nell'apprendimento automatico si affidarono a caratteristiche di alta qualità, selezionate manualmente e specializzate in un determinato campo per costruire modelli di intelligenza artificiale ma, con l'avvento dell'apprendimento profondo, i modelli sono diventati abili nell'estrarre le caratteristiche automaticamente. Sono qui elencati alcuni approcci comuni utilizzati negli algoritmi di estrazione di caratteristiche:

- Suddivisione (_tiling_) dello spazio
- Proiezioni casuali
- Classificatori polinomiali (caratteristiche tra prodotti)
- Funzioni di base radiale
- Macchine a Kernel

A causa della natura compositiva dei dati, le caratteristiche apprese hanno una gerarchia rappresentativa con livello crescente di astrazioni. Ad esempio:

-  Immagini - al massimo livello di granularità le immagini possono essere pensate come pixel. La combinazione di pixel costituisce le linee che, quando combinate, formano i textons (forme a multi linee). I textons formano i motivi ed i motivi formano le parti di un'immagine. Combinando queste parti insieme otteniamo l'immagine finale.
-  Testo - Allo stesso modo c'è una gerarchia inerente ai dati testuali. I caratteri formano le parole, che combinate assieme ci danno gruppi di parole, quindi clausole che combinate tra loro ci danno frasi. Le frasi infine ci dicono quale storia viene narrata.
-  Parlato - Nel parlato i campioni compongono le bande che compongono i suoni che compongono i foni, quindi i fonemi, quindi parole intere e frasi, mostrando così una chiara gerarchia nella rappresentazione.


## [Rappresentazioni di apprendimento](https://www.youtube.com/watch?v=0bMe_vCZo30&t=4767s)

Quelli che rifiutano l'apprendimento profondo: se possiamo approssimare ogni funzione con 2 strati, perchè averne di più?

Per esempio: le SVM trovano un iperpiano di separazione "nell'intervallo dei dati", che significa che le predizioni si basano sul confronto con gli esempi di addestramento. Le SVM sono essenzialmente delle reti neurali a 2 strati molto semplicistiche, in cui il primo strato definisce i "template" mentre il secondo è un classificatore lineare. Il problema con l'errore di due strati è che la complessità e la dimensione dello strato intermedio è esponenziale in N (per lavorare bene con un compito difficile, c'è bisogno di una grande quantità di template). Se però espandi il numero di strati nel log(N), ogni strato diventa lineare in N. Esiste un compromesso tra il tempo e lo spazio.

Un analogia è il disegno di un circuito per il calcolo di una funzione booleana con non più di due strati di porte logiche - possiamo calcolare **ogni funzione booleana** in questo modo! ma la complessità e le risorse del primo strato (il numero delle porte logiche) diventa velocemente infattibile per funzioni complesse.

Cosa è "profondo"?

- Una SVM non è profonda perchè ha solo 2 strati
- Un albero di classificazione non è profondo perchè ogni livello analizza le stesse caratteristiche (grezze)
- Una rete profonda ha molti strati e li usa per costruire una **gerarchia di caratteristiche di complessità incrementale**

Come possono i modelli apprendere le rappresentazioni (buone caratteristiche)?

Ipotesi della varietà (_manifold_): i dati naturali vivono in una varietà a poche dimensioni. L'insieme di immagini possibili è essenzialmente infinito, l'insieme di immagini "naturali" è un piccolo sottoinsieme.
Per esempio: data l'immagine di una persona, l'insieme delle possibili immagini è nell'ordine di grandezza dei muscoli facciali che egli può muovere (gradi di libertà) di ~ 50. Un estrattore di caratteristiche ideale (ed irrealistico) rappresenta tutti i fattori di variazione (ogni muscolo, illuminazione, *ecc*).

Q&A (domande e risposte):

- Nell'esempio del viso qualche altra tecnica di riduzione di dimensionalità (*ad es.* PCA) potrebbe estrarre queste caratteristiche?
  - Risposta: potrebbe funzionare solo se la superficie chiusa è un iperpiano, ma non lo è.

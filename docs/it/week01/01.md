---
lang: it
lang-ref: ch.01
title: Settimana 1
translation-date: 26 Mar 2020
translator: Nantas Nardelli
---


## Lezione parte A

Parliamo delle motivazioni dietro all'apprendimento profondo (DL, Deep Learning).
Prima di tutto, iniziamo con la storia e l'ispirazione dietro a questa materia.
Continuiamo con la storia del riconoscimento di pattern (Pattern Recognition) e
introduciamo la discesa del gradiente (Gradient Descent) e la retropropagazione
(Backpropagation). Alla fine parliamo della rappresentazione gerarchica della
corteccia visiva.


## Lezione parte B

Per prima cosa, parliamo l'evoluzione delle CNN, da Fukushima a LeCun, fino a
arrivare a AlexNet. Point parliamo di alcune applicazioni di CNN, come la
segmentazione di immagini, veicoli autonomi e analisi di immagini mediche.
Parliamo della natura ierarchica di deep networks, e degli attributi dei deep
netowrks che li rendono vantaggiosi. Finiamo con una discussione sul generare e
imparare rappresentazioni / caratteristiche (Features).


## Pratica

Parliamo della motivazione per l'applicazione delle trasformazioni di punti di
dati visualizzati in uno spazio. Parliamo di algebra lineare e di applicazioni
di trasformazioni lineari e non-lineari. Parliamo dell'uso di visualizzazioni
per capire le funzioni e gli effetti di queste trasformazioni. Ne parliamo
attravero esempi in un Jupyter notebook, e concludiamo con una discussione su
funzioni rappresentate da reti neurali.

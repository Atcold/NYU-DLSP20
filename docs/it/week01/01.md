---
lang: it
lang-ref: ch.01
title: Settimana 1
translation-date: 26 Mar 2020
translator: Nantas Nardelli
---

<!--
## Lecture part A
-->
## Lezione parte A

<!--
We discuss the motivation behind deep learning. We begin with the history
and inspiration of deep learning. Then we discuss the history of pattern
recognition and introduce gradient descent and its computation by
backpropagation. Finally, we discuss the hierarchical representation of the
visual cortex.
-->

Parliamo delle motivazioni dietro all'apprendimento profondo (Deep Learning, DL).
Prima di tutto, iniziamo con la storia e l'ispirazione dietro a questa materia.
Continuiamo con la storia del riconoscimento di pattern (Pattern Recognition) e
introduciamo la discesa del gradiente (Gradient Descent) e la retropropagazione
(Backpropagation). Alla fine parliamo della rappresentazione gerarchica della
corteccia visiva.

<!--
## Lecture part B 
-->
## Lezione parte B

<!--
We first discuss the evolution of CNNs, from Fukushima to LeCun to AlexNet. We
then discuss some applications of CNN's, such as image segmentation, autonomous
vehicles, and medical image analysis. We discuss the hierarchical nature of deep
networks and the attributes of deep networks that make them advantageous. We
conclude with a discussion of generating and learning features/representations.
-->

Per prima cosa, parliamo l'evoluzione delle CNN, da Fukushima a LeCun, fino a
arrivare a AlexNet. Point parliamo di alcune applicazioni di CNN, come la
segmentazione di immagini, veicoli autonomi e analisi di immagini mediche.
Parliamo della natura ierarchica di deep networks, e degli attributi dei deep
netowrks che li rendono vantaggiosi. Finiamo con una discussione sul generare e
imparare rappresentazioni / caratteristiche (Features).

<!--
## Practicum
-->
## Pratica

<!-- =
We discuss the motivation for applying transformations to data points
visualized in space. We talk about Linear Algebra and the application of linear
and non-linear transformations. We discuss the use of visualization to
understand the function and effects of these transformations. We walk through
examples in a Jupyter Notebook and conclude with a discussion of functions
represented by neural networks.
-->

Parliamo della motivazione per l'applicazione delle trasformazioni di punti di
dati visualizzati in uno spazio. Parliamo di algebra lineare e di applicazioni
di trasformazioni lineari e non-lineari. Parliamo dell'uso di visualizzazioni
per capire le funzioni e gli effetti di queste trasformazioni. Ne parliamo
attravero esempi in un Jupyter notebook, e concludiamo con una discussione su
funzioni rappresentate da reti neurali.

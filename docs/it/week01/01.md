---
lang: it
lang-ref: ch.01
title: Settimana 1
translation-date: 26 Mar 2020
translator: Nantas Nardelli
---


## Lezione parte A

Parliamo delle motivazioni per all'apprendimento profondo (Deep Learning, DL). Prima di tutto, iniziamo con la storia e l'ispirazione dietro a questa materia. Continuiamo con la storia del riconoscimento di pattern (_Pattern Recognition_) e introduciamo la discesa del gradiente (_Gradient Descent_) e la retropropagazione (_Backpropagation_). Alla fine parliamo della rappresentazione gerarchica della corteccia visiva.
<!--

## Lecture part A

We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.
-->

## Lezione parte B

Per prima cosa, parliamo l'evoluzione delle CNN, da Fukushima a LeCun, fino a arrivare a AlexNet. Poi parliamo di alcune applicazioni di CNN, come la segmentazione di immagini, veicoli autonomi e analisi di immagini mediche. Parliamo della natura gerarchica delle reti profonde (deep networks), e delle propriet√† che le rendono vantaggiose. Finiamo con una discussione su come generare ed apprendere rappresentazioni / caratteristiche (_features_).

<!--
## Lecture part B

We first discuss the evolution of CNNs, from Fukushima to LeCun to AlexNet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.
-->

## Pratica

Parliamo della motivazione per le trasformazioni dei dati visualizzati nello spazio. Parliamo di algebra lineare e di trasformazioni lineari e non-lineari degli _input_. Visualizziamo alcune di queste trasformazioni per capirne le funzioni e gli effetti. Ne seguiamo alcuni esempi in un Jupyter notebook. Concludiamo con una discussione sui diversi tipi di funzioni rappresentate dalle reti neurali.

<!--
## Practicum

We discuss the motivation for applying transformations to data points visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to understand the function and effects of these transformations. We walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by neural networks.
-->

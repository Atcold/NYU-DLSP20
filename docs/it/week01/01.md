---
lang: it
lang-ref: ch.01
title: Settimana 1
translation-date: 26 Mar 2020
translator: edran
---


## Lezione parte A

Discutiamo le motivazioni dietro a deep learning. Prima di tutto, iniziamo con la storia e l'ispirazione dietro a deep learning.
Continuiamo con la storia di pattern recognition, e introduciamo gradient
descent e computation by backpropagation.
Finalmente, discutiamo la rappresentazione ierarchica della corteccia visiva.


## Lezione parte B

Per prima cosa, discutiamo l'evoluzione delle CNN, da Fukushima a LeCun, fino a
arrivare a AlexNet. Point discutiamo di alcune applicazioni di CNN, come la
segmentazione di immagini, veicoli autonomi e analisi di immagini mediche.
Discutiamo della natura ierarchica di deep networks, e degli attributi dei deep
netowrks che li rendono vantaggiosi. Finiamo con una discussione sul generare e
imparare rappresentazioni / features.


## Pratica

Discutiamo la motivazione per l'applicazione di trasformazioni di punti di dati
visualizzati in uno spazio. Parliamo di algebra linear e di applicazioni di
trasformazioni lineari e non-lineari. Parliamo dell'usuo di visualizzazioni per
capire le funzioni e gli effetti di queste trasformazioni. Ne parliamo attravero
esempi in un Notebook Jupyter, e concludiamo con una discussione su funzioni
rappresentate da neural networks.

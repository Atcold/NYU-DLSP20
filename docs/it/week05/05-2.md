---
lang-ref: ch.05-2
title: Tecniche di ottimizzazione II
authors: Guido Petri, Haoyue Ping, Chinmay Singhal, Divya Juneja
date: 29 February 2020
translation-date: 13 Apr 2020
translator: marcozullich
---


## [Metodi adattivi](https://www.youtube.com/watch?v=--NZb480zlg&t=2675s)

L'SGD con momento rappresenta attualmente lo stato dell'arte dei metodi di ottimizzazione per molti problemi di ML. Ma ci sono altri metodi, generalmente chiamati adattivi, innovati nel corso degli anni, particolarmente utili per problemi mal condizionati (nel caso in cui SGD non funzioni).

Nella formulazione di SGD, ogni singolo peso della rete è aggiornato utilizzando un'equazione con lo stesso livello di apprendimento ($\gamma$ globale). Qui, per metodi adattivi intendiamo che *adattiamo il livello di apprendimento individualmente per ogni peso*. Per questo scopo, è utilizzata l'informazione fornitaci dai gradienti per ogni peso.

Le reti più frequentemente utilizzate hanno diverse strutture in diverse parti di esse. Per esempio, la prima frazione di una CNN può essere molto spesso composta da strati convoluzionali su immagini grandi, mentre più in fondo alla rete possiamo avere convoluzioni di un gran numero di canali su immagini piccole. Queste due opreazioni sono molto diverse, cosicché un livello di apprendimento che funzioni bene per la prima parte della rete potrebbe non andare bene per le sezioni finali. Ciò implica i livelli di apprendimento adattivi potrebbero essere di una certa utilità.

I pesi nella parte finale della rete (4096 nella Fig. 1 di cui sotto) sfociano direttamente nell'output e hanno un forte effetto su quest'ultimo. Di conseguenza, necessitiamo di un livello di apprendimento più basso per loro. D'altro canto, i pesi degli strati iniziali hanno un effetto individuale più limitato sull'output, specialmente quando inizializzati in maniera casuale.

<center>
<img src="{{site.baseurl}}/images/week05/05-2/5_2_vgg.png" style="zoom:40%"><br>
<b>Fig. 1: </b> la CNN VGG16
</center>


### RMSprop

L'intuizione dietro a RMSprop (acronimo di *Root Mean Square Propagation*) è che il gradiente viene normalizzato attraverso la radice della media quadratica di quest'ultimo.

Nell'equazione di cui sotto, l'elevazione a quadrato del gradiente viene effettuata individualmente per ogni elemento del vettore.

$$
\begin{aligned}
v_{t+1} &= {\alpha}v_t + (1 - \alpha) \nabla f_i(w_t)^2 \\
w_{t+1} &=  w_t - \gamma \frac {\nabla f_i(w_t)}{ \sqrt{v_{t+1}} + \epsilon}
\end{aligned}
$$

dove $\gamma$ è il livello di apprendimento globale, $\epsilon$ è una costante molto piccola (dell'ordine di  $10^{-7}$ o $10^{-8}$) utilizzata al fine di evitare errori di divisione per 0 e $v_{t+1}$ è una stima del secondo momento.

Aggiorniamo $v$ al fine di stimare una quantità *rumorosa* tramite una *media mobile esponenziale* (un metodo standard per mantenere una media di una quantità che può cambiare con il tempo). Abbiamo bisogno di dare più peso ai valori più recenti in quanto apportano più informazione. Un modo per fare ciò è smorzare in maniera esponenziale i valori meno recenti. Nel calcolo di $v$, i valori più vecchi vengono ridotti di un fattore costante $\alpha \in (0;1)$ ad ogni passo. Questo smorza i valori meno recenti finché non sono più una parte importante della media mobile esponenziale.

Il metodo originario tiene una media mobile esponenziale del secondo momento non centrato, cosicché non serve che sottraiamo la media in questo caso. Il *secondo momento* è usato per normalizzare il gradiente elemento per elemento, ovvero, ogni elemento del gradiente viene diviso per la radice della stima del secondo momento. Se il valore atteso del gradiente è piccolo, questo processo è simile alla divisione del gradiente per la sua deviazione standard.

Utilizzando un $\epsilon$ piccolo al denominatore non si causano divergenze, in quanto, quando $v$ è molto piccolo, anche il momento è molto piccolo di conseguenza.





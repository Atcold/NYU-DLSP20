---
lang: it
lang-ref: ch.05
title: Settimana 5
translation-date: 07 Apr 2020
translator: Marco Zullich
---

## Lezione parte A

Iniziamo introducendo la discesa del gradiente (GD). Ne discutiamo l'intuizione e parliamo, inoltre, di come la grandezza del passo gioca un ruolo importante nel raggiungimento della soluzione. Quindi ci muoviamo verso la discesa stocastica del gradiente (SGD) e ne compariamo la performance rispetto al GD a *batch* pieno. Infine, parliamo di aggiornamento del momento, specificamente della regola dei due aggiornamenti, dell'intuizione che sta dientro al momento e del suo effetto sulla convergenza.


## Lezione parte B

Discutiamo dei metodi adattivi per SGD quali RMSprop e ADAM. Parliamo anche degli strati di normalizzazione e dei loro effetti sul processo di addestramento delle reti neurali. Infine, discutiamo un esempio reale di rete neurale utilizzata nell'industria per rendere più veloci ed efficienti le scansioni di *imaging* a risonanza magnetica.


## Pratica

Rivediamo brevemente i prodotti matriciali e le convoluzioni. Il punto chiave è l'utilizzo dei filtri tramite impilamento e traslazione. Prima di tutto comprendiamo "a mano" il caso monodimensionale delle convoluzioni, dopodiché utilizziamo PyTorch per imparare le dimensioni dei filtri e la grandezza dell'output in convoluzioni mono- e bi-dimensionali. Inoltre, usiamo PyTorch per imparare come funzionano il gradiente automatico e i *custom-grads*.

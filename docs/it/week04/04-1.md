---
lang: it
lang-ref: ch.04-1
title: Algebra lineare e convoluzioni
data: 18 febbraio 2020
data traduzione: 5 aprile 2020
traduttore: marcozullich
---

## [Ripasso di algebra lineare](https://www.youtube.com/watch?v=OrBEon3VlQg&t=68s)

Questo capitolo è un ripasso di algebra lineare base nel contesto delle reti neurali. Iniziamo con un semplice strato nascosto $\boldsymbol{h}$:

$$
\boldsymbol{h} = f(\boldsymbol{z})
$$

L'output è una funzione nonlineare $f$ applicata a un vettore $z$. Qui $z$ è l'output di una trasformazione affine $\boldsymbol{A} \in\mathbb{R^{m\times n}}$ applicata al vettore di input $\boldsymbol{x} \in\mathbb{R^n}$:

$$
\boldsymbol{z} = \boldsymbol{A} \boldsymbol{x}
$$

Per semplicità, i *bias* sono omessi. L'equazione lineare può essere espansa come:

$$
\boldsymbol{A}\boldsymbol{x} =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix} \begin{pmatrix}
x_1 \\ \vdots \\x_n \end{pmatrix} =
\begin{pmatrix}
    \text{---} \; \boldsymbol{a}^{(1)} \; \text{---} \\
    \text{---} \; \boldsymbol{a}^{(2)} \; \text{---} \\
    \vdots \\
    \text{---} \; \boldsymbol{a}^{(m)} \; \text{---} \\
\end{pmatrix}
\begin{matrix}
    \rvert \\ \boldsymbol{x} \\ \rvert
\end{matrix} =
\begin{pmatrix}
    {\boldsymbol{a}}^{(1)} \boldsymbol{x} \\ {\boldsymbol{a}}^{(2)} \boldsymbol{x} \\ \vdots \\ {\boldsymbol{a}}^{(m)} \boldsymbol{x}
\end{pmatrix}_{m \times 1}
$$

dove $\boldsymbol{a}^{(i)}$ è la $i$-esima riga della matrice $\boldsymbol{A}$.

Per comprendere il significato di questa trasformazione, analizziamo una componente di $\boldsymbol{z}$ come $a^{(1)}\boldsymbol{x}$. Sia $n=2$, quindi $\boldsymbol{a} = (a_1,a_2)$ e $\boldsymbol{x}  = (x_1,x_2)$.

$\boldsymbol{a}$ e $\boldsymbol{x}$ possono essere visualizzati come vettori nel piano cartesiano. Ora, se l'angolo tra $\boldsymbol{a}$ e $\hat{\boldsymbol{\imath}}$ è $\alpha$ è l'angolo fra $\boldsymbol{x}$ e $\hat{\boldsymbol{\imath}}$ è $\xi$, allora le formule trigonometriche $a^\top\boldsymbol{x}$ possono essere espanse come:

$$
\begin {aligned}
\boldsymbol{a}^\top\boldsymbol{x} &= a_1x_1+a_2x_2\\
&=\lVert \boldsymbol{a} \rVert \cos(\alpha)\lVert \boldsymbol{x} \rVert \cos(\xi) + \lVert \boldsymbol{a} \rVert \sin(\alpha)\lVert \boldsymbol{x} \rVert \sin(\xi)\\
&=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert \big(\cos(\alpha)\cos(\xi)+\sin(\alpha)\sin(\xi)\big)\\
&=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert \cos(\xi-\alpha)
\end {aligned}
$$

L'output misura l'allineamento dell'input ad una specifica riga della matrice $\boldsymbol{A}$. Ciò può essere compreso dall'osservazione dell'angolo fra due vettori, $\xi-\alpha$. Quando $\xi = \alpha$, i due vettori sono perfettamente allineati ed il massimo è raggiunto. If $\xi - \alpha = \pi$, allora $\boldsymbol{a}^\top\boldsymbol{x}$ raggiunge il suo minimo e i due vettori sono orientati in versi opposti. Riassumendo, la trasformazione lineare permette di visualizzare la proiezione di un input in vari orientamenti, come definito da $A$. Questa intuizione è pure espandibile in un numero maggiore di dimensioni.

Un'altro modo di capire la trasformazione lineare passa per la comprensione che $\boldsymbol{z}$ può anche essere espanso come:

$$
\boldsymbol{A}\boldsymbol{x} =
\begin{pmatrix}
    \vert            & \vert            &        & \vert             \\
    \boldsymbol{a}_1 & \boldsymbol{a}_2 & \cdots & \boldsymbol{a}_n  \\
    \vert            & \vert            &        & \vert             \\
\end{pmatrix}
\begin{matrix}
    \rvert \\ \boldsymbol{x} \\ \rvert
\end{matrix} =
x_1 \begin{matrix} \rvert \\ \boldsymbol{a}_1 \\ \rvert \end{matrix} +
x_2 \begin{matrix} \rvert \\ \boldsymbol{a}_2 \\ \rvert \end{matrix} +
    \cdots +
x_n \begin{matrix} \rvert \\ \boldsymbol{a}_n \\ \rvert \end{matrix} +
$$

L'output è una somma pesata di colonne della matrice $\boldsymbol{A}$. Di conseguenza, il segnale non è null'altro che una composizione dell'input.


## [Estensione dell'algebra lineare alle convoluzioni](https://www.youtube.com/watch?v=OrBEon3VlQg&t=1030s)

Ora, estendiamo l'algebra lineare alle convoluzioni utilizzando come esempio l'analisi di dati audio. Iniziamo rappresentando uno strato densamente connesso come una forma di moltiplicazione matriciale:

$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13}\\
w_{21} & w_{22} & w_{23}\\
w_{31} & w_{32} & w_{33}\\
w_{41} & w_{42} & w_{43}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

In questo esempio, la matrice dei pesi ha una dimensione di $4 \times 3$, il vettore di input ha una dimensione di $3 \times 1$ e il vettore di output ha una dimensione di $4 \times 1$.

Comunque, i dati audio sono molto più lunghi (ben più di 3 campionamenti). Il numero di campionamenti dei dati audio è uguale alla durata dell'audio (es. 3 secondi) moltiplicato per il rapporto di campionamento (es. 22.05 kHz). Come mostrato sotto, il vettore di input $\boldsymbol{x}$ sarà piuttosto lungo. In maniera analoga, la matrice dei pesi diventera piuttosto "grossa".

$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13} & w_{14} & \cdots &w_{1k}& \cdots &w_{1n}\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

La formulazione qui sopra sarà difficile da addestrare. Fortunatamente vi sono metodi per semplificarla.


### Proprietà: localizzazione

A causa della localizzazione (ovvero, non siamo interessati a punti lontani) dei dati, $w_{1k}$ della matrice sopra può essere riempito di zeri quando $k$ è relativamente grande. Di conseguenza, la prima riga della matrice diviene un nucleo di dimensione 3. Denotiamo questo nucleo come $\boldsymbol{a}^{(1)} = \begin{bmatrix} a_1^{(1)}  & a_2^{(1)}  & a_3^{(1)} \end{bmatrix}$.

$$
\begin{bmatrix}
a_1^{(1)}  & a_2^{(1)}  & a_3^{(1)}  & 0 & \cdots &0& \cdots &0\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

### Proprietà: stazionarietà

I segnali di dati naturali hanno la proprietà di stazionarietà (ovvero, certi motivi si ripeteranno). Ciò ci aiuta a riutilizzare il nucleo $\mathbf{a}^{(1)}$ che abbiamo definito precedentemente. Usiamo questo nucleo posizionandolo avanti di una posizione per ogni istante (ovvero, il passo è 1), ottenendo quanto segue:

$$
\begin{bmatrix}
a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0 & 0 & 0 & 0&\cdots  &0\\
0 & a_1^{(1)}  & a_2^{(1)} & a_3^{(1)}  & 0&0&0&\cdots &0\\
0 & 0 & a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0&0&\cdots &0\\
0 & 0 & 0& a_1^{(1)}  & a_2^{(1)}  &a_3^{(1)} &0&\cdots &0\\
0 & 0 & 0& 0 & a_1^{(1)}  &a_2^{(1)} &a_3^{(1)} &\cdots &0\\
\vdots&&\vdots&&\vdots&&\vdots&&\vdots
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix}
$$

Sia la parte in alto a destra che quella in basso a sinistra della matrice sono riempite con zeri grazie alla localizzazione, dando origine a sparsità. L'utilizzo continuo di un certo nucleo viene denominato condivisione dei pesi.
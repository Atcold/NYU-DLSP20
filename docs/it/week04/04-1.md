---
lang: it
lang-ref: ch.04-1
title: Algebra lineare e convoluzioni
data: 18 febbraio 2020
data traduzione: 5 aprile 2020
traduttore: marcozullich
---

## [Ripasso di algebra lineare](https://www.youtube.com/watch?v=OrBEon3VlQg&t=68s)

Questo capitolo è un ripasso di algebra lineare base nel contesto delle reti neurali. Iniziamo con un semplice strato nascosto $\boldsymbol{h}$:

$$
\boldsymbol{h} = f(\boldsymbol{z})
$$

L'output è una funzione nonlineare $f$ applicata a un vettore $z$. Qui $z$ è l'output di una trasformazione affine $\boldsymbol{A} \in\mathbb{R^{m\times n}}$ applicata al vettore di input $\boldsymbol{x} \in\mathbb{R^n}$:

$$
\boldsymbol{z} = \boldsymbol{A} \boldsymbol{x}
$$

Per semplicità, i *bias* sono omessi. L'equazione lineare può essere espansa come:

$$
\boldsymbol{A}\boldsymbol{x} =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix} \begin{pmatrix}
x_1 \\ \vdots \\x_n \end{pmatrix} =
\begin{pmatrix}
    \text{---} \; \boldsymbol{a}^{(1)} \; \text{---} \\
    \text{---} \; \boldsymbol{a}^{(2)} \; \text{---} \\
    \vdots \\
    \text{---} \; \boldsymbol{a}^{(m)} \; \text{---} \\
\end{pmatrix}
\begin{matrix}
    \rvert \\ \boldsymbol{x} \\ \rvert
\end{matrix} =
\begin{pmatrix}
    {\boldsymbol{a}}^{(1)} \boldsymbol{x} \\ {\boldsymbol{a}}^{(2)} \boldsymbol{x} \\ \vdots \\ {\boldsymbol{a}}^{(m)} \boldsymbol{x}
\end{pmatrix}_{m \times 1}
$$

dove $\boldsymbol{a}^{(i)}$ è la $i$-esima riga della matrice $\boldsymbol{A}$.

Per comprendere il significato di questa trasformazione, analizziamo una componente di $\boldsymbol{z}$ come $a^{(1)}\boldsymbol{x}$. Sia $n=2$, quindi $\boldsymbol{a} = (a_1,a_2)$ e $\boldsymbol{x}  = (x_1,x_2)$.
---
lang: it
lang-ref: ch.11
title: Settimana 11
translation-date: 15 Aug 2020
translator: Francesca Guiso
---

## Lezione parte A
<!-- ## Lecture part A -->

In questa sezione, abbiamo discusso di alcune funzioni di attivazione comuni di PyTorch. In particolare, abbiamo messo a confronto due tipi di funzioni di attivazione: non-differenziabili (es. con punti angolosi) e differenziabili (lisce). Le prime sono preferibili per le reti neurali profonde poiché le seconde potrebbero essere soggette al problema dello svanimento del gradiente (_vanishing gradient problem_). Poi, abbiamo discusso delle funzioni di perdita comuni di PyTorch.
<!-- In this section, we discussed about the common activation functions in Pytorch. In particular, we compared activations with kink(s) versus smooth activations - the former is preferred in a deep neural network as the latter might suffer with gradient vanishing problem. We then learned about the common loss functions in Pytorch. -->

## Lezione parte B
<!-- ## Lecture part B -->

In questa sezione, abbiamo continuato la discussione sulle funzioni di perdita, particolarmente le funzioni di perdita a margine (*margin-based loss functions*) e le loro applicazioni. Abbiamo poi discusso come concepire una funzione di perdita adeguata ai modelli ad energia (*energy-based models, EBM*) e abbiamo visto degli esempi ben noti di funzioni di perdita per gli EBM. Abbiamo dato particolare attenzione alle funzioni di perdita a margine e al concetto di "errore più grave".
<!-- In this section, we continued to learn about loss functions - in particular, margin-based losses and their applications. We then discussed how to design a good loss function for EBMs as well as examples of well-known EBM loss functions. We gave particular attention to margin-based loss function here, as well as explaining the idea of "most offending incorrect answer. -->

## Pratica
<!-- ## Practicum -->

Questa pratica propone metodi di apprendimento delle politiche efficaci (_effective policy learning_) per la guida nel traffico denso. Abbiamo addestrato diverse politiche, "srotolando" un modello addestrato sulla realtà, tramite l'ottimizzazione di diverse funzioni di costo (*cost functions*). L'idea è di introdurre un termine di costo che rappresenta la "distanza" del modello dagli stati su cui è stato addestrato, minimizzandone così l'incertezza nelle previsioni.
<!-- This practicum proposed effective policy learning for driving in dense traffic. We trained multiple policies by unrolling a learned model of the real world dynamics by optimizing different cost functions. The idea is to minimize the uncertainty in the model's prediction by introducing a cost term that represents the model's divergence from the states it is trained on.   -->

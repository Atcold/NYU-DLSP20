---
lang-ref: ch.02
lang: it
title: Settimana 2
translation-date: 1 Apr 2020
translator: Edoardo Gomba
---


## Lezione parte A

Iniziamo comprendendo quali sono i modelli parametrizzati e quindi discutiamo cos'è una funzione di perdita (loss function). Analizziamo quindi i metodi basati sul gradiente (Gradient Based) e il modo in cui viene utilizzato nell'algoritmo di backpropagation in una rete neurale tradizionale. Concludiamo questa sezione imparando come implementare una rete neurale in PyTorch seguita da una discussione su una forma più generalizzata di backpropagation.

## Lezione parte B

Iniziamo con un esempio concreto di backpropagation e discutiamo delle dimensioni delle matrici giacobine. Esaminiamo quindi vari moduli di rete neurale di base e calcoliamo i loro gradienti, seguiti da una breve discussione su softmax e logsoftmax. L'altro argomento di discussione in questa parte è Trucchi Pratici (Practical Tricks) per la backpropagation.


## Pratica

Diamo una breve introduzione all'apprendimento supervisionato usando reti neurali artificiali. Esaminiamo la formulazione del problema e le convenzioni dei dati utilizzati per formare queste reti. Discutiamo anche come addestrare una rete neurale per la classificazione multi-classe e come eseguire l'inferenza una volta che la rete è stata addestrata.

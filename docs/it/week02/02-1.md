---
lang-ref: ch.02-1
lang: it
lecturer: Yann LeCun
title: Introduzione al gradient descent e alla retropropagazione
authors: Amartya Prasad, Dongning Fang, Yuxin Tang, Sahana Upadhya
date: 3 Feb 2020
translation-date: 8 Apr 2020
translator: Edoardo Gomba
---


## [Gradient Descent optimization algorithm](https://www.youtube.com/watch?v=d9vdh3b787Y&t=29s)


### Parametrised models

$$
\bar{y} = G(x,w)
$$

I modelli parametrizzati sono semplicemente funzioni che dipendono da input e parametri addestrabili. Non vi è alcuna differenza fondamentale tra i due, tranne per il fatto che i parametri addestrabili sono condivisi tra i campioni di addestramento, mentre l'input varia da campione a campione. Nella maggior parte dei framework di deep learning, i parametri sono impliciti, cioè non vengono passati quando viene chiamata la funzione. Sono "salvati all'interno della funzione", per così dire, almeno nelle versioni orientate agli oggetti (object-oriented) dei modelli.
The parametrised model (function) takes in an input, has a parameter vector and produces an output. In supervised learning, this output goes into the cost function ($C(y,\bar{y}$)), which compares the true output (${y}$) with the model output ($\bar{y}$). The computation graph for this model is shown in Figure 1.
Il modello (funzione) parametrizzato accetta un input, ha un vettore di parametro e produce un output. Nell'apprendimento supervisionato, questo output passa alla funzione di costo ($ C (y, \ bar {y} $)), che confronta l'output reale ($ {y} $) con l'output del modello ($ \ bar {y} $ ). Il grafico di calcolo per questo modello è mostrato nella Figura 1.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure1.jpg" alt="Figure1" style="zoom: 33%;" /></center> |
| <center>Figure 1: Rappresentazione del grafico di calcolo per un modello con parametri </center>|

Esempi di funzioni parametrizzate -

- Modello lineare - Somma ponderata dei componenti del vettore di input :
  $$
  \bar{y} = \sum_i w_i x_i \text{  ;  } C(y,\bar{y}) = ||y - \bar{y}||^2
  $$

- Vicino più vicino (Nearest Neighbor)- C'è un input x e una matrice di peso W con ogni riga della matrice indicizzata da k. L'output è il valore di k che corrisponde alla riga di W più vicina a x :
  $$
  \bar{y} = \text{argmin}_k ||x - w_{k,.} ||^2
  $$
  I modelli con parametri potrebbero anche comportare funzioni complicate.


#### Notazioni del diagramma a blocchi per i grafici di calcolo

- Variabili (tensori, scalari, continue, discrete)
    - <img src="{{site.baseurl}}/images/week02/02-1/x.PNG" alt="x" style="zoom:50%;" /> è un ingresso osservato al sistema
    - <img src="{{site.baseurl}}/images/week02/02-1/y.PNG" alt="y" style="zoom:50%;" /> è una variabile calcolata che è prodotta da una funzione deterministica

- Funzione deterministica

    <img src="{{site.baseurl}}/images/week02/02-1/deterministic_function.PNG" alt="deterministic_function" style="zoom:50%;" />

    - Accetta ingressi multipli e può produrre uscite multiple
    - ????Ha una variabile con parametro implicito (${w}$)
    - Il lato arrotondato indica la direzione in cui è facile da calcolare. Nel diagramma sopra, è più facile calcolare ${\bar{y}}$ da ${x}$ rispetto al contrario

- Funzione con valore scalare (scalar-valued)

  <img src="{{site.baseurl}}/images/week02/02-1/scalar-valued.PNG" alt="scalar-valued" style="zoom:50%;" />

    - Utilizzata per rappresentare le funzioni di costo
    - Ha un'uscita implicitamente scalare
    - Accetta più input e genera un singolo valore in uscita (normalmente la distanza fra gli ingressi)


#### Funzioni di perdita(Loss functions)

La funzione di perdita è una funzione minimizzata durante l'allenamento. Esistono due tipi di perdite:

1) Perdita a campione -
$$
 L(x,y,w) = C(y, G(x,w))
$$
2) Perdita media -

​	Per ogni insieme di campioni $$S = \{(x[p],y[p])/p=0,1...P-1 \}$$

​	Perdita media su un Set S  è data da :  $$L(S,w) = \frac{1}{P} \sum_{(x,y)} L(x,y,w)$$

| <center><img src="{{site.baseurl}}/images/week02/02-1/Average_Loss.png" alt="Average_Loss" style="zoom:33%;" /></center> |
|   <center>Figura 2: Grafico di calcolo per modello con Perdita media    </center>|

Nel paradigma standard di Apprendimento Supervisionato (Supervised Learning ), la perdita (per campione) è semplicemente l'uscita della funzione di costo. L'apprendimento automatico (Machine Learning) riguarda principalmente l'ottimizzazione delle funzioni (in genere le minimizza). Potrebbe anche implicare la ricerca del Nash Equilibria tra due funzioni come con i GAN (Generative Adversarial Networks). Questo viene fatto usando i metodi basati sul gradiente, sebbene non necessariamente la discesa del gradiente (Gradient Descent).
In the standard Supervised Learning paradigm, the loss (per sample) is simply the output of the cost function. Machine Learning is mostly about optimizing functions (usually minimizing them). It could also involve finding Nash Equilibria between two functions like with GANs. This is done using Gradient Based Methods, though not necessarily Gradient Descent.


### Discesa del Gradiente(Gradient descent)

Un  **Gradient Based Method** è un metodo/algoritmo che trova il minimo della funzione, assumendo che si possa facimente calcolare il gradiente di quella funzione. Si assume che la funzione sia continua e differenziabile quasi ovunque (non è necessario che sia differenziabile ovunque).

**Descrizione intuitiva della Discesa del Gradiente (Gradient Descent)** - Immagina di essere in montagna nel mezzo di una notte nebbiosa. Dato che vuoi andare al villaggio e hai una visione limitata, ti guardi intorno nelle immediate vicinanze per trovare la direzione della discesa più ripida e fai un passo in quella direzione.

**Metodi differenti della Discesa del Gradiente**

- Regola di aggiornamento della discesa gradiente completa (batch) :
  $$
  w \leftarrow w - \eta \frac{\partial L(S,w)}{\partial w}
  $$

- Per la SGD (Stochastic Gradient  Descent), la regola di aggiornamento diventa :
  - Scegliere un $p$ nel $\text{0...P-1}$, poi aggiornare
    $$
    w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
    $$

Dove $${w}$$ rappresenta il parametro che deve essere ottimizzato.

$\eta \text{ é una costante qui ma in algoritmi più sofisticati ,potrebbe essere una matrice}$.

Se è una matrice semi-definita positiva, ci sposteremo comunque in discesa ma non necessariamente nella direzione della discesa più ripida. In effetti la direzione della discesa più ripida potrebbe non essere sempre la direzione in cui vogliamo spostarci.

Se la funzione non è differenziabile, ovvero ha un buco o è simile a una scala o è piatta, in cui il gradiente non fornisce alcuna informazione, è necessario ricorrere ad altri metodi, chiamati metodi di ordine 0 (0-th Order Methods) o metodi senza gradiente (Gradient-Free Methods) . Il Deep Learning riguarda i metodi basati sul gradiente.

Tuttavia, il RL (Reinforcement Learning) comporta la **Stima del gradiente (Gradient Estimation)** senza la forma esplicita per il gradiente. Un esempio è un robot che impara ad andare in bicicletta dove il robot cade di tanto in tanto. La funzione obiettivo misura per quanto tempo la bici rimane in piedi senza cadere. Sfortunatamente, non esiste un gradiente per la funzione obiettivo. Il robot deve provare algoritmi differenti.

La funzione di costo del RL non è differenziabile per la maggior parte del tempo, ma la rete che calcola l'output è basata sul gradiente. Questa è la differenza principale tra apprendimento supervisionato (supervised learning) e apprendimento di rinforzo(reinforcement learning). Con quest'ultimo, la funzione di costo C non è differenziabile. In realtà è completamente sconosciuta. Restituisce semplicemente un output quando gli input vengono alimentati, come una blackbox. Ciò lo rende altamente inefficiente ed è uno dei principali inconvenienti di RL, in particolare quando il vettore dei parametri è ad elevata dimensionalità (il che implica un enorme spazio di soluzioni in cui cercare, rendendo difficile trovare verso dove muoversi).

Una tecnica molto popolare nel RL sono gli Actor Critic Methods. Un metodo critico consiste fondamentalmente di un secondo modulo C che è un modulo noto e addestrabile. Si può addestrare il modulo C, che è differenziabile, per approssimare la funzione di costo / funzione di ricompensa. La ricompensa è un costo negativo, più simile a una punizione. Questo è un modo per rendere la funzione di costo differenziabile, o almeno per approssimarla con una funzione di differenziabile in modo che si possa retropropagazione.


## [Vantaggi della SGD e della retropropagazione per le reti neurali tradizionali nets](https://www.youtube.com/watch?v=d9vdh3b787Y&t=1036s)


### Vantaggi della discesa del gradiente stocastico (Stochastic Gradient Descent, SGD)

In pratica, si usa il gradiente stocastico per calcolare il gradiente della funzione obiettivo rispetto ai parametri. Invece di calcolare l'intero gradiente della funzione obiettivo, che è la media di tutti i campioni, il gradiente stocastico prende solo un campione, calcola la perdita,$L$, e il gradiente della perdita rispetto ai parametri, quindi fa un passo nella direzione del gradiente negativo.

$$
w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
$$

Nella formula, $w$ veine approssimato da  $w$ meno la dimensione del gradino, volte il gradiente della funzione di perdita, per campione, rispetto ai parametri per un dato campione, ($x[p]$,$y[p]$).

Se lo facciamo su un singolo campione, otterremo una traiettoria molto rumorosa, come mostrato nella Figura 3. Al posto della perdita che va direttamente in discesa, questa è stocastica. Ogni campione attirerà la perdita in una direzione diversa. È solo la media che ci porta al minimo della media. Sebbene appaia inefficiente, è molto più veloce della discesa del gradiente batch almeno nel contesto dell'apprendimento automatico (machine learning) quando i campioni presentano una certa ridondanza.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure2.png" alt="Figure2" style="zoom:80%;" /></center> |
| <center>Figure 3: Traiettoria della discesa stocastica del gradiente   (SGD) nel caso di aggiornamento per campione </center>|

In pratica, utilizziamo i batch invece di eseguire la discesa stocastica del gradiente  su un singolo campione. Calcoliamo la media del gradiente su un lotto (batch) di campioni, non un singolo campione, quindi facciamo un passo. L'unico motivo per farlo è che possiamo usare in modo più efficiente l'hardware esistente (ad esempio GPU, CPU multicore) se utilizziamo i batch, poiché è più facile parallelizzare. Il batch è il modo più semplice per parallelizzare.

### Reti neurali tradizionali

Le reti neurali tradizionali sono fondamentalmente strati intervallati di operazioni lineari e operazioni non lineari puntuali. Per operazioni lineari, concettualmente è solo una moltiplicazione matrice-vettore. Prendiamo il vettore (input) moltiplicato per una matrice formata dai pesi. Il secondo tipo di operazione consiste nel prendere tutti i componenti del vettore sommato ponderato e passarlo attraverso una semplice non linearità(i.e. $\texttt{ReLU}(\cdot)$, $\tanh(\cdot)$, …).

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure3.png" alt="Figure3" style="zoom:30%;" /></center> |
|             <center>Figura 4: Reti neurali tradizionali             </center>|

Figura 4 è un esempio di una rete a 2 strati, perché ciò che conta sono le coppie (cioè lineare + non lineare). Alcune persone lo chiamano una rete a 3 strati perché contano le variabili. Si noti che se non ci sono non linearità nel mezzo, potremmo anche avere un singolo strato perché il prodotto di due funzioni lineari è una funzione lineare.

La figura 5 mostra come i blocchi funzionali lineari e non lineari dello stack di rete:

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure4.png" alt="Figure4" style="zoom:30%;" /></center> |
|  <center>Figure 5: Guardando all'interno dei blocchi lineari e non-lineari   </center>|

Nel grafo, $s[i]$ è la somma pesata dell'unità ${i}$ che è calcolata come:

$$
s[i]=\Sigma_{j \in UP(i)}w[i,j]\cdot z[j]
$$

dove $UP(i)$ denota i predecessori di $i$ e  $z[j]$ è la $j$ma uscita dallo strato precedente.

L'uscita $z[i]$ è calcolata come:

$$
z[i]=f(s[i])
$$

dove $f$ è una funzione non-lineare.


### Retropropagazione (backpropagation) through a non-linear function

The first way to do backpropagation is to backpropagate through a non linear function. We take a particular non-linear function $h$ from the network and leave everything else in the blackbox.
Il primo modo per eseguire la retropropagazione è retropropagare attraverso una funzione non lineare. Prendiamo una particolare funzione non lineare $h$ dalla rete e lasciamo tutto il resto nella scatola nera.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure5.png" alt="Figure5" style="zoom: 25%;" /></center> |
|    <center>Figura 6: Retropropagazione attraverso una funzione non lineare     </center>|

Useremo la regola della catena per calcolare i gradienti:

$$
g(h(s))' = g'(h(s))\cdot h'(s)
$$

mentre $h'(s)$ è la derivata di $z$ rispetto a $s$ rrappresentata da $\frac{\mathrm{d}z}{\mathrm{d}s}$.
Per chiarire la connessione fra le derivate, riscriviamo la formula sopra come :

$$
\frac{\mathrm{d}C}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot \frac{\mathrm{d}z}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot h'(s)
$$

Quindi se abbiamo una catena di queste funzioni nella rete, possiamo backpropagare moltiplicando per le derivate di tutte le funzioni ${h}$ una dopo l'altra fino in fondo.

È più intuitivo pensarlo in termini di perturbazioni. Perturbando $s$ da $\mathrm{d}s$ perturberemo $z$ da:

$$\mathrm{d}z = \mathrm{d}s \cdot h'(s)$$

Questo a sua volta perturberebbe C da:

$$
\mathrm{d}C = \mathrm{d}z\cdot\frac{\mathrm{d}C}{\mathrm{d}z} = \mathrm{d}s\cdot h’(s)\cdot\frac{\mathrm{d}C}{\mathrm{d}z}
$$

Una volta di nuovo, noi finiamo con la stessa formula di quella mostrata sopra.


### Retropropagazione attraverso una somma pesata

Per un modulo lineare, eseguiamo la retropropagazione attraverso una somma ponderata. Qui vediamo l'intera rete come una blackbox ad eccezione di 3 connessioni che vanno da una variabile ${z}$ a un gruppo di variabili $s$.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure6.png" alt="Figure6" style="zoom: 25%;" /></center> |
|        <center>Figura 7: Retropropagazione attraverso somma pesata </center>|


Questa volta la perturbazione è una somma ponderata. Z influenza diverse variabili. Perturbando $z$ da $\mathrm{d}z$ perturberà $s[0]$, $s[1]$ e $s[2]$ da:

$$
\mathrm{d}s[0]=w[0]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[1]=w[1]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[2]=w[2]\cdot\mathrm{d}z
$$

 Questo perturberà C da

$$
\mathrm{d}C = \mathrm{d}s[0]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[0]}+\mathrm{d}s[1]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[1]}+\mathrm{d}s[2]\cdot\frac{\mathrm{d}C}{\mathrm{d}s[2]}
$$

Quindi C sta per variare la somma di 3 variazioni:

$$
\frac{\mathrm{d}C}{\mathrm{d}z} = \frac{\mathrm{d}C}{\mathrm{d}s[0]}\cdot w[0]+\frac{\mathrm{d}C}{\mathrm{d}s[1]}\cdot w[1]+\frac{\mathrm{d}C}{\mathrm{d}s[2]}\cdot w[2]
$$


## [Implementation PyTorch di reti neurali e un algoritmo generalizzato di retropropagazione](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2288s)


### Diagramma a blocchi di una rete neurale tradizionale

- Blocchi lineari $s_{k+1}=w_kz_k$
- Blocchi non-lineari $z_k=h(s_k)$

  <center><img src="{{site.baseurl}}/images/week02/02-1/Figure 7.png" alt="Figure 7" style="zoom: 33%;" /></center>

$w_k$: matrci $z_k$: vettori $h$: applicazione della funzione scalare ${h}$ a ogni componente. Questa è una rete neurale a 3 strati con coppie di funzioni lineari e non lineari, sebbene la maggior parte delle reti neurali moderne non abbia separazioni lineari e non lineari così chiare e siano più complesse.

### Implementazione PyTorch

```python
import torch
from torch import nn
image = torch.randn(3, 10, 20)
d0 = image.nelement()

class mynet(nn.Module):
    def __init__(self, d0, d1, d2, d3):
        super().__init__()
        self.m0 = nn.Linear(d0, d1)
        self.m1 = nn.Linear(d1, d2)
        self.m2 = nn.Linear(d2, d3)

    def forward(self,x):
        z0 = x.view(-1)  # flatten input tensor
        s1 = self.m0(z0)
        z1 = torch.relu(s1)
        s2 = self.m1(z1)
        z2 = torch.relu(s2)
        s3 = self.m2(z2)
        return s3
model = mynet(d0, 60, 40, 10)
out = model(image)
```

- Possiamo implementare reti neurali con classi orientate agli oggetti in PyTorch. Per prima cosa definiamo una classe per la rete neurale e inizializziamo gli strati lineari nel costruttore usando la classe predefinita nn.Linear. I livelli lineari devono essere oggetti separati perché ognuno di essi contiene un vettore di parametri. La classe nn.Linear aggiunge anche implicitamente il vettore di distorsione (bias). Quindi definiamo una funzione in avanti (forward) su come calcolare le uscite con la funzione $\text{torch.relu}$ come attivazione non lineare. Non è necessario inizializzare funzioni relu separate perché esse non hanno parametri.

- Non abbiamo bisogno di calcolare noi stessi il gradiente poiché PyTorch sa come retropropagare e calcolare i gradienti una volta data la funzione forward.


### Retropropagazione attraverso un modulo funzionale

Ora noi presentiamo una forma più generalizzata di retropropagazione.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure9.png" alt="Figure9" style="zoom:33%;" /></center> |
|    <center>Figure 8: Retropropagazione attraverso un modulo funzionale     </center>|


- Usando la regola della catena per le funzioni dei vettori

  $$
   z_g : [d_g\times 1]
  $$

  $$
   z_f:[d_f\times 1]
  $$

  $$
  \frac{\partial c}{\partial{z_f}}=\frac{\partial c}{\partial{z_g}}\frac{\partial {z_g}}{\partial{z_f}}
  $$

  $$
  [1\times d_f]= [1\times d_g]\times[d_g\times d_f]
  $$

    Questa è la formula di base per $\frac{\partial c}{\partial{z_f}}$ usando la regola della catena. Si noti che il gradiente di una funzione scalare rispetto a un vettore è un vettore della stessa dimensione del vettore rispetto al quale si differenzia. Per rendere coerenti le notazioni, si tratta di un vettore di riga anziché di un vettore colonna.

- Matrice jacobiana

  $$
  \left(\frac{\partial{z_g}}{\partial {z_f}}\right)_{ij}=\frac{(\partial {z_g})_i}{(\partial {z_f})_j}
  $$

  Abbiamo bisogno di $\frac{\partial {z_g}}{\partial {z_f}}$ (elementi della matrice jacobiana) per calcolare il gradiente della funzione costo rispetto a $z_f$ dati il gradiente della funzione costo rispetto a $z_g$. Ogni elemento $ij$ è uguale alla derivata parziale della $i$ma componente del vettore di uscita rispetto alla $j$ma componente del vettore di ingresso.

  If we have a cascade of modules, we keep multiplying the Jacobian matrices of all the modules going down and we get the gradients w.r.t all the internal variables.
  Se abbiamo una cascata di moduli, scendendo continuiamo a moltiplicare le matrici jacobiane di tutti i moduli e otteniamo i gradienti rispetto a tutte le variabili interne.

### Retropropagazione attraverso un grafo a più stadi

Considera una pila di molti moduli in una rete neurale come mostrato in Figura 9.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure10.png" alt="Figure10" style="zoom:33%;" /></center> |
|         <center>Figure 9: Retropropagazione attraverso un grafo a più stadi         </center>|

For the backprop algorithm, we need two sets of gradients - one with respect to the states (each module of the network) and one with respect to the weights (all the parameters in a particular module). So we have two Jacobian matrices associated with each module. We can again use chain rule for backprop.
Per l'algoritmo di retropropagazione, abbiamo bisogno di due serie di gradienti: uno rispetto agli stati (ogni modulo della rete) e uno rispetto ai pesi (tutti i parametri in un particolare modulo). Quindi abbiamo due matrici jacobiane associate a ciascun modulo. Possiamo di nuovo usare la regola della catena per la retropropagazione.
- Usando la regola della catena per le funzioni vettori

  $$
  \frac{\partial c}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {z_k}}
  $$

  $$
  \frac{\partial c}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {w_k}}
  $$

- Due matrici jacobiane per il modulo
    - Una rispetto a $z[k]$
    - Una rispetto a $w[k]$


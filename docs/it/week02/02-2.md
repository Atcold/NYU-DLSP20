---
lang: it
lang-ref: ch.02-2
lecturer: Yann LeCun
title: Calcolare i gradienti per i moduli NN e trucchi pratici per la retropropagazione
authors: Micaela Flores, Sheetal Laad, Brina Seidel, Aishwarya Rajan
date: 3 Feb 2020
translation-date: 30 Mar 2020
translator: Yuri Gardinazzi
---


## [Esempio concreto di retropropagazione e introduzione ai moduli basilari di reti neurali](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2989s)


### Esempio

Successivamente considereremo un esempio concreto di retropropagazione (backpropagation) accompagnato da un grafico.
La funzione arbitraria $G(w)$ è l'input della funzione costo $C$, che può essere rappresentata come un grafo.
Attraverso la manipolazione della moltiplicazione delle matrici Jacobiane, possiamo trasformare questo grafo nel grafo che calcolerà i gradienti andando all'indietro. (Nota che Pytorch e TensorFLow fanno questo in automatico per l'utente, i.e., il grafo in avanti è automaticamente "invertito" per creare il grafo derivato che retropropaga il gradiente.)


<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-1.png" alt="Gradient diagram" style="zoom:40%;" /></center>

In questo esempio, il grafo verde sulla destra rappresenta il grafo gradiente, Successivamente il grafo dal nodo più alto, che lo segue.

$$
\frac{\partial C(y,\bar{y})}{\partial w}=1 \cdot \frac{\partial C(y,\bar{y})}{\partial\bar{y}}\cdot\frac{\partial G(x,w)}{\partial w}
$$

In termini di dimensioni, $\frac{\partial C(y,\bar{y})}{\partial w}$ è un vettore riga di dimensione $1\times N$ dove $N$ è il numero di componenti di $w$; $\frac{\partial C(y,\bar{y})}{\partial \bar{y}}$  è un vettore riga di dimensione $1\times M$, dove $M$ è la dimensione dell'output; $\frac{\partial \bar{y}}{\partial w}=\frac{\partial G(x,w)}{\partial w}$ è una matrice $M\times N$, dove $M$ è il numero di output di $G$ e $N$ è la dimensione di $w$.

Nota che delle complicazioni potrebbero sorgere quando l'architettura del grafo non è fissa, ma dipende dai dati. Per esempio, potremmo fissare un modulo di una rete neurale che dipende dalla lunghezza del vettore in input. Anche se questo è possibile, diventa di difficoltà sempre superiore gestire questa variazione quando il numero dei cicli eccede una quantità ragionevole


### Moduli base di reti neurali

Esistono diversi dipi di moduli pre-costruiti oltre ai familiari moduli Linear e ReLU. Questi sono utili perché sono ottimizzati individualmente per eseguire le loro rispettive funzioni (invece di essere costruiti da una combinazione di altri moduli elementari)

- Linear: $Y=W\cdot X$

$$
\begin{aligned}
\frac{dC}{dX} &= W^\top \cdot \frac{dC}{dY} \\
\frac{dC}{dW} &= \frac{dC}{dY} \cdot X^\top
\end{aligned}
$$

- ReLU: $y=(x)^+$

  $$
  \frac{dC}{dX} =
      \begin{cases}
        0 & x<0\\
        \frac{dC}{dY} & \text{altrimenti}
      \end{cases}
  $$

- Duplicate: $Y_1=X$, $Y_2=X$

  - Simile a "Y - splitter" dove entrambi gli output sono uguali all'input.

  - Quando si retropropaga, i gradienti sono sommati

  - Può essere diviso in n rami allo stesso modo

    $$
    \frac{dC}{dX}=\frac{dC}{dY_1}+\frac{dC}{dY_2}
    $$


- Somma: $Y=X_1+X_2$

  - Con due variabili sommate, quando una è perturpata, L'output sarà perturbato dalla stessa quantità, i.e.,

    $$
    \frac{dC}{dX_1}=\frac{dC}{dY}\cdot1 \quad \text{e}\quad \frac{dC}{dX_2}=\frac{dC}{dY}\cdot1
    $$


- Massimo: $Y=\max(X_1,X_2)$

  -  Dal momento che questa funzione può essere rappresentata come

$$
Y=\max(X_1,X_2)=\begin{cases}
      X_1 & X_1 > X_2 \\
      X_2 & \text{altro}
   \end{cases}
\Rightarrow
\frac{dY}{dX_1}=\begin{cases}
      1 & X_1 > X_2 \\
      0 & \text{altro}
   \end{cases}
$$

 - - Perciò, dalla regola della catena,

$$
\frac{dC}{dX_1}=\begin{cases}
      \frac{dC}{dY}\cdot1 & X_1 > X_2 \\
      0 & \text{altro}
   \end{cases}
$$


## [LogSoftMax vs SoftMax](https://www.youtube.com/watch?v=d9vdh3b787Y&t=3953s)

*SoftMax*, che è anche un modulo Pytorch, è un modo conveniente di trasformare un gruppo di numeri in un gruppo di numeri positivi tra 0 e 1 che sommano a uno.
Questi numeri possono essere interpretati come una distribuzione di probabilità. Quindi come risultato, è comunemente utilizzata nei problemi di classificazione. $y_i$ nell'equazione sotto è un vettore di probabilità di tutte le categorie.

$$
y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$

Tuttavia, l'uso di softmax lascia la rete suscettibile ai gradienti evanescenti (vanishing gradients). I gradienti evanescenti sono un problema, impedisce ai pesi a valle di essere modificati dalla rete neurale, il che potrebbe fermare completamente la rete da ulteriori allenamenti.
La funzione logistica sigma (sigmoid), che è la funzione softmax per un valore, mostra che quando s è grande, $h(s)$ è 1, e quando s è piccolo, $h(s)$ è 0. Perché la funzione sigma è piatta a $h(s) = 0 $ e $h(s) = 1$, il gradiente è 0, che risulta in un gradiente evanescente.

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-2.png" alt="Sigmoid function to illustrate vanishing gradient" style="background-color:#DCDCDC;" /></center>

$$
h(s) = \frac{1}{1 + \exp(-s)}
$$

I matematici si son fatti venire l'idea della logsoftmax per risolvere il problema dei gradienti evanescenti creati dalla softmax. *LogSoftMax* è un altro modulo base in PyTorch. Come si può vedere nell'equazione sotto, *LogSoftMax* è una combinazione di softmax e log.

$$
\log(y_i )= \log\left(\frac{\exp(x_i)}{\Sigma_j \exp(x_j)}\right) = x_i - \log(\Sigma_j \exp(x_j)
$$

L'equazione sotto mostra un altro modo di vedere la stessa equazione. La figura sotto mostra $\log(1 + \exp(s))$ parte della funzione. Quando s è molto piccolo, il valore è 0, e quando s è molto grande, il valore è s. Come risultato non satura, e il problema del gradiente evanescente è evitato.

$$
\log\left(\frac{\exp(s)}{\exp(s) + 1}\right)= s - \log(1 + \exp(s))
$$

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-3.png" width='400px' alt="Plot of logarithmic part of the functions" /></center>


## [Trucchi pratici per la retropropagazione](https://www.youtube.com/watch?v=d9vdh3b787Y&t=4891s)


### Usare ReLU come funzione d'attivazione non lineare

ReLu funziona al meglio nelle reti con molti strati, il che ha causato alle alternative come la funzione sigma e alla tangente iperbolica $\tanh(\cdot)$ di cadere a suo favore. La ragione per cui la ReLu funziona meglio è dovuta alla singola peculiarità che fa si che scali in modo equivariante.


### Uso della perdità di entropia incrociata come funzione obiettivo per problemi di classificazione

Log softmax, come discusso nella lezione precedentemente, è un caso speciale di perdità di entropia-incrociata (cross-entropy loss). In PyTorch, bisogna assicursarsi di fornire una funzione di perdità di entropia-incrociata con *log* softmax come input (al contrario della normale softmax).


### Uso della discesa stocastica del gradiente come minibatch durante l'allenamento

Come discusso precedentemente, i minibatch permettono di allenare più efficientemente perché c'è ridondanza nei dati let; non si dovrebbe aver bisogno di fare predizioni e calcolare la perdita su ogni singola osservazione a ogni singolo step per stimare il gradiente.


### Mischiare l'ordine degli esempi per l'allenamento quando si usa una discesa stocastica del gradiente

L'ordine conta. Se il modello vede solo esempi di una singola classe durante ogni singolo step d'allenamento, allora imparerà a predirre la classe senza imparare perché dovrebbe essere predetta quella classe. Per esempio, se state provando a classificare le cifre dal dataset MNIST e i dati non sono ordinati, i parametri distorti nell'ultimo strato predirranno semplicemente sempre 0, quindi si adatta a predirre sempre 1, poi  2, ecc. Idealmente, dovresti avere campioni da ogni classe in ogni minibatch.

Tuttavia, c'è ancora un dibattito in atto sulla necessità di cambiare l'ordine dei campioni ad ogni passo (epoca).


### Normalizzare gli input per avere media zero e varianza unitaria
Prima dell'allenamento, è utile normalizzare ogni caratteristica in input, cosi avrà media 0 e la deviazione standard di 0. Quando si usano immagini RGB è comune prendere la media e la deviazione standard di ogni canale e normalizzare l'immagine riflettendo sui canali. Per esempio, si prende la media $m_b$ e la deviazione standard $\sigma_b$ di tutti i valori blu nel dataset, poi si normalizzano i valori blu di ogni singola immagine come .

$$
b_{[i,j]}^{'} = \frac{b_{[i,j]} - m_b}{\max(\sigma_b, \epsilon)}
$$

dove $\epsilon$ è un numero arbitrariarmente piccolo che si usa per evitare la divisione per 0. Si ripete lo stesso processo per i canali verde e rosso. Questo è necessario per avere un segnale significativo dalle immagini in diverse luci; Per esempio, le immagini illuminate di giorno hanno molto rosso, mentre le immagini sott'acqua quasi non ne hanno.


### Usa un programma per diminuire il tasso d'apprendimento

Il tasso d'apprendimento dovrebbe diminuire con l'avanzamento dell'allenamento. In pratica, i modelli più avanzati sono allenati usando algoritmi come Adam che adattano il tasso d'apprendimento invece del semplice SGD con un tasso d'apprendimento costante.



### Usa la regolarizzazione L1 e/o L2 per il decadimento del peso

Puoi aggiunere un costo per pesi grandi alla funzione costo. Per esempio, usando la regolarizzazione L2, noi vorremmo definire la perdita $L$ e aggiornare i pesi $w$ come segue:

$$
L(S, w) = C(S, w) + \alpha \Vert w \Vert^2\\
\frac{\partial R}{\partial w_i} = 2w_i\\
w_i = w_i - \eta\frac{\partial L}{\partial w_i} = w_i - \eta \left( \frac{\partial C}{\partial w_i} + 2 \alpha w_i \right)
$$

Per capire perché questo è chiamato decadimento del peso, nota che si può riscrive la formula sopra per mostrare come moltiplicando $w_i$ per una costante minore di uno durante l'aggiornamento

$$
w_i = (1 - 2 \eta \alpha) w_i - \eta\frac{\partial C}{\partial w_i}
$$

La regolarizzazione L1 (Lasso) è simile, salvo che usa $\sum_i \vert w_i\vert$ al posto di $\Vert w \Vert^2$.

Essenzialmente, la regolarizzaione prova a dire al sistema di minimizzare la funzione costo con minor vettore peso possibile. Con la regolarizzazione L1 i pesi che non sono utili vengono ridotti a 0.

### Inizializzazione dei pesi

I pesi devono essere inizializzato causalmente, tuttavia, non dovrebbero essere troppo grandi o troppo piccoli cosicché l'output abbia approssimativamente la stessa varianza dell'input.
Ci sono vari trucchi per inizializzare i pesi in PyTorch. Uno di questi trucchi che funziona bene per i modelli profondi (deep models) è l'inizializzazione Kaiming, dove la varianza dei pesi è inversamente proporzionale alla radice quadrata del numero di input.

### Usa il dropout

Il dropout è un altra forma di regolarizzazione. Può essere pensato come un altro strato della rete neurale: prende degli input, setta casualmente $n/2$ degli input a 0, restituisce il risultato come output. Questo obbliga il sistema a prendere informazione da tutte le unità in input, piuttosto che fare troppo affidamento a un numero piccolo di input, cosi si distrubuisce l'informazione tra tutte le unità di uno strato. Questo metodo è stato inizialmente proposto da <a href="https://arxiv.org/abs/1207.0580">Hinton et al (2012)</a>.

Per altri trucchi, guarda  <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">LeCun et al 1998</a>.

Infine, nota che la retropropagazione non funziona solo per i modelli impilati; può funzionare per ogni grafo diretto aciclico (DAG) finché vi è un ordine parziale dei moduli.


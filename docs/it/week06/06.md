---
lang-ref: ch.06
lang: it
title: Settimana 6
translation-date: 21 Apr 2020
translator: fg746
---

## Lezione parte A

Discutiamo tre applicazioni delle reti neurali convoluzionali. Cominciamo con il riconoscimento di cifre e l'applicazione al riconoscimento di codici postali composti da 5 cifre. Nell'ambito della rilevazione degli oggetti, parliamo dell'utilizzo di architetture multi-scala per il riconoscimento di facce. Infine vediamo come le CNN vengono utilizzate per la segmentazione semantica, mostrando l'esempio di un sistema di visione robotica che fa segmentazione degli oggetti in un contesto urbano.

<!-- ## Lecture part A

We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment.
-->

## Lezione parte B

Esaminiamo le reti neurali ricorrenti (_Recurrent Neural Networks, RNN_), le problematiche che presentano e le tecniche comunemente utilizzate per mitigarle. Revisiamo vari moduli che sono stati sviluppati per risolvere le difficoltà delle RNN, fra cui l'attenzione, le _GRU_ (_Gated Recurrent Unit_, unità ricorrenti con _gate_), le reti _LSTM_ (_Long Short-Term Memory_ memoria a breve-termine lunga), e _Seq2Seq_ (_Sequence to Sequence_, da sequenza a sequenza).

<!-- ## Lecture part B

We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq.
-->

## Pratica

Discutiamo le architetture più semplici di _RNN_ e _LSTM_ e paragoniamo la _performance_ fra le due. Le LSTM mantengono alcuni vantaggi delle RNN e risolvono allo stesso tempo alcune loro debolezze perché incorporano una "cella di memoria" (_memory cell_) che conserva l'informazione a lungo. Le _LSTM_ sono nettamente superiori nella performnace rispetto alle _RNN_.

<!-- ## Practicum
We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models.
-->

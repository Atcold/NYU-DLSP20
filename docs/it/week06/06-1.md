---
lang-ref: ch.06-1
lang: it
title: Applicazioni di Reti Convoluzionali
authors: Shiqing Li, Chenqin Yang, Yakun Wang, Jimin Tan
date: 4 Mar 2020
translation-date: 06 May 2020
translator: Alessio Salman
---

## [Riconoscimento di codici ZIP](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=43s)

// TODO: Majority vote 

Nella lezione precedente, abbiamo dimostrato come una rete convoluzionale (ConvNet o CNN, Convolutional Neural Network) sia in grado di riconoscere le cifre, tuttavia una questione rimane aperta: come fa il modello a selezionare ciascuna cifra ed evitare perturbazioni sulle cifre vicine. Il passaggio successivo consiste nel rilevare oggetti non sovrapposti e utilizzare l'approccio generale della soppressione dei non-massimi (in inglese, NMS). Ora, assumendo che l'input sia una serie di cifre non sovrapposte, la strategia è quella di addestrare diverse reti convoluzionali ed utilizzare il voto di maggioranza o scegliere le cifre corrispondenti al punteggio più alto generato dalla rete convoluzionale.


### Riconoscimento mediante CNN // TODO: chiedere dubbio

Qui presentiamo il compito di riconoscere 5 codici postali non sovrapposti. Al sistema non sono state fornite istruzioni su come separare ogni cifra ma sa che è necessario prevedere 5 cifre. Il sistema (Figura 1) è costituito da 4 reti convoluzionali di dimensioni diverse, ciascuna delle quali produce un insieme di output. L'output è rappresentato in matrici. Le quattro matrici di output provengono da modelli con una larghezza del kernel diversa nell'ultimo livello. In ogni output ci sono 10 righe che rappresentano le 10 categorie, da 0 a 9. Il quadrato bianco più grande rappresenta un punteggio più alto in quella categoria. In questi quattro blocchi di output, le dimensioni orizzontali del kernel degli ultimi layer sono rispettivamente 5, 4, 3 e 2. La dimensione del kernel decide la larghezza della finestra di visualizzazione del modello sull'input, pertanto ogni modello prevede cifre basate su dimensioni della finestra diverse. Il modello quindi prende il voto di maggioranza e seleziona la categoria che corrisponde al punteggio più alto in quella finestra. Per estrarre informazioni utili, è necessario tenere presente che non tutte le combinazioni di caratteri sono possibili, pertanto correggere gli errori facendo leva sulle restrizioni sull'input è utile per garantire che gli output siano veri codici postali.

<center>
<img src="{{site.baseurl}}/images/week06/06-1/O1IN3JD.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<b>Figura 1:</b> Diversi classificatori su riconoscimento di codici zip
</center>

Ora bisogna imporre l'ordine dei caratteri. Il trucco è utilizzare un algoritmo di cammino minimo. Poiché ci vengono forniti intervalli di caratteri possibili e il numero totale di cifre da prevedere, possiamo affrontare questo problema calcolando il costo minimo di generazione delle cifre e le transizioni tra le cifre. Il percorso deve essere continuo dalla cella in basso a sinistra sul grafo alla cella in alto a destra, e può contenere solo i movimenti da sinistra a destra e dal basso verso l'alto. Si noti che se lo stesso numero viene ripetuto accanto all'altro, l'algoritmo dovrebbe essere in grado di distinguere che è in presenza di due numeri ripetuti invece di prevedere una singola cifra.

## [Riconoscimento facciale](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=1241s)

Le reti neurali convoluzionali svolgono bene le attività di rilevamento ed anche il rilevamento dei volti non fa eccezione. Per eseguire il rilevamento facciale, raccogliamo un set di dati di immagini con volti e senza volti, su cui addestriamo una rete convoluzionale con una dimensione della finestra di circa 30 $\times$ 30 pixel e la interroghiamo chiedendo se c'è un volto o meno. Una volta addestrata, applichiamo il modello a una nuova immagine e se ci sono facce approssimativamente all'interno di una finestra di 30 $\times$ 30 pixel, la rete convoluzionale illuminerà l'output nelle posizioni corrispondenti. Esistono tuttavia due problemi. 


- **Falsi positivi**: ci sono molti modi in cui una patch di un'immagine non rappresenti un volto. Durante la fase di addestramento, il modello potrebbe non vederli tutti (ovvero un set non completamente rappresentativo). Pertanto, il modello potrebbe soffrire di molti falsi positivi al momento del test.

- **Diverse dimensioni del viso:** Non tutti i volti sono 30 $\times$ 30 pixel, quindi i volti di dimensioni diverse potrebbero non essere rilevati. Un modo per gestire questo problema è generare più versioni della stessa immagine su divrse scale. Il rilevatore originale rileverà volti di circa 30 $\times$ 30 pixel. Se si riscala l'immagine di un fattore $\sqrt 2$, il modello rileverà volti più piccoli nell'immagine originale poiché ciò che era 30 $\times$ 30 è ora diventato 20 $\times$ 20 pixel all'incirca. Per rilevare volti più grandi, possiamo rimpicciolire l'immagine. Questo processo è economico poiché metà del costo deriva dall'elaborazione dell'immagine originale non in scala. La somma dei costi di tutte le altre reti combinate è quasi uguale all'elaborazione dell'immagine originale non ridimensionata. La dimensione della rete è il quadrato della dimensione dell'immagine su un lato, quindi se riduci l'immagine di $\sqrt 2$, la rete che devi eseguire è più piccola di un fattore 2. Quindi il costo complessivo è $1+1/2+1/4+1/8+1/16…$, che è 2. Da cui segue che il costo d'esecuzione di un modello multiscala è solamente il doppio di quello normale. 

### Un sistema di rilevamento facciale multiscala

<center>
<img src="{{site.baseurl}}/images/week06/06-1/8R3v0Dj.png" style="zoom: 30%; background-color:#DCDCDC;"/><br>
<b>Figura 2:</b> Sistema di rilevamento facciale
</center>

Le mappe mostrate in (Figura 3) indicano i punteggi dei rilevatori di volti. Questo rilevatore di volti riconosce volti di dimensioni 20 $\times $ 20 pixel. In scala fine (Scala 3) ci sono molti punteggi alti ma non sono molto definiti. Quando il fattore di ridimensionamento aumenta (Scala 6), vediamo più aree bianche raggruppate. Quelle regioni bianche rappresentano i volti rilevati. Quindi applichiamo la soppressione dei non-massimi per ottenere la posizione finale del viso.

<center>
<img src="{{site.baseurl}}/images/week06/06-1/CQ8T00O.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<b>Figura 3:</b> Punteggi dei rilevatori di volti per diversi fattori di scala
</center>


// TODO: bounding box 
### Soppressione dei non-massimi

In ogni regione con punteggio elevato vi è probabilmente sotto una faccia. Se vengono rilevati più volti molto vicini al primo, significa che solo uno dovrebbe essere considerato corretto e il resto sbagliato. Con la soppression dei non-massimi, prendiamo il punteggio più alto tra le bounding boxes sovrapposte e rimuoviamo gli altri. Il risultato sarà una unica bounding box nella posizione ottimale.


### Estrazione negativa

Nell'ultima sezione, abbiamo discusso di come il modello si imbatterà in un gran numero di falsi positivi al momento del test poiché ci sono molti modi per far apparire un oggetto simile a una faccia. Nessun set di addestramento includerà tutti gli oggetti che sembrano volti ma non lo sono. Possiamo mitigare questo problema attraverso il mining negativo. Nel mining negativo, creiamo un set di dati negativo di patch che non contengono volti che il modello rileva erroneamente come facce. I dati vengono raccolti eseguendo il modello su input noti per non contenere volti. Quindi addestriamo nuovamente il rivelatore utilizzando il set di dati negativi. Possiamo ripetere questo processo per aumentare la solidità del nostro modello contro i falsi positivi.

# Segmentazione semantica

La segmentazione semantica è il compito di assegnare una categoria a ogni pixel in un'immagine di input.


### [CNN per visione robotica adattiva a lungo raggio] (https://www.youtube.com/watch?v=ycbMGyCPzvE&t=1669s)

In questo progetto, l'obiettivo era etichettare le regioni delle immagini di input in modo che un robot potesse distinguere tra strade e ostacoli. Nella figura, le regioni verdi sono aree su cui il robot può guidare e le regioni rosse sono ostacoli, come ad esempio l'erba alta. Per addestrare la rete per questa attività, abbiamo preso una patch dall'immagine ed l'abbiamo manualmente attraversabile o meno (verde o rosso). Addestriamo quindi la rete convoluzionale sulle patch chiedendole di prevedere il colore della patch. Una volta che il sistema è sufficientemente addestrato, viene applicato all'intera immagine, etichettando tutte le regioni dell'immagine come verdi o rosse.

<center>
<img src="{{site.baseurl}}/images/week06/06-1/5mM7dTT.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<b>Figura 4:</b>CNN per visione robotica adattiva a lungo raggio (programma DARPA LAGR 2005-2008)
</center>

Esistono cinque categorie da classificare: 1) super verde, 2) verde, 3) viola: linea del piede ostacolo, 4) rosso: ostacolo 5) super rosso: sicuramente un ostacolo.


**Etichette stereo** (Figura 4, Colonna 2)
 Le immagini vengono catturate dalle 4 telecamere sul robot e poi raggruppate in 2 coppie di visione stereo. 
 
Utilizzando le distanze note tra le telecamere della coppia stereo possiamo stimare le posizioni di ogni pixel nello spazio 3D; 

 Utilizzando le distanze note tra le telecamere della coppia stereo, le posizioni di ogni pixel nello spazio 3D vengono quindi stimate misurando le distanze relative tra i pixel che appaiono in entrambe le telecamere in una coppia stereo. 
 
 Questo è lo stesso processo che i nostri cervelli usano per stimare la distanza degli oggetti che vediamo. Utilizzando le informazioni sulla posizione stimata, si esegue il fitting di una superficie piana al terreno ed i pixel vengono quindi etichettati come verdi se si trovano vicino al suolo e rossi se si trovano sopra di esso.

* **Limitazioni e motivazione per ConvNet**: la visione stereo funziona solo fino a 10 metri e la guida di un robot richiede una visione a lungo raggio. Una ConvNet, tuttavia, è in grado di rilevare oggetti a distanze molto maggiori se addestrata correttamente.

<center>
<img src="{{site.baseurl}}/images/week06/06-1/rcxY4Lb.png" style="zoom: 100%; background-color:#DCDCDC;"/><br>
<b>Figura 5:</b> Piramide invariante per scala di immagini normalizzate sulla distanza</center>

* **Usata come input del modello**: un'importante pre-elaborazione include la costruzione di una piramide scala invariante di immagini normalizzate sulla distanza (Figura 5). È simile a quello che abbiamo fatto prima di questa lezione quando abbiamo cercato di rilevare i volti di più dimensioni.

**Output del modello** (Figura 4, colonna 3)

Il modello genera un'etichetta per ogni pixel dell'immagine **fino all'orizzonte**. Queste sono gli output di classificazione di una rete convoluzionale multiscala.

* **Come il modello diventa adattivo**: i robot hanno accesso continuo alle etichette stereo, permettendo alla rete di riaddestrarsi, adattandosi al nuovo ambiente in cui si trova. Si noti che solo l'ultimo strato della rete sarebbe riaddestrato. I livelli precedenti vengono addestrati in laboratorio e mantenuti costanti.

**Prestazione del sistema**

Quando cercava di raggiungere una coordinata GPS dall'altra parte di una barriera, il robot "vedeva" la barriera da molto lontano e pianificava un percorso che la evitava. Questo grazie alla CNN che rileva oggetti a 50-100m di distanza.

**Limitazione**

Negli anni 2000, le risorse di calcolo erano limitate. Il robot era in grado di elaborare circa 1 frame al secondo, il che significa che non sarebbe stato in grado di rilevare una persona che lo incrociava per un intero secondo prima di essere in grado di reagire. La soluzione per questa limitazione è un modello **di odometria visiva a basso costo**. Non si basa su reti neurali, ha una visione di ~2,5m ma reagisce rapidamente.

### Analisi ed etichettatura delle scene

In questa attività, il modello genera una categoria di oggetti (edifici, automobili, cielo, ecc.) per ogni pixel. L'architettura è anche multi-scala (Figura 6).

<center>
<img src="{{site.baseurl}}/images/week06/06-1/VpVbkl5.jpg" style="zoom: 30%; background-color:#DCDCDC;"/><br>
<b>Figura 6:</b> CNN multi-scala per l'analisi di scene
</center>

Si noti che se si proietta indietro un output della CNN sull'input, corrisponde a una finestra di input di dimensioni $46 \ times 46$ sull'immagine originale nella parte inferiore della Piramide Laplaciana. Significa che **stiamo usando il contesto di $46 \times 46$ pixel per decidere la categoria del pixel centrale**.

Tuttavia, a volte questa dimensione del contesto non è sufficiente per determinare la categoria di oggetti più grandi.

**L'approccio multiscala consente una visione più ampia fornendo ulteriori immagini ridimensionate come input.** I passaggi sono i seguenti:

1. Prendi la stessa immagine, riducila di un fattore 2 e di un fattore 4, separatamente.
2. Queste due immagini extra che sono state ridimensionate vengono inviate alla stessa **ConvNet** (stessi pesi, stessi kernel) e otteniamo altri due set di features di Livello 2.
3. **Upsample** di queste features in modo che abbiano le stesse dimensioni delle features di livello 2 dell'immagine originale.
4. **Impila** le tre serie di features (ricampionate) insieme e dalle in input ad un classificatore.


Ora la più grande dimensione effettiva del contenuto, che deriva questodall'immagine 1/4 ridimensionata, è $184 \times 184\, (46 \times 4 = 184)$.

**Prestazioni**: senza post-elaborazione ed eseguendolo frame-by-frame, il modello funziona molto velocemente anche su hardware standard. Ha una dimensione piuttosto piccola di dati di allenamento (2k~3k), ma i risultati sono ancora da record.
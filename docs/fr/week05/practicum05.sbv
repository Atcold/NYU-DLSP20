0:00:00.000,0:00:05.339
La dernière fois que nous avons vu qu'une matrice peut être écrite… laissez-moi la dessiner

0:00:05.339,0:00:12.719
Donc nous avions plusieurs lignes puis nous avons multiplié ça par une colonne.

0:00:12.719,0:00:18.210
Donc chaque fois que nous multiplions ces types, vous pouvez voir ces

0:00:18.210,0:00:23.340
deux types différents de représentation équivalente. Vous pouvez-voir ?

0:00:23.340,0:00:28.980
C’est lisible ? [Etudiant] Donc vous pouvez voir que le résultat

0:00:28.980,0:00:35.430
de ce produit est une séquence de… comme la première ligne fois ce vecteur

0:00:35.430,0:00:40.469
colonne et puis encore une fois… je les rétrécis… cela devrait être

0:00:40.469,0:00:46.170
de la même taille car sinon vous ne pouvez pas les multiplier.

0:00:46.170,0:00:52.170
Donc vous avez ça et ainsi de suite jusqu'au dernier. Cela est mon vecteur final.

0:00:52.170,0:01:00.960
Nous avons vu que chacun de ces trucs ici… Que sont-ils ?

0:01:00.960,0:01:05.339
Parlez moi s'il vous plaît. Il y a un produit scalaire mais que

0:01:05.339,0:01:08.820
représentent-ils ? Qu’est ce que c’est ? Comment pouvons-nous appeler ça ? L’autre nom pour appeler

0:01:08.820,0:01:13.290
un produit scalaire. Je vous ai montré la dernière fois une démonstration 

0:01:13.290,0:01:18.119
avec de la trigonométrie. Qu'est-ce que c'est ? Ce sont donc toutes les projections, si vous

0:01:18.119,0:01:22.619
parler de géométrie, ou vous pouvez penser à cela comme la valeur d’un cosinus 

0:01:22.619,0:01:29.310
non normalisé. Donc celui-ci est ma projection d'un noyau ou

0:01:29.310,0:01:36.030
mon signal d'entrée sur le noyau. Donc ce sont des projections. Projection.

0:01:36.030,0:01:40.619
Il y a aussi une autre interprétation de ceci, une autre façon

0:01:40.619,0:01:45.390
de voir les choses, qui est quoi ? En gros nous avons la première colonne

0:01:45.390,0:01:53.579
de la matrice A multipliée par le premier élément de X, de ce vecteur.

0:01:53.579,0:01:58.260
Elément numéro un. Vous avez la deuxième colonne fois

0:01:58.260,0:02:04.020
le deuxième élément du vecteur X jusqu'à ce que vous arriviez à la dernière colonne

0:02:04.020,0:02:11.100
fois le derniers élément. Supposé que ce soit de longueur n. Et ceci est

0:02:11.100,0:02:16.110
m fois n. La hauteur est la dimension vers nous tirons

0:02:16.110,0:02:19.550
et la largeur d'une matrice est la dimension d'où nous venons.

0:02:19.550,0:02:24.810
La deuxième partie est la suivante. Nous avons dit qu'au lieu d'utiliser cette matrice ici

0:02:24.810,0:02:29.450
puisque nous faisons des convolutions car nous voulons exploiter l’éparsité,

0:02:29.450,0:02:35.400
la stationnarité et la compositionnalité nous utilisons toujours la même matrice ici.

0:02:35.400,0:02:41.370
Nous utilisons le même type ici, mais ensuite ces noyaux nous allons les

0:02:41.370,0:02:45.510
utiliser encore et encore : le même noyau sur l'ensemble du signal.

0:02:45.510,0:02:51.360
Dans ce cas, la largeur de cette matrice n'est plus n comme c'était le cas ici.

0:02:51.360,0:02:56.820
Cela va être k, la taille du noyau.

0:02:56.820,0:03:03.090
Donc ici je dessine ma matrice plus fine avec k en bas

0:03:03.090,0:03:10.140
et la hauteur, on peut peut-être encore l'appeler m. Donc disons ici que

0:03:10.140,0:03:18.230
j’ai plusieurs noyaux. Par exemple, un noyau cyan, puis

0:03:18.230,0:03:25.080
un autre vert… non mettons du rose au lieu du vert.

0:03:25.080,0:03:33.180
Puis nous pouvons avoir le vert et ainsi de suite. Donc comment utiliser ces noyaux ?

0:03:33.180,0:03:38.280
Nous les utilisons basiquement en les empilant et en les déplaçant un peu.

0:03:38.280,0:03:43.650
Donc nous pouvons sortir le premier noyau et ensuite vous allez avoir

0:03:43.650,0:03:50.519
en gros… vous obtenez le premier gars ici puis vous le déplacez

0:03:50.519,0:03:58.290
jusqu'à ce que vous obteniez la matrice complète. Et nous mettons un 0

0:03:58.290,0:04:02.100
ici et un 0 ici. C'est juste un récapitulatif. Vous avez celui-là pour la

0:04:02.100,0:04:11.379
couleur bleue. Maintenant vous faites de la magie : un copier-coller.

0:04:11.379,0:04:19.370
Vous pouvez aussi faire la couleur. De la magie, fantastique. Nous avons le 

0:04:19.370,0:04:25.360
rose et la même chose pour le dernier : copie. Fantastique.

0:04:25.360,0:04:29.080
Vous ne pouvez pas faire de copier-coller sur papier.

0:04:29.080,0:04:38.419
Donc couleur, vert clair. Ok. Donc on a juste dupliqué.

0:04:38.419,0:04:44.479
Combien de matrices avons-nous maintenant ? Combien de couches ? Ne comptez pas,

0:04:44.479,0:04:50.600
il y a des lettres sur l'écran. [Etudiant] k ou m ? Qu’est-ce que k ? 

0:04:50.600,0:05:00.620
[Etudiant]. Vous devinez. Vous ne devriez pas deviner, vous devriez me

0:05:00.620,0:05:07.120
donner la bonne réponse. Pensez à ça comme un entretien d'embauche. Je vous entraîne.

0:05:07.120,0:05:14.990
Donc combien de cartes nous avons ? [Etudiant] m. Donc celle-ci est de longueur m qui

0:05:14.990,0:05:21.470
est le nombre de lignes de cette chose ici.

0:05:21.470,0:05:30.289
Quelle est la largeur de ce petit noyau ici ? [Etudiant] Ok k. 

0:05:30.289,0:05:41.349
Quelle est la hauteur de cette matrice ? [Etudiant]

0:05:42.340,0:05:45.480
Vous êtes sûr ?  [Etudiant] Réessayez. [Etudiant]

0:05:49.220,0:06:04.310
Je ne peux pas entendre. [Etudiant] n-k+1, ok. Quelle est la sortie de cette chose ?

0:06:04.310,0:06:18.660
La sortie sera donc un vecteur de la même hauteur : n-k+1…. Oui c’est correct. 

0:06:19.430,0:06:27.890
Mais quelle est l'épaisseur de ce vecteur final ? [Etudiant] m, juste.

0:06:27.890,0:06:34.600
Ce truc est aussi épais que m. C'est là où nous nous sommes arrêtés la dernière fois mais ensuite

0:06:34.600,0:06:40.770
quelqu'un m'a demandé… Ici nous avons autant de colonnes que de couleurs différentes.

0:06:40.770,0:06:45.170
Par exemple dans ce cas, si je dessine pour m'assurer que vous comprenez,

0:06:45.170,0:06:53.730
vous avez ça ici, puis la deuxième ici, et la troisième là.

0:06:53.730,0:07:03.650
Alors la dernière fois, quelqu'un m’a demandé à la fin du cours, comment faire la convolution

0:07:03.650,0:07:09.760
lorsque nous nous retrouvons dans cette situation ? Car ici nous supposons que

0:07:09.760,0:07:14.990
mes noyaux sont de longueur, disons k=3, mais ensuite ils ne sont

0:07:14.990,0:07:21.380
qu'un petit vecteur. Et donc quelqu'un m'a dit : « que faites-vous d’ici ?

0:07:21.380,0:07:24.950
Comment continuer ? » Car maintenant nous avons une épaisseur alors

0:07:24.950,0:07:32.510
qu’avant nous avons commencé avec ce vecteur qui avait juste n éléments. 

0:07:32.510,0:07:35.690
Vous suivez jusqu'à présent ? Je vais plus vite car nous avons déjà vu ces

0:07:35.690,0:07:44.030
choses que je suis en train de revoir mais êtes-vous avec moi jusqu'à présent ? Oui ? Non ? Oui. Ok.

0:07:44.030,0:07:47.720
Fantastique. Donc voyons comment nous continuons.

0:07:47.720,0:07:51.680
Ce que je viens de vous montrer, c'est que nous supposons commencer avec ce long vecteur

0:07:51.680,0:08:01.400
qui était de hauteur… Quelle était la hauteur ? [Etudiant] n, ok. Mais dans ce cas, cela signifie

0:08:01.400,0:08:13.060
aussi que nous avons quelque chose qui ressemble à ça. Donc vous avez ça et

0:08:13.060,0:08:20.720
ici c'est 1 et là aussi 1. Donc nous n'avons qu'un signal monophonique par exemple.

0:08:20.720,0:08:26.300
Et la hauteur est n. Donc supposons que maintenant nous utilisons un

0:08:26.300,0:08:33.950
système stéréophonique. Que va être mon domaine ici ? Mon x peut être

0:08:33.950,0:08:39.740
pensé comme une fonction qui va du domaine à ℝ^{nombre de canaux}.

0:08:39.740,0:08:47.840
Qu'est-ce que ce type-là ? [Etudiant] x est une dimension et quelque part, qu'est-ce qu’Ω ?

0:08:47.840,0:08:59.930
Nous avons vu cette diapositive à la fin du cours de mardi. [Etudiant] Répétez. [Etudiant] Ω n’est

0:08:59.930,0:09:11.720
pas un ensemble de chiffres réels. Les autres, essayez. Nous utilisons des ordinateurs. [Etudiant]

0:09:11.720,0:09:16.520
C’est chronologique oui. Combien d'échantillons ? Vous avez échantillon numéro un, échantillon numéro deux,

0:09:16.520,0:09:21.710
ou l'échantillon numéro trois, donc vous avez un sous-ensemble de l'espace naturel.

0:09:21.710,0:09:30.860
Donc celui-ci va être quelque chose comme 0, 1, 2, etc… Un ensemble qui est 

0:09:30.860,0:09:36.410
un sous-ensemble de ℕ. Donc ce n'est pas ℝ. ℝ va être si vous avez un domaine

0:09:36.410,0:09:45.850
de temps continu. Qu’est c dans le cas que je viens de vous montrer ? [Etudiant]

0:09:45.850,0:09:55.160
Qu’est c dans ce cas ? [Etudiant] Le nombre de ? [Etudiant] Le nombre de canaux d'entrée

0:09:55.160,0:10:00.740
car c'est mon x. C’est mon entrée. Donc dans ce cas présent ici, nous n’en

0:10:00.740,0:10:07.220
utilisions qu'un, ce qui signifie que nous avons un son monophonique.

0:10:07.220,0:10:22.780
Faisons maintenant l'hypothèse que ce type vaut 2 et donc que nous parlons de signal stéréophonique.

0:10:23.200,0:10:27.380
Donc voyons comment ce truc change.

0:10:27.380,0:10:38.450
Dans ce cas… laissez-moi réfléchir… oui alors comment dessiner ? Je vais juste dessiner.

0:10:38.450,0:10:43.400
Plaignez-vous si vous ne suivez pas. Vous suivez jusqu'à présent ? 

0:10:43.400,0:10:46.550
Car si je regarde ma tablette je ne vous vois pas. Donc plaignez-vous si

0:10:46.550,0:10:50.750
quelque chose n'a pas de sens. Sinon cela devient ennuyeux d’attendre

0:10:50.750,0:10:56.690
puis vous regardez tout le temps. Oui ? Non ? Ok je suis ennuyeux. Merci.

0:10:56.690,0:11:00.080
Bien, nous avons donc ici ce signal

0:11:00.080,0:11:07.280
et maintenant nous avons une certaine épaisseur. Dans ce cas, qu'est-ce que 

0:11:07.280,0:11:14.660
l'épaisseur de ce type ? [Etudiant] C, ok. Donc dans ce cas, celui-ci est c 

0:11:14.660,0:11:18.589
et dans le cas du signal stéréophonique il y a deux canaux : gauche et droit.

0:11:18.589,0:11:30.170
Et ça continue à descendre. Donc nos noyaux… si je veux

0:11:30.170,0:11:35.030
effectuer une convolution sur ce signal, donc de vous avez

0:11:35.030,0:11:44.150
différents échantillons. Et ainsi de suite. Si je veux faire une

0:11:44.150,0:11:47.089
Convolution 1D, je ne parle pas de convolution 2D, car elles utilisent

0:11:47.089,0:11:52.670
toujours le domaine qui est ici 1. Donc c'est important.

0:11:52.670,0:11:58.510
Donc si je vous demande quel type de signal c'est, vous devez

0:11:58.510,0:12:02.890
essentiellement regarder ce numéro ici. Donc nous parlons d'un

0:12:02.890,0:12:12.490
signal unidimensionnel qui est un domaine unidimensionnel : domaine 1D.

0:12:12.490,0:12:17.710
Nous utilisons donc toujours un signal 1D, mais dans ce cas, vous avez deux

0:12:17.710,0:12:25.750
valeurs par point. Donc quel type de noyaux allons-nous utiliser ? Je vais 

0:12:25.750,0:12:31.450
le dessiner. Dans ce cas, nous allons utiliser quelque chose de similaire., 

0:12:31.450,0:12:37.990
Donc je dessine ce type. Disons que j'ai k ici, qui va être ma largeur

0:12:37.990,0:12:42.700
du noyau mais dans ce cas, je vais aussi avoir une certaine épaisseur. 

0:12:42.700,0:12:56.230
Donc en gros, vous appliquez ce truc ici, ok. Ensuite vous pouvez aller à 

0:12:56.230,0:13:04.060
la deuxième ligne, la troisième ligne et ainsi de suite. Vous pouvez encore avoir ici m noyaux.

0:13:04.060,0:13:11.590
Mais dans ce cas, vous avez également une certaine épaisseur qui doit correspondre

0:13:11.590,0:13:17.680
l’autre épaisseur. Donc cette épaisseur doit ici correspondre à l'épaisseur 

0:13:17.680,0:13:23.980
de la taille d'entrée. Laissez-moi vous montrer comment appliquer la convolution.

0:13:23.980,0:13:37.980
Vous obtenez une de ces tranches ici puis ensuite vous allez appliquer ceci ici.

0:13:39.320,0:13:46.190
Ensuite vous descendez simplement de cette façon.

0:13:46.190,0:13:53.870
Donc chaque fois que vous les appliquez, vous réalisez ce type, le produit intérieur

0:13:53.870,0:14:04.410
avec ces derniers, qu’obtenez-vous ? C'est en fait un par un, c’est un scalaire.

0:14:04.410,0:14:09.540
Chaque fois que j'utilise ce truc orange ici à gauche et que je fais un 

0:14:09.540,0:14:14.190
produit scalaire avec celui-ci, j’obtiens juste un scalaire. Donc c'est en fait ma

0:14:14.190,0:14:19.620
convolution en 1D. Convolution 1D signifie que ça descend de cette façon et

0:14:19.620,0:14:27.480
seulement d'une manière, c'est pourquoi on l'appelle 1D. Mais nous multiplions chaque élément de ce

0:14:27.480,0:14:36.290
masque fois ce gars ici. Maintenant une deuxième rangée et ce gars ici.

0:14:36.290,0:14:41.090
Vous les multipliez tous, vous les additionnez tous et vous obtenez votre première sortie.

0:14:41.090,0:14:47.250
Chaque fois que je fais cette multiplication, j'obtiens ma première sortie.

0:14:47.250,0:14:52.050
Je continue à faire glisser ce noyau vers le bas et vous obtenez la 

0:14:52.050,0:14:58.380
deuxième sortie, troisième, quatrième et ainsi de suite jusqu'à ce que vous descendiez à la fin. 

0:14:58.380,0:15:03.780
Ensuite qu’arrive-t-il ? Je vais ramasser un noyau différent. 

0:15:03.780,0:15:07.950
Je vais…  disons que j'obtiens le deuxième.

0:15:07.950,0:15:23.050
J’effectue la même opération. Et j’obtiens ça. Mettons ça sous la forme d’une matrice.

0:15:26.940,0:15:33.790
Vous descendez jusqu'au dernier, qui sera le n-ième.

0:15:33.790,0:15:45.450
Le n-ième noyau. Cela descend de cette façon et vous obtenez le dernier ici.

0:15:51.680,0:15:58.790
Ok ? Oui ? Non ? Clarification confuse ? Donc c'est la question que j'ai reçue à la fin du dernier cours.

0:15:58.790,0:16:10.339
[Etudiant] Car c'est un produit scalaire de toutes ces valeurs entre…

0:16:10.339,0:16:18.259
Essentiellement cela fait la projection de cette partie du signal sur ce noyau.

0:16:18.259,0:16:22.879
Vous aimeriez voir quelle est la contribution comme quel est l'alignement de cette partie

0:16:22.879,0:16:27.850
du signal sur ce sous-espace spécifique. C'est ainsi qu'une convolution fonctionne.

0:16:27.850,0:16:31.850
Lorsque vous avez plusieurs canaux… Jusqu'à présent, je vous ai montré avec un seul canal,

0:16:31.850,0:16:53.319
maintenant nous avons plusieurs canaux. [Etudiant] Oui dans une seconde. [Etudiant]

0:16:54.259,0:16:59.509
Un en haut et un autre en bas, ce qui fait que vous perdez la première ligne ici

0:16:59.509,0:17:04.850
et vous perdez la dernière ligne ici. Donc à la fin, dans ce cas, la sortie 

0:17:04.850,0:17:10.490
est n-3+1. Donc vous perdez 2. Dans ce cas vous

0:17:10.490,0:17:15.140
perdez deux en bas. Si vous faites effectivement au centre de la convolution,

0:17:15.140,0:17:20.390
vous en perdez généralement 1 au début et 1 à la fin. Chaque fois que vous

0:17:20.390,0:17:24.409
effectuez une convolution, vous perdez le numéro de la dimension du noyau

0:17:24.409,0:17:28.789
moins un. Vous pouvez essayer. Si vous mettez votre main comme ceci et avez un noyau de 3,

0:17:28.789,0:17:34.340
vous obtenez le premier ici, cela correspond, puis vous échangez un puis 

0:17:34.340,0:17:39.440
vous échangez deux. Bon disons un noyau de deux. Donc vous avec un signal

0:17:39.440,0:17:44.149
de cinq et un noyau de deux. Vous avez, un, deux, trois et quatre.

0:17:44.149,0:17:49.070
Donc on a commencé avec cinq et vous finissez avec quatre car vous utilisez 

0:17:49.070,0:17:54.500
un noyau de taille deux. Si vous utilisez un noyau de taille trois, vous avez un, deux et trois.

0:17:54.500,0:17:57.289
Donc vous perdez deux. Si vous utilisez un noyau de taille trois.

0:17:57.289,0:18:01.010
Donc vous pouvez essayer de faire ça. Donc je vais vous montrer maintenant 

0:18:01.010,0:18:07.040
les dimensions de ces noyaux et les sorties avec PyTorch.

0:18:07.040,0:18:18.500
Pouvez-vous voir quelque chose ?

0:18:18.500,0:18:25.520
Je zoome un peu plus. Ok donc maintenant on fait

0:18:25.520,0:18:33.770
conda activate pDL, pytorch Deep Learning.

0:18:33.770,0:18:40.520
Donc ici nous pouvons lancer ipython. Avec Ctrl L, je nettoie l’écran.

0:18:40.520,0:18:49.820
Nous pouvons importer torch et « from torch import nn ». Voyons maintenant 

0:18:49.820,0:18:54.500
par exemple ma couche convolutive conv.

0:18:54.500,0:18:59.930
C’est égal à « nn. » puis je peux continuer jusqu'à ce que choisisse celui-ci.

0:18:59.930,0:19:04.220
Disons que je n'ai aucune idée de comment utiliser cette fonction.

0:19:04.220,0:19:08.750
Je mets un point d'interrogation, j'appuie sur Entrée et je vais voir ici la documentation.

0:19:08.750,0:19:13.460
Donc dans ce cas, le premier élément va être les entrées des canaux,

0:19:13.460,0:19:19.820
puis j'ai les canaux de sortie, puis j'ai la taille du noyau.

0:19:19.820,0:19:24.290
Par exemple, nous allons mettre ici des canaux d'entrée pour un signal stéréo

0:19:24.290,0:19:30.530
donc nous mettons deux canaux. Pour le nombre de noyaux, nous avons dit m,  

0:19:30.530,0:19:36.650
disons que nous avons m=16 noyaux. Donc c'est le nombre de noyaux que je vais utiliser.

0:19:36.650,0:19:41.810
Mettons la taille de notre noyau qui est la même que celle que j'utilise ici, donc prenons k,

0:19:41.810,0:19:47.570
taille du noyau égale à 3. Donc ici je vais définir mon premier objet de 

0:19:47.570,0:19:52.910
convolution. Si j’affiche ça, vous voyez que nous avons une convolution 2D.

0:19:52.910,0:19:57.580
Erf désolé. Il nous faut une convolution 1D. Donc nous avons une convolution 1D

0:20:02.149,0:20:08.869
qui passe par deux canaux, stéréophonique à 16 canaux, ce qui signifie que

0:20:08.869,0:20:16.039
j’utilise 16 noyaux. La taille des noyaux est de 3 et le pas est de 1.

0:20:16.039,0:20:23.859
Donc dans ce cas, je vais vérifier quels seront mes poids convolutionnels.

0:20:27.429,0:20:33.379
Quelle est la taille des poids ? Combien de poids avons-nous ? 

0:20:33.379,0:20:40.069
Combien de plans avons-nous pour les poids ? [Etudiant] 16, donc nous avons 16 poids.

0:20:40.069,0:20:53.649
Quelle est la longueur des noyaux ? [Etudiant : 3] Oh qu'est-ce que ce 2 ?

0:20:54.549,0:21:00.349
[Etudiant] Les canaux. J'ai donc 16 de ces noyaux qui ont une épaisseur de 2 

0:21:00.349,0:21:05.539
et de longueur de 3. Cela fait sens car vous allez appliquer chacun de ces

0:21:05.539,0:21:11.629
16 sur l'ensemble du signal. Alors mon signal maintenant, va être égal

0:21:11.629,0:21:20.599
torch.randn(64)

0:21:20.599,0:21:25.129
Je dois aussi dire que j'ai un batch de taille 1. Donc j'ai juste un signal.

0:21:25.129,0:21:31.879
Ensuite ce sera 64. Combien de canaux nous avons dit que cela a ? [Etudiant] 2.

0:21:31.879,0:21:37.819
J'ai donc un signal, un exemple qui a deux 2 et 64 échantillons.

0:21:37.819,0:21:47.689
Donc voici mon x. Quelle est la taille du biais ? [Etudiant]

0:21:48.320,0:21:54.380
Ok 16 Car vous avez un biais par plan. Que va être la taille 

0:21:54.380,0:22:07.539
de ma convolution de x ? Donc je vais avoir encore un échantillon.

0:22:07.539,0:22:15.919
Combien de canaux ? [Etudiant] 16. Quelle est la longueur du signal ? [Etudiant]

0:22:15.919,0:22:22.700
62. Ok fantastique. Qu’est-ce que j’ai si j’utilise une convolution

0:22:22.700,0:22:32.240
avec un noyau de taille 5 ? Qu'est-ce que j’ai ? [Etudiant] Criez, je ne peux pas vous entendre.

0:22:32.240,0:22:36.320
[Etudiant] 60, ok vous suivez, fantastique.

0:22:36.320,0:22:44.059
Essayons maintenant d'utiliser une image hyperspectrale avec une convolution 2D.

0:22:44.059,0:22:49.100
Donc je vais coder ma convolution ici.

0:22:49.100,0:22:55.490
Dans ce cas cela va être nn.Conv2d. A nouveau si je ne sais pas comment

0:22:55.490,0:22:59.059
l'utiliser, je peux mettre un point d'interrogation. J'ai ici le canal d'entrée, le canal de sortie,

0:22:59.059,0:23:05.450
la taille du noyau, le pas, le rembourrage. Ok donc je vais mettre des canaux d’entrées.

0:23:05.450,0:23:10.429
C'est une image hyperspectrale avec 20 plans donc qu’est l’entrée dans

0:23:10.429,0:23:16.149
ce cas ? [Etudiant] 20 ok car vous partez de 20 bandes spectrales.

0:23:16.149,0:23:20.419
Nous allons entrer le nombre de canaux de sortie, disons que nous en

0:23:20.419,0:23:25.330
utilisons à nouveau 16 ici. Entrons la taille du noyau.

0:23:25.330,0:23:33.440
Puisque j'ai l'intention d'utiliser… ok définissons le signal d’abord.

0:23:33.440,0:23:52.820
Donc mon x égal torch.rand(échantillon=1, canaux=20, hauteur=64, largeur=128) 

0:23:52.820,0:23:58.820
Donc il s’agit de mes données d’entrée.

0:23:58.820,0:24:04.370
Donc ma convolution maintenant peut être quelque chose comme ça : 

0:24:04.370,0:24:15.050
nn.Conv2D(canaux_entrée=20, canaux=16, taille_noyau=(3,5))

0:24:15.050,0:24:24.580
Donc que va être la sortie ? Quelle est la taille du noyau ?

0:24:29.170,0:24:47.630
Quelqu'un ? Oui ? Non ? [Etudiant] Non. 20 canaux est le nombre pour les canaux des données d'entrée.

0:24:47.630,0:24:51.680
Combien de noyaux avez-vous ici ? [Etudiant] 16, ok.

0:24:51.680,0:24:56.420
Nous avons 16 noyaux qui ont 20 canaux de sorte qu'ils peuvent se superposer

0:24:56.420,0:25:03.410
sur l’entrée 3 par 5.  Court mais grand. Ok donc que va être ma

0:25:03.410,0:25:08.140
conv(x).size ? [1, 16, 62, 124].

0:25:08.310,0:25:22.190
Disons que je voudrais avoir la même dimension. Je peux ajouter du rembourrage.

0:25:22.190,0:25:25.730
Donc ici il y aura le pas dont la valeur ici est 1,

0:25:25.730,0:25:29.930
1encore une fois si vous ne vous souvenez pas de la syntaxe, vous pouvez simplement mettre un point d’interrogation.

0:25:29.930,0:25:35.120
Puis combien de pas dois-je ajouter dans la direction y ?

0:25:35.120,0:25:41.870
Oui, désolé. Combien de rembourrage dois-je ajouter dans la direction y ?

0:25:41.870,0:25:46.490
[Etudiant] 1, car il y a 1 en haut et 1 en bas. Puis sur l’axe des x ?

0:25:46.490,0:25:51.890
[Etudiant] 2, ok vous suivez, c’est fantastique.

0:25:51.890,0:25:57.320
En exécutant, on retrouve la taille initiale. Donc maintenant vous avez les deux : 1D et 2D.

0:25:57.320,0:26:05.500
Le point est : quelle est la dimension d'un noyau et d’un symbole convolutif

0:26:05.500,0:26:12.470
pour un signal dimensionnel ? Je répète encore une fois. Qu’elle est la

0:26:12.470,0:26:20.049
dimensionnalité de la collection de noyaux utilisés pour des données bidimensionnelles ? [Etudiant]

0:26:20.860,0:26:27.679
Répétez [Etudiant] Donc 4 est le nombre de dimensions requises pour

0:26:27.679,0:26:35.659
stocker la collection de noyaux quand vous effectuez des convolutions 2D. [Etudiant] Le 1 est le pas.

0:26:35.659,0:26:40.370
Si vous ne savez pas comment cela fonctionne, vous n'avez qu'à mettre un

0:26:40.370,0:26:44.000
point d'interrogation et cela vous dit ici. Donc un pas de 1 est que

0:26:44.000,0:26:50.929
vous déplacez à chaque fois le noyau d'une unité. [Etudiant] Le premier 1

0:26:50.929,0:26:55.460
signifie que vous n'avez que la taille du batch. Torch s'attend à ce que vous utilisiez toujours des batchs,

0:26:55.460,0:27:00.110
c'est-à-dire le nombre de signaux que vous utilisez. Ici juste 1. C’est ce qu’on attend.

0:27:00.110,0:27:04.549
Si vous envoyez un tenseur d'entrée de dimension 3,

0:27:04.549,0:27:12.289
Torch va se casser et se plaindre. Nous avons encore un peu de temps

0:27:12.289,0:27:18.049
pour aller à la deuxième partie. Cette deuxième partie sera…

0:27:18.049,0:27:23.779
Vous avez calculé quelques dérivés pour le premier devoir à la maison.

0:27:23.779,0:27:31.909
Pour le devoir à la maison suivant, vous devez calculer ça.

0:27:31.909,0:27:35.510
Vous êtes supposé rire, c'est une blague [rires] Ok, voilà, fantastique.

0:27:35.510,0:27:43.340
C’est donc ce que vous pouviez écrire dans les années 90 pour le calcul des

0:27:43.340,0:27:50.029
gradients, les gradients pour LSTM que nous verrons la prochaine fois.

0:27:50.029,0:27:54.950
Ils avaient à faire des choses comme ça. C’est un peu fou. 

0:27:54.950,0:28:00.769
Nous nous pouvons utiliser PyTorch pour avoir un calcul automatique des

0:28:00.769,0:28:06.500
gradients. Regardons comment fonctionnent ces gradients automatiques.

0:28:06.500,0:28:28.159
Donc nous faisons maintenant le notebook numéro trois, qui est… invisible.  

0:28:28.490,0:28:33.590
Laissez-moi voir si je peux le surligner, c'est encore pire. 

0:28:33.590,0:28:41.619
03-autograd_tutorial. Je passe en plein écran. 

0:28:41.619,0:28:53.029
Donc le tutoriel. Ici je crée juste mon tenseur qui a l’argument

0:28:53.029,0:28:57.499
« requires_grad=True ». Ici je demande à Torch :

0:28:57.499,0:29:02.539
« s'il te plaît suit tous les calculs de gradient », les calculs sur le

0:29:02.539,0:29:07.749
tenseur de sorte que nous puissions calculer des dérivés partielles.

0:29:07.749,0:29:13.279
Dans ce cas, je vais avoir mon y qui va…. Ici x va simplement être

0:29:13.279,0:29:20.419
1, 2, 3, 4. Le y va être x-2.

0:29:20.419,0:29:26.869
Nous pouvons maintenant remarquer qu'il y a cette fonction « grad_fn ».

0:29:26.869,0:29:32.059
Voyons voir ce que c'est. Oh, c’est un « SubBackward ».

0:29:32.059,0:29:37.629
Cela signifie que le y a été généré par un module qui effectue la

0:29:37.629,0:29:43.669
soustraction entre x et 2. Donc vous avez x - 2 donc si vous vérifiez

0:29:43.669,0:29:51.860
qui a généré le y, il y a un module de soustraction. Donc que va être

0:29:51.860,0:30:01.009
« x.grad_fn » ? Vous êtes censé répondre. [Etudiant]

0:30:01.009,0:30:11.580
Pourquoi « None » ? Il aurait dû écrire « Alfredo a généré ça », mais « None » est bien aussi.

0:30:12.020,0:30:17.000
Alors mettons le nez à l'intérieur. Nous étions là, nous pouvons accéder

0:30:17.000,0:30:23.770
au premier élément. Vous avez l'accumulation. Pourquoi l'accumulation ?

0:30:25.090,0:30:29.830
Je ne sais pas, j'ai oublié. Mais si vous allez à l'intérieur, vous voyez 

0:30:29.830,0:30:34.760
le tenseur initial que nous utilisons : 1, 2, 3, 4.

0:30:34.760,0:30:41.390
Donc à l'intérieur de ce graphique de calcul, vous pouvez également trouver le tenseur d'origine.

0:30:41.390,0:30:46.880
Prenons maintenant le z qui vaut y au carré fois 3.

0:30:46.880,0:30:51.620
Je calcule ma moyenne a, ce sera la moyenne de z. Donc si je calcule

0:30:51.620,0:30:56.330
le carré de cette chose, je multiplie par trois puis prends la moyenne,

0:30:56.330,0:31:00.500
voici la partie carrée multipliée par 3 et ensuite voici la moyenne. Vous pouvez

0:31:00.500,0:31:06.200
essayer si vous ne me croyez pas. Donc voyons à quoi ressemble cette chose.

0:31:06.200,0:31:10.549
J’affiche ici toutes ces séquences de calculs. Donc nous avons commencé par

0:31:10.549,0:31:16.669
une matrice 2 par 2. Quel était ce gars ici, le 2 par 2 ?  [Etudiant] x

0:31:16.669,0:31:22.399
Ok, vous suivez, cool. Puis nous avons soustrait 2 et ensuite nous avons

0:31:22.399,0:31:27.440
multiplié par y deux fois. C'est pourquoi il y a deux flèches. Vous obtenez 

0:31:27.440,0:31:31.669
la même soustraction qui est le y = x-2 multiplié par lui-même.

0:31:31.669,0:31:36.649
Vous avez une autre multiplication. Qu’est-ce que c’est ? [Etudiant] Ok, multiplication par trois

0:31:36.649,0:31:42.980
Puis le « MeanBackward ». Pourquoi c’est en vert ? Car c’est la moyenne.

0:31:42.980,0:31:51.140
[blague sur le son de « mean » et « green »]. Merci de rire. Ok donc je calcule la rétropropagation.

0:31:51.140,0:31:59.409
Que fait la rétropropagation ? Que fait cette ligne ? [Etudiant]

0:32:00.360,0:32:08.610
Je veux entendre tout le monde. Vous savez déjà vous deux. Nous calculons quoi ? [Etudiant]Les gradients ok.

0:32:08.610,0:32:11.580
La rétropropagation est la façon dont vous calculez les gradients. Nous entraînons les réseaux de neurones avec ? [Etudiant]

0:32:11.580,0:32:20.730
Descente de gradient ok. Ou quoi que ce soit qu'Aaron a présenté hier.

0:32:20.730,0:32:27.000
La répropagation est utilisée pour le calcul du gradient, c’est des choses

0:32:27.000,0:32:29.970
complètement différentes. Gardez les séparées, ne les fusionnez pas.

0:32:29.970,0:32:34.559
Quand les gens ne me voient plus, après un certain temps ils ont tendance 

0:32:34.559,0:32:43.740
à associer les deux. Ne le faites pas, c’est douloureux.

0:32:43.740,0:32:51.659
Donc devinez quoi ? On va calculer les gradients.

0:32:51.659,0:33:02.580
Allons sur une nouvelle page. Que va être a ? La moyenne [a pour « average »]

0:33:02.580,0:33:10.529
Donc c'est 1/4 somme de tous ces zᵢ.

0:33:10.529,0:33:17.460
Je vais de 1 à 4. Donc qu’est-ce que zᵢ ?

0:33:17.460,0:33:27.539
zᵢ = 3yᵢ². Des questions ? Non, ok.

0:33:27.539,0:33:36.840
C’est égale à 3(x-2)². Donc à quoi appartient a ?

0:33:36.840,0:33:44.199
[Etudiant] ℝ. a est un scalaire. 

0:33:44.279,0:33:51.200
Donc maintenant nous pouvons calculer ∂a/∂x.

0:33:51.200,0:33:58.110
Quel est ce truc ? Nous avons le 1/4.

0:33:58.110,0:34:03.090
Ici mettons un i car ça concerne l’élément xᵢ.

0:34:03.090,0:34:09.179
Donc on va avoir celui-là que je branche dans le zᵢ, les 3yᵢ², 

0:34:09.179,0:34:15.899
le 3(xᵢ- 2)². Le 3 vient ici.

0:34:15.899,0:34:22.080
Le 2 de la puissance descend et on multiplie par (xᵢ – 2).

0:34:22.080,0:34:33.260
Cela devrait être correct jusqu'à présent. Ok fantastique. Donc mon x était cet élément ici.

0:34:33.589,0:34:38.190
Laissez-moi simplifier ici. Donc cela devient

0:34:38.190,0:34:47.690
1,5 fois xᵢ – 3.

0:34:55.159,0:35:06.780
Bien alors que va être ma ∂a/∂x ?

0:35:06.780,0:35:11.339
J'écris en fait la transposition ici. Donc pour le premier élément

0:35:11.339,0:35:18.859
vous avez 1,5 * 1 – 3 donc -1,5.

0:35:18.859,0:35:23.670
Le deuxième est 1,5 * 2 – 3 donc vous obtenez 0.

0:35:23.670,0:35:27.420
Peut-être que je devrais tout écrire pour que vous suiviez. Donc vous avez

0:35:27.420,0:35:37.589
1,5 – 3, là 3 – 3. En dessous vous avez 4,5 - 3  et ensuite le

0:35:37.589,0:35:47.160
dernier est 6 – 3. Ce qui est égal à 

0:35:47.160,0:35:59.789
-1,5   0   1,5   3    Vous êtes d'accord ? Ok. Laissez-moi juste écrire ceci ici.

0:35:59.789,0:36:06.149
Nous avons déjà calculé la rétropropagation ici.

0:36:06.149,0:36:14.609
J’affiche juste le gradient. Tada ! C'est la même chose qu’ici.

0:36:14.609,0:36:27.630
[Etudiant] Je n'ai pas à transposer ici. Chaque fois que vous faites 

0:36:27.630,0:36:33.209
la dérivée partielle dans PyTorch, vous obtenez la même forme comme dimension 

0:36:33.209,0:36:37.469
d'entrée. Donc si vous avez un poids, quelle que soit la dimension, alors lorsque vous calculez

0:36:37.469,0:36:41.069
les partielles, vous avez toujours la même dimension. On ne tourne pas.

0:36:41.069,0:36:44.789
C’est utilisé juste pour des raisons pratiques.

0:36:44.789,0:36:49.919
Le gradient devrait être la transposition de cette chose. Désolé.

0:36:49.919,0:36:54.479
La jacobienne qui est la transposition du gradient si c'est un vecteur mais 

0:36:54.479,0:37:08.130
ceci est un tenseur. Quoi que nous ayons, nous utilisons la même forme. [Etudiant] Non ça devrait être un retournement.

0:37:08.130,0:37:13.639
J'ai peut-être tort, mais je ne pense pas.

0:37:13.639,0:37:19.919
Ceci est du PyTorch basique. Maintenant nous pouvons faire des trucs fous 

0:37:19.919,0:37:26.609
car nous aimons la folie. Enfin, moi j’aime et je pense que si vous m'aimez, vous aimez ça aussi.

0:37:26.609,0:37:34.259
Donc ici je crée mon vecteur x est un tensor 1D de trois éléments.

0:37:34.259,0:37:43.769
Je multiplie x par 2, puis j'appelle cela y.

0:37:43.769,0:37:49.859
Puis je commence mon compteur à 0 et tant que la norme de y vaut moins de 1000,

0:37:49.859,0:37:56.699
je continue à doubler y. Ainsi vous pouvez obtenir comme un graphe dynamique.

0:37:56.699,0:38:01.529
Le graphe est conditionné à l'initialisation aléatoire que vous

0:38:01.529,0:38:04.979
ne pouvez même pas dire ici car je n'ai même pas utilisé de « seed ».

0:38:04.979,0:38:08.999
Si vous lancez tout ça, vous allez tous avoir des nombres différents.

0:38:08.999,0:38:11.910
Ici ce sont les valeurs finales de y. Pouvez-vous me dire

0:38:11.910,0:38:23.549
le nombre d'itérations que nous avons effectué ? [Etudiant] La moyenne de ce truc est inférieure à 1000

0:38:23.549,0:38:27.630
oui mais alors je vous demande si vous savez combien de fois on a du faire

0:38:27.630,0:38:41.119
cette boucle ? [Etudiant] Non. Bien. Pourquoi ? [Etudiant] C’est aléatoire. C'est une mauvaise question.

0:38:41.119,0:38:45.539
A propos de mauvaises questions, la prochaine fois j'aurai quelque chose pour vous.

0:38:45.539,0:38:51.569
Donc j’affiche ça et vous dis que le x.grad vaut 2048.

0:38:51.569,0:38:55.589
Il suffit de vérifier la valeur centrale pour le moment. C’est le gradient.

0:38:55.589,0:39:04.739
Donc pouvez-vous me dire maintenant combien de fois on est passé sur la boucle ? [Etudiant] Quelqu'un a dit 11 fois.

0:39:04.739,0:39:14.420
Levez la main ceux qui pense 11. Ok que 4 personnes ont levé la main. Qu'en est-il des autres ? [Etudiant]

0:39:14.809,0:39:17.809
21. D’autres propositions ? [Etudiants : 11 ou 10]

0:39:25.529,0:39:30.749
Quelqu'un a la bonne solution. Cette boucle s’est produite 10 fois.

0:39:30.749,0:39:35.759
Pourquoi ça ? Car vous avez la première multiplication par 2 ici

0:39:35.759,0:39:40.589
puis la boucle se répète encore et encore et multiplie par 2.

0:39:40.589,0:39:45.239
Donc le nombre final est le nombre le d'itérations dans la boucle plus

0:39:45.239,0:39:50.779
la multiplication supplémentaire en dehors. Oui ? Non ?

0:39:50.779,0:39:56.670
Vous dormez ? Peut-être ? Je vous prévenu de ne pas manger avant les cours sinon vous

0:39:56.670,0:40:05.009
devenez groggy. Ok donc l’inférence. C'est cool. Ici je vais juste avoir

0:40:05.009,0:40:09.420
mes x et w  et faisons juste une régression linéaire. Linéaire ou peu

0:40:09.420,0:40:17.670
importe. L'opérateur « @ » est juste le produit scalaire. Donc x et w

0:40:17.670,0:40:21.589
ont tous les deux l’argument « requires_grad=True ».

0:40:21.589,0:40:27.119
Cela signifie que nous allons suivre les gradients et les

0:40:27.119,0:40:31.290
graphes de calcul. Donc si j'exécute ça, vous obtenez les dérivés

0:40:31.290,0:40:37.710
partielles du produit intérieur par rapport à l’entrée qui 

0:40:37.710,0:40:43.920
seront les poids. Donc « arange » est l’entrée et « ones » sont les poids.

0:40:43.920,0:40:47.160
Donc les dérivées partielles par rapport à l'entrée seront les poids.

0:40:47.160,0:40:50.070
Les dérivées partielles par rapport aux poids seront l’entrée.

0:40:50.070,0:40:56.670
Oui ? Non ? Oui. Ok. D'habitude celui-ci est le k.

0:40:56.670,0:41:00.359
J'ai « requires_grad » dans mes paramètres car que je vais utiliser

0:41:00.359,0:41:06.030
les gradients pour la mise à jour ultérieure des paramètres de mon modèle.

0:41:06.030,0:41:12.300
Dans ce cas, j’ai « None ». Dans ce cas-ci, je veux faire de l’inférence.

0:41:12.300,0:41:17.250
Quand je fais de l’inférence, je dis à Torch : « Hey Torch arrêter de 

0:41:17.250,0:41:22.950
suivre toutes les sortes d’opérations ». Donc : « torch.no_grad ». Donc peu importe que 

0:41:22.950,0:41:28.859
votre entrée ou vos poids aient l’argument « requires_grad » égal « True » ou « False », quand je mets

0:41:28.859,0:41:35.060
« torch.no_grad », il n’y a pas de calcul de graphe pris en compte.  

0:41:35.060,0:41:41.130
Donc si j'essaie d'effectuer une rétropropagation sur un tenseur qui a été

0:41:41.130,0:41:46.320
généré à partir de… n'a pas de graphe car celui-ci n’a pas de graphe,

0:41:46.320,0:41:50.940
vous obtenez une erreur. Donc si je lance cette cellule, j’ai une erreur 

0:41:50.940,0:41:55.410
et un visage fâché car c'est une erreur. Et ensuite le message :

0:41:55.410,0:42:00.720
L’élément 0 du tenseur ne nécessite pas de « grad » et n'a pas de « grad_fn ».

0:42:00.720,0:42:11.400
Donc « e »… euh le z enfaites, vous ne pouvez pas rétropropager z car il n'y a pas de graphe attaché à z.

0:42:11.400,0:42:19.710
Des questions ? C’est très puissant. Vous ne pouvez pas faire ces choses avec TensorFlow.

0:42:19.710,0:42:29.790
TensorFlow est comme… Peu importe. Davantage de chose arrivent maintenant. 

0:42:30.600,0:42:36.340
Alors nous retournons ici. Nous avons à l'intérieur du dossier « extra » 

0:42:36.340,0:42:40.450
des choses sympathiques. Je voulais couvrir les deux mais je pense qu’on va faire que la seconde.

0:42:40.450,0:42:47.290
Donc la seconde est la suivante. Donc dans ce cas nous créons

0:42:47.290,0:42:52.750
nos propres modules spécifiques. J'aimerais définir ma propre

0:42:52.750,0:42:58.030
fonction qui est super spéciale, une fonction géniale.

0:42:58.030,0:43:02.560
Si je veux l'utiliser pour entraîner les réseaux, j'ai besoin d’une phase avant

0:43:02.560,0:43:06.220
et également savoir quelle est la dérivée partielle de l’entrée par rapport à la sortie

0:43:06.220,0:43:10.930
de sorte que je puisse utiliser ce module dans n'importe quel point de mon

0:43:10.930,0:43:15.670
code. En utilisant la rétropropagation, la règle de la chaîne, vous branchez

0:43:15.670,0:43:19.320
les choses. Yann a évoqué ça plusieurs fois. Tant que vous connaissez la 

0:43:19.320,0:43:23.410
dérivé partielle de la sortie par rapport à l'entrée vous pouvez brancher 

0:43:23.410,0:43:31.690
ces choses n'importe où dans votre chaîne d'opérations. Donc dans ce cas nous définissons « MyAdd » qui

0:43:31.690,0:43:35.620
effectue l'addition des deux entrées dans ce cas. Mais lorsque vous

0:43:35.620,0:43:41.130
faisons la rétropropagation, si vous avez une addition, qu'est la rétropropagation ?

0:43:41.130,0:43:47.020
Si vous avez une addition des deux choses, vous obtenez un résultat lorsque vous descendez

0:43:47.020,0:43:53.320
les gradients, qu'est-ce qui se passe avec le gradient ? [Etudiant]

0:43:53.320,0:43:57.160
Les deux côtés sont copiés. C'est pourquoi vous obtenez les deux copies,

0:43:57.160,0:44:01.390
la même chose. Et elles sont envoyées par un côté de l'autre. Vous pouvez

0:44:01.390,0:44:05.170
exécuter ce truc. Vous voyez qu’on a le même gradient dans les deux sens.

0:44:05.170,0:44:09.460
Dans ce cas, j'ai une séparation. Donc je viens de la même chose, puis je sépare et

0:44:09.460,0:44:13.180
j'ai ces deux choses qui font autre chose. Si je descends avec le gradient 

0:44:13.180,0:44:19.080
Qu’est-ce que je fais ? [Etudiant] Je les ajoute. C’est pourquoi nous avons ici la somme. 

0:44:19.080,0:44:23.680
Vous pouvez exécuter celui-ci et voir que nous avons eu ces deux gradients

0:44:23.680,0:44:30.710
initiaux puis quand vous descendez, les deux choses, les deux gradients s’additionnent ici.

0:44:30.790,0:44:36.190
Encore une fois, si vous utilisez des choses préfaites dans PyTorch, elles sont correctes.

0:44:36.190,0:44:41.080
Vous pouvez vous amuser, y mettre toutes sortes de choses différentes pour

0:44:41.080,0:44:47.950
la fonction avant et arrière. Je pense nous n’avons plus de temps. D’autres questions

0:44:47.950,0:44:58.800
avant de partir ? Non. Ok. Donc je vous vois lundi. Restez au chaud. Bye.

---
lang: fr
lang-ref: ch.05
title: Semaine 5
translation-date: 05 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A

We begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on convergence.
-->


## Cours magistral partie A

Nous commençons par introduire la méthode de descente de gradient (GD pour *Gradient Descent*). Nous discutons de l'intuition et expliquons également comment la taille des pas joue un rôle important dans l'obtention de la solution. Nous passons ensuite à la descente de gradient stochastique (SGD) et à ses performances par rapport à la descente de gradient « *Full Batch* ». Enfin, nous parlons des mises à jour dans le *momentum*, en particulier des deux règles de mise à jour, de l'intuition derrière le *momentum* et de son effet sur la convergence.

<!--
## Lecture part B

We discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient.
-->

## Cours magistral partie B

Nous discutons des méthodes adaptatives pour la SGD telles que RMSprop et ADAM. Nous parlons également des couches de normalisation et de leurs effets sur le processus d'entraînement des réseaux neuronaux. Enfin, nous discutons d'un exemple concret de réseaux de neurones utilisés dans l'industrie pour rendre les IRMs plus rapides et plus efficaces.


<!--
## Practicum

We briefly review the matrix-multiplications and then discuss the convolutions. Key point is we use kernels by stacking and shifting. We first understand the 1D convolution by hand, and then use PyTorch to learn the dimension of kernels and output width in 1D and 2D convolutions examples. Furthermore, we use PyTorch to learn about how automatic gradient works and custom-grads.
-->

## Travaux dirigés
Nous passons brièvement en revue les multiplications matricielles et discutons ensuite des convolutions. Le point essentiel est que nous utilisons les noyaux en les empilant et en les déplaçant. Nous commençons par comprendre la convolution 1D à la main, puis nous utilisons PyTorch pour apprendre la dimension des noyaux et la largeur de sortie dans des exemples de convolutions 1D et 2D. De plus, nous utilisons PyTorch pour apprendre comment fonctionne le gradient automatique et les gradations personnalisées.


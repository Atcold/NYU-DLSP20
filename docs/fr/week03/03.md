---
lang: fr
lang-ref: ch.03
title: Semaine 3
translation-date: 03 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A

We first see a visualization of a 6-layer neural network. Next we begin with the topic of Convolutions and Convolution Neural Networks (CNN). We review several types of parameter transformations in the context of CNNs and introduce the idea of a kernel, which is used to learn features in a hierarchical manner. Thereby allowing us to classify our input data which is the basic idea motivating the use of CNNs.
-->


## Conférence partie A

Nous voyons d'abord une visualisation d'un réseau de neurones à 6 couches. Ensuite, nous commençons par le sujet des convolutions et des réseaux neuronaux à convolution (ConvNets). Nous passons en revue plusieurs types de transformations de paramètres dans le contexte des ConvNets et introduisons l'idée d'un noyau, qui est utilisé pour apprendre des caractéristiques de manière hiérarchique. Cela nous permet de classer nos données d'entrée, ce qui est l'idée de base motivant l'utilisation des ConvNets.

<!--
## Lecture part B

We give an introduction on how CNNs have evolved over time. We discuss in detail different CNN architectures, including a modern implementation of LeNet5 to exemplify the task of digit recognition on the MNIST dataset. Based on its design principles, we expand on the advantages of CNNs which allows us to exploit the compositionality, stationarity, and locality features of natural images.
-->

## Conférence partie B

Nous présentons une introduction sur l'évolution des ConvNets au fil du temps. Nous discutons en détail des différentes architectures de ConvNets, y compris une mise en œuvre moderne de LeNet5 pour illustrer la tâche de reconnaissance numérique sur le jeu de données du MNIST. Sur la base de ses principes de conception, nous développons les avantages des ConvNets qui nous permettent d'exploiter les caractéristiques de composition, de stationnarité et de localisation des images naturelles.

<!--
## Practicum

Properties of natural signals that are most relevant to CNNs are discussed in more detail, namely: Locality, Stationarity, and Compositionality. We explore precisely how a kernel exploits these features through sparsity, weight sharing and the stacking of layers, as well as motivate the concepts of padding and pooling. Finally, a performance comparison between FCN and CNN was done for different data modalities.
-->

## Pratique
Les propriétés des signaux naturels qui sont les plus pertinentes pour les ConvNets sont discutées plus en détail, à savoir : localité, stationnarité et compositionnalité. Nous explorons précisément comment un noyau exploite ces caractéristiques par l'éparsité (sparsity), le partage des poids (weight sharing) et l'empilement des couches (the stacking of layers) et abordons les concepts de rembourrage (padding) et de pooling. Enfin, une comparaison des performances entre FCN (fully connected network) et ConvNets est effectuée pour différents types de données.


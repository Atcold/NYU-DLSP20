---
lang: fr
lang-ref: ch.03
title: Semaine 3
translation-date: 03 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A

We first see a visualization of a 6-layer neural network. Next we begin with the topic of Convolutions and Convolution Neural Networks (CNN). We review several types of parameter transformations in the context of CNNs and introduce the idea of a kernel, which is used to learn features in a hierarchical manner. Thereby allowing us to classify our input data which is the basic idea motivating the use of CNNs.
-->


## Cours magistral partie A

Nous voyons d'abord une visualisation d'un réseau de neurones à 6 couches. Ensuite, nous commençons par le sujet des convolutions et des réseaux neuronaux à convolution (ConvNets). Nous passons en revue plusieurs types de transformations de paramètres dans le contexte des ConvNets et introduisons l'idée d'un noyau, qui est utilisé pour apprendre des caractéristiques de manière hiérarchique. Cela nous permet de classer nos données d'entrée, ce qui est l'idée de base motivant l'utilisation des ConvNets.

<!--
## Lecture part B

We give an introduction on how CNNs have evolved over time. We discuss in detail different CNN architectures, including a modern implementation of LeNet5 to exemplify the task of digit recognition on the MNIST dataset. Based on its design principles, we expand on the advantages of CNNs which allows us to exploit the compositionality, stationarity, and locality features of natural images.
-->

## Cours magistral partie B

Nous présentons l'évolution des ConvNets au fil du temps. Nous discutons en détails des différentes architectures de ConvNets, y compris une implémentation moderne de LeNet5 pour illustrer la tâche de reconnaissance des chiffres sur le jeu de données du MNIST. Sur la base des principes de conception, nous développons les avantages des ConvNets qui nous permettent d'exploiter les caractéristiques de compositionnalité, de stationnarité et de localisation des images naturelles.

<!--
## Practicum

Properties of natural signals that are most relevant to CNNs are discussed in more detail, namely: Locality, Stationarity, and Compositionality. We explore precisely how a kernel exploits these features through sparsity, weight sharing and the stacking of layers, as well as motivate the concepts of padding and pooling. Finally, a performance comparison between FCN and CNN was done for different data modalities.
-->

## Travaux dirigés
Nous discutons des propriétés des signaux naturels qui sont les plus pertinentes pour les ConvNets. A savoir la localité, la stationnarité et la compositionnalité. Nous explorons précisément comment un noyau exploite ces caractéristiques par l'éparsité, le partage des poids et l'empilement des couches puis abordons les concepts de rembourrage et de *pooling*. Enfin, une comparaison des performances entre les réseaux entièrement connectés et les ConvNets est effectuée pour différents types de données.


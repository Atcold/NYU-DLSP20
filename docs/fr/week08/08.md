---
lang: fr
lang-ref: ch.08
title: Semaine 8
translation-date: 08 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A

In this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence.
-->


## Conférence partie A

Dans cette section, nous nous focalisons sur l'introduction des méthodes contrastives dans les modèles à base d’énergie (EBM) sous plusieurs aspects. Tout d'abord, nous discutons de l'avantage apporté par l'application des méthodes contrastives dans l'auto-apprentissage. Ensuite, nous discutons de l'architecture des auto-encodeurs débruiteur et de leur faiblesse dans les tâches de reconstruction d'images. Nous évoquons également d'autres méthodes contrastives, comme la divergence contrastive et la divergence contrastive persistante.

<!--
## Lecture part B

In this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved.
-->

## Conférence partie B

Dans cette section, nous discutons en détail des EBM à variables latentes régularisées en couvrant les concepts de versions conditionnelles et inconditionnelles de ces modèles. Nous discutons ensuite des algorithmes ISTA, FISTA et LISTA. Nous examinons des exemples de codage épars et de filtres appris d’encodeurs épars convolutifs. Enfin, nous parlons des auto-encodeurs variationnels et des concepts sous-jacents impliqués.

<!--
## Practicum

In this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples.
-->

## Pratique
Dans cette section, nous discutons d'un type spécifique de modèle génératif appelé auto-encodeurs variationnels (VAE) et comparons leurs fonctionnalités et avantages par rapport aux auto-encodeurs classiques. Nous explorons en détail la fonction objective du VAE, en comprenant comment elle impose une certaine structure dans l'espace latent. Enfin, nous mettons en œuvre et entraînons un VAE sur l'ensemble de données MNIST et l’utilisons pour générer de nouveaux échantillons.




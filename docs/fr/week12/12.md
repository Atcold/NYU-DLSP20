---
lang: fr
lang-ref: ch.12
title: Semaine 12
translation-date: 12 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A


In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively. 
-->


## Cours magistral partie A

Dans cette section, nous discutons des différentes architectures utilisées dans les applications de traitement du langage naturel, en commençant par les ConvNets, les RNNs et en couvrant finalement l'architecture de pointe, les transformers. Nous abordons ensuite les différents modules qui composent les transformers et leurs avantages pour les tâches de texte. Enfin, nous discutons des astuces qui permettent d’entraîner efficacement les transformers.

<!--
## Lecture part B

In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT.
-->

## Cours magistral partie B

Dans cette section, nous présentons la recherche en faisceau comme un consensus entre le décodage gourmand et la recherche exhaustive. Nous considérons le cas de vouloir échantillonner à partir de la distribution générative (c'est-à-dire lors de la génération de texte) et introduisons l'échantillonnage *top-k*. Ensuite, nous introduisons les modèles de séquence à séquence (avec une variante pour les transformers) et la rétrotraduction. Nous voyons enfin des approches d'apprentissage non supervisées pour l'apprentissage des enchâssements et discutons de word2vec, GPT et BERT.

<!--
## Practicum


We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures.
-->

## Travaux dirigés
Nous introduisons l'attention, en nous concentrant sur l'auto-attention et ses représentations de couches cachées des entrées. Ensuite, nous abordons le paradigme des valeurs des clés et discutons de la manière de représenter les requêtes, les clés et les valeurs comme des rotations d'une entrée. Enfin, nous utilisons l'attention pour interpréter l'architecture du transformer, en prenant une propagation avant à travers un transformer de base. Enfin nous comparons le paradigme de l'encodeur-décodeur à celui des architectures séquentielles.

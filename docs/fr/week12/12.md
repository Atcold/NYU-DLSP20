---
lang: fr
lang-ref: ch.12
title: Semaine 12
translation-date: 12 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A


In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively. 
-->


## Conférence partie A

Dans cette section, nous discutons des différentes architectures utilisées dans les applications de NLP, en commençant par les ConvNets, les RNN, et en couvrant finalement l'architecture de pointe, les transformers. Nous abordons ensuite les différents modules qui composent les transformers et comment ils rendent les transformers avantageux pour les tâches de NLP. Enfin, nous discutons des astuces qui permettent d’entraîner efficacement les transformers.

<!--
## Lecture part B

In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT.
-->

## Conférence partie B

Dans cette section, nous présentons la recherche par faisceau comme un consensus entre le décodage gourmand et la recherche exhaustive. Nous considérons le cas de vouloir échantillonner à partir de la distribution générative (c'est-à-dire lors de la génération de texte) et introduisons l'échantillonnage "top-k". Ensuite, nous introduisons les modèles de séquence à séquence (avec une variante pours les transformers) et la rétro-traduction. Nous introduisons ensuite des approches d'apprentissage non supervisées pour l'apprentissage des embeddings et discutons de word2vec, GPT et BERT.

<!--
## Practicum


We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures.
-->

## Pratique
Nous introduisons l'attention, en nous concentrant sur l'auto-attention et ses représentations en couches cachées des intrants. Ensuite, nous introduisons le paradigme des valeurs des clés et discutons de la manière de représenter les requêtes, les clés et les valeurs comme des rotations d'une entrée. Enfin, nous utilisons l'attention pour interpréter l'architecture du transformer, en prenant un passage en avant à travers un transformer de base. Enfin nous comparons le paradigme de l'encodeur-décodeur à celui des architectures séquentielles.





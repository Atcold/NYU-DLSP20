---
lang: fr
lang-ref: ch.06
title: Semaine 6
translation-date: 06 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A

We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment.
-->


## Cours magistral partie A

Nous discutons de trois applications des réseaux de neurones convolutifs. Nous commençons par la reconnaissance d'un code postal à 5 chiffres. En ce qui concerne la détection d'objets, nous abordons la manière d'utiliser une architecture multi-échelle dans le cadre de détection de visages. Enfin, nous voyons comment les ConvNets sont utilisés dans des tâches de segmentation sémantique avec des exemples concrets dans un système de vision robotique et la segmentation d'objets dans un environnement urbain.

<!--
## Lecture part B

We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq.
-->

## Cours magistral partie B

Nous examinons les réseaux neuronaux récurrents (RNNs pour *Recurrent Neural Networks*), leurs problèmes et les techniques courantes permettant de les atténuer. Nous examinons ensuite divers modules développés pour résoudre les problèmes liés aux modèles RNNs, notamment l’Attention, les GRUs (*Gated Recurrent Unit*), les LSTMs (*Long Short-Term Memory*) et le Seq2Seq.

<!--
## Practicum

We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models.
-->

## Travaux dirigés
Nous discutons de l'architecture des modèles RNNs de base et des LSTMs et comparons les performances entre les deux. Les LSTMs héritent des avantages des RNNs, tout en améliorant leurs faiblesses en incluant une cellule mémoire afin de stocker les informations pendant de longues périodes. Les modèles LSTMs sont nettement plus performants que les modèles RNNs.

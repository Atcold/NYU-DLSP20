---
lang: fr
lang-ref: ch.07
title: Semaine 7
translation-date: 07 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A

We introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions.
-->


## Cours magistral partie A


Nous introduisons le concept des modèles à base d’énergie (EBMs pour *energy-based models*) et l'intention d'adopter des approches différentes autres que les réseaux *feed-forward*. Pour résoudre la difficulté de l'inférence chez les EBMs, des variables latentes sont utilisées pour fournir des informations auxiliaires et permettre plusieurs prédictions. Enfin, les EBMs peuvent être généralisés aux modèles probabilistes avec des fonctions de notation plus souples.


<!--
## Lecture part B

We discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map.
-->

## Cours magistral partie B

Nous discutons de l'apprentissage autosupervisé, présentons la manière d’entraîné un EBM, discutons de la gestion des variables latentes, en particulier avec l’exemple des K-means. Nous présentons également les méthodes contrastives, expliquons le fonctionnement d'un auto-encodeur débruiteur avec une carte topographique ainsi que le processus d’entraînement et la façon dont il peut être utilisé. Nous poursuivons avec une introduction à BERT. Enfin, nous parlons de la divergence contrastive, également expliquée à l'aide d'une carte topographique.


<!--
## Practicum

We discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder.
-->

## Travaux dirigés
Nous discutons de certaines applications des auto-encodeurs et des raisons pour lesquelles nous voulons les utiliser. Nous parlons ensuite des différentes architectures d'auto-encodeurs (sous ou sur une couche cachée complète), de la manière d'éviter les problèmes de surentraînement et des fonctions de perte que nous devrions utiliser. Enfin, nous mettons en place un auto-encodeur standard et un auto-encodeur débruiteur.

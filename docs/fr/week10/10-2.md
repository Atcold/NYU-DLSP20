---
lang: fr
lang-ref: ch.10-2
title: Apprentissage auto-supervisé, ClusterFit et PIRL
lecturer: Ishan Misra
authors: Zhonghui Hu, Yuqing Wang, Alfred Ajay Aureate Rajakumar, Param Shah
date: 6 Apr 2020
translation-date: 10 Aug 2020
translator: Loïck Bourdois
---


<!--
### [What is missing from "pretext" tasks? The hope of generalization](https://www.youtube.com/watch?v=0KeR6i1_56g&t=3710s)

Pretext task generally comprises of pretraining steps which is self-supervised and then we have our transfer tasks which are often classification or detection. We *hope* that the pretraining task and the transfer tasks are "aligned", meaning, solving the pretext task will help solve the transfer tasks very well. So, a lot of research goes into designing a pretext task and implementing them really well.

However, it is very unclear why performing a non-semantic task should produce good features?. For example, why should we expect to learn about “semantics” while solving something like Jigsaw puzzle? Or why should “predicting hashtags” from images be expected to help in learning a classifier on transfer tasks? Therefore, the question remains. How should we design good pre-training tasks which are well aligned with the transfer tasks?

One way to evaluate this problem is by looking at representations at each layer (refer Fig. 1). If the representations from the last layer are not well aligned with the transfer task, then the pretraining task may not be the right task to solve.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig01.png" height="75%" width="75%"/><br>
<b>Fig. 1</b>: Feature representations at each layer
</center>

Fig. 2 plots the Mean Average Precision at each layer for Linear Classifiers on VOC07 using Jigsaw Pretraining. It is clear that the last layer is very specialized for the Jigsaw problem.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig02.png" height="70%" width="80%"/><br>
<b>Fig. 2</b>: Performance of Jigsaw based on each layer
</center>
-->

### [Que manque-t-il aux tâches de "prétexte" ? L'espoir de la généralisation](https://www.youtube.com/watch?v=0KeR6i1_56g&t=3710s)

La tâche de prétexte comprend généralement des étapes de pré-entraînement qui sont auto-supervisées et ensuite nous avons nos tâches de transfert qui sont souvent de classification ou de détection. Nous *espérons* que la tâche de pré-entraînement et les tâches de transfert sont "alignées", ce qui signifie que la résolution de la tâche de prétexte aidera à résoudre les tâches de transfert. Donc, beaucoup de recherches sont nécessaires pour concevoir une tâche de prétexte et la mettre en œuvre de façon optimale.

Cependant, il est très difficile de savoir pourquoi l'exécution d'une tâche non sémantique devrait produire de bonnes caractéristiques. Par exemple, pourquoi devrions-nous nous attendre à apprendre la "sémantique" tout en résolvant quelque chose comme un puzzle ? Ou pourquoi "prédire les mots-dièse" à partir d'images devrait-il aider à apprendre un classificateur sur des tâches de transfert ? La question reste donc posée. Comment concevoir de bonnes tâches de pré-entraînement qui soient bien alignées avec les tâches de transfert ?

Une façon d'évaluer ce problème est d'examiner les représentations à chaque couche (voir Fig. 1). Si les représentations de la dernière couche ne sont pas bien alignées avec la tâche de transfert, alors la tâche de pré-entraînement peut ne pas être la bonne tâche à résoudre.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig01.png" height="75%" width="75%"/><br>
<b>Figure 1</b> : Représentation des éléments à chaque couche
</center>

La Fig. 2 représente la précision moyenne à chaque couche pour les classificateurs linéaires sur VOC07 avec un pré-entraînement Jigsaw. Il est clair que la dernière couche est très spécialisée pour le problème du puzzle.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig02.png" height="70%" width="80%"/><br>
<b>Figure 2</b> : Performance de Jigsaw en fonction de chaque couche
</center>


<!--
### What we want from pre-trained features?

* Represent how images relate to one another

  * ClusterFit: Improving Generalization of Visual Representations

* Be robust to "nuisance factors" -- Invariance

  *E.g.* exact location of objects, lighting, exact colour

  * PIRL: Self-supervised learning of Pre-text Invariant Representations

Two ways to achieve the above properties are **Clustering** and **Contrastive Learning**. They have started performing much better than whatever pretext tasks that were designed so far.  One method that belongs to clustering is **ClusterFit** and another falling into invariance is **PIRL**.
-->

### Qu'attendons-nous des fonctionnalités pré-entraînées ?

* Représenter la façon dont les images sont liées les unes aux autres

  * ClusterFit : améliorer la généralisation des représentations visuelles

* Être robuste aux "facteurs de nuisance" : Invariance

  *par exemple* emplacement exact des objets, éclairage, couleur exacte

  * PIRL : Apprentissage auto-supervisé des représentations invariantes du prétexte

Deux moyens d'atteindre les propriétés ci-dessus sont le **Clustering** et l' **Apprentissage Contrastif**. Ils ont commencé à fonctionner bien mieux que les tâches prétextes qui ont été conçues jusqu'à présent. Une méthode qui appartient au clustering est **ClusterFit** et une autre qui tombe dans l'invariance est **PIRL**.


<!--
## ClusterFit: Improving Generalization of Visual Representations

Clustering the feature space is a way to see what images relate to one another.
-->

## ClusterFit : Améliorer la généralisation des représentations visuelles

Le clustering de l'espace de présentation est un moyen de voir quelles images sont liées les unes aux autres.

<!--
### Method

ClusterFit follows two steps.  One is the cluster step, and the other is the predict step.
-->

### Méthode

ClusterFit suit deux étapes.  L'une est l'étape du cluster, l'autre est l'étape de la prédiction.

<!--
#### Cluster: Feature Clustering

We take a pretrained network and use it to extract a bunch of features from a set of images. The network can be any kind of pretrained network. K-means clustering is then performed on these features, so each image belongs to a cluster, which becomes its label.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig03.png"
height="75%" width="75%" /><br>
<b>Fig. 3</b>: Cluster step
</center>
-->

#### Cluster : Regroupement des caractéristiques

Nous prenons un réseau pré-entraîné et l'utilisons pour extraire un ensemble de caractéristiques d'un ensemble d'images. Le réseau peut être n'importe quel type de réseau pré-entraîné. Le clustering K-means est alors effectué sur ces caractéristiques, de sorte que chaque image appartient à un cluster, qui devient son label.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig03.png"
height="75%" width="75%" /><br>
<b>Figure 3</b> : Etape du cluster
</center>


<!--
#### Fit: Predict Cluster Assignment

For this step, we train a network from scratch to predict the pseudo labels of images. These pseudo labels are what we obtained in the first step through clustering.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig04.png"
height="75%" width="75%"/><br>
<b>Fig. 4</b>: Predict step
</center>

A standard pretrain and transfer task first pretrains a network and then evaluates it in downstream tasks, as it is shown in the first row of Fig. 5. ClusterFit performs the pretraining on a dataset $D_{cf}$ to get the pretrained network $N_{pre}$. The pretrained network $N_{pre}$ are performed on dataset $D_{cf}$ to generate clusters. We then learn a new network $N_{cf}$from scratch on this data. Finally, use $N_{cf}$ for all downstream tasks.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig05.png"
height="75%" width="75%"/><br>
<b>Fig. 5</b>: "Standard" pretrain + transfer *vs.* "Standard" pretrain + ClusterFit
</center>
-->

#### Formation : Prévoir l'affectation des clusters

Pour cette étape, nous entraînons un réseau à partir de zéro afin de prévoir les pseudo labels des images. Ces pseudo labels sont ceux que nous avons obtenus lors de la première étape par le clustering.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig04.png"
height="75%" width="75%"/><br>
<b>Fig. 4</b> : Etape de prédiction
</center>

Une tâche standard de pré-entraînement et de transfert pré-entraîne d'abord un réseau et l'évalue ensuite sur des tâches en aval, comme le montre la première ligne de la figure 5. ClusterFit effectue le pré-entraînement sur un ensemble de données $D_{cf}$ pour obtenir le réseau pré-entraîné $N_{pre}$. Le pré-entraînement $N_{pre}$ est effectué sur un ensemble de données $D_{cf}$ pour générer des clusters. Nous apprenons ensuite un nouveau réseau $N_{cf}$ à partir de zéro sur ces données. Enfin, on utilise $N_{cf}$ pour toutes les tâches en aval.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig05.png"
height="75%" width="75%"/><br>
<b>Fig. 5</b> : Pré entraînement "standard" + transfert *vs.* Pré entraînement "standard" + ClusterFit
</center>

<!--
### Why ClusterFit Works

The reason why ClusterFit works is that in the clustering step only the essential information is captured, and artefacts are thrown away making the second network learn something slightly more generic.

To understand this point, a fairly simple experiment is performed.  We add label noise to ImageNet-1K, and train a network based on this dataset. Then, we evaluate the feature representation from this network on a downstream task on ImageNet-9K. As it is shown in Fig. 6, we add different amounts of label noise to the ImageNet-1K, and evaluate the transfer performance of different methods on ImageNet-9K.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig06.png"
height="50%" width="50%"/>
<br><b>Fig. 6</b>: Control Experiment
</center>

The pink line shows the performance of pretrained network, which decreases as the amount of label noise increases. The blue line represents model distillation where we take the initial network and use it to generate labels. Distillation generally performs better than pretrained network.  The green line, ClusterFit, is consistently better than either of these methods. This result validates our hypothesis.

* Question: Why use distillation method to compare. What's the difference between distillation and ClusterFit?

In model distillation we take the pre-trained network and use the labels the network predicted in a softer fashion to generate labels for our images. For example, we get a distribution over all the classes and use this distribution to train the second network. The softer distribution helps enhance the initial classes that we have. In ClusterFit we don't care about the label space.
-->

### Pourquoi ClusterFit fonctionne ?

La raison pour laquelle ClusterFit fonctionne est que lors de l'étape de clustering, seules les informations essentielles sont saisies et les artefacts sont jetés, ce qui permet au second réseau d'apprendre quelque chose de légèrement plus générique.

Pour comprendre ce point, une expérience assez simple est réalisée.  Nous ajoutons un bruit de label à ImageNet-1K, et entraînons un réseau basé sur cet ensemble de données. Ensuite, nous évaluons la représentation des caractéristiques de ce réseau sur une tâche en aval sur ImageNet-9K. Comme le montre la figure 6, nous ajoutons différentes quantités de bruit de label au réseau ImageNet-1K et nous évaluons les performances de transfert de différentes méthodes sur ImageNet-9K.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig06.png"
height="50%" width="50%"/>
<br><b>Fig. 6</b> : Expérience de contrôle
</center>

La ligne rose indique les performances du réseau pré-entraîné, qui diminuent à mesure que le bruit du label augmente. La ligne bleue représente la distillation du modèle où nous prenons le réseau initial et l'utilisons pour générer des labels. La distillation est généralement plus performante que le réseau pré-entraîné.  La ligne verte, ClusterFit, est toujours meilleure que l'une ou l'autre de ces méthodes. Ce résultat valide notre hypothèse.

* Question : Pourquoi utiliser la méthode de distillation pour comparer ? Quelle est la différence entre la distillation et ClusterFit ?

Dans la distillation de modèle, nous prenons le réseau pré-entraîné et utilisons les labels que le réseau a prédits de manière plus douce pour générer des labels pour nos images. Par exemple, nous obtenons une répartition sur toutes les classes et utilisons cette répartition pour entraîner le second réseau. La distribution plus douce permet d'améliorer les classes initiales que nous avons. Dans ClusterFit, nous ne nous soucions pas de l'espace du label.

<!--
### Performance

We apply this method to self-supervised learning. Here Jigsaw is applied to obtain the pretrained network $N_{pre}$ in ClusterFit. From Fig. 7 we see that the transfer performance on different datasets shows a surprising amount of gains, compared to other self-supervised methods.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig07.png"
height="70%" width="70%"/><br><b>Fig. 7</b>: Transfer performance on different datasets
</center>

ClusterFit works for any pre-trained network. Gains without extra data, labels or changes in architecture can be seen in Fig. 8. So in some way, we can think of the ClusterFit as a self-supervised fine-tuning step, which improves the quality of representation.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig08.png"
height="70%" width="70%"/><br><b>Fig. 8</b>: Gains without extra data, labels or changes in architecture
</center>
-->

### Performance

Nous appliquons cette méthode à l'apprentissage auto-supervisé. Ici, Jigsaw est appliqué pour obtenir le réseau pré-entraîné $N_{pre}$ dans ClusterFit. La figure 7 montre que les performances de transfert sur différents ensembles de données montrent des gains surprenants par rapport à d'autres méthodes auto-supervisées.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig07.png"
height="70%" width="70%"/><br><b>Fig. 7</b> : Performances de transfert sur différents jeux de données
</center>

ClusterFit fonctionne pour tout réseau ayant reçu un entraînement préalable. Les gains sans données supplémentaires, labels ou changements d'architecture sont visibles dans la figure 8. D'une certaine manière, on peut donc considérer le ClusterFit comme une étape de fine-tuning auto-supervisée, qui améliore la qualité de la représentation.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig08.png"
height="70%" width="70%"/><br><b>Fig. 8</b> : Des gains sans données supplémentaires, sans labels ni changements d'architecture
</center>

<!--
## [Self-supervised Learning of Pretext Invariant Representations (PIRL)](https://www.youtube.com/watch?v=0KeR6i1_56g&t=4748s)
-->

## [Apprentissage auto-supervisé des représentations invariantes du prétexte (Pretext Invariant Representations  - PIRL)](https://www.youtube.com/watch?v=0KeR6i1_56g&t=4748s)

<!--
### Contrastive Learning

Contrastive learning is basically a general framework that tries to learn a feature space that can combine together or put together points that are related and push apart points that are not related.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig09.png"
height="20%" width="20%"/><br><b>Fig. 9</b>: Groups of Related and Unrelated Images
</center>

In this case, imagine like the blue boxes are the related points, the greens are related, and the purples are related points.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig10.png"
height="100%" width="100%"/><br><b>Fig. 10</b>: Contrastive Learning and Loss Function
</center>

Features for each of these data points would be extracted through a shared network, which is called Siamese Network to get a bunch of image features for each of these data points. Then a contrastive loss function is applied to try to minimize the distance between the blue points as opposed to, say, the distance between the blue point and the green point. Or the distance basically between the blue points should be less than the distance between the blue point and green point or the blue point and the purple point. So, embedding space from the related samples should be much closer than embedding space from the unrelated samples. So that's the general idea of what contrastive learning is and of course Yann was one of the first teachers to propose this method. So contrastive learning is now making a resurgence in self-supervised learning pretty much a lot of the self-supervised state of the art methods are really based on contrastive learning.
-->

### Apprentissage contrastif

L'apprentissage contrastif est essentiellement un cadre général qui tente d'apprendre un espace de caractéristiques qui peut combiner ou rassembler des points qui sont liés et écarter des points qui ne sont pas liés.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig09.png"
height="20%" width="20%"/><br><b>Fig. 9</b> : Groupes d'images liées et non liées
</center>

Dans ce cas, imaginez que les cases bleues sont les points liés, les verts sont liés et les violets sont les points liés.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig10.png"
height="100%" width="100%"/><br><b>Fig. 10</b> : Fonction d'apprentissage et de perte contrastive
</center>

Les caractéristiques de chacun de ces points de données seraient extraites par le biais d'un réseau partagé, appelé "réseau siamois", afin d'obtenir un ensemble de caractéristiques d'images pour chacun de ces points de données. Ensuite, une fonction de perte contrastive est appliquée pour essayer de minimiser la distance entre les points bleus par opposition, par exemple, à la distance entre le point bleu et le point vert. Ou alors, la distance entre les points bleus devrait être inférieure à la distance entre le point bleu et le point vert ou le point bleu et le point violet. Ainsi, l'espace d'enchâssement des échantillons apparentés devrait être beaucoup plus proche que l'espace d'enchâssement des échantillons non apparentés. C'est donc l'idée générale de ce qu'est l'apprentissage contrastif et, bien sûr, Yann a été l'un des premiers à proposer cette méthode. L'apprentissage contrastif fait donc maintenant un retour en force dans l'apprentissage auto-supervisé ; une grande partie des méthodes de pointe auto-supervisées sont en fait basées sur l'apprentissage contrastif.

<!--
### How to define related or unrelated?

And the main question is how to define what is related and unrelated. In the case of supervised learning that's fairly clear all of the dog images are related images, and any image that is not a dog is basically an unrelated image. But it's not so clear how to define the relatedness and unrelatedness in this case of self-supervised learning. The other main difference from something like a pretext task is that contrastive learning really reasons a lot of data at once.  If you look at the loss function, it always involves multiple images. In the first row it involves basically the blue images and the green images and in the second row it involves the blue images and the purple images. But as if you look at a task like say Jigsaw or a task like rotation, you're always reasoning about a single image independently. So that's another difference with contrastive learning: contrastive learning reasons about multiple data points at once.

Similar techniques to what was discussed earlier could be used: frames of video or sequential nature of data. Frames that are nearby in a video are related and frames, say, from a different video or which are further away in time are unrelated. And that has formed the basis of a lot of self- supervised learning methods in this area. This method is called CPC, which is contrastive predictive coding, which relies on the sequential nature of a signal and it basically says that samples that are close by, like in the time-space, are related and samples that are further apart in the time-space are unrelated. A fairly large amount of work basically exploiting this: it can either be in the speech domain, video, text, or particular images. And recently, we've also been working on video and audio so basically saying a video and its corresponding audio are related samples and video and audio from a different video are basically unrelated samples.
-->

### Comment définir ce qui est lié ou non lié ?

Et la question principale est de savoir comment définir ce qui est lié et ce qui ne l'est pas. Dans le cas de l'apprentissage supervisé, il est assez clair que toutes les images de chiens sont des images liées, et toute image qui n'est pas un chien est fondamentalement une image non liée. Mais il n'est pas aussi clair de définir ce qui est lié et ce qui ne l'est pas dans le cas d'apprentissage auto-supervisé. L'autre grande différence avec une tâche de prétexte est que l'apprentissage contrastif raisonne avec vraiment beaucoup de données à la fois.  Si vous regardez la fonction de perte, elle implique toujours plusieurs images. Dans la première ligne, il s'agit essentiellement d'images bleues et d'images vertes, et dans la deuxième ligne, d'images bleues et d'images violettes. Mais comme si vous regardiez une tâche comme par exemple Jigsaw ou une tâche comme la rotation, vous raisonnez toujours sur une seule image de manière indépendante. C'est donc une autre différence avec l'apprentissage contrastif : l'apprentissage contrastif raisonne sur plusieurs points de données à la fois.

On pourrait utiliser des techniques similaires à celles qui ont été évoquées précédemment : des images vidéo ou la nature séquentielle des données. Les images qui sont proches dans une vidéo sont liées et les images, par exemple, d'une autre vidéo ou qui sont plus éloignées dans le temps ne sont pas liées. Et cela a constitué la base de nombreuses méthodes d'apprentissage auto-supervisées dans ce domaine. Cette méthode est appelée CPC (codage prédictif contrastif) repose sur la nature séquentielle d'un signal et dit essentiellement que les échantillons qui sont proches, comme dans l'espace-temps, sont liés et que les échantillons qui sont plus éloignés dans l'espace-temps ne sont pas liés. Une quantité assez importante de travaux exploite essentiellement ce principe : il peut s'agir de la parole, de la vidéo, du texte ou d'images particulières. Et récemment, nous avons également travaillé sur la vidéo et l'audio, ce qui signifie qu'une vidéo et son audio correspondant sont des échantillons liés et que la vidéo et l'audio d'une autre vidéo sont des échantillons non liés.

<!--
### Tracking Objects

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig11.png"
height="80%" width="80%"/><br><b>Fig. 11</b>: Tracking the Objects
</center>

Some of the early work, like self-supervised learning, also uses this contrastive learning method and they really defined related examples fairly interestingly. You run a tracked object tracker over a video and that gives you a moving patch and what you say is that any patch that was tracked by the tracker is related to the original patch. Whereas, any patch from a different video is not a related patch. So that basically gives out these bunch of related and unrelated samples. In figure 11(c), you have this like distance notation. What this network tries to learn is basically that patches that are coming from the same video are related and patches that are coming from different videos are not related. In some way, it automatically learns about different poses of an object. It tries to group together a cycle, viewed from different viewing angles or different poses of a dog.
-->

### Suivi des objets (tracking)

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig11.png"
height="80%" width="80%"/><br><b>Fig. 11</b> : Suivi des objets
</center>

En passant un tracker d'objets sur une vidéo cela vous donne un patch mobile et ce que celui-ci vous dites est que tout patch qui a été suivi par le tracker est lié au patch original. En revanche, tout patch provenant d'une autre vidéo n'est pas un patch apparenté. Cela donne donc un ensemble d'échantillons liés et non liés. Dans la figure 11(c), vous avez cette notation de distance. Ce que ce réseau essaie d'apprendre, c'est que les patchs provenant d'une même vidéo sont liés et que les patchs provenant de vidéos différentes ne sont pas liés. D'une certaine manière, il apprend automatiquement les différentes poses d'un objet. Il essaie de regrouper un cycle, vu sous différents angles de vue ou différentes poses d'un chien.

<!--
### Nearby patches *vs.* distant patches of an Image

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig12.png"
height="50%" width="50%"/><br><b>Fig. 12</b>: Nearby patches *vs.* distant patches of an Image
</center>

In general, talking about images, a lot of work is done on looking at nearby image patches versus distant patches, so most of the CPC v1 and CPC v2 methods are really exploiting this property of images. So image patches that are close are called as positives and image patches that are further apart are translated as negatives, and the goal is to minimize the contrastive loss using this definition of positives and negatives.
-->

### Patchs proches *vs.* patchs lointains d'une image

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig12.png"
height="50%" width="50%"/><br><b>Fig. 12</b> : Patchs proches *vs.* patchs éloignés d'une image
</center>

En général, en ce qui concerne les images, beaucoup de travail est fait sur l'examen des correctifs d'images proches par rapport aux correctifs éloignés, donc la plupart des méthodes CPC v1 et CPC v2 exploitent vraiment cette propriété des images. Ainsi, les plages d'images proches sont appelées positives et les plages d'images plus éloignées sont traduites en négatifs, et le but est de minimiser la perte de contraste en utilisant cette définition des positifs et des négatifs.

<!--
### Patches of an image *vs.* patches of other images

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig13.png"
height="60%" width="60%"/><br><b>Fig. 13</b>: Patches of an Image *vs.* Patches of Other Images
</center>

The more popular or performant way of doing this is to look at patches coming from an image and contrast them with patches coming from a different image. This forms the basis of a lot of popular methods like instance discrimination, MoCo, PIRL, SimCLR. The idea is basically what's shown in the image. To go into more details, what these methods do is to extract completely random patches from an image. These patches can be overlapping, they can actually become contained within one another or they can be completely falling apart and then apply some data augmentation. In this case, say, a colour chattering or removing the colour or so on. And then these two patches are defined to be positive examples. Another patch is extracted from a different image. And this is again a random patch and that basically becomes your negatives. And a lot of these methods will extract a lot of negative patches and then they will basically perform contrastive learning. So there are relating two positive samples, but there are a lot of negative samples to do contrastive learning against.
-->

### Patchs d'une image *vs.* patchs d'autres images

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig13.png"
height="60%" width="60%"/><br><b>Fig. 13</b> : Patchs d'une image *vs.* Patchs d'autres images
</center>

La façon la plus populaire ou la plus performante de procéder consiste à regarder les patchs provenant d'une image et à les mettre en contraste avec les patchs provenant d'une autre image. Ceci constitue la base de nombreuses méthodes populaires comme la discrimination d'instance, MoCo, PIRL, SimCLR. L'idée est essentiellement ce qui est montré dans l'image. Pour aller plus loin dans les détails, ces méthodes consistent à extraire d'une image des taches complètement aléatoires. Ces patchs peuvent se chevaucher, être contenus les uns dans les autres ou s'effondrer complètement, puis voir appliquer une augmentation des données. Prenons le cas, par exemple, d’une couleur qui se brouille ou qui s'efface. Ces deux taches sont alors définies comme des exemples positifs. Un autre patch est extrait d'une image différente. Il s'agit là encore d'une tache aléatoire qui devient en gros vos négatifs. Beaucoup de ces méthodes permettent d'extraire de nombreuses taches négatives et d'effectuer un apprentissage contrastif. Il y a donc deux échantillons positifs liés, mais il y a beaucoup d'échantillons négatifs contre lesquels on peut faire un apprentissage contrastif.

<!--
### Underlying Principle for Pretext Tasks

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig14.png"
height="50%" width="50%"/><br><b>Fig. 14</b>: Pretext Image Transform and Standard Pretext Learning
</center>

Now moving to PIRL a little bit, and that's trying to understand what the main difference of pretext tasks is and how contrastive learning is very different from the pretext tasks. Again, pretext tasks always reason about a single image at once.  So the idea is that given an image your and prior transform to that image, in this case a Jigsaw transform, and then inputting this transformed image into a ConvNet and trying to predict the property of the transform that you applied to, the permutation that you applied or the rotation that you applied or the kind of colour that you removed and so on. So the pretext tasks always reason about a single image. And the second thing is that the task that you're performing in this case really has to capture some property of the transform. So it really needs to capture the exact permutation that are applied or the kind of rotation that are applied, which means that the last layer representations are actually going to go PIRL very a lot as the transform the changes and that is by design, because you're really trying to solve that pretext tasks. But unfortunately, what this means is that the last layer representations capture a very low-level property of the signal. They capture things like rotation or so on. Whereas what is designed or what is expected of these representations is that they are invariant to these things that it should be able to recognize a cat, no matter whether the cat is upright or that the cat is say,  bent towards like by 90 degrees. Whereas when you're solving that particular pretext task you're imposing the exact opposite thing. We're saying that we should be able to recognize whether this picture is upright or whether this picture is basically turning it sideways. There are many exceptions in which you really want these low-level representations to be covariant and a lot of it really has to do with the tasks that you're performing and quite a few tasks in 3D really want to be predictive. So you want to predict what camera transforms you have: you're looking at two views of the same object or so on. But unless you have that kind of a specific application for a lot of semantic tasks, you really want to be invariant to the transforms that are used to use that input.
-->

### Principe sous-jacent pour les tâches de prétexte

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig14.png"
height="50%" width="50%"/><br><b>Fig. 14</b> : Transformation des images du prétexte et apprentissage du prétexte standard
</center>

Passons maintenant un peu à PIRL, et essayons de comprendre quelle est la principale différence entre les tâches de prétexte et comment l'apprentissage contrastif est très différent des tâches de prétexte. Là encore, les tâches de prétexte raisonnent toujours à propos d'une seule image à la fois.  L'idée est donc qu'en donnant à une image votre transformation préalable (une transformation Jigsaw ici), puis en entrant cette image transformée dans un ConvNet et vous essayez de prédire la propriété de la transformation que vous avez appliquée (permutation, rotation, changement de couleur, etc…). Ainsi, les tâches de prétexte raisonnent toujours à propos d'une seule image. Et la deuxième chose est que la tâche que vous effectuez dans ce cas doit vraiment capturer une certaine propriété de la transformation. Elle doit donc capturer la nature exacte de la transformation, ce qui veut dire que les représentations de la dernière couche vont en fait passer très souvent au fur et à mesure que la transformation change. Mais malheureusement, cela signifie que les représentations de la dernière couche capturent une propriété de très bas niveau du signal. Elles capturent des choses comme la rotation, etc... Alors que ce qui est conçu ou ce que l'on attend de ces représentations, c'est qu'elles soient invariantes à ces choses qu'il devrait être capable de reconnaître un chat, peu importe si le chat est debout ou que le chat est tourné à 90 degrés. Alors que lorsque vous résolvez cette tâche de prétexte particulier, vous imposez la chose exactement inverse. Nous disons que nous devrions être capables de reconnaître si cette image est droite ou si elle est en fait en train de la tourner sur le côté. Il y a de nombreuses exceptions où l'on veut vraiment que ces représentations de bas niveau soient covariantes, et une grande partie de ces exceptions est liée aux tâches que vous effectuez. Vous voulez donc prédire les transformations de la caméra : deux vues du même objet, etc... Mais à moins d'avoir ce genre d'application spécifique pour de nombreuses tâches sémantiques, le souhait est d’être invariant aux transformations qui sont utilisées pour utiliser cette entrée.

<!--
### How important has invariance been?

Invariance has been the word course for feature learning. Something like SIFT, which is a fairly popular handcrafted feature where we inserted here is transferred invariant. And supervise networks, for example, supervised Alex nets, they're trained to be invariant data augmentation. You want this network to classify different crops or different rotations of this image as a tree, rather than ask it to predict what exactly was the transformation applied for the input.
-->

### Quelle est l'importance de l'invariance ?

L'invariance a été le mot d'ordre pour l'apprentissage des caractéristiques. SIFT qui est une fonctionnalité artisanale assez populaire est transféré invariant. Les réseaux de supervision, par exemple, les réseaux AlexNet, sont entraînés à être invariants pour l'augmentation des données. Vous voulez que ce réseau classifie différentes générations ou différentes rotations de cette image comme un arbre, plutôt que de lui demander de prédire quelle a été exactement la transformation appliquée pour l'entrée.


<!--
### PIRL

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig15.png"
height="70%" width="70%"/><br><b>Fig. 15</b>: PIRL
</center>

This is what inspired PIRL. So PIRL stands for pretext invariant representation learning, where the idea is that you want the representation to be invariant or capture as little information as possible of the input transform. So you have the image, you have the transformed version of the image, you feed-forward both of these images through a ConvNet, you get a representation and then you basically encourage these representations to be similar. In terms of the notation referred earlier, the image $I$ and any pretext transformed version of this image $I^t$ are related samples and any other image is underrated samples. So in this way when you frame this network, representation hopefully contains very little information about this transform $t$. And assume you are using contrastive learning. So the contrastive learning part is basically you have the saved feature $v_I$ coming from the original image $I$ and you have the feature $v_{I^t}$ coming from the transform version and you want both of these representations to be the same. And the book paper we looked at is two different states of the art of the pretext transforms, which is the jigsaw and the rotation method discussed earlier. In some way, this is like multitask learning, but just not really trying to predict both designed rotation. You're trying to be invariant of Jigsaw rotation.
-->

### PIRL

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig15.png"
height="70%" width="70%"/><br><b>Fig. 15</b> : PIRL
</center>

C'est ce qui a inspiré PIRL. PIRL est donc l'acronyme de pretext invariant representation learning, où l'idée est que vous voulez que la représentation soit invariante ou capture le moins d'informations possible de la transformation d'entrée. Ainsi, vous avez l'image, vous avez la version transformée de l'image, vous faites suivre ces deux images par un ConvNet, vous obtenez une représentation et ensuite vous encouragez essentiellement ces représentations à être similaires. En ce qui concerne la notation mentionnée plus haut, l'image $I$ et toute version transformée de cette image $I^t$ sont des échantillons apparentés et toute autre image est un échantillon sous-estimé. Ainsi, lorsque vous encadrez ce réseau, la représentation contient, espérons-le, très peu d'informations sur cette transformée $t$. 
Supposons que vous utilisiez l'apprentissage contrastif. La partie apprentissage contrastif consiste donc à avoir la fonction sauvegardée $v_I$ provenant de l'image originale $I$ et la fonction $v_{I^t}$ provenant de la version transformée et à vouloir que ces deux représentations soient identiques. D'une certaine manière, c'est comme un apprentissage multitâche, mais sans vraiment essayer de prédire les deux rotations conçues. Vous essayez d'être invariant de la rotation de Jigsaw.

<!--
###  Using a Large Number of Negatives

The key thing that has made contrastive learning work well in the past, taking successful attempts is using a large number of negatives. [One of the good paper](https://arxiv.org/abs/1805.01978) taking successful attempts, is instance discrimination paper from 2018, which introduced this concept of a memory bank. This is powered, most of the research methods which are state of the art techniques hinge on this idea for a memory bank. The memory bank is a nice way to get a large number of negatives without really increasing the sort of computing requirement. What you do is you store a feature vector per image in memory, and then you use that feature vector in your contrastive learning.
-->

### Utilisation d'un grand nombre de négatifs

L'élément clé qui a fait que l'apprentissage contrastif a bien fonctionné dans le passé est l'utilisation d'un grand nombre de négatifs. L'un des [bons papiers](https://arxiv.org/abs/1805.01978) qui a réussi, est le celui de 2018 qui a introduit le concept de banque de mémoire. C'est un outil puissant, la plupart des méthodes de recherche qui sont des techniques de pointe s'articulent autour de cette idée. La banque de mémoire est un bon moyen d'obtenir un grand nombre de négatifs sans vraiment augmenter le type de besoin informatique. Ce que vous faites, c'est que vous stockez un vecteur de caractéristiques par image dans la mémoire, puis vous utilisez ce vecteur de caractéristiques dans votre apprentissage contrastif.

<!--
### How it works

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig16.png"
height="75%" width="75%"/><br><b>Fig. 16</b>: How does the Memory Bank Work
</center>

Let's first talk about how you would do this entire PIRL setup without using a memory bank. So you have an image $I$ and you have an image $I^t$, and you feed-forward both of these images, you get a feature vector $f(v_I)$ from the original image $I$, you get a feature $g(v_{I^t})$ from the transform versions, the patches, in this case. What you want is the features $f$ and $g$ to be similar. And you want features from any other unrelated image to basically be dissimilar. In this case, what we can do now is if you want a lot of negatives, we would really want a lot of these negative images to be feed-forward at the same time, which really means that you need a very large batch size to be able to do this. Of course, a large batch size is not really good, if not possible, on a limited amount of GPU memory. The way to do that is to use something called a memory bank. So what this memory bank does is that it stores a feature vector for each of the images in your data set, and when you're doing contrastive learning rather than using feature vectors, say, from a different from a negative image or a different image in your batch, you can just retrieve these features from memory. You can just retrieve features of any other unrelated image from the memory and you can just substitute that to perform contrastive learning. Simply dividing the objective into two parts, there was a contrasting term to bring the feature vector from the transformed image $g(v_I)$, similar to the representation that we have in the memory so $m_I$. And similarly, we have a second contrastive term that tries to bring the feature $f(v_I)$ close to the feature representation that we have in memory. Essentially $g$ is being pulled close to $m_I$ and $f$ is being pulled close to $m_I$. By transitivity, $f$ and $g$ are being pulled close to one another. And the reason for separating this out was that it stabilized training and we were not able to train without doing this. Basically, the training would not really converge. By separating this out into two forms, rather than doing direct contrastive learning between $f$ and $g$, we were able to stabilize training and actually get it working.
-->

### Comment cela focntionne

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig16.png"
height="75%" width="75%"/><br><b>Fig. 16</b> : Comment fonctionne la banque de mémoire
</center>

Parlons d'abord de la façon dont vous feriez toute cette installation PIRL sans utiliser une banque de mémoire. Vous avez donc une image $I$ et une image $I^t$, et vous faites suivre ces deux images. Vous obtenez un vecteur de caractéristique $f(v_I)$ de l'image originale $I$, vous obtenez une caractéristique $g(v_{I^t})$ des versions de transformation, les patches, dans ce cas. Ce que vous voulez, c'est que les caractéristiques $f$ et $g$ soient similaires. Et vous voulez que les caractéristiques de toute autre image sans rapport soient fondamentalement différentes. Ce que nous voudrions maintenant est que beaucoup d’images négatives soient alimentées en même temps, ce qui signifie que vous avez besoin d'une très grande taille de lot pour pouvoir faire cela. Bien sûr, une grande taille de lot n'est pas vraiment bonne pour la mémoire. Le moyen d'y parvenir est d'utiliser ce qu'on appelle une banque de mémoire. Ainsi, cette banque de mémoire stocke un vecteur de caractéristiques pour chacune des images de votre ensemble de données, et lorsque vous faites un apprentissage contrastif plutôt que d'utiliser des vecteurs de caractéristiques, par exemple, à partir d'une image différente d'une image négative ou d'une image différente de votre lot, vous pouvez simplement récupérer ces caractéristiques en mémoire. En divisant l'objectif en deux parties, un terme contrasté a été utilisé pour amener le vecteur de caractéristiques de l'image transformée $g(v_I)$, similaire à la représentation que nous avons en mémoire, donc $m_I$. Et de même, nous avons un second terme contrasté qui tente de rapprocher la caractéristique $f(v_I)$ de la représentation de la caractéristique que nous avons en mémoire. Essentiellement, $g$ est rapproché de $m_I$ et $f$ est rapproché de $m_I$. Par transitivité, $f$ et $g$ sont rapprochés l'un de l'autre. Et la raison de cette séparation est que cela stabilise l'entraînement et que nous ne pouvons pas nous entraîner sans cela. En gros, l'entraînement ne convergeait pas vraiment. En séparant les deux types de formation, plutôt que de faire un apprentissage direct et contrasté entre $f$ et $g$, nous avons pu stabiliser l'entraînement et le faire fonctionner.

<!--
### PIRL Pre-training

The way to evaluate this is basically by standard pre-training evaluation set-up. For transfer learning, we can pretrain on images without labels. The standard way of doing this is to take an image net, throw away the labels and pretend as unsupervised.
-->

### PIRL Pré-entraînement

L'évaluation se fait essentiellement par le biais d'une évaluation standard avant l'entraînement. Pour l'apprentissage par transfert, nous pouvons faire un pré-entraînement sur des images sans label. La méthode standard consiste à prendre un réseau d'images, à jeter les labels et à faire semblant de ne pas être supervisé.

<!--
### [Evaluation](https://www.youtube.com/watch?v=0KeR6i1_56g&t=5889s)

Evaluation can be performed by full fine-tuning (initialisation evaluation) or training a linear classifier (feature evaluation). PIRL robustness has been tested by using images in-distribution by training it on in-the-wild images. So we just took 1 million images randomly from Flickr, which is the YFCC data set. And then we basically performed pre-training on these images and then performed transplanting on different data sets.
-->

### [Evaluation](https://www.youtube.com/watch?v=0KeR6i1_56g&t=5889s)

L'évaluation peut être effectuée par un fine tuning complet (évaluation d'initialisation) ou par l'entraînement d'un classificateur linéaire (évaluation de caractéristiques). La robustesse du PIRL a été testée par l'utilisation d'images en distribution en l'entraînant sur des images en milieu naturel. Nous avons donc pris 1 million d'images au hasard sur Flickr, qui est l'ensemble de données du YFCC. Nous avons ensuite effectué un entraînement préalable sur ces images, puis nous avons effectué des greffes sur différents ensembles de données.

<!--
#### Evaluating on Object Detection task

PIRL was first evaluated on object detection task (a standard task in vision) and it was able to outperform ImageNet supervised pre-trained networks on both **VOC07+12** and **VOC07** data sets. In fact, PIRL outperformed even in the stricter evaluation criteria, $AP^{all}$ and that's a good positive sign.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig17.png"
height="70%" width="70%"/><br><b>Fig. 17</b>: Object detection performance on different datasets
</center>
-->

#### Evaluation sur la tâche de détection d'objets

PIRL a d'abord été évalué sur la tâche de détection d'objets (une tâche standard en vision) et il a pu surpasser les réseaux pré-entraînés supervisés par ImageNet sur les ensembles de données **VOC07+12** et **VOC07**. En fait, PIRL a surpassé même les critères d'évaluation les plus stricts, $AP^{all}$ et c'est un bon signe positif.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig17.png"
height="70%" width="70%"/><br><b>Fig. 17</b> : Performances de détection d'objets sur différents jeux de données
</center>

<!--
#### Evaluating on Semi-supervised Learning

PIRL was then evaluated on semi-supervised learning task. Again, PIRL performed fairly well. In fact, PIRL was better than even the pre-text task of Jigsaw. The only difference between the first row and the last row is that, PIRL is an invariant version, whereas Jigsaw is a covariant version.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig18.png"
height="70%" width="70%"/><br><b>Fig. 18</b>: Semi-supervised learning on ImageNet
</center>
-->

#### Évaluation sur l'apprentissage semi-supervisé

PIRL a ensuite été évalué sur une tâche d'apprentissage semi-supervisée. Là encore, il a obtenu d'assez bons résultats. En fait, il a même été meilleur que la tâche pré-texte de Jigsaw. La seule différence entre la première et la dernière rangée est que PIRL est une version invariante, alors que Jigsaw est une version covariante.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig18.png"
height="70%" width="70%"/><br><b>Fig. 18</b> : Apprentissage semi-supervisé sur ImageNet
</center>

<!--
#### Evaluating on Linear Classification

Now when evaluating on Linear Classifiers, PIRL was actually on par with the CPCv2, when it came out. It also worked well on a bunch of parameter settings and a bunch of different architectures. And of course, now you can have fairly good performance by methods like SimCLR or so. In fact, the Top-1 Accuracy for SimCLR would be around 69-70, whereas for PIRL, that'd be around 63.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig19.png"
height="75%" width="75%"/><br><b>Fig. 19</b>: ImageNet classification with linear models
</center>
-->

#### Évaluation sur la classification linéaire

Lors de l'évaluation des Linear Classifiers, PIRL était au même niveau que le CPCv2, lorsqu'il est sorti. Il a également bien fonctionné sur un certain nombre de paramètres et d'architectures différentes. Maintenant vous pouvez avoir des performances assez bonnes avec des méthodes comme SimCLR ou autres. En fait, la précision Top-1 pour SimCLR serait d'environ 69-70, alors que pour PIRL, elle serait d'environ 63.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig19.png"
height="75%" width="75%"/><br><b>Fig. 19</b> : Classification ImageNet avec modèles linéaires
</center>

<!--
#### Evaluating on YFCC images

PIRL was evaluated on *"In-the-wild" Flickr* images from the YFCC data set. It was able to perform better than Jigsaw, even with $100$ times smaller data set. This shows the power of taking invariance into consideration for the representation in the pre-text tasks, rather than just predicting pre-text tasks.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig20.png"
height="70%" width="70%"/><br><b>Fig. 20</b>: Pre-training on uncurated YFCC images
</center>
-->

#### Evaluation sur les images du YFCC

PIRL a été évalué sur des images Flickr "dans la nature" provenant de l'ensemble des données du YFCC. Il a été plus performant que Jigsaw, même avec un ensemble de données $100$ fois plus petit. Cela montre la puissance de la prise en compte de l'invariance pour la représentation dans les tâches de pré-texte, plutôt que de simplement prédire les tâches de pré-texte.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig20.png"
height="70%" width="70%"/><br><b>Fig. 20</b> : Pré-entraînement sur les images non traitées du YFCC
</center>

<!--
### Semantic Features

Now, going back to verifying the semantic features, we look at the Top-1 accuracy for PIRL and Jigsaw for different layers of representation from `conv1` to `res5`. It's interesting to note that the accuracy keeps increasing for different layers for both PIRL and Jigsaw, but drops in the 5th layer for Jigsaw. Whereas, the accuracy keeps improving for PIRL, *i.e.* more and more semantic.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig21.png"
height="70%" width="70%"/><br><b>Fig. 21</b>: Quality of PIRL representations per layer
</center>
-->


### Caractéristiques sémantiques

Maintenant, pour en revenir à la vérification des caractéristiques sémantiques, nous examinons la précision Top-1 pour PIRL et Jigsaw pour différentes couches de représentation de "conv1" à "res5". Il est intéressant de noter que la précision continue d'augmenter pour les différentes couches pour PIRL et Jigsaw, mais qu'elle diminue dans la 5e couche pour Jigsaw. En revanche, la précision continue à s'améliorer pour PIRL, *c'est-à-dire de plus en plus sémantique*.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig21.png"
height="70%" width="70%"/><br><b>Fig. 21</b> : Qualité des représentations PIRL par couche
</center>

<!--
### Scalability

PIRL is very good at handling problem complexity because you're never predicting the number of permutations, you're just using them as input. So, PIRL can easily scale to all 362,880 possible permutations in the 9 patches. Whereas in Jigsaw, since you're predicting that, you're limited by the size of your output space.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig22.png"
height="70%" width="70%"/><br><b>Fig. 22</b>: Effect of varying the number of patch permutations
</center>

The paper "*Misra & van der Maaten, 2019, PIRL*" also shows how PIRL could be easily extended to other pretext tasks like Jigsaw, Rotations  and so on. Further, it could even be extended to combinations of those tasks like Jigsaw+Rotation.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig23.png"
height="70%" width="70%"/><br><b>Fig. 23</b>: Using PIRL with (combinations of) different pretext tasks
</center>
-->

### Évolutivité

Le PIRL est très efficace pour gérer la complexité des problèmes, car on ne prédit jamais le nombre de permutations, on les utilise simplement comme données d'entrée. Ainsi, PIRL peut facilement s'adapter aux 362 880 permutations possibles dans les 9 patchs. Alors que dans Jigsaw, puisque vous prévoyez cela, vous êtes limité par la taille de votre espace de sortie.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig22.png"
height="70%" width="70%"/><br><b>Fig. 22</b> : Effet de la variation du nombre de permutations de patchs
</center>

Le document "*Misra & van der Maaten, 2019, PIRL*" montre également comment PIRL pourrait être facilement étendu à d'autres tâches prétextes comme le Jigsaw, les rotations, etc. En outre, elle pourrait même être étendue à des combinaisons de ces tâches comme Jigsaw+Rotation.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig23.png"
height="70%" width="70%"/><br><b>Fig. 23</b> : Utilisation de PIRL avec (combinaisons de) différentes tâches de prétexte
</center>

<!--
### Invariance *vs.* performance

In terms of invariance property, one could, in general, assert that the invariance of PIRL is more than that of the Clustering, which in turn has more invariance than that of the pretext tasks. And similarly, the performance to is higher for PIRL than Clustering, which in turn has higher performance than pretext tasks. This suggests that taking more invariance in your method could improve performance.
-->

### Invariance *vs.* performance

En termes de propriété d'invariance, on pourrait, en général, affirmer que l'invariance du PIRL est plus que celle du Clustering, qui à son tour a plus d'invariance que celle des tâches de prétexte. Et de même, la performance est plus élevée pour le PIRL que pour le Clustering, qui à son tour a une performance plus élevée que les tâches de prétexte. Cela suggère que le fait de prendre plus d'invariance dans votre méthode pourrait améliorer les performances.

<!--
### Shortcomings

1. It's not very clear as to which set of data transforms matter. Although Jigsaw works, it's not very clear why it works.
2. Saturation with model size and data size.
3. What invariances matter? (One could think about what invariances work for a particular supervised task in general as future work.)

So in general, we should try to predict more and more information and try to be as invariant as possible.
-->

### Lacunes

1. Il n'est pas très clair de savoir quel ensemble de données transforme la matière. Bien que Jigsaw fonctionne, on ne sait pas très bien pourquoi il fonctionne.
2. Saturation avec la taille du modèle et la taille des données.
3. Quelles sont les invariances importantes ? (On pourrait penser aux invariances qui fonctionnent pour une tâche supervisée particulière en général comme un travail futur).

Donc, en général, nous devrions essayer de prévoir de plus en plus d'informations et essayer d'être aussi invariants que possible.

<!--
## Some important questions asked as doubts
-->

## Quelques questions importantes posées comme des doutes

<!--
### Contrastive learning and batch norms

1. Wouldn't the network learn only a very trivial way of separating the negatives from the positives if the contrasting network uses the batch norm layer (as the information would then pass from one sample to the other)?

**Ans**: *In PIRL, no such phenomenon was observed, so just the usual batch norm was used*

2. So is it fine to use batch norms for any contrasting networks?

**Ans**: *In general, yeah. In SimCLR, a variant of the usual batch norm is used to emulate a large batch size. So, batch norm with maybe some tweaking could be used to make the training easier*

3. Does the batch norm work in the PIRL paper only because it's implemented as a memory bank - as all the representations aren't taken at the same time? (As batch norms aren't specifically used in the MoCo paper for instance)

**Ans**: *Yeah. In PIRL, the same batch doesn't have all the representations and possibly why batch norm works here, which might not be the case for other tasks where the representations are all correlated within the batch*

4. So, other than memory bank, are there any other suggestions how to go about for n-pair loss? Should we use AlexNet or others that don't use batch norm? Or is there a way to turn off the batch norm layer? (This is for a video learning task)

**Ans**: *Generally frames are correlated in videos, and the performance of the batch norm degrades when there are correlations. Also, even the simplest implementation of AlexNet actually uses batch norm. Because, it's much more stable when trained with a batch norm. You could even use a higher learning rate and you could also use for other downstream tasks. You could use a variant of batch norm for example, group norm for video learning task, as it doesn't depend on the batch size*
-->

### Apprentissage contrastif et batch normalisation

1. Le réseau n'apprendrait-il pas une façon très triviale de séparer les négatifs des positifs si le réseau utilise la couche de batch norm (car l'information passerait alors d'un échantillon à l'autre) ?

**Rep** : *Dans PIRL, aucun phénomène de ce type n'a été observé, donc seule la batch norm habituelle a été utilisée*

2. Est-il donc acceptable d'utiliser des batch norm pour des réseaux contrastifs ?

**Rep** : *En général, oui. Dans SimCLR, une variante de la batch norm habituelle est utilisée pour émuler une grande taille de lot. Ainsi, la batch norm avec peut-être quelques modifications pourrait être utilisée pour faciliter l'entraînement*.

3. La batch norm fonctionne-t-elle dans le papier PIRL uniquement parce qu'elle est mise en œuvre en tant que banque de mémoire - étant donné que toutes les représentations ne sont pas prises en même temps ? (Comme les batch norm ne sont pas spécifiquement utilisées dans le document MoCo par exemple)

**Rep** : *Oui. Dans PIRL, le même lot n'a pas toutes les représentations et peut-être pourquoi la batch norm fonctionne ici, ce qui pourrait ne pas être le cas pour d'autres tâches où les représentations sont toutes corrélées dans le batch*

4. Outre la banque de mémoire, existe-t-il d'autres suggestions sur la manière de procéder en cas de perte de n-paires ? Devrions-nous utiliser AlexNet ou d'autres qui n'utilisent pas la batch norm ? Ou existe-t-il un moyen de désactiver la couche de batch norm ? (Ceci est pour une tâche d'apprentissage vidéo)

**Rep** : *Généralement, les images sont corrélées dans les vidéos, et la performance de la batch norm se dégrade lorsqu'il y a des corrélations. De plus, même la plus simple des implémentations d'AlexNet utilise en fait la batch norm. En effet, il est beaucoup plus stable lorsqu'il est entraîné avec une batch norm. Vous pourriez même utiliser un taux d'apprentissage plus élevé et vous pourriez également l'utiliser pour d'autres tâches en aval. Vous pouvez utiliser une variante de la batch norm, par exemple, la groupe norm pour les tâches d'apprentissage vidéo, car elle ne dépend pas de la taille du batch*.


<!--
### Loss functions in PIRL

1. In PIRL, why is NCE (Noise Contrastive Estimator) used for minimizing loss and not just the negative probability of the data distribution: $h(v_{I},v_{I^{t}})$?

**Ans**: *Actually, both could be used. The reason for using NCE has more to do with how the memory bank paper was set up. So, with $k+1$ negatives, it's equivalent to solving $k+1$ binary problem. Another way of doing it is using a softmax, where you apply a softmax and minimize the negative log-likelihood*
-->

### Fonctions de perte dans PIRL

1. Dans PIRL, pourquoi utilise-t-on NCE (Noise Contrastive Estimator) pour minimiser les pertes et pas seulement la probabilité négative de la distribution des données : $h(v_{I},v_{I^{t}})$ ?

**Rep** : *En fait, les deux pourraient être utilisés. La raison de l'utilisation de NCE a plus à voir avec la façon dont le papier de la banque de données a été mis en place. Ainsi, avec $k+1$ négatifs, cela équivaut à résoudre $k+1$ problème binaire. Une autre façon de procéder est d'utiliser un softmax, où vous appliquez un softmax et minimisez la log-vraisemblance négative*

<!--
### Self-supervised learning project related tips

How do we get a simple self-supervised model working? How do we begin the implementation?

**Ans**: *There are a certain class of techniques that are useful for the initial stages. For instance, you could look at the pretext tasks. Rotation is a very easy task to implement. The number of moving pieces are in general good indicator. If you're planning to implement an existing method, then you might have to take a closer look at the details mentioned by the authors, like - the exact learning rate used, the way batch norms were used, etc. The more number of these things, the harder the implementation. Next very critical thing to consider is data augmentation. If you get something working, then add more data augmentation to it.* 
-->

### Conseils relatifs aux projets d'apprentissage auto-supervisés

Comment faire fonctionner un modèle simple et auto-supervisé ? Comment en amorcer la mise en œuvre ?

**Rep** : *Il existe une certaine classe de techniques qui sont utiles pour les étapes initiales. Par exemple, vous pouvez examiner les tâches de prétexte. La rotation est une tâche très facile à mettre en œuvre. Le nombre de pièces en mouvement est en général un bon indicateur. Si vous envisagez de mettre en œuvre une méthode existante, vous devrez peut-être examiner de plus près les détails mentionnés par les auteurs, comme le taux d'apprentissage exact utilisé, la manière dont les normes de lot ont été utilisées, etc. Plus ces éléments sont nombreux, plus la mise en œuvre est difficile. Le prochain point très important à prendre en compte est l'augmentation des données. Si quelque chose fonctionne, il faut y ajouter une augmentation des données*.

<!--
### Generative models

Have you thought of combining generative models with contrasting networks?

**Ans**: *Generally, it's good idea. But, it hasn't been implemented partly because it is tricky and non-trivial to train such models. Integrative approaches are harder to implement, but perhaps the way to go in the future.*
-->

### Modèles générateurs

Avez-vous pensé à combiner des modèles générateurs avec des réseaux contrastifs ?

**Rep** : *Généralement, c'est une bonne idée. Mais, elle n'a pas été mise en œuvre en partie parce qu'il est délicat et non trivial d'entraîner de tels modèles. Les approches intégratives sont plus difficiles à mettre en œuvre, mais c'est peut-être la voie à suivre à l'avenir.*

<!--
### Distillation

Wouldn't the uncertainty of the model increase when richer targets are given by softer distributions? Also, why is it called distillation?

**Ans**: *If you train on one hot labels, your models tend to be very overconfident. Tricks like label smoothing are being used in some methods. Label smoothing is just a simple version of distillation where you are trying to predict a one hot vector. Now, rather than trying to predict the entire one-hot vector, you take some probability mass out of that, where instead of predicting a one and a bunch of zeros, you predict say $0.97$ and then you add $0.01$, $0.01$ and $0.01$ to the remaining vector (uniformly). Distillation is just a more informed way of doing this. Instead of randomly increasing the probability of an unrelated task, you have a pre-trained network to do that. In general softer distributions are very useful in pre-training methods. Models tend to be over-confident and so softer distributions are easier to train. They converge faster too. These benefits are present in distillation*
-->

### Distillation

L'incertitude du modèle n'augmenterait-elle pas lorsque des cibles plus riches sont données par des distributions plus douces ? Aussi, pourquoi l'appelle-t-on distillation ?

**Rep** : *Si vous vous entraînez sur un seul label, vos modèles ont tendance à être trop confiants. Des astuces comme le lissage des labels sont utilisées dans certaines méthodes. Le lissage de label est une simple version de la distillation où vous essayez de prédire un vecteur chaud unique. Maintenant, plutôt que d'essayer de prédire le vecteur one hot entier, vous en retirez une certaine masse de probabilité, où au lieu de prédire un 1 et un tas de 0, vous prédisez par exemple $0,97$ et vous ajoutez ensuite $0,01$, $0,01$ et $0,01$ au vecteur restant (uniformément). La distillation est simplement une façon plus éclairée de procéder. Au lieu d'augmenter de manière aléatoire la probabilité d'une tâche sans rapport, vous disposez d'un réseau pré entraîné pour le faire. En général, les distributions plus douces sont très utiles dans les méthodes d'entraînement préalable. Les modèles ont tendance à être trop confiants et les distributions plus douces sont plus faciles à entraîner. Elles convergent aussi plus rapidement. Ces avantages sont présents dans la distillation*.



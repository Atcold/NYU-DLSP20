---
lang: fr
lang-ref: ch.14
title: Semaine 14
translation-date: 14 Aug 2020
translator: Loïck Bourdois
---

<!--
## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net.
-->


## Conférence partie A

Dans cette section, nous discutons de la prédiction structurée. Nous présentons d'abord le graphe des facteurs à base d’énergie et l'inférence efficace pour celui-ci. Ensuite, nous donnons quelques exemples simples avec des facteurs "peu profonds". Enfin, nous discutons du réseau transformer de graphe.

<!--
## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.
-->

## Conférence partie B

La deuxième partie de la conférence traite plus en détail de l'application des méthodes de modélisation graphique aux modèles à base d’énergie. Après avoir passé un certain temps à comparer différentes fonctions de perte, nous discutons de l'application de l'algorithme de Viterbi et de l'algorithme de retransmission aux réseaux transformer de graphe. Nous passons ensuite à la discussion de la formulation lagrangienne de la rétropropagation, puis à l'inférence variationnelle pour les EBM.

<!--
## Practicum


When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.
-->

## Pratique
Lors de l’entraînement de modèles hautement paramétrés tels que les réseaux neuronaux profonds, il existe un risque de surentraînement des données d’entraînement. Cela conduit à une plus grande erreur de généralisation. Pour aider à réduire cela, nous pouvons introduire une régularisation dans notre entraînement, en décourageant certaines solutions pour diminuer la mesure dans laquelle nos modèles s'adapteront au bruit.





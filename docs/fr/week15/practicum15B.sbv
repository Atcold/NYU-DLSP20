0:00:01.920,0:00:08.160
Donc, partage d'écran et j'ouvre le chat.

0:00:08.160,0:00:12.160
Très bien, j'ai ouvert le chat donc vous pouvez interagir avec moi.

0:00:12.160,0:00:19.080
Un petit rappel de la dernière fois. Nous avons parlé de l'énergie de l'inférence.

0:00:19.080,0:00:24.560
Comment trouver z, comment trouver z^check, comment calculer F et E.

0:00:24.560,0:00:30.080
Donc laissez-moi juste commencer par la dernière diapositive de

0:00:30.080,0:00:35.120
la dernière fois. Donc nous avions calculé F∞ qui est

0:00:35.120,0:00:39.680
appelée la limite vers 0 de la température, l’énergie libre.

0:00:39.680,0:00:44.239
C’est une fonction de mon y et y est un vecteur 2D.

0:00:44.239,0:00:52.520
Donc chaque fois que je vais tracer F∞(y), ce sera un champ de scalaire, des hauteurs sur une région 2D.

0:00:52.520,0:01:00.800
Donc on a déjà vu ce truc, représentant une hauteur différente via une couleur différente.

0:01:00.800,0:01:14.760
En violet, la hauteur = 0, en vert F∞ = 1 et en jaune quand l’énergie libre est supérieure ou égale à 2.

0:01:14.799,0:01:22.159
Donc voici à quoi ça ressemble.

0:01:22.159,0:01:26.240
Je vous rappelle que cette énergie libre est la distance euclidienne

0:01:26.240,0:01:31.600
quadratique de la surface du modèle. Donc tous les points qui sont

0:01:31.600,0:01:36.720
à l'intérieur de la surface du modèle ont un coût nul…

0:01:36.720,0:01:40.000
Désolé, une énergie libre nulle car, encore une fois,

0:01:40.000,0:01:44.240
la distance entre eux et la surface est égale à 0. Donc le carré de 0 est égal à 0.

0:01:44.240,0:01:49.360
Puis, à mesure que vous vous éloignez, cela va augmenter quadratiquement.

0:01:49.360,0:01:57.000
Donc jusqu'à présent tout devrait être connu/compris. Vous aviez une semaine pour revoir ce truc.

0:01:57.000,0:02:01.759
Donc je suppose que tout le monde est assez familier avec ça.

0:02:01.759,0:02:07.079
Quelque chose que vous pouvez remarquer se trouve au centre de cette ellipse.

0:02:07.079,0:02:11.039
Vous avez comme une région qui est légèrement plus claire.

0:02:11.039,0:02:14.879
Vous pouvez voir un degré plus clair de violet.

0:02:14.879,0:02:18.879
Donc qu’est ce qui se passe là ? Laissez-moi vous montrer cette même image

0:02:18.879,0:02:26.440
avec une hauteur qui proportionnelle à la vraie hauteur de cette énergie libre.

0:02:26.440,0:02:32.319
Donc je change la carte de couleur de façon à ce que vous puissiez voir clairement ce qui se passe.

0:02:32.319,0:02:36.720
Et je vais utiliser cette échelle de chaleur chaud/froid.

0:02:36.720,0:02:40.720
Le froid, en bleu, signifie que F∞ = 0.

0:02:40.720,0:02:45.120
Pour F∞ = 0,5 j’utilise une couleur grise.

0:02:45.120,0:02:53.400
Puis pour tout ce qui est supérieur ou égal à 1, j’utilise du rouge.

0:02:53.400,0:02:59.680
Et donc ceci est l'image précédente mais simplement vu de haut.

0:02:59.680,0:03:07.120
Ici je vous montre le contour, de sorte que chaque ligne ici partagent la même valeur d'énergie libre.

0:03:07.360,0:03:11.519
Ok, alors laissez-moi faire tourner ce petit gars.

0:03:11.519,0:03:14.560
Vous pouvez alors voir tout autour.

0:03:14.560,0:03:23.400
Comme vous pouvez le remarquer, les régions/les hauteurs autour de l'ellipse, la surface de l'ellipse, a une énergie nulle.

0:03:23.440,0:03:27.440
Et à mesure que vous vous en éloignerez, vous avez comme une chose quadratique.

0:03:27.440,0:03:33.519
Donc vous allez avoir comme une parabole. Vous pouvez remarquer que dans le centre…

0:03:33.519,0:03:39.840
Donc à l'extérieur, bien sûr, c’est comme une parabole, mais au centre ces deux choses montent en un pic.

0:03:39.840,0:03:46.720
Et cela peut ou non être voulu.

0:03:46.720,0:03:50.879
Et nous allons commencer la leçon d'aujourd'hui en apprenant comment détendre

0:03:50.879,0:03:55.500
cette énergie libre, cette limite vers 0 de la température énergie libre

0:03:55.500,0:03:59.599
en une énergie libre sans minima locaux,

0:03:59.599,0:04:03.920
pour que ce soit un peu plus lisse.

0:04:03.920,0:04:07.599
Laissez-moi prendre ici une coupe transversale de cette baignoire.

0:04:07.599,0:04:14.640
Avec y1 = 0, donc je vais la découper en une correspondance de y1 = 0.

0:04:15.519,0:04:19.359
Donc ce que nous obtenons est la chose suivante. Vous voyez maintenant

0:04:19.359,0:04:23.520
ces deux branches qui sont mes branches paraboliques.

0:04:23.520,0:04:27.520
Alors encore une fois, qu'est-ce que cette énergie libre ?

0:04:27.520,0:04:34.360
C’est la distance au carrée d’un point donné au point le plus proche de la surface.

0:04:34.360,0:04:38.759
Donc si vous êtes sur la surface qui est comme l'emplacement

0:04:38.759,0:04:47.120
0,4 par exemple, la distance qui vous sépare et la surface va être nulle. Donc le carré de 0 c’est 0.

0:04:47.280,0:04:50.240
Quand vous vous éloignez… Disons que nous nous éloignons vers

0:04:50.240,0:04:58.759
le côté droit de ce 0,4. En se déplaçant linéairement vers la droite, vous allez augmenter quadratiquement.

0:04:58.759,0:05:04.320
C’est pourquoi nous observons cette énergie libre augmenter de façon quadratique.

0:05:04.320,0:05:07.919
De même, cela se passe de l'autre côté.

0:05:07.919,0:05:11.919
Et bien sur la même chose se produit lorsque vous vous dirigez vers 0.

0:05:11.919,0:05:16.560
En allant vers 0, vous allez comme essayer de grimper cette parabole.

0:05:16.560,0:05:24.160
Et nous avons ce pic ici. Donc dans la prochaine diapositive, nous allons apprendre à lisser ce pic.

0:05:24.160,0:05:31.039
Je vous dirais plus tard c’est très utile, pourquoi nous voudrions avoir ça.

0:05:31.039,0:05:38.479
Donc l'énergie libre. Nous connaissons.

0:05:38.479,0:05:42.400
La valeur minimale de l'énergie E(y,z).

0:05:42.400,0:05:53.600
Donc pour un y donné nous avons une énergie sur z et l'énergie libre est la valeur

0:05:53.600,0:05:58.000
de l'énergie correspondant à l'endroit où se trouve la valeur minimale.

0:05:58.000,0:06:04.000
Donc la valeur minimale de E est mon énergie libre.

0:06:04.000,0:06:12.479
Maintenant que je vais introduire une version détendue qui ce F violet.

0:06:12.479,0:06:17.000
Ce F violet est une fonction paramétrée par β et est cette expression.

0:06:17.000,0:06:21.199
Qu'est-ce que ce β ?

0:06:21.199,0:06:29.479
En physique, ce β est appelé la température inverse. C’est le β thermodynamique [appelé aussi « coldness » en anglais].

0:06:29.759,0:06:39.060
C’est tout simplement 1/kB, kB étant la constante de Boltzmann, multipliée par la température.

0:06:39.080,0:06:50.080
Donc si T la température est très très très élevée, très chaude comme si vous étiez sur le soleil, β va être extrêmement petit. Il sera 0.

0:06:50.080,0:07:02.400
A la place, si la température est froide, comme le zéro absolu, alors automatiquement vous avez β valant +∞.

0:07:02.520,0:07:12.240
Maintenant vous pouvez comprendre pourquoi j'appelle F∞, la limite vers 0 de la température, l’énergie libre.

0:07:12.240,0:07:18.800
Donc la température en 0 c'est super froid. T est 0 signifiant que β est +∞.

0:07:18.800,0:07:25.680
Donc encore une fois si vous avez cette énergie libre, cette énergie libre froide alors elle est exactement le minimum.

0:07:25.680,0:07:33.840
Si vous relâchez cette contrainte en chauffant un peu cette énergie libre, l'énergie libre sera la somme de plusieurs choses.

0:07:33.840,0:07:38.800
Donc ce s est le s pour la somme.

0:07:38.800,0:07:43.680
Une somme de toutes ces composantes ici, multipliée par l’intervalle.

0:07:43.680,0:07:53.280
Ce symbole ici c'est simplement la mesure du domaine de z.

0:07:53.280,0:07:56.960
Donc dans notre cas, z va de 0 à 2π.

0:07:56.960,0:08:00.560
Cet élément ici signifie simplement 2π.

0:08:00.560,0:08:07.520
Mais qui se souvient de ce qu’est ce kB T ?

0:08:07.520,0:08:11.199
Qu'est-ce que c'est ? Pourquoi parlons-nous d’énergie ?

0:08:11.199,0:08:14.960
Donc encore une fois, en cours d’introduction de physique,

0:08:14.960,0:08:20.319
vous avez vu que l'énergie cinétique moyenne de translation est 2/3 de kB T.

0:08:20.319,0:08:29.039
Donc kB T ou 2/3 de kB exprime l'énergie cinétique.

0:08:29.199,0:08:33.200
Disons d’un gaz, avec toutes ces particules.

0:08:33.200,0:08:36.959
Donc la température vous permet d'exprimer l'énergie.

0:08:36.959,0:08:41.360
Donc vous la température et que l'énergie sont connectées.

0:08:41.360,0:08:46.720
Vous pouvez rapidement contrôler β,

0:08:46.720,0:08:51.920
car il est l'inverse de kB T et est 1/joule.

0:08:51.920,0:08:58.800
Donc ici, nous avons 1/joule ce qui signifie que ce truc est joule

0:08:58.800,0:09:01.839
et donc F est une énergie.

0:09:01.839,0:09:04.880
Puis à l'intérieur de cette exponentielle, nous avons

0:09:04.880,0:09:09.279
1/joule fois E qui est joule, puis si vous multipliez les deux,

0:09:09.279,0:09:12.800
les deux unités s'annulent.

0:09:12.800,0:09:18.560
Tout fonctionne très bien. Bien, bien, bien.

0:09:18.560,0:09:21.760
Également, la dimension de z s'annule avec

0:09:21.760,0:09:24.080
cette dimension donc tout est en nombre pur.

0:09:24.080,0:09:29.040
Encore une fois, ce n’est pas de l’apprentissage machine, c'est de la physique.

0:09:29.040,0:09:36.320
Juste pour vous donner une vue d'ensemble sur d’où ces choses viennent.

0:09:36.320,0:09:39.839
Cela vient de nos amis du département de physique.

0:09:39.839,0:09:45.600
Bien. Donc je veux calculer cette énergie libre dans cette version détendue.

0:09:45.600,0:09:53.360
Puisque je ne sais pas comment calculer cette intégrale,

0:09:53.360,0:09:58.959
j'utilise simplement une discrétisation. Je remplace donc ce latin s par un

0:09:58.959,0:10:02.320
s grecque et puis je remplace ce d latin avec un d grec.

0:10:02.320,0:10:06.800
Tout le reste est inchangé.

0:10:06.800,0:10:11.680
Je passe d’un temps continu à une discrétisation très simple.

0:10:11.680,0:10:15.519
Cela marche dans notre cas car z est comme une dimension.

0:10:15.519,0:10:22.160
Tout est alors assez facile. En plus ici, je vais définir…

0:10:22.160,0:10:25.760
Faites attention à ce que je suis en train de définir pour cette classe.

0:10:25.760,0:10:32.880
Je défini donc ceci : le softmin de E.

0:10:32.880,0:10:39.120
Donc l'énergie libre, en violet, est simplement la détente de la limite

0:10:39.120,0:10:42.079
vers 0 de la température. C’est simplement le softmin.

0:10:42.079,0:10:48.360
Cela signifie donc que la température en zéro, la plus froide, est simplement le minimum.

0:10:48.360,0:10:56.640
Alors que si je détends, si j’augmente la température comme si j'augmentais le thermostat, je vais avoir

0:10:56.640,0:11:02.399
cette softmin qui est la somme logarithmique de l'exponentielle.

0:11:02.399,0:11:05.920
Et j’appelle ça « actual softmin ». Pourquoi j’appelle ça comme ça ?

0:11:05.920,0:11:12.560
Car d'autres personnes, la plupart des personnes en dehors de cette classe, vont appeler ça softmin autre chose.

0:11:12.560,0:11:18.480
Vous en saurez un peu plus à ce sujet dans quelques diapositives.

0:11:18.480,0:11:26.680
Une chose super intéressante est de calculer la limite de cette énergie libre pour β allant vers 0.

0:11:26.680,0:11:31.440
Donc chaque fois que vous augmentez la température,

0:11:31.440,0:11:40.000
comme la température sur le soleil, quelque chose de très chaud, quelle est la version la plus détendue de ce min ?

0:11:40.000,0:11:46.480
Donc si vous faites cela, vous allez voir que ce truc finit par être la moyenne.

0:11:46.480,0:11:50.720
Mais encore une fois, ce n’est pas très important.

0:11:50.720,0:11:52.720
C’est la dérivation.

0:11:52.720,0:11:57.559
Je vous montre juste ça pour que vous puissiez y avoir accès plus tard.

0:11:57.559,0:12:02.279
La limite de cette énergie libre pour β tendant vers 0…

0:12:02.279,0:12:09.920
Donc il fait très chaud… est simplement la moyenne de l'énergie à travers z.

0:12:09.920,0:12:15.279
A nouveau… N’ayez pas peur de ce calcul.

0:12:15.279,0:12:19.440
Donc calculons cette énergie libre pour les cas que nous avons vues avant.

0:12:19.440,0:12:25.000
Donc nous faisons toujours de l’inférence comme la dernière fois mais au lieu d'utiliser

0:12:25.000,0:12:29.360
l'inférence froide, l'énergie libre froide, nous utilisons la version détendue.

0:12:29.360,0:12:39.000
Pour Y[23]. Donc si vous vous souvenez, Y[23] le x vert sur le côté droit.

0:12:39.000,0:12:44.800
Ici l'énergie libre était le carré de la distance entre le x bleu et le x vert.

0:12:44.800,0:12:50.440
Donc la distance était de 0,5² soit 0,25. C’était donc l'énergie libre.

0:12:50.519,0:12:57.040
C’était la limite vers 0 de la température.

0:12:57.040,0:13:03.440
Mais dans ce cas, nous devons maintenant considérer toutes ces contributions.

0:13:03.440,0:13:12.560
Donc je vais vous montrer comment tous ces petits z vont contribuer à cette énergie libre.

0:13:12.560,0:13:17.519
Donc nous choisissons β = 1 et avons maintenant ceci.

0:13:17.519,0:13:23.760
Donc étant donné que y’ est ce x sur le côté droit,

0:13:23.760,0:13:31.040
mon énergie libre vient de l’addition de tous ces  termes ici.

0:13:31.040,0:13:35.120
L’exponentielle de moins l'énergie de tout ça.

0:13:35.120,0:13:40.000
Donc l'exponentielle de moins ces carrés.

0:13:40.000,0:13:44.560
Donc vous pouvez constater, les points qui sont

0:13:44.560,0:13:48.560
proche du x ont une

0:13:48.560,0:13:51.920
plus petite énergie et donc l'exponentielle est plus grande.

0:13:51.920,0:13:55.920
C'est pourquoi vous pouvez les voir. Mais pour l'énergie éloignée,

0:13:55.920,0:14:00.240
une énergie très élevée, vous faites l'exponentielle de moins

0:14:00.240,0:14:07.079
un grand nombre et vous obtenez pratiquement 0. Donc elles ne comptent pas dans cette somme, dans cette intégrale.

0:14:07.120,0:14:17.600
Première question pour vous pour vérifier si vous suivez bien.

0:14:15.120,0:14:21.120
D'où vient cette valeur ici de 0,75 ?

0:14:21.120,0:14:28.279
Vous êtes censé écrire sur le chat de façon à ce que je puisse lire à haute voix ce que vous dites.

0:14:28.279,0:14:36.199
Donc une nouvelle fois, d'où vient cette valeur de 0,75 ?

0:14:39.440,0:14:45.839
Et quelqu'un doit répondre. [Chat : contribution à l'énergie]

0:14:49.760,0:14:56.399
Non, le nombre 0,75. J’ai besoin que vous me disiez comment calculer ce 0,75.

0:14:56.399,0:15:02.000
D’où vient ce nombre ? Vous avez… [Chat : ỹ les plus proches]

0:15:02.000,0:15:05.600
Oui, dites-moi comment je fais pour calculer.

0:15:05.600,0:15:12.800
[Chat : 0,5π] Non. [Chat : exp(-βE)] Ok. Que vaut E ?

0:15:12.800,0:15:17.600
E est la distance carrée.

0:15:22.240,0:15:26.639
Donc c'est combien ? [Chat : E = 0,25] Ok. Correct.

0:15:26.639,0:15:34.079
Donc exp(- 0,25) vaut 0,75 [environ 0,779]

0:15:34.079,0:15:40.000
Ok. Donc Jesse a eu la bonne réponse. Bon travail.

0:15:40.000,0:15:43.839
Donc maintenant nous savons d’où provient ce numéro.

0:15:43.839,0:15:48.000
Donc à chaque fois que vous voyez ce diagramme, bien qu'il semble très épars,

0:15:48.000,0:15:53.500
et jolie et quoique ce soit, vous devez toujours faire attention au numéro que je mets sur l’écran.

0:15:53.560,0:16:00.560
Ce ne sont pas des nombres aléatoires. Ils sont calculés par mon ordinateur

0:16:00.560,0:16:06.160
et vous devez toujours vérifier sur un bout de papier que ces chiffres ont un sens.

0:16:06.160,0:16:11.600
Car s'ils n'en ont pas, alors vous ne comprenez pas ce qui se passe.

0:16:11.600,0:16:15.600
Faites attention aux chiffres.

0:16:15.600,0:16:23.519
Je suis physicien, donc dans mon esprit, j’ai toujours à l'avance ce que mon programme/mon réseau est censé faire.

0:16:23.519,0:16:28.000
Si je fais un circuit électronique, je dois comprendre,

0:16:28.000,0:16:34.360
je dois à l'avance connaître le voltage avant de le mesurer.

0:16:34.720,0:16:39.600
Sinon ça n'avance pas beaucoup.

0:16:39.600,0:16:47.680
Très bien. Donc passons à autre chose et considérons maintenant le cas pour quand j'ai y’= Y[10]

0:16:50.240,0:16:58.560
Donc l’élément en haut. Dans ce cas, je vais obtenir tous ces points ici,

0:16:58.560,0:17:02.160
contribuant à l'énergie libre.

0:17:02.160,0:17:09.520
Ici nous avons 0,26/0,27. Quelqu'un d'autre que Jesse

0:17:09.520,0:17:13.039
peut il écrire dans le chat d'où vient ce numéro ?

0:17:13.039,0:17:18.799
D’où est-ce que vient ce 0,26 ? Je pense que vous devez avoir compris maintenant.

0:17:18.799,0:17:24.439
[Chat : exp(-1)] Oui à peu près.

0:17:25.559,0:17:31.600
Donc la distance ici est de 1,1.  1,1 au carré donne 1,2.

0:17:31.600,0:17:36.160
Puis vous prenez exp(-1,2) ce qui fait 0,26.

0:17:36.160,0:17:39.000
C'est correct.

0:17:39.000,0:17:47.840
Ok donc question suivante : que se passe-t-il maintenant si y’ est l'origine ?

0:17:48.480,0:17:53.760
Que se passe-t-il si y’ est l'origine de la température zéro ?

0:17:53.760,0:17:57.280
Vous obtenez la distance carrée de l'un ou l'autre.

0:17:57.280,0:18:03.120
Dans ce cas, quelle est la différence principale si vous réchauffez la température ?

0:18:03.120,0:18:10.559
Donc pas la température zéro, ce n'est pas le froid glacial, nous augmentons un peu la température.

0:18:10.559,0:18:14.400
Comment cette énergie libre change par rapport à avant ?

0:18:14.400,0:18:21.360
Vous pouvez écrire dans le chat. [Chat : c'est symétrique] Oui parfaitement.

0:18:21.360,0:18:26.320
Comment savez-vous ça ? Vous avez déjà vu les diapositives avant ou vous

0:18:26.320,0:18:31.919
avez réellement bien compris ? Je suppose que vous avez bien compris.

0:18:32.000,0:18:35.360
Très bien, ok, c'est parfait. C'est symétrique.

0:18:35.360,0:18:45.120
Donc un point à l'intérieur…  [Alfredo lit le chat] Oh ok. Je ne sais pas si c'est il ou elle mais a étudié la physique en premier cycle.

0:18:45.120,0:18:49.919
Ok cool d'accord.

0:18:49.919,0:18:54.080
Donc dans ce cas, vous avez tous ces points en haut, en bas,

0:18:54.080,0:18:58.720
contribuant à l’énergie libre. Etant donné que je choisis que

0:18:58.720,0:19:04.880
y’ doit être au centre. Donc c’est à peu près tout sur…

0:19:04.880,0:19:09.520
Oh ! mais pourquoi nous parlons de ça ? Nous en sommes venus à ça

0:19:09.520,0:19:15.720
car nous avons le problème avec le centre pointu que je vous ai montré avant.

0:19:15.720,0:19:19.360
La baignoire tournante et la coupe transversale ici.

0:19:19.360,0:19:23.750
Nous avions cette chose pointue venant de l’énergie libre froide.

0:19:23.750,0:19:27.000
Laissez-moi vous montrer maintenant ce qui se passe si je

0:19:27.000,0:19:32.880
choisi l'énergie libre chaude. Donc si je fais cela, j’obtiens…

0:19:32.880,0:19:38.919
Si je peux faire défiler mon écran… Tada ! Oh vous ne voyez rien.

0:19:39.679,0:19:43.840
Laissez-moi cliquez. Ok.

0:19:43.840,0:19:47.120
En rouge c’était donc le β super froid.

0:19:47.120,0:19:50.120
Encore une fois un β élevé est froid

0:19:50.120,0:19:54.880
et puis nous augmentons la température et vous pouvez constater que

0:19:54.880,0:19:58.720
la partie pointue devient lisse, lisse, lisse,

0:19:58.720,0:20:09.120
jusqu'à ce qu'elle devienne une parabole avec un seul minimum global.

0:20:09.200,0:20:15.039
Vous vous souvenez ce qui se passe si β va vers 0 ?

0:20:15.039,0:20:21.200
Vous obtenez la moyenne. Vous récupérer donc la MSE.

0:20:21.200,0:20:28.480
Ok. Je donne juste des petites informations, des pilules, peu importe.

0:20:28.480,0:20:36.679
Mais encore une fois, quand nous augmentons la température vous détendez jusqu'à ce que vous n'en ayez plus qu'un minimum unique.

0:20:36.700,0:20:43.840
Il n’y a plus de latente car nous faisons la moyenne de tout, sans ces poids.

0:20:43.840,0:20:50.500
Je pense que si vous avez besoin d’implémenter ces choses en PyTorch, vous allez être très frustré car

0:20:50.500,0:20:55.280
ils utilisent des noms différents pour les choses que je viens de définir.

0:20:55.280,0:20:59.440
Quelqu'un a dit : « Oh vous auriez dû utiliser leurs noms » Non.

0:20:59.440,0:21:04.159
Car ils sont faux. J’utilise les noms qui sont corrects, ceux qui sont logiques.

0:21:04.159,0:21:07.520
Je vais essayer de vous le vendre de cette façon.

0:21:07.520,0:21:12.159
Donc laissez-moi vous expliquer un peu la nomenclature que j'utilise,

0:21:12.159,0:21:15.440
de sorte que cela ait du sens. Au moins pour moi.

0:21:15.440,0:21:19.000
Sinon les choses n'ont pas de sens pour moi.

0:21:19.000,0:21:23.440
Donc c’est la vraie softmax, pas celui que les gens utilisent en dehors de ce cours.

0:21:23.440,0:21:30.840
C’est la softmax réelle : 1/β log de la somme des exponentielles.

0:21:30.900,0:21:38.000
Je l’ai développée [la formule] J’ai développé le z.

0:21:38.000,0:21:42.480
Je l'ai enlevé du logarithme. Donc j'ai juste séparé les deux choses.

0:21:42.480,0:21:46.480
Donc comment implémenter ce truc dans PyTorch ? Il suffit d'utiliser la

0:21:46.480,0:21:51.640
fonction appelée « torch.logsumexp » qui est cette softmax réelle.

0:21:51.640,0:21:55.600
Et ensuite plus ou moins cette constante supplémentaire là.

0:21:55.600,0:22:03.440
Donc c'est comme ça que vous voulez utiliser, implémenter ça. Car c’est numériquement stable.

0:22:03.440,0:22:09.320
De plus, si vous… Ceci est la définition de la softmin réelle.

0:22:09.320,0:22:13.640
Vous pouvez voir que c’est ce que j'ai écrit avant.

0:22:13.640,0:22:19.240
Vous pouvez voir que c'est très similaire la softmax réelle.

0:22:19.360,0:22:23.240
Quelle est la seule différence ? Il y a deux moins.

0:22:23.240,0:22:26.640
Donc vous pouvez obtenir ça en mettant un moins devant.

0:22:26.640,0:22:30.480
Donc vous annulez le premier moins. Et vous mettez un moins à l'intérieur.

0:22:30.480,0:22:34.320
Donc vous annulez l'autre moins. C’est donc la softmin est simplement…

0:22:34.320,0:22:38.400
Vous pouvez l’implémenter comme une softmax avec deux moins.

0:22:38.400,0:22:42.320
A nouveau la softmax réelle. Puis quelqu'un va bien sûr

0:22:42.320,0:22:46.000
demander : « mais quelle est la softmax que nous utilisons en classe

0:22:46.000,0:22:51.840
à chaque fois ? » Donc ceci est en faites la softargmax.

0:22:51.840,0:22:56.880
Pourquoi cela ? Car argmax est comme un vecteur « one hot ».

0:22:56.880,0:23:02.799
Le 1 vous indiquant quel est l'indice de l'élément qui a le maximum.

0:23:02.799,0:23:07.720
Donc le maximum vous permette de récupérer la valeur maximale.

0:23:07.720,0:23:14.320
Et l’argmax vous dit où se trouve l'indice pointant vers cette valeur maximale.

0:23:14.320,0:23:18.000
Donc c'est comme un vecteur « one hot » et l'autre est un scalaire.

0:23:18.000,0:23:21.440
De la même manière, chaque fois que je calcule la softmax,

0:23:21.440,0:23:26.159
la version douce du max, ce max n'est pas seulement le max.

0:23:26.159,0:23:30.880
C’est la somme de ça : le logarithme de la somme des exponentielles.

0:23:30.880,0:23:34.000
Qui vous permet de changer la température.

0:23:34.000,0:23:37.200
Si vous avez la température super froide vous récupérez le maximum.

0:23:37.200,0:23:41.839
Si vous augmentez la température, vous obtenez quelque chose comme une somme pondérée.

0:23:41.839,0:23:48.960
Et pour la softargmax, si c'est super froid, c’est comme l'argmax, vous avez un « one hot »,

0:23:48.960,0:23:55.679
mais si vous augmentez la température, vous allez obtenir une distribution de probabilité.

0:23:55.679,0:24:00.200
Donc chaque fois que quelqu'un dit : « oh la softmax vous donne la distribution de probabilité »

0:24:00.200,0:24:06.720
Non ! C’est la softargmax. Argmax, ou limite zero de température, c’est pour le « one hot ».

0:24:06.720,0:24:10.000
Si vous augmentez la température, vous obtenez une distribution.

0:24:10.000,0:24:14.080
Donc ce sont les noms corrects que personne n'utilise à part moi.

0:24:14.080,0:24:21.360
J'espère donc que je n'ai pas créé de confusion. Si c’est cas, désolé.

0:24:21.360,0:24:27.840
Mais c’est la bonne façon de voir les choses, car c'est logique.

0:24:27.840,0:24:33.279
Donc si vous avez le maximum… Si vous avez une fonction et vous voulez trouver le maximum, c'est ici.

0:24:33.279,0:24:38.240
Si vous avez cette fonction et vous voulez trouver le min, prenez cette fonction, retourner la

0:24:38.240,0:24:42.339
et trouvez le maximum. Puis retourner la à nouveau et vous obtenez le min.

0:24:42.339,0:24:45.039
C'est ce que je vous montre ici.

0:24:45.039,0:24:54.000
Je vous montre que softmin est simplement la version retournée, le négatif, du max avec un argument retourné.

0:24:54.000,0:25:01.520
Très bien, j’arrête de parler mathématiques et tout ça. J'espère que c'était ok.

0:25:01.520,0:25:08.640
Donc c’est partie qui concluait la dernière leçon.

0:25:08.640,0:25:12.880
Donc c'est la fin de l’inférence.

0:25:12.880,0:25:16.500
Nous avons donc dit qu'il y a l'énergie libre.

0:25:16.500,0:25:24.000
Qu’il y a plusieurs versions : froide, tiède ou chaude. La version chaude
est la moyenne.

0:25:24.000,0:25:30.400
La version tiède est quelque chose qui vous plaira peut-être, comme cette marginalisation de la latente.

0:25:30.400,0:25:37.200
Puis la version super froide, la limite vers 0 de la température est exactement la valeur minimale.

0:25:37.200,0:25:44.480
Ce que je vous ai montré c'est le fait que ce modèle est très mal entraîné.

0:25:44.500,0:25:51.760
Car ces régions à faible énergie ne se produisaient pas autour de

0:25:51.760,0:25:56.480
l’ensemble d’entraînement. Laissez-moi vous montrer à nouveau le même

0:25:56.480,0:26:00.480
diagramme que je vous ai montré au début de la leçon d'aujourd'hui.

0:26:00.480,0:26:04.320
Celui-là. Donc ici je vous montre avec ces

0:26:04.320,0:26:08.559
y^check quelques échantillons sur la surface du modèle.

0:26:08.559,0:26:17.000
Les y bleus sont l'échantillon d’entraînement, mais nous n’utilisons jamais d'échantillon d’entraînement.

0:26:17.200,0:26:21.039
Je l’utilise juste pour calculer l'énergie libre.

0:26:21.039,0:26:25.480
Nous ne l’utilisons pas pour apprendre. Nous n'avons pas parlé d'apprendre.

0:26:25.480,0:26:33.120
Seulement d'inférence jusqu'à présent. Donc devinez quelle est ma prochaine partie de la leçon d'aujourd'hui. L’entraînement !

0:26:34.080,0:26:42.640
Donc nous allons commencer à apprendre comment entraîner les modèles à base d’énergie.

0:26:42.600,0:26:48.960
Sauf s’il y a des questions dans le chat. Pas de questions. Tout est clair ?

0:26:50.039,0:26:57.600
[Chat : Meta-learning] [rires] Ceci est un sujet différent, une prochaine fois.

0:27:00.000,0:27:07.000
Ok donc ce n’est pas important. C’est juste de l’inférence. Nous n'avons pas parlé de trucs fous.

0:27:07.000,0:27:12.279
Nous avons déjà parlé d’inférence toute la dernière leçon.

0:27:12.559,0:27:17.000
Je suppose donc que nous pouvons avancer et commencer l’entraînement.

0:27:17.000,0:27:21.750
Trouver une fonction d’énergie qui se comporte bien.

0:27:21.750,0:27:25.200
Qu'est-ce que cela signifie ? Que nous devons introduire

0:27:25.200,0:27:30.240
une fonctionnelle de perte. Qu'est-ce qu'une fonctionnelle de perte ?

0:27:30.240,0:27:35.440
C'est une métrique, c'est une fonction scalaire

0:27:35.440,0:27:40.000
indiquant à quel point votre fonction d’énergie est bonne.

0:27:40.000,0:27:45.520
Donc nous avons une fonction d’énergie qui est cette énergie libre

0:27:45.520,0:27:49.279
et puis nous allons avoir une fonction de ma fonction.

0:27:49.279,0:27:53.840
Ce qui me donne un scalaire me disant juste à quel point cette

0:27:53.840,0:28:03.679
fonction d’énergie est. Donc une fonctionnelle de perte me donne un scalaire étant donné que j'alimente une fonction.

0:28:03.679,0:28:07.679
Et ici je vous montre juste que si j'ai ce l bouclé,

0:28:07.679,0:28:13.600
la fonctionnelle de perte pour l'ensemble du batch, mon ensemble de données entier,

0:28:13.600,0:28:19.600
je peux aussi exprimer ça comme la moyenne de ces fonctionnelles de perte par échantillon.

0:28:19.600,0:28:24.600
Donc je fais juste la moyenne de ces fonctionnelles de perte par échantillon.

0:28:23.919,0:28:30.000
Donc de quoi je parle ? Je vous « hype » mais je ne vous ai rien dit jusqu'à présent.

0:28:30.080,0:28:35.120
Nous connaissons déjà ce genre de choses via le l'apprentissage automatique

0:28:35.120,0:28:39.360
et les leçons précédentes. Voici donc la première fonctionnelle de perte

0:28:39.360,0:28:43.200
qui est la fonctionnelle de perte d'énergie.

0:28:43.200,0:28:49.360
Donc cette fonctionnelle de perte d'énergie est simplement l'énergie libre

0:28:49.360,0:28:54.720
F évaluée dans mon y où y est le point de données de l’ensemble de données.

0:28:54.720,0:29:03.000
Quand entraînons ces modèles, nous minimisons la fonctionnelle de perte.

0:29:03.200,0:29:08.000
Donc, dans ce cas, la fonctionnelle de perte

0:29:08.000,0:29:14.399
est en fait l'énergie libre au point d’entraînement.

0:29:14.399,0:29:18.559
Que doit faire la fonction d’énergie, l'énergie libre ?

0:29:18.559,0:29:22.559
Être faible pour les données provenant de la distribution de l’entraînement

0:29:22.559,0:29:26.960
et grande ailleurs. Quel est le moyen le plus simple de faire ça ?

0:29:26.960,0:29:31.679
Nous allons juste avoir la fonctionnelle de perte étant l'énergie libre

0:29:31.679,0:29:37.600
évaluée au point d’entraînement. Donc si elle est supérieure à 0, alors

0:29:37.600,0:29:41.039
l’entraînement du réseau, modifier les paramètres de telle sorte que nous

0:29:41.039,0:29:46.520
minimisons la fonctionnelle de perte va comprimer l’énergie libre sur ces points.

0:29:46.520,0:29:53.679
Donc vous avez le point, vous avez l’énergie libre : Boum !

0:29:48.640,0:29:53.679
Point, énergie libre : Boum.

0:29:53.679,0:30:00.000
Nous réduisions l'énergie libre correspondant à tous ces y.

0:30:00.000,0:30:10.240
Pourquoi il y a un check ? Car je veux souligner le fait que nous essayons de pousser l'énergie vers le bas à ces endroits.

0:30:10.240,0:30:13.440
Donc je pousse vers le bas.

0:30:13.440,0:30:19.600
C’est une flèche pointant vers le bas. Je peux paraître idiot mais

0:30:19.600,0:30:27.000
peu importe que je m'aime bien. Donc à la place, nous allons introduire ces méthodes contrastives.

0:30:27.000,0:30:35.440
Qu’est-ce qu’une méthode contrastive ? Dans le cas présent, cette méthode contrastive a un y check qui en bleu. Pourquoi bleu ?

0:30:35.440,0:30:38.320
Car il fait froid. Donc nous voulons essayer d’avoir une

0:30:38.320,0:30:41.600
énergie basse. L'énergie et la température étant connectées.

0:30:41.600,0:30:46.320
Donc l’énergie basse est froide, en bleu et puis j'ai un y chaud.

0:30:46.320,0:30:51.520
Pour ŷ est rouge ? Je veux augmenter l'énergie.

0:30:51.520,0:30:56.720
C'est pourquoi le chapeau pointe vers le haut. Donc dans ce cas,

0:30:56.720,0:31:10.000
étant donné que m est un nombre positif, le réseau va essayer de rendre la différence F(ŷ) moins F(y^check) plus grande que m.

0:31:10.880,0:31:22.000
Donc aussi longtemps que la différence est plus petite que m, alors cette valeur ici sera positive.

0:31:22.159,0:31:34.719
Chaque fois que F(ŷ) moins F(y^check) est plus grande que m, la sortie sera nulle.

0:31:35.000,0:31:39.360
Car il y a une partie positive.

0:31:39.360,0:31:46.000
Cette hinge loss tentera simplement d'obtenir que la différence soit plus importante que la marge m.

0:31:46.000,0:31:50.960
Afin d'avoir comme une version plus lisse de cette marge…

0:31:51.120,0:31:53.960
C'est très binaire.

0:31:53.960,0:32:01.279
Si vous êtes inférieur à la marge, vous poussez. Supérieur à la marge vous cessez de pousser.

0:32:01.279,0:32:07.679
Vous pouvez utiliser cette autre version : le logarithme de la fonctionnelle de perte.

0:32:07.679,0:32:13.279
C’est une marge lisse. Vous pouvez voir. Chaque fois que vous

0:32:13.279,0:32:18.440
êtes à l'intérieur de cette parenthèse, vous avez très négatif.

0:32:18.441,0:32:23.600
Donc disons que ceci est très grand et ceci 0. Vous allez avoir le x d'un

0:32:23.600,0:32:25.919
nombre très négatif qui est à peu près 0.

0:32:25.919,0:32:31.279
Puis le log de 1, qui est : « arrêtez de pousser, il n’y a plus rien ».

0:32:31.279,0:32:37.600
Si cette valeur est grande et cette valeur est négative ou égale à 0,

0:32:37.600,0:32:40.640
vous allez avoir l'exponentielle de ce nombre qui va être très important.

0:32:40.640,0:32:44.240
Puis 1 + cette exponentielle.

0:32:44.240,0:32:51.240
Le 1 est négligeable et vous n'obtenez pas le log de ce x mais

0:32:51.840,0:32:57.279
la perte sera proportionnelle à l'énergie. Si c’est très grand.

0:32:57.279,0:33:03.519
Mais encore une fois pour notre cas nous avons juste une latente 1D très petite.

0:33:03.519,0:33:06.399
Donc nous n'avons pas besoin de faire ça.

0:33:06.399,0:33:17.000
L'échantillonnage contrastif, l'apprentissage contrastif est nécessaire quand vous avez une latente en grandes dimensions, etc…

0:33:17.000,0:33:24.559
Donc entraînons ce modèle, car je ne l'ai pas encore entraîné, avec cette fonctionnelle de perte d'énergie.

0:33:24.559,0:33:33.840
Donc j’entraîne le modèle. Cela prend une époque pour converger. C'est ridiculement rapide mais c'est exemple.

0:33:33.840,0:33:38.000
Vous comprenez cela. Donc je vais commencer par vous montrer

0:33:38.000,0:33:43.600
la limite vers 0 de la température, l’énergie libre super froide.

0:33:43.600,0:33:48.559
A gauche je vais montrer vous la version non entraînée qui est celle que nous avons déjà vue.

0:33:48.559,0:33:54.559
Donc dans ce cas, pour chaque point d’entraînement, le point bleu, j'ai un

0:33:54.559,0:33:58.919
x correspondant qui est l'emplacement sur la surface du modèle

0:33:58.919,0:34:04.559
qui est le plus proche de ce point d’entraînement.

0:34:04.559,0:34:11.200
Quand j’entraîne, j’obtiens un gradient. Ce gradient va… je vous l’ai dit avant.

0:34:11.200,0:34:17.280
Si j’ai le min, j’ai un élément. Puis si je fais la dérivée, j’ai l’argmin

0:34:17.280,0:34:22.040
qui n'est que 1 correspondant à la valeur la plus faible.

0:34:22.040,0:34:25.919
On représente ça ici

0:34:25.919,0:34:31.359
par cette flèche à droite. Donc cette flèche ici

0:34:31.359,0:34:35.440
est la dérivé de l'énergie

0:34:35.440,0:34:39.599
qui est juste la distance : y – y^check.

0:34:39.599,0:34:43.440
Puis vous allez multiplier par

0:34:43.440,0:34:48.240
celle correspondant à l'endroit le plus proche de notre point.

0:34:48.240,0:34:56.879
Ce qui signifie que pendant l’entraînement chaque fois que nous utilisons la ZTL, la limite vers 0 de la température, vous obtenez

0:34:56.040,0:35:02.240
l'emplacement sur la surface qui est le plus proche de votre point d’entraînement.

0:35:02.240,0:35:05.520
Puis vous allez obtenir ce point pour le déplacer.

0:35:05.520,0:35:08.800
Vous avez ce point d’entraînement, vous avez cet emplacement qui se trouve sur la surface

0:35:08.800,0:35:12.800
proche de ce point, et vous obtenez alors un gradient qui monte ici.

0:35:12.800,0:35:15.119
De même vous avez un point d’entraînement ici,

0:35:15.119,0:35:20.000
le point proche de la surface ici, vous obtenez ce point, et le gradient descend ici.

0:35:20.000,0:35:26.079
Donc voici la procédure d’entraînement lors de l'utilisation de cette limite vers 0 de la température.

0:35:26.079,0:35:31.520
Une époque plus tard vous obtenez à droite : boom !

0:35:31.520,0:35:38.720
Tous ces x ont automatiquement réussi à arriver à destination. Terminé.

0:35:38.720,0:35:50.400
C'est donc un modèle bien entraîné où l'énergie va à 0 dans tous les lieux

0:35:50.880,0:35:54.640
correspondant à mon ensemble de données d’entraînement. Les points d’entraînement en bleus.

0:35:54.640,0:36:07.359
[Chat : qu’arrive t’il si vous avez deux points étant les plus proches sur la surface ? Par exemple si y est à (0,0)]

0:36:09.280,0:36:17.240
Dans la limite vers 0 de la température que vous allez obtenir un seul point.

0:36:17.119,0:36:23.480
Et c’est très sujet au surentraînement. Disons que notre z n'est pas seulement en 1D, qu’il est plus grand.

0:36:23.280,0:36:29.599
Donc au lieu d’avoir comme une ellipse que vous avez comme une pomme de terre.

0:36:29.599,0:36:33.200
Si vous…. Ok, laissez-moi finir la réponse. Si vous avez une pomme de terre,

0:36:33.200,0:36:37.440
vous allez obtenir tous ces endroits sur la pomme de terre allant

0:36:37.440,0:36:43.000
à ces points d’entraînement. Donc si votre z est une variable latente de haute dimension,

0:36:43.000,0:36:47.400
vous commencez par une pomme de terre et vous finissez par un porc-épic.

0:36:47.400,0:36:51.359
Avec tous ces pics qui sortent. Et c'est essentiellement du surentraînement.

0:36:51.359,0:36:54.280
Vous mémorisez seulement le jeu d’entraînement.

0:36:54.280,0:36:58.320
Dans notre cas, cela ne se produit pas car notre latence est 1D.

0:36:58.320,0:37:04.320
Donc on ne peut pas vraiment tirer des pics de ce truc.

0:37:05.119,0:37:11.839
Mais nous pourrions néanmoins vouloir trouver une solution à ce surentraînement

0:37:11.839,0:37:16.240
en utilisant la régularisation de la température. Donc avant que je

0:37:16.240,0:37:21.760
vous montrais qu'il y a un pic s'il y a une limite vers 0 de la température

0:37:21.760,0:37:25.760
mais si vous augmentez la température, vous lissez ce pic.

0:37:25.760,0:37:30.640
Et donc ici je vais vous montrer… Puis je réponds à l'autre question.

0:37:30.640,0:37:34.079
En fait, laissez-moi voir ce qui se passe ici.

0:37:34.079,0:37:38.800
[Chat : comment mettons-nous à jour la fonction d’énergie ? Est-elle paramétrée avec…]

0:37:38.800,0:37:42.000
Oh, c’est la définition de la dernière fois.

0:37:42.000,0:37:47.440
Donc ma fonction d’énergie est celle-ci.

0:37:47.440,0:37:52.000
Donc ma fonction d’énergie est mon modèle.

0:37:52.000,0:37:56.000
C’est la différence carrée entre les lieux et code de la latente pour

0:37:56.000,0:38:00.640
la première composante et le code de la latente pour la deuxième composante,

0:38:00.640,0:38:03.280
C'est ainsi que E est paramétrée.

0:38:03.359,0:38:08.079
[Chat : est-ce que l'apprentissage interpole entre les points ?

0:38:08.079,0:38:14.200
Est-ce que cet algorithme apprend toute l'ellipse ou seulement les points bleus ?]

0:38:14.880,0:38:18.640
Ok, je vais parler de ça. [Chat : Y a-t-il une visualisation

0:38:18.640,0:38:24.240
pour les pointes du surentraînement ?] Je vais en parler aussi.

0:38:24.240,0:38:28.160
Ok, donc nous parlions de comment entraîner

0:38:28.160,0:38:32.079
cette fonction d’énergie. Donc cette fonction d’énergie

0:38:32.079,0:38:35.280
est ce truc en couleur que je vous montre ici.

0:38:35.280,0:38:39.280
C'est une représentation différente, c'est simplement la localisation

0:38:39.280,0:38:43.680
de cette ellipse violette.

0:38:43.680,0:38:48.400
L’entraînement pour la limite vers 0 de la température signifie que vous prenez

0:38:48.400,0:38:51.760
ce point de l’ellipse et vous essayez de le tirer vers le haut.

0:38:51.760,0:38:55.760
Comment faire ça ? Les deux seuls paramètres que nous avons dans ce

0:38:55.760,0:39:00.240
modèle sont w1 et w2 qui contrôlent le

0:39:00.240,0:39:04.960
rayon x et le rayon y. Nous avons donc deux paramètres.

0:39:04.960,0:39:08.079
Et avec deux paramètres, nous essayons d’entraîner tous ces y.

0:39:08.079,0:39:11.440
Et donc, fondamentalement, le réseau…

0:39:11.440,0:39:15.200
la procédure d'entraînement de descente de gradient

0:39:15.200,0:39:19.200
essaye de modifier la taille de cette ellipse de manière à ce qu'elle

0:39:19.200,0:39:24.240
grandisse et match tous ces points bleus

0:39:24.240,0:39:29.040
Le truc avec des pointes dont je parlais, c'est ça.

0:39:29.040,0:39:33.200
Si vous avez un z de grande dimension… Ici dans ce cas, z est 1D donc

0:39:33.200,0:39:36.960
vous avez une ligne comme ça. Si c'est en 2D,

0:39:36.960,0:39:41.119
c’est toute la surface. Donc maintenant c’est trivial de surentraîner,

0:39:41.119,0:39:44.160
vous pouvez vous déplacer n'importe où dans plan. Il n'y a plus de contrainte

0:39:44.160,0:39:47.359
de vivre sur cette ligne. Donc voyons

0:39:47.359,0:39:51.200
comment éviter le surentraînement. Ici dans ce cas cela n'arrive pas, mais

0:39:51.200,0:39:54.720
nous pouvons maintenant constater qu'en augmentant la température

0:39:54.720,0:39:59.599
nous ne choisissons plus les points individuellement. Nous utilisons donc cette marginalisation,

0:39:59.599,0:40:06.160
ce truc visuel. Donc sur la partie du bas, c’est la marginalisation.

0:40:06.160,0:40:11.200
A gauche, je vous montre comment fonctionne l’entraînement.

0:40:11.200,0:40:15.440
Donc vous avez tous ces endroits contribuant au gradient,

0:40:15.440,0:40:21.119
n’étant que la moyenne de ces flèches ici.

0:40:21.119,0:40:26.079
Donc, étant donné que nous choisissons un y, qui est ce x vert ici,

0:40:26.079,0:40:32.960
vous obtenez tous ces points sur cette surface qui contribuent

0:40:32.960,0:40:37.760
et sont attirés. Ici, avant, on a qu’un seul point tiré vers le haut.

0:40:37.760,0:40:43.000
Ici, nous avons tous ces points tirés vers le haut. Donc c’est beaucoup plus difficile de surentraîner

0:40:43.000,0:40:48.280
Un élément auquel vous voulez prêter attention ici c'est comment calculer le gradient.

0:40:48.280,0:40:52.000
Donc je calcule le gradient de cette softmin et donc

0:40:52.000,0:40:57.040
automatiquement, nous allons avoir une softargmin. Donc si vous avez un maximum,

0:40:57.040,0:41:01.000
vous faites le gradient et obtenez l'argmax ou si vous avez un minimum,

0:41:01.000,0:41:04.240
le gradient va être l'argmin. Ici nous avons une

0:41:04.240,0:41:08.400
softmin et donc le gradient va être la softargmin

0:41:08.400,0:41:11.920
multipliée par la dérivée de cette énergie.

0:41:11.920,0:41:15.920
Et cela est simplement ce vecteur. L’énergie est la distance carrée.

0:41:15.920,0:41:19.520
Si vous faites la dérivée, vous obtenez le vecteur ici en blanc

0:41:19.520,0:41:29.440
et ensuite la hauteur est donnée par le vecteur multiplié la softargmin.

0:41:29.440,0:41:36.560
C'est beaucoup à prendre je pense,

0:41:36.560,0:41:39.599
mais c'est… je pense que c'est juste super…

0:41:39.599,0:41:44.240
Finalement, j’entraîne et j’obtiens quelque chose comme ça sur

0:41:44.240,0:41:48.800
le côté droit. Donc avant que je vous montre

0:41:48.800,0:41:53.839
la section pour la version non entraînée sur le côté gauche, je vais vous

0:41:53.839,0:41:56.800
montrer la coupe transversale de cette version entraînée.

0:41:56.800,0:42:01.359
Donc pour la limite vers 0 de température, la froide, je vais avoir ceci en rouge

0:42:01.359,0:42:05.280
avec un pic et puis, à mesure que vous augmentez la température,

0:42:05.280,0:42:08.400
à mesure que vous réduisez ce β,

0:42:08.400,0:42:14.400
vous obtenez cette version moyenne, cette parabole bleue.

0:42:14.400,0:42:18.800
Ok, ok, ok. Et donc tout cela était à propos de

0:42:18.800,0:42:24.680
l'apprentissage non supervisé. Jusqu'à présent, nous n'avons vu que les y.

0:42:24.680,0:42:30.880
Où sont les x ? Donc hier soir je me suis dit :

0:42:30.880,0:42:34.560
ok je ne vais peut-être pas parler de l'apprentissage supervisé.

0:42:34.560,0:42:39.400
Combien de temps me faudra-t-il pour entraîner un modèle avec les x et tout ?

0:42:39.400,0:42:42.400
Je ne veux pas le faire.

0:42:42.400,0:42:46.680
Mais j’ai changé juste une ligne de code et tout a fonctionné.

0:42:46.680,0:42:53.600
Donc tout ce que nous avons vu jusqu'à présent est exactement pareil que pour l'inconditionnel, l’apprentissage non supervisé.

0:42:53.600,0:43:00.640
En changeant une ligne, vous obtenez le supervisé, l'autosupervisé, le conditionnel.

0:43:00.640,0:43:03.760
Donc dans les cinq dernières minutes

0:43:03.760,0:43:06.880
nous allons parler de l'apprentissage autosupervisé

0:43:06.880,0:43:10.000
cas conditionnel. Qu'est-ce que cela signifie ?

0:43:10.000,0:43:13.280
Revenons aux données d’entraînement. Les voici.

0:43:13.280,0:43:19.119
Nus essayons d'apprendre cette corne qui commence par une bouche horizontale,

0:43:19.119,0:43:25.240
comme si c'était une bouche fermée comme ça. Puis ça devient très grand et étroit.

0:43:25.400,0:43:31.760
Et l'enveloppe est exponentielle.

0:43:31.760,0:43:38.240
Donc le rayon passe de β à α.

0:43:38.240,0:43:41.839
C’est multiplié par l'exponentielle de deux fois le x

0:43:41.839,0:43:45.920
et dans l'autre cas, allant d’α à β est également multiplié par

0:43:45.920,0:43:50.560
l'exponentielle. Donc voyons si nous pouvons apprendre ce truc.

0:43:50.560,0:43:54.960
Je pensais que c'était difficile à faire mais c’est en faites très facile.

0:43:54.960,0:43:59.119
Donc la surface du modèle non entraîné.

0:43:59.119,0:44:03.119
Voyons donc à quoi ressemble mon modèle maintenant.

0:44:03.119,0:44:07.359
J'ai donc un z et comme je contrôle le z, je prends

0:44:07.359,0:44:12.079
de 0 à 2π avec π exclus, c'est pourquoi la parenthèse est inversée,

0:44:12.079,0:44:19.359
avec un intervalle de π/24. Donc j'ai une ligne là. Je mets ce

0:44:19.359,0:44:26.319
z dans le décodeur et obtiens mon ỹ qui se déplace autour de l’ellipse

0:44:26.319,0:44:32.960
car c'est ainsi que mon réseau est acheminé dans le décodeur.

0:44:32.960,0:44:37.520
De plus, nous avons notre y observé.

0:44:37.520,0:44:41.280
Vous pouvez voir qu’il est observé car il y a une ombre dans ce cercle.

0:44:41.280,0:44:44.200
Et maintenant nous avons un prédicteur.

0:44:44.200,0:44:49.000
Le décodeur ne prend pas seulement ma latente z mais aussi un prédicteur.

0:44:49.000,0:44:56.480
Le prédicteur est alimenté avec mon x observé. Et puisque je contrôle z

0:44:56.480,0:45:02.960
je peux simplement dire qu'il passe de 0 à 1 avec un intervalle de 0,02.

0:45:02.960,0:45:07.520
Laissez-moi vous montrer à quoi ressemble la variété du modèle non entrainé.

0:45:07.520,0:45:13.599
Voici à quoi elle ressemble. Donc comment puis-je entraîner ça ?

0:45:13.599,0:45:18.000
Je fais juste l’entraînement de l’énergie libre en la limite vers 0 de la température.

0:45:18.000,0:45:22.640
Donc étant donné ma corne, je prends un point y

0:45:22.640,0:45:25.839
et trouve le point le plus proche sur ma surface et puis

0:45:25.839,0:45:29.440
j'essaie de le remonter. Je prends cet autre point, je prends le point le plus proche

0:45:29.440,0:45:33.280
et je le pose là. Je prends ce point-là, je prends le point le plus proche

0:45:33.280,0:45:36.000
et ensuite le mets ici. Je prends ce point ici sur

0:45:36.000,0:45:39.200
la corne, je prends le point le plus proche sur la surface

0:45:39.200,0:45:42.240
et je l'abaisse. Je fais ça pour une époque.

0:45:42.240,0:45:47.119
Je vous ai dit que c’était très facile d’entraîner ce modèle

0:45:47.119,0:45:51.599
et nous obtenons… En fait je dois d'abord définir ce qu'est la fonction d’énergie.

0:45:51.599,0:45:56.240
Donc ma fonction d’énergie dans ce cas est

0:45:56.240,0:46:01.200
E(x,y,z) où, à nouveau, ces deux composantes

0:46:01.200,0:46:05.040
sont la somme des distances carrées.

0:46:05.040,0:46:10.560
Mais dans ce cas, j'ai f et g. Donc nous avons un prédicteur f.

0:46:10.560,0:46:16.800
Tous deux vont de ℝ dans ℝ². Puis f est un réseau neuronal.

0:46:16.800,0:46:20.160
Associant mon entrée x à travers une couche linéaire et

0:46:20.160,0:46:23.440
une ReLU à une couche cachée en huit dimensions.

0:46:23.440,0:46:26.880
Puis je passe à nouveau par une autre couche linéaire et ReLU

0:46:26.880,0:46:29.680
à une autre couche cachée en huit dimensions. Puis j'ai ma

0:46:29.680,0:46:32.880
couche linéaire pour finir en deux dimensions. Donc j'ai

0:46:32.880,0:46:38.560
un réseau à 4 couches : entrée, 2 cachées de taille 8 et une sortie de taille 2.

0:46:38.560,0:46:42.000
Ma fonction g est simplement ce qui me permet d'obtenir

0:46:42.000,0:46:47.839
ce z dans une boucle. Mais le fait est que ces 2

0:46:47.839,0:46:53.359
composantes vont être mises à l'échelle par la sortie de f.

0:46:53.359,0:46:58.000
C'est donc mon modèle. Un très petit modèle.

0:46:58.000,0:47:02.760
Et je vais l’entraîner. Puis je vous montrerai sa surface.

0:47:02.760,0:47:05.040
Donc je prends encore une fois la même

0:47:05.040,0:47:09.040
discrétisation pour z et x et donc voici

0:47:09.040,0:47:16.240
ce à quoi la surface du modèle entraîné ressemble. C’est génial.

0:47:16.240,0:47:22.839
Et ceci n'a rien pris en temps de calcul.

0:47:22.859,0:47:28.200
Donc que faisons-nous après ? Comment avant à partir de là ?

0:47:28.200,0:47:34.079
Il existe quelques moyens d'étendre ce système à autres choses que des exemples basiques.

0:47:34.079,0:47:37.359
Jusqu'à présent, j'ai été une sorte de tricheur.

0:47:37.359,0:47:41.520
J’ai toujours enchâssé dans le décodeur le fait que mon

0:47:41.520,0:47:45.359
z tourne en rond. Mais je ne le sais pas.

0:47:45.359,0:47:49.920
Nous ne le savons pas. Donc nous devrions utiliser quelque chose comme ça.

0:47:49.920,0:47:56.559
Dans ce cas, ma fonction g prend mes f et z.

0:47:56.559,0:47:59.839
et alors g peut aussi être un réseau neuronal.

0:47:59.839,0:48:02.960
Dans ce cas, je dois apprendre le fait que ce truc

0:48:02.960,0:48:09.920
se déplace autour de cercles. Donc je dois apprendre ce sinus et ce cosinus.

0:48:09.920,0:48:15.839
Mais alors comment puis-je savoir que z est en fait 1D ? Je le sais car

0:48:15.839,0:48:21.559
j'ai généré les données. Donc je suis le propriétaire du processus de génération de données.

0:48:21.559,0:48:25.680
Je savais que le thêta était un objet 1D.

0:48:25.680,0:48:29.359
Donc j peux certainement utiliser une latente qui est 1D mais personne

0:48:29.359,0:48:33.599
ne peut me dire pour les images naturelles ou quoi que ce soit d'autre.

0:48:33.599,0:48:37.760
Donc c’est l'autre grand problème. Comment allons-nous traiter le fait que

0:48:37.760,0:48:42.240
nous savons pas quelle est la taille correcte de ma latente ?

0:48:42.240,0:48:45.839
Car, encore une fois, si vous choisissez une grande latent, vous allez

0:48:45.839,0:48:49.839
très facilement surentraîner. Donc dans ce cas

0:48:49.839,0:48:53.920
ce qui change par rapport à la diapositive précédente, celle-ci, c'est que maintenant z

0:48:53.920,0:49:00.400
est un vecteur. Donc z est un vecteur et non plus une simple ligne.

0:49:00.400,0:49:03.200
Cela devrait être un vecteur peu importe la forme.

0:49:03.200,0:49:12.000
Et maintenant mon g part de la dimension de f et par le produit cartésien de la dimension de z, va dans ℝ².

0:49:12.000,0:49:18.240
Le problème maintenant est de régulariser cette fonctionnelle de perte.

0:49:18.240,0:49:21.920
Car sinon vous allez radicalement surapprendre.

0:49:21.920,0:49:28.880
Et c'est donc la recherche actuelle que Yann, mes étudiants et moi faisons.

0:49:28.880,0:49:42.440
Nous essayons de comprendre les moyens de régulariser la variable latente de façon à ce que la latente ne surentraîne pas. Et c'est tout.

0:49:42.480,0:49:45.680
C'est tout ce que j'avais à vous dire sur les EBMs à variables latentes.

0:49:45.680,0:49:53.680
Sur l’inférence, l’entraînement, la limite vers 0 de la température et une énergie libre un peu plus chaude.

0:49:53.680,0:49:57.920
Puis nous avons vu le cas inconditionnel / l'apprentissage non supervisé,

0:49:57.920,0:50:02.640
ainsi que le cas conditionnel avec l'apprentissage autosupervisé

0:50:02.640,0:50:07.280
où nous avons accès à ces x. Également le code pour entraîner

0:50:07.280,0:50:11.760
ces deux modèles. Le code que j'utilise pour l’entraînement du cas conditionnel

0:50:11.760,0:50:16.480
est le même code que j'utilise pour le non supervisé mais

0:50:16.480,0:50:20.079
avec une ligne changée. Donc cela ne nécessite vraiment pas

0:50:20.079,0:50:25.680
beaucoup d’efforts pour y parvenir. Il en a fallu pour dessiner

0:50:25.680,0:50:30.640
les diapositives, mais encore une fois, c'est juste car j'aime rendre les choses jolies.

0:50:30.640,0:50:37.119
C'est tout. Merci d'avoir écouté. Des questions ?

0:50:37.119,0:50:41.520
Je veux dire que la classe est finie mais

0:50:41.520,0:50:44.720
vous pouvez demander tout ce que vous voulez. Si vous êtes encore éveillé.

0:50:48.280,0:50:55.119
Quelqu'un est réveillé ! [Chat : pouvez-vous de nouveau expliquer la dimension d'entrée de g ?]

0:50:55.119,0:51:02.000
Oui, je peux vous expliquer autant que vous voulez. Maintenant c'est les heures de bureau, vous pouvez demander tout ce que vous voulez.

0:51:02.000,0:51:06.500
Attendez ! Première question. [Chat : Pouvez-vous expliquer la dimension d'entrée de g ?]

0:51:06.500,0:51:09.680
Laissez-moi revenir au premier cas.

0:51:09.680,0:51:15.839
Dans ce cas, g est 1 car a été alimenté avec z. La sortie était

0:51:15.839,0:51:20.559
ces g1 et g2 qui sont le cosinus et le sinus.

0:51:20.559,0:51:24.559
Dans le second cas, l'entrée est ce f dont nous

0:51:24.559,0:51:28.160
ne connaissons pas exactement la dimension, cela peut être n'importe quoi.

0:51:28.160,0:51:33.839
Donc la dimension de f et puis z. Etant donné que je sais que z est 1D.

0:51:33.839,0:51:40.599
Enfin, le cas réel qui est le plus réaliste est celui-ci.

0:51:41.520,0:51:48.000
Où nous ne savons pas nécessairement quelle est la dimension de la latente.

0:51:48.079,0:51:51.359
Et donc maintenant nous allons utiliser une

0:51:51.359,0:51:54.800
Variable latente de n’importe quelle dimension mais

0:51:54.800,0:51:59.280
c’est nécessaire de régulariser la fonctionnelle de

0:51:59.280,0:52:04.400
perte sinon, comme je le soulignais, vous pouvez facilement surentraîner

0:52:04.400,0:52:09.920
en utilisant cette limite vers 0 de la température. Néanmoins vous pouvez

0:52:09.920,0:52:13.599
faire monter la température et d'utiliser cela comme un moyen de régularisation.

0:52:13.599,0:52:18.880
Vous avez compris ? Oui.

0:52:18.880,0:52:23.200
La question suivante est de savoir à quoi cela ressemble sans variable latente.

0:52:23.200,0:52:26.880
Sans variable latente, c'est exactement comme si on mettait

0:52:26.880,0:52:33.920
β à 0. Donc avec β à 0 vous faites juste la moyenne

0:52:33.920,0:52:39.680
sur toutes les valeurs possibles. Qu’avez-vous à la fin

0:52:39.680,0:52:44.319
si vous commencez ici sur le côté gauche ?

0:52:44.319,0:52:47.119
Au lieu d'avoir toutes ces flèches qui

0:52:47.119,0:52:52.400
ont une forme comme ça, toutes ces flèches auront la même longueur.

0:52:52.400,0:52:56.880
En faites que ces points ici seront encore plus longs car sont les plus éloignés.

0:52:56.880,0:53:03.280
Donc, ces ellipses seront tirées dans toutes les directions.

0:53:03.280,0:53:07.200
La façon de minimiser cette énergie est en fait de la faire s'effondrer

0:53:07.200,0:53:12.079
dans un point unique centré en zéro.

0:53:12.079,0:53:16.160
Et c'est une très bonne question. Quel est le mode classique de

0:53:16.160,0:53:22.000
défaillance dans le réseau neuronal ? Quand vous avez de multiples cibles

0:53:22.000,0:53:26.880
associés à la même entrée, vous finissez par prédire la moyenne de toutes

0:53:26.880,0:53:31.520
les cibles possibles. Dans ce cas, la moyenne de toutes les

0:53:31.520,0:53:34.640
Cibles, tous ces points de l'ellipse, est

0:53:34.640,0:53:39.680
juste le point à l'origine qui est comme l'effondrement de votre modèle.

0:53:39.680,0:53:42.880
C'est donc une très bonne question et le fait est que

0:53:42.880,0:53:49.760
si vous essayez d'apprendre un ensemble de données multimodales avec

0:53:49.760,0:53:56.640
une MSE sans latent, avec un β nul, une température infinie,

0:53:56.640,0:54:00.559
cela s'effondre à la moyenne.

0:54:04.000,0:54:09.599
Très bien autre question : pour être clair, la limite vers 0 de

0:54:09.599,0:54:14.000
la température, la perte ne tient compte que de l'énergie

0:54:14.000,0:54:21.440
du point le plus proche ? Oui.  Et à mesure que nous réchauffons, la perte utilise

0:54:21.440,0:54:26.640
une somme pondérée de tous les points ? Oui et les pondérations que vous

0:54:26.640,0:54:32.480
utilisez pour la somme pondérée sont les poids qui viennent de la softarmin.

0:54:32.480,0:54:38.400
Si vous prenez la softmin, vous avez la softmin de l'énergie.

0:54:38.400,0:54:41.920
C'est ce que vous obtenez. Vous avez la softmin de l'énergie.

0:54:41.920,0:54:45.280
Donc le f̃ est la softmin de l'énergie.

0:54:45.280,0:54:49.520
Vous prenez la dérivée de la softmin, vous obtenez

0:54:49.520,0:54:53.680
l'exponentielle divisée par la somme des exponentielles.

0:54:53.680,0:54:56.799
C'est donc la softargmin.

0:54:56.799,0:55:03.000
Multipliée par E’. Qu’est-ce que E’ ? E est la distance carrée. Donc si vous prenez

0:55:03.000,0:55:05.599
la dérivée de la distance carrée vous obtenez

0:55:05.599,0:55:10.079
le vecteur qui est multiplié par cette softargmin.

0:55:10.079,0:55:13.359
Donc c’est exactement ce que vous avez dit, un très bon résumé.

0:55:13.359,0:55:18.000
Je vais juste le relire et je montre l'autre tableau.

0:55:18.000,0:55:22.720
Donc votre commentaire : pour être clair à la limite vers 0 de

0:55:22.720,0:55:26.799
La température, la perte ne considère que l'énergie du point le plus proche…

0:55:26.799,0:55:29.839
La distance au carré du point le plus proche.

0:55:29.839,0:55:34.160
… Et à mesure que vous réchauffez, la perte est la somme pondérée de…

0:55:34.160,0:55:42.480
Pas les points. C’est la somme de toutes ces contributions.

0:55:42.480,0:55:46.319
Exponentielle de -βE.

0:55:46.319,0:55:50.319
C'est ce qui a été écrit ici en haut.

0:55:50.319,0:55:56.799
Donc en réchauffant, vous allez obtenir cette exponentielle qui est la softmin. Et si vous calculez la

0:55:56.799,0:56:00.960
dérivée, vous obtenez la softargmin multipliée par la

0:56:00.960,0:56:04.400
dérivée de l'énergie. Qui sont les flèches multipliées par la softargmin.

0:56:04.400,0:56:09.359
[Chat : qu’arrive t’il si on laisse z se déplacer

0:56:09.359,0:56:13.839
librement dans l'espace ?] Nous allons avoir un effondrement.

0:56:13.839,0:56:17.119
Ce modèle peut simplement produire 0 partout.

0:56:17.119,0:56:21.520
Et c'est là que vous devrez peut-être utiliser les cas contrastifs

0:56:21.520,0:56:26.319
Donc dans ce cas, un moyen très facile d'obtenir

0:56:26.319,0:56:31.200
une énergie nulle est de mettre tout à 0. Mais dans ce cas, vous pouvez

0:56:31.200,0:56:35.599
utiliser le cas contrastif. Vous pouvez dire : « Oh non dans ce cas cela

0:56:35.599,0:56:45.760
devrait être plus grande qu'une certaine marge ». C'est ainsi que vous pouvez traiter le cas z en 2D.

0:56:45.760,0:56:52.000
[Chat : prendre β à 0 irait à l'encontre de l'objectif d'avoir une variable latente] Oui exactement.

0:56:52.000,0:56:56.720
C’est quelque chose que je vous ai montré brièvement

0:56:56.720,0:57:02.000
mais dont je n'ai pas parlé. C'est une rapide dérivation.

0:57:02.000,0:57:06.559
Si vous avez β = 0, la limite pour β tendant vers 0,

0:57:06.559,0:57:10.480
vous récupérez la moyenne sur l'ensemble des latentes.

0:57:10.480,0:57:14.640
Et c'est en gros ce à quoi vous vous retrouvez avec la MSE.

0:57:14.640,0:57:18.240
On finit par jeter toutes ces sortes de goodies.

0:57:18.240,0:57:23.760
Et c'était à peu près tout.

0:57:23.760,0:57:28.480
Comment tirer le meilleur parti de cette leçon ? D'abord la compréhension.

0:57:28.480,0:57:33.119
Si quelque chose n'était pas clair, demandez-moi n’importe quoi dans la section commentaire ci-dessous.

0:57:33.119,0:57:38.640
Si vous souhaitez avoir les dernières nouvelles, suivez-moi sur twitter via @alfcnz.

0:57:38.640,0:57:45.680
Si vous souhaitez être notifié quand je mets en ligne une vidéo, n'oubliez pas de vous abonner à la chaîne

0:57:45.680,0:57:50.079
et activez la cloche de notification. Si vous aimez cette vidéo, n'oubliez

0:57:50.079,0:57:54.400
pas de mettre un pouce. Cette vidéo a une transcription en anglais.

0:57:54.400,0:57:57.359
Si vous souhaitez contribuer à la traduction dans votre langue,

0:57:57.359,0:58:01.839
veuillez me le faire savoir. Ici, comme vous pouvez le voir, nous avons les

0:58:01.839,0:58:07.920
notes écrites. Toutes les vidéos y ont été transcrites en anglais.

0:58:07.920,0:58:13.040
Et comme je l'ai déjà dit, si nous allons sur la page d’accueil,

0:58:13.040,0:58:16.240
nous pouvons voir ici le drapeau anglais et

0:58:16.240,0:58:20.799
nous pouvons sélectionner différentes langues. Nous avons l’arabe, l’espagnol,

0:58:20.799,0:58:24.960
le français, l’italien, le japonais, le coréen, le russe, le turc et le chinois.

0:58:24.960,0:58:28.640
La traduction de votre langue n'attend plus que vous.

0:58:28.640,0:58:35.520
Ensuite, jouez avec le notebook et PyTorch afin de vous familiariser avec toutes ces nouvelles.

0:58:35.520,0:58:41.520
Enfin, si vous trouvez une coquille, des erreurs ou quoi que ce soit d'autre, veuillez simplement me le faire savoir

0:58:41.520,0:58:46.079
directement sur GitHub ou si vous vous sentez assez courageux, vous pouvez même envoyer une PR.

0:58:46.079,0:58:54.240
Cela sera grandement apprécié. Merci de votre écoute et n'oubliez pas d'aimer, de partager et de vous abonner, bye bye.

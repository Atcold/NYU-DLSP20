---
lang: fr
lang-ref: ch.15
title: Semaine 15
translation-date: 6 Dec 2020
translator: Loïck Bourdois
---


<!--
## Practicum part A

When encountering the data with multiple outputs for a single input, feed-forward networks cannot capture such implicit dependencies. 
Instead, latent-variable energy-based models (EBMs) come to the rescue. We developed a toy ellipse example with a fixed input and the optimal model formulation. 
Then, we applied latent-variable EBMs to inference the best latent variables that can learn the implicit relationships.
-->

## Travaux dirigés partie A

Lorsque les réseaux *feed-forward* rencontrent des données à sorties multiples pour une seule entrée, ils ne peuvent pas saisir les dépendances implicites. 
C’est là que les modèles à base d'énergie à variable latente (LV-EBMs pour *Latent variable energy based models*) viennent à la rescousse. 
Nous développons un exemple d'ellipse avec une entrée fixe et la formulation optimale du modèle. 
Ensuite, nous appliquons les EBMs à variables latentes pour inférer les meilleures variables latentes pouvant apprendre les relations implicites.


<!--
## Practicum part B

This section starts from introducing a relaxed version of free energy by modifying the "temperature" to smooth the energy function. 
Then we demonstrate how to train EBMs by minimizing loss functionals with several examples. 
Finally we give a concrete example of self-supervised learning, where we train a EBM to learn a horn-like data manifold.
-->

## Travaux dirigés partie B

Cette section introduit une version détendue de l'énergie libre en modifiant la « température » afin de lisser la fonction d’énergie.
Ensuite, nous montrons comment entraîner les EBMs en minimisant les pertes fonctionnelles à l'aide de plusieurs exemples. 
Enfin, nous donnons un exemple concret d'apprentissage autosupervisé, où nous entraînons un EBM à l'apprentissage d'une variété de type conique.

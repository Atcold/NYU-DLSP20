---
lang-ref: ch.12
title: সপ্তাহ ১২ 
lang: bn
translation-date: 19 Dec 2020
translator: Mobasshir Bhuiya Shagor
---

<!-- ## Lecture part A -->

## লেকচার পার্ট এ

<!-- In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively. -->

প্রথম অংশে আমরা ন্যাচারাল ল্যাংগুয়েজ প্রোসেসিং অ্যাপ্লিকেশনগুলিতে ব্যবহৃত বিভিন্ন আর্কিটেকচার তথা Convolutional Neural Network, Recurrent Neural Network দিয়ে শুরু করে স্টেট অফ দ্য আর্ট আর্কিটেকচার, ট্রান্সফর্মারগুলো পর্যন্ত আলোচনা করব। ট্রান্সফর্মারগুলো এর বিভিন্ন সমন্বয়ক মডিউল এবং উক্ত মডিউলগুলো কীভাবে ন্যাচারাল ল্যাংগুয়েজ প্রোসেসিং কাজের জন্য ট্রান্সফর্মারগুলোকে অগ্রগামী করে তুলে আমরা তা দেখবো। পরিশেষে, এমন কৌশলগুলি নিয়ে আলোচনা করব যা ট্রান্সফর্মারগুলোকে কার্যকরভাবে প্রশিক্ষিত হতে সাহায্য করে।

<!-- ## Lecture part B -->

## লেকচার পার্ট বি

<!-- In this section we introduce beam search as a middle ground between greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (_i.e._ when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT. -->

দ্বিতীয় অংশে গ্রিডি ডিকোডিং এবং একজস্টিভ সার্চের মধ্যবর্তী মাধ্যম হিসেবে বিম সার্চের সাথে পরিচিত হব। জেনেরেটিভ ডিসট্রিবিউশন থেকে নমুনা সংগ্রহ ক্ষেত্র বিবেচনা করা (উদাহরণস্বরূপ, টেক্সট উৎপন্ন এর সময়) এবং "টপ কে" নমুনা সিলেক্ট করা নিয়ে আলচনা করব। এরপর শুরু করব সিকুয়েন্স টু সিকুয়েন্স মডেলগুলো (একটি ট্রান্সফর্মার ভেরিয়েন্ট সহ) এবং ব্যাকট্রান্সলেশন পরিচিতি। এছাড়া আন-সুপারভাইজড লার্নিং পদ্ধতি এর মাধ্যমে এমবেডিংগুলি শিখব এবং word2vec, GPT, BERT নিয়ে আলোকপাত করব।

<!-- ## Practicum -->

## প্রাক্টিক্যাল কোর্সওয়ার্ক

<!-- We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures. -->

শেষ অংশে আমরা পরিচিত হব অ্যাটেনশন, সেলফ-অ্যাটেনশন এর সাথে এবং এখানে মূলত মনোযোগ দিব ইনপুটগুলির হিডেন-লেয়ার রিপ্রেজেন্টেশন এর দিকে। আমরা আরও দেখবো key-value আকারে তথ্য জমা করার পদ্ধতি এবং আলোচনা করব কিভাবে জিজ্ঞাসাগুলো, চিহ্নগুলো এবং মানগুলো ইনপুট হিসেবে পরিবর্তন করা যায়। অবশেষে, একটা বেসিক ট্রান্সফর্মার এর মধ্য দিয়ে একটা ফরওয়ার্ড পাস করে এবং এনকোডার-ডিকোডার পদ্ধতিটিকে সিকুয়েন্সিয়াল পদ্ধতি এর আর্কিটেকচারগুলোর সাথে তুলনা করে, ট্রান্সফর্মারগুলোর আর্কিটেকচারের ব্যাখ্যা প্রদানের ক্ষেত্রে অ্যাটেনশন এর ব্যাবহার দেখবো।

---
lang-ref: ch.06-3
title: RNN এবং LSTM মডেলের আর্কিটেকচার
lecturer: Alfredo Canziani
authors: Zhengyuan Ding, Biao Huang, Lin Jiang, Nhung Le
date: 3 Mar 2020
lang: bn
translation-date: 11 Mar 2021
translator: Mahbuba Tasmin
---


## [সার্বিক পরিদর্শন](https://www.youtube.com/watch?v=8cAffg2jaT0&t=21s)


<!--RNN is one type of architecture that we can use to deal with sequences of data. What is a sequence? From the CNN lesson, we learned that a signal can be either 1D, 2D or 3D depending on the domain. The domain is defined by what you are mapping from and what you are mapping to. Handling sequential data is basically dealing with 1D data since the domain is the temporal axis. Nevertheless, you can also use RNN to deal with 2D data, where you have two directions.-->
আরএনএন হল এক ধরণের আর্কিটেকচার যা আমরা ব্যবহার করতে পারি ডেটা সিক্যুয়েন্সগুলি নিয়ে কাজ  করতে। ক্রম কি? সিএনএন পাঠ থেকে আমরা শিখেছি যে ডোমেনের উপর নির্ভর করে একটি সংকেত 1D, 2D বা 3D হতে পারে। আপনি কী থেকে ম্যাপিং করছেন এবং আপনি কী ম্যাপিং করছেন তা দ্বারা ডোমেনটি সংজ্ঞায়িত করা হয়। অনুক্রমিক ডেটা হ্যান্ডলিং মূলত 1D ডেটা নিয়ে কাজ করে যেহেতু ডোমেনটি অস্থায়ী অক্ষ হয়। তবুও, আপনি 2 ডি ডেটা ব্যবহার করতে আরএনএন ব্যবহার করতে পারেন, যেখানে আপনার দুটি দিক রয়েছে।

### ভ্যানিলা *বনাম.* রিকারেন্ট এনএন


<!--Figure 1 is a vanilla neural network diagram with three layers. "Vanilla" is an American term meaning plain. The pink bubble is the input vector x, in the center is the hidden layer in green, and the final blue layer is the output. Using an example from digital electronics on the right, this is like a combinational logic, where the current output only depends on the current input.-->
চিত্র-১ হল তিনটি স্তর সহ একটি ভ্যানিলা নিউরাল নেটওয়ার্কের ডায়াগ্রাম। "ভ্যানিলা" একটি আমেরিকান শব্দ যার অর্থ সমতল। গোলাপী বুদ্বুদ হল ইনপুট ভেক্টর X, এর মাঝখানে সবুজ রঙের মধ্যে হিডেন লেয়ার এবং চূড়ান্ত নীল স্তরটি আউটপুট। ডানদিকের ডিজিটাল ইলেকট্রনিক্স থেকে একটি উদাহরণ ব্যবহার করে, এটি একটি যৌথ যুক্তির মতো, যেখানে বর্তমান আউটপুট কেবলমাত্র বর্তমান ইনপুটটির উপর নির্ভর করে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/vanilla.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 1:</b> Vanilla Architecture-->
চিত্র ১ঃ ভ্যানিলা আর্কিটেকচার
</center>

<!--In contrast to a vanilla neural network, in recurrent neural networks the current output depends not only on the current input but also on the state of the system, shown in Figure 2. This is like a sequential logic in digital electronics, where the output also depends on a "flip-flop" (a basic memory unit in digital electronics). Therefore the main difference here is that the output of a vanilla neural network only depends on the current input, while the one of RNN depends also on the state of the system.-->
ভ্যানিলা নিউরাল নেটওয়ার্কের বিপরীতে, রিকারেন্ট নিউরাল নেটওয়ার্কগুলিতে বর্তমান আউটপুট কেবল বর্তমান ইনপুটের উপর নির্ভর করে না, চিত্র 2-তে প্রদর্শিত সিস্টেমের অবস্থার উপরও নির্ভর করে, এটি ডিজিটাল ইলেক্ট্রনিক্সের অনুক্রমিক যুক্তির মতো, যেখানে আউটপুটটিও একটি "ফ্লিপ-ফ্লপ" (ডিজিটাল ইলেকট্রনিক্সের একটি বেসিক মেমরি ইউনিট) এর উপর নির্ভর করে। সুতরাং এখানে মূল পার্থক্যটি হল ভ্যানিলা নিউরাল নেটওয়ার্কের আউটপুট কেবলমাত্র বর্তমান ইনপুটের উপর নির্ভর করে, যখন RNN এর একটি সিস্টেমের অবস্থার উপরও নির্ভর করে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/rnn.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 2:</b> RNN Architecture-->
চিত্র ২ঃ আরএনএন আর্কিটেকচার
</center>

<center>
<img src="{{site.baseurl}}/images/week06/06-3/basic_neural_net.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 3:</b> Basic NN Architecture-->
চিত্র ৩ঃ প্রাথমিক এনএন আর্কিটেকচার
</center>

<!--Yann's diagram adds these shapes between neurons to represent the mapping between one tensor and another(one vector to another). For example, in Figure 3, the input vector x will map through this additional item to the hidden representations h. This item is actually an affine transformation *i.e.* rotation plus distortion. Then through another transformation, we get from the hidden layer to the final output. Similarly, in the RNN diagram, you can have the same additional items between neurons.-->
ইয়ান এর ডায়াগ্রামে এই আকৃতি গুলোকে নিউরনের মধ্যে যুক্ত করা হয় যাতে করে একটি টেনসর এবং অন্যটির মধ্যে একটি ম্যাপিং উপস্থাপন করা হয় (একটি ভেক্টর থেকে অন্য ভেক্টরে)। উদাহরণস্বরূপ, চিত্র ৩-এ, ইনপুট ভেক্টর X এই অতিরিক্ত আইটেমটির মাধ্যমে হিডেন উপস্থাপনা h- এর জন্য মানচিত্র তৈরি করবে । এই আইটেমটি আসলে একটি অ্যাফাইন ট্রান্সফরমেশন  * অর্থাত্ * ঘূর্ণন প্লাস বিকৃতি। তারপরে অন্য রূপান্তরের মাধ্যমে আমরা হিডেন লেয়ার থেকে চূড়ান্ত আউটপুটটিতে পৌঁছে যাই। একইভাবে,  আরএনএন ডায়াগ্রামে নিউরনের মধ্যে একই অতিরিক্ত আইটেম থাকতে পারে।

<center>
<img src="{{site.baseurl}}/images/week06/06-3/yann_rnn.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 4:</b> Yann's RNN Architecture-->
চিত্র ৪ঃ ইয়ানের আরএনএন আর্কিটেকচার
</center>


<!--### Four types of RNN Architectures and Examples-->
### চার ধরণের আরএনএন আর্কিটেকচার এবং উদাহরণ

<!--The first case is vector to sequence. The input is one bubble and then there will be evolutions of the internal state of the system annotated as these green bubbles. As the state of the system evolves, at every time step there will be one specific output.-->
প্রথম কেসটি সিকোয়েন্সের ভেক্টর। ইনপুটটি একটি বুদবুদ এবং তারপরে এই সবুজ বুদবুদ হিসাবে বর্ণিত সিস্টেমের অভ্যন্তরীণ অবস্থার বিবর্তন হবে। সিস্টেমের অবস্থা যেমন বিকশিত হয় ততবার প্রতিটি ধাপে একটি নির্দিষ্ট আউটপুট আসবে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/vec_seq.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 5:</b> Vec to Seq-->
চিত্র ৫ঃ ভেক্টর টু সিকোয়েন্স
</center>

<!--An example of this type of architecture is to have the input as one image while the output will be a sequence of words representing the English descriptions of the input image. To explain using Figure 6, each blue bubble here can be an index in a dictionary of English words. For instance, if the output is the sentence "This is a yellow school bus". You first get the index of the word "This" and then get the index of the word "is" and so on. Some of the results of this network are shown below. For example, in the first column the description regarding the last picture is "A herd of elephants walking across a dry grass field.", which is very well refined. Then in the second column, the first image outputs "Two dogs play in the grass.", while it's actually three dogs. In the last column are the more wrong examples such as "A yellow school bus parked in a parking lot." In general, these results show that this network can fail quite drastically and perform well sometimes. This is the case that is from one input vector, which is the representation of an image, to a sequence of symbols, which are for example characters or words making up the English sentences. This kind of architecture is called an autoregressive network. An autoregressive network is a network which gives an output given that you feed as input the previous output.-->
এই ধরণের আর্কিটেকচারের উদাহরণ হল ইনপুটটি একটি চিত্র হিসাবে থাকে তবে আউটপুটটি শব্দের অনুক্রম হবে যা ইনপুট চিত্রের ইংরেজী বিবরণ উপস্থাপন করে। চিত্র-৬ ব্যবহার করে ব্যাখ্যা করার জন্য, এখানে প্রতিটি নীল বুদ্বুদ ইংরেজি শব্দের অভিধানে একটি সূচক হতে পারে। উদাহরণস্বরূপ, যদি আউটপুটটি এই বাক্যটি হয় "এটি হলুদ স্কুল বাস"। আপনি প্রথমে "এটি" শব্দের সূচি টি পান এবং তারপরে "হয়" শব্দের সূচি পেতে পারেন ইত্যাদি । এই নেটওয়ার্কের কিছু ফলাফল নীচে দেখানো হয়েছে। উদাহরণস্বরূপ, প্রথম কলামে শেষ চিত্রটি  সম্পর্কিত বিবরণটি হ'ল "একটি শুকনো ঘাসের ক্ষেত পেরিয়ে হাতির একটি পাল হাঁটছে", যা খুব ভালভাবে পরিশ্রুত। তারপরে দ্বিতীয় কলামে, প্রথম চিত্রটি "দুটি কুকুর ঘাসে খেলবে" ", যখন এটিতে আসলে তিনটি কুকুর আছে। শেষ কলামে আরও বেশি ভুল উদাহরণ রয়েছে যেমন "একটি পার্কিংয়ের জায়গায় একটি হলুদ স্কুল বাস"। সাধারণভাবে, এই ফলাফলগুলি দেখায় যে এই নেটওয়ার্কটি বেশ মারাত্মকভাবে ব্যর্থ হতে পারে এবং কখনও কখনও ভাল সম্পাদন করতে পারে। এটি এমন একটি প্রেক্ষাপট যেখানে একটি ইনপুট ভেক্টর থেকে, যা একটি চিত্রের প্রতিনিধিত্ব করে, প্রতীকগুলির অনুক্রম হয়, যা উদাহরণস্বরূপ অক্ষর বা শব্দের জন্য ইংরেজি বাক্যগুলি তৈরি করে। এই ধরণের আর্কিটেকচারকে অটোরেগ্রেসিভ নেটওয়ার্ক বলা হয়। একটি অটোরেগ্রেসিভ নেটওয়ার্ক এমন একটি নেটওয়ার্ক যাতে পূর্ববর্তী আউটপুটটিকে ইনপুট হিসাবে ব্যবহার করার পরিপ্রেক্ষিতে এমন একটি আউটপুট দেয়।

<center>
<img src="{{site.baseurl}}/images/week06/06-3/image_to_text_vec2seq.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 6:</b> vec2seq Example: Image to Text-->
চিত্র ৬ঃ ভেক্টর টূ সিকোয়েন্স উদাহরণঃ ছবি থেকে পাঠ্য
</center>

<!--The second type is sequence to a final vector. This network keeps feeding a sequence of symbols and only at the end gives a final output. An application of this can be using the network to interpret Python. For example, the input are these lines of Python program.-->
দ্বিতীয় প্রকারটি একটি চূড়ান্ত ভেক্টরের ক্রম। এই নেটওয়ার্কটি প্রতীকগুলির ক্রমকে ব্যবহার করতে থাকে এবং কেবল শেষে একটি চূড়ান্ত আউটপুট দেয়। পাইথনের ব্যাখ্যার জন্য এর একটি অ্যাপ্লিকেশনটি নেটওয়ার্কটি ব্যবহার করতে পারে। উদাহরণস্বরূপ, ইনপুটটি পাইথন প্রোগ্রামের এই লাইনগুলি।

<center>
<img src="{{site.baseurl}}/images/week06/06-3/seq2vec.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 7:</b> Seq to Vec-->
চিত্র ৭ঃ সিকোয়েন্স টু ভেক্টর
</center>

<center>
<img src="{{site.baseurl}}/images/week06/06-3/second_1.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 8:</b> Input lines of Python Codes-->
চিত্র ৮ঃ পাইথন কোডের ইনপুট লাইনস
</center>

<!--Then the network will be able to output the correct solution of this program. Another more complicated program like this:-->
তারপরে নেটওয়ার্ক এই প্রোগ্রামটির সঠিক সমাধানটি আউটপুট করতে সক্ষম হবে। এর মতো আরও জটিল প্রোগ্রাম:
<center>
<img src="{{site.baseurl}}/images/week06/06-3/second_2.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 9:</b> Input lines of Python Codes in a more Completed Case-->
চিত্র ৯ঃ অপেক্ষাকৃত পূর্ণ ক্ষেত্রের পাইথন কোডের ইনপুট লাইনস
</center>

<!--Then the output should be 12184. These two examples display that you can train a neural network to do this kind of operation. We just need to feed a sequence of symbols and enforce the final output to be a specific value.-->
তারপরে আউটপুটটি ১২১৮৪ হওয়া উচিত। এই দুটি উদাহরণ প্রদর্শিত হয় যে আপনি এই ধরণের অপারেশন করতে নিউরাল নেটওয়ার্ককে প্রশিক্ষণ দিতে পারেন। আমাদের কেবল একটি চিহ্নের ক্রম ব্যবহার করতে হবে এবং একটি নির্দিষ্ট মান হিসাবে চূড়ান্ত আউটপুট প্রয়োগ করতে হবে।
<!--The third is sequence to vector to sequence, shown in Figure 10. This architecture used to be the standard way of performing language translation. You start with a sequence of symbols here shown in pink. Then everything gets condensed into this final h, which represents a concept. For instance, we can have a sentence as input and squeeze it temporarily into a vector, which is representing the meaning and message that to send across. Then after getting this meaning in whatever representation, the network unrolls it back into a different language. For example "Today I'm very happy" in a sequence of words in English can be translated into Italian or Chinese. In general, the network gets some kind of encoding as inputs and turns them into a compressed representation. Finally, it performs the decoding given the same compressed version. In recent times we have seen networks like Transformers, which we will cover in the next lesson, outperform this method at language translation tasks. This type of architecture used to be the state of the art about two years ago (2018).-->
তৃতীয়টি ভেক্টর থেকে সিক্যুয়েন্সের ক্রম, চিত্র -১০ এ দেখানো হয়েছে। এই আর্কিটেকচারটি ভাষা অনুবাদ সম্পাদনের মানক উপায় হিসাবে ব্যবহৃত হত। আপনি এখানে গোলাপীতে দেখানো প্রতীকগুলির ক্রম দিয়ে শুরু করুন । তারপরে সবকিছু এই চূড়ান্ত-h এ সংশ্লেষিত হয়ে যায় যা একটি ধারণার প্রতিনিধিত্ব করে। উদাহরণস্বরূপ, আমরা ইনপুট হিসাবে একটি বাক্য পেতে পারি এবং এটি একটি ভেক্টরে সাময়িকভাবে চেপে ধরতে পারি, যা অর্থ এবং বার্তাটি উপস্থাপন করে যা প্রেরণ করতে পারে। তারপরে যেকোন উপস্থাপনায় এই অর্থটি পাওয়ার পরে, নেটওয়ার্ক এটিকে আবার আলাদা ভাষায় তালিকাভুক্ত করে। উদাহরণস্বরূপ, "আজ আমি খুব খুশি" ইংরেজি শব্দের অনুক্রমে ইতালীয় বা চীনা অনুবাদ করা যেতে পারে। সাধারণভাবে, নেটওয়ার্ক ইনপুট হিসাবে এক ধরণের এনকোডিং পায় এবং এগুলি সংকীর্ণ উপস্থাপনায় রূপান্তরিত করে। অবশেষে, এটি একই সংকোচিত সংস্করণে ডিকোডিং সম্পাদন করে।
সাম্প্রতিক সময়ে আমরা ট্রান্সফরমারগুলির মতো নেটওয়ার্ক দেখেছি, যা আমরা পরবর্তী পাঠে কভার করব, ভাষা অনুবাদ কার্যগুলিতে এই পদ্ধতিটিকে ছাড়িয়ে যাবে। এই ধরণের আর্কিটেকচারটি প্রায় দুই বছর আগে (2018) স্টেট অফ দ্য আর্ট হিসাবে ব্যবহৃত হত।

<center>
<img src="{{site.baseurl}}/images/week06/06-3/seq2vec2seq.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 10:</b> Seq to Vec to Seq-->
চিত্র ১০ঃ সিকোয়েন্স টু ভেক্টর টু সিকোয়েন্স
</center>

<!--If you do a PCA over the latent space, you will have the words grouped by semantics like shown in this graph.-->
আপনি যদি সুপ্ত স্পেসের উপর একটি পিসিএ করেন, আপনাকে শব্দার্থবিজ্ঞানের (সেমান্টিকস) দ্বারা শব্দগুলি এই গ্রাফের মতো দেখানো হবে।

<center>
<img src="{{site.baseurl}}/images/week06/06-3/third_1.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 11:</b> Words Grouped by Semantics after PCA-->
চিত্র ১১ঃ পিসিএ পরবর্তী সেমান্টিকস দ্বারা গুচ্ছকৃত শব্রি
</center>

<!--If we zoom in, we will see that the in the same location there are all the months, like January and November.-->
আমরা যদি জুম করি তবে আমরা দেখতে পাব যে একই অবস্থানটিতে জানুয়ারি এবং নভেম্বর এর মতো সমস্ত মাস রয়েছে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/third_2.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 12:</b> Zooming in Word Groups-->-->
চিত্র ১২ঃ শব্দ গুচ্ছে জুম করে দেখা
</center>

<!--If you focus on a different region, you get phrases like "a few days ago " "the next few months" etc.-->
আপনি যদি কোনও আলাদা অঞ্চলে মনোনিবেশ করেন তবে আপনি "কয়েক দিন আগে" "পরের কয়েক মাস" ইত্যাদির মত বাক্যাংশ পান
<center>
<img src="{{site.baseurl}}/images/week06/06-3/third_3.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 13:</b> Word Groups in another Region-->
চিত্র ১৩ঃ অন্য অঞ্চলের শব্দ গুচ্ছ
</center>

<!--From these examples, we see that different locations will have some specific common meanings.-->
এই উদাহরণগুলি থেকে আমরা দেখতে পাচ্ছি যে বিভিন্ন অবস্থানের কিছু নির্দিষ্ট সাধারণ অর্থ থাকবে।

<!--Figure 14 showcases how how by training this kind of network will pick up on some semantics features. For exmaple in this case you can see there is a vector connecting man to woman and another between king and queen, which means woman minus man is going to be equal to queen minus king. You will get the same distance in this embeddings space applied to cases like male-female. Another example will be walking to walked and swimming to swam. You can always apply this kind of specific linear transofmation going from one word to another or from country to capital.-->
চিত্র -১৪ দেখায় যে কীভাবে এই ধরণের নেটওয়ার্ক প্রশিক্ষণ দেওয়ার মাধ্যমে কিছু শব্দার্থবিজ্ঞানের বৈশিষ্ট্য দেখা যায়। এক্ষেত্রে উদাহরণ রূপে আপনি দেখতে পাচ্ছেন যে একজন ভেক্টর পুরুষকে মহিলার সাথে এবং আরেকজন রাজা ও রানির সাথে সংযুক্ত করছেন, যার অর্থ মহিলা বিয়োগ পুরুষ রানী বিয়োগের রাজার সমান হতে চলেছে। আপনি এই এমবেডিং স্পেসে একই দূরত্বটি পাবেন যা পুরুষ-মহিলার মতো ক্ষেত্রে প্রযোজ্য। আরেকটি উদাহরণ হবে হাঁটছে থেকে হেঁটেছিল এবং সাঁতার করছে থেকে  সাঁতার করেছিল। আপনি সর্বদা এই রকম শব্দের নির্দিষ্ট রৈখিক রূপান্তর একরকম শব্দ থেকে অন্য শব্দে কিংবা দেশ থেকে রাজধানীতে প্রয়োগ করতে পারেন।

<center>
<img src="{{site.baseurl}}/images/week06/06-3/fourth.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 14:</b> Semantics Features Picked during Training-->
চিত্র ১৪ঃ ট্রেইনিং কালে বাছাইকৃত সেমান্টিকস বৈশিষ্ট্য
</center>

<!--The fourth and final case is sequence to sequence. In this network, as you start feeding in input the network starts generating outputs. An example of this type of architecture is T9, if you remember using a Nokia phone, you would get text suggestions as you were typing. Another example is speech to captions. One cool example is this RNN-writer. When you start typing "the rings of Saturn glittered while", it suggests the following "two men looked at each other". This network was trained on some sci-fi novels so that you can just type something and let it make suggestions to help you write a book. One more example is shown in Figure 16. You input the top prompt and then this network will try to complete the rest.-->
চতুর্থ এবং চূড়ান্ত কেসটি সিকোয়েন্সের ক্রম। এই ইনপুটটিতে আপনি ইনপুট ব্যবহার শুরু করার সাথে সাথে নেটওয়ার্ক আউটপুট উত্পন্ন করতে শুরু করে। এই ধরণের আর্কিটেকচারের উদাহরণ T9, আপনি যদি কোনও নোকিয়া ফোন ব্যবহার করে থাকেন, তবে আপনি টাইপ করার সাথে সাথে লেখ্যর পরামর্শ পেয়েছিলেন । অন্য উদাহরণটি হচ্ছে বক্তব্য থেকে  শিরোনাম থেকে বক্তব্য। একটি দুর্দান্ত উদাহরণ হলেন এই আরএনএন-রাইটার। আপনি যখন "শনির আংটি জ্বলজ্বল করার সময়" টাইপ করা শুরু করেন, এটি উল্লেখিত  "দু'জন একে অপরের দিকে তাকাতে" পরামর্শ দেয়। এই নেটওয়ার্কটি কিছু বিজ্ঞান ভিত্তিক উপন্যাসে প্রশিক্ষিত হয়েছিল যাতে আপনি কেবল কিছু টাইপ করতে পারেন এবং আপনাকে একটি বই লিখতে সহায়তা করার জন্য পরামর্শ দেওয়ার সুযোগ দেয়। আরও একটি উদাহরণ চিত্র-১৬ এ প্রদর্শিত হয়েছে। আপনি শীর্ষ প্রম্পটটি ইনপুট করুন এবং তারপরে এই নেটওয়ার্কটি বাকিটি সম্পূর্ণ করার চেষ্টা করবে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/seq2seq.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 15:</b> Seq to Seq-->
চিত্র ১৫ঃ সিকোয়েন্স টূ সিকোয়েন্স
</center>

<center>
<img src="{{site.baseurl}}/images/week06/06-3/seq2seq_model_completion.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 16:</b> Text Auto-Completion Model of Seq to Seq Model-->
চিত্র ১৬ঃ সিকোয়েন্স টূ সিকোয়েন্স মডেলের টেক্সট অটো-কমপ্লিশন মডেল
</center>

## [সময়ের সাথে ব্যাক-প্রোপাগেশন](https://www.youtube.com/watch?v=8cAffg2jaT0&t=855s)


<!--### Model architecture-->
###  মডেল আর্কিটেকচার

<!--In order to train an RNN, backpropagation through time (BPTT) must be used. The model architecture of RNN is given in the figure below. The left design uses loop representation while the right figure unfolds the loop into a row over time.-->
আরএনএনকে প্রশিক্ষণের জন্য, সময়ের সাথে ব্যাক-প্রোপাগেশন (বিপিটিটি) অবশ্যই ব্যবহার করতে হবে। আরএনএন এর মডেল আর্কিটেকচারটি নীচের চিত্রে দেওয়া হয়েছে। বাম নকশাটি লুপের উপস্থাপনা ব্যবহার করে যখন ডান চিত্রটি সময়ের সাথে সাথে একটি সারিতে লুপটি প্রকাশ করে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/bptt.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 17:</b> Back Propagation through time-->
চিত্র ১৭ঃ সময়ের সাথে ব্যাক-প্রোপাগেশন
</center>

<!--Hidden representations are stated as-->
হিডেন উপস্থাপনা বর্ণিত হয় নিম্নরূপে

$$
\begin{aligned}
\begin{cases}
h[t]&= g(W_{h}\begin{bmatrix}
x[t] \\
h[t-1]
\end{bmatrix}
+b_h)  \\
h[0]&\dot=\ \boldsymbol{0},\ W_h\dot=\left[ W_{hx} W_{hh}\right] \\
\hat{y}[t]&= g(W_yh[t]+b_y)
\end{cases}
\end{aligned}
$$

<!--The first equation indicates a non-linear function applied on a rotation of a stack version of input where the previous configuration of the hidden layer is appended. At the beginning, $h[0]$ is set 0. To simplify the equation, $W_h$ can be written as two separate matrices, $\left[ W_{hx}\ W_{hh}\right]$, thus sometimes the transformation can be stated as.-->
প্রথম সমীকরণটি ইনপুটটির স্ট্যাক সংস্করণের ঘূর্ণনের উপর প্রয়োগ করা একটি নন -লিনিয়ার ফাংশন নির্দেশ করে যেখানে হিডেন স্তরের পূর্ববর্তী কনফিগারেশন সংযুক্ত করা হয়। শুরুতে, $h[0]$ সেট করা আছে 0 মান হিসেবে।  সমীকরণটি সহজ করার জন্য, $W_h$ কে দুটি পৃথক ম্যাট্রিক হিসাবে লেখা যায়, $\left[ W_{hx}\ W_{hh}\right]$, এভাবে কখনও কখনও রূপান্তর হিসাবে বলা যেতে পারে ।

$$
W_{hx}\cdot x[t]+W_{hh}\cdot h[t-1]
$$

<!--which corresponds to the stack representation of the input.-->
যা ইনপুটটির স্ট্যাক উপস্থাপনার সাথে মিলে যায়।


<!--$y[t]$ is calculated at the final rotation and then we can use the chain rule to backpropagate the error to the previous time step.-->
$y[t]$ চূড়ান্ত রোটেশনের সময় গণনা করা হয় এবং তারপরে আমরা ত্রুটিটিকে আগের সময়ের ধাপে ব্যাকপ্রোপাগেট করতে চেইন বিধিটি ব্যবহার করতে পারি।

<!--### Batch-Ification in Language Modeling-->
### ভাষা মডেলিংয়ে ব্যাচ-ইফিকেশন

<!--When dealing with a sequence of symbols, we can batchify the text into different sizes. For example, when dealing with sequences shown in the following figure, batch-ification can be applied first, where the time domain is preserved vertically. In this case, the batch size is set to 4.-->
প্রতীকগুলির ক্রম নিয়ে কাজ করার সময়, আমরা লেখ্যটি বিভিন্ন আকারে দলভুক্ত করতে পারি। উদাহরণস্বরূপ, নিম্নলিখিত চিত্রটিতে প্রদর্শিত ক্রমগুলির সাথে কাজ করার সময়, ব্যাচ-ইফিকেশন প্রথমে প্রয়োগ করা যেতে পারে, যেখানে সময় ডোমেনটি উল্লম্বভাবে সংরক্ষণ করা হয়। এই ক্ষেত্রে, ব্যাচের আকার ৪  এ সেট করা আছে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/batchify_1.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 18:</b> Batch-Ification-->
চিত্র ১৮ঃ ব্যাচ-ইফিকেশন
</center>

<!--If BPTT period $T$ is set to 3, the first input $x[1:T]$ and output $y[1:T]$ for RNN is determined as-->
বিপিটিটি পিরিয়ড $T$ ৩ এ সেট করা থাকলে, আরএনএন-এর জন্য প্রথম ইনপুট $x[1: T]$ এবং আউটপুট $y[1: T]$ হিসাবে নির্ধারিত হয়।

$$
\begin{aligned}
x[1:T] &= \begin{bmatrix}
a & g & m & s \\
b & h & n & t \\
c & i & o & u \\
\end{bmatrix} \\
y[1:T] &= \begin{bmatrix}
b & h & n & t \\
c & i & o & u \\
d & j & p & v
\end{bmatrix}
\end{aligned}
$$

<!--When performing RNN on the first batch, firstly, we feed $x[1] = [a\ g\ m\ s]$ into RNN and force the output to be $y[1] = [b\ h\ n\ t]$. The hidden representation $h[1]$ will be sent forward into next time step to help the RNN predict $y[2]$ from $x[2]$. After sending $h[T-1]$ to the final set of $x[T]$ and $y[T]$, we cut gradient propagation process for both $h[T]$ and $h[0]$ so that gradients will not propagate infinitely(.detach() in Pytorch). The whole process is shown in figure below.-->
প্রথম ব্যাচে আরএনএন করার সময়, প্রথমত, আমরা আরএনএন-তে $x[1] = [a\ g\ m\ s]$  ব্যবহার করি এবং আউটপুটকে $y[1] = [b\ h\ n\ t]$  হতে বাধ্য করি। $x[2]$ থেকে
$y[2]$ কে  আরএনএন দ্বারা  পূর্বাভাসে  সহায়তা করার জন্য হিডেন উপস্থাপনা $h[1]$ টিকে পরবর্তী সময় ধাপে প্রেরণ করা হবে।  $x[T]$ এবং $y[T]$ এর চূড়ান্ত সেটে
$h[T-1]$ প্রেরণের পরে, আমরা $h[T]$ এবং $h[0]$ উভয়ের জন্য গ্র্যাডিয়েন্ট প্রোপাগেশন  প্রক্রিয়াটি কেটে দিলাম যাতে করে গ্র্যাডিয়েন্ট গুলো  অসম্পূর্ণভাবে প্রোপাগেট করবে না (.detach () in Pytorch) । পুরো প্রক্রিয়াটি নীচের চিত্রে দেখানো হয়েছে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/batchify_2.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 19:</b> Batch-Ification-->
চিত্র ১৯ঃ ব্যাচ-ইফিকেশন
</center>


## ভ্যানিশিং এবং এক্সপ্লোডিং গ্র্যাডিয়েন্ট


<!--### Problem-->
### সমস্যা
<center>
<img src="{{site.baseurl}}/images/week06/06-3/rnn_3.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 20:</b> Vanishing Problem-->
চিত্র ২০ঃ বিলীয়মান সমস্যা
</center>

<!--The figure above is a typical RNN architecture. In order to perform rotation over previous steps in RNN, we use matrices, which can be regarded as horizontal arrows in the model above. Since the matrices can change the size of outputs, if the determinant we select is larger than 1, the gradient will inflate over time and cause gradient explosion. Relatively speaking, if the eigenvalue we select is small across 0, the propagation process will shrink gradients and leads to the gradient vanishing.-->
উপরের চিত্রটি একটি সাধারণ আরএনএন আর্কিটেকচার। আরএনএন-এ পূর্ববর্তী পদক্ষেপগুলির উপর রোটেশন করার জন্য, আমরা ম্যাট্রিক্স ব্যবহার করি, যা উপরের মডেলটিতে অনুভূমিক তীর হিসাবে বিবেচিত হতে পারে। যেহেতু ম্যাট্রিকগুলি আউটপুটগুলির আকার পরিবর্তন করতে পারে, নির্বাচিত নির্ধারক যদি 1 এর চেয়ে বড় হয় তবে গ্র্যাডিয়েন্ট  সময়ের সাথে সাথে উত্থিত হয় এবং গ্র্যাডিয়েন্ট এক্সপ্লোশন  ঘটায়। তুলনামূলকভাবে বলতে গেলে, আমরা যে আইজেন ভ্যালু টি নির্বাচন করি তা যদি 0 এর কাছাকাছি  ছোট হয় তবে প্রোপাগেশন প্রক্রিয়াটি গ্র্যাডিয়েন্টগুলি সঙ্কুচিত করবে এবং গ্র্যাডিয়েন্ট বিলুপ্ত হয়ে যাবে।
<!--In typical RNNs, gradients will be propagated through all the possible arrows, which provides the gradients a large chance to vanish or explode. For example, the gradient at time 1 is large, which is indicated by the bright color. When it goes through one rotation, the gradient shrinks a lot and at time 3, it gets killed.-->
সাধারণ আরএনএনগুলিতে, গ্র্যাডিয়েন্টগুলি সমস্ত সম্ভাব্য তীরগুলির মধ্য দিয়ে প্রোপাগেট  করা হবে, যা গ্র্যাডিয়েন্টগুলি বিলুপ্ত বা এক্সপ্লোশনের একটি বৃহৎ সুযোগ তৈরি করে। উদাহরণস্বরূপ, সময় ধাপ ১-এর গ্র্যাডিয়েন্টটি বড়, যা উজ্জ্বল রঙ দ্বারা নির্দেশিত। যখন এটি একটি আবর্তনের মধ্য দিয়ে যায়, গ্র্যাডিয়েন্টটি অনেকগুলি সঙ্কুচিত হয় এবং সময়-৩ এ এটি শেষ হয়।

<!--### Solution-->
### সমাধান
<!--An ideal to prevent gradients from exploding or vanishing is to skip connections. To fulfill this, multiply networks can be used.-->
গ্র্যাডিয়েন্টগুলি এক্সপ্লোড হওয়া বা বিলীন হওয়া থেকে রক্ষা করার জন্য একটি আদর্শ হল সংযোগগুলি এড়িয়ে যাওয়া (স্কিপ কানেকশন)। এটি পূরণ করতে, একাধিক নেটওয়ার্ক ব্যবহার করা যেতে পারে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/rnn_2.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 21:</b> Skip Connection-->
চিত্র ২১ঃ স্কিপ কানেকশন
</center>

<!--In the case above, we split the original network into 4 networks. Take the first network for instance. It takes in a value from input at time 1 and sends the output to the first intermediate state in the hidden layer. The state has 3 other networks where the $\circ$s allows the gradients to pass while the $-$s blocks propagation. Such a technique is called gated recurrent network.-->
উপরের ক্ষেত্রে, আমরা মূল নেটওয়ার্কটি ৪টি নেটওয়ার্কে বিভক্ত করি। উদাহরণস্বরূপ প্রথম নেটওয়ার্ক নিন। এটি সময় ধাপ ১-এ ইনপুট থেকে একটি মান গ্রহণ করে এবং আউটপুটটিকে হিডেন লেয়ারের প্রথম মধ্যবর্তী স্টেটে প্রেরণ করে। স্টেটটিতে আরও ৩টি নেটওয়ার্ক রয়েছে যেখানে $\circ$s\গ্র্যাডিয়েন্ট গুলোকে  কাজ করার সুযোগ দেয় যখন $-$s প্রোপাগেশনে বাধা দেয় । যেমন একটি কৌশলকে গেটেড রিকারেন্ট নেটওয়ার্ক বলা হয়।
<!--LSTM is one prevalent gated RNN and is introduced in detail in the following sections.-->
এলএসটিএম হল একটি প্রচলিত গেটেড আরএনএন এবং নিম্নলিখিত বিভাগে বিস্তারিতভাবে চালু করা হয়েছে।

## [লং শর্ট-টার্ম মেমরি](https://www.youtube.com/watch?v=8cAffg2jaT0&t=1838s)
<!--### Model Architecture-->
### মডেল আর্কিটেকচার

<!--Below are equations expressing an LSTM. The input gate is highlighted by yellow boxes, which will be an affine transformation. This input transformation will be multiplying $c[t]$, which is our candidate gate.-->
নীচে একটি এলএসটিএম প্রকাশের সমীকরণ রয়েছে। ইনপুট গেটটি হলুদ বাক্স দ্বারা হাইলাইট করা হয়েছে, এটি একটি অ্যাফাইন ট্রান্সফর্মেশন হবে। এই ইনপুট রূপান্তরটি গুণমান হবে $c[t]$, যা আমাদের প্রার্থী গেট।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/lstm.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 22:</b> LSTM Architecture-->
চিত্র ২২ঃ এলএসটিএম আর্কিটেকচার
</center>

<!--Don’t forget gate is multiplying the previous value of cell memory $c[t-1]$. Total cell value $c[t]$ is don’t forget gate plus input gate. Final hidden representation is element-wise multiplication between output gate $o[t]$ and hyperbolic tangent version of the cell $c[t]$, such that things are bounded. Finally, candidate gate $\tilde{c}[t]$ is simply a recurrent net. So we have a $o[t]$ to modulate the output, a $f[t]$ to modulate the don’t forget gate, and a $i[t]$ to modulate the input gate. All these interactions between memory and gates are multiplicative interactions. $i[t]$, $f[t]$ and $o[t]$ are all sigmoids, going from zero to one. Hence, when multiplying by zero, you have a closed gate. When multiplying by one, you have an open gate.-->
ডোন্ট ফরগেট গেটটি সেল মেমরির আগের মানটি $c[t-1]$ গুণ করে চলেছে। $c[t]$ সেলের মোট মান হচ্ছে ইনপুট গেট এবং ডোন্ট ফরগেট গেটের সমষ্টি। চূড়ান্ত হিডেন উপস্থাপনা হল আউটপুট গেট $o[t]$ এবং $c[t]$ সেলের হাইপারবোলিক ট্যানজেন্ট সংস্করণ এর মধ্যে উপাদান অনুসারে গুণ,যাতে করে জিনিসগুলি আবদ্ধ থাকে । পরিশেষে, প্রার্থীর গেট $\tilde{c}[t]$ কেবল একটি রিকারেন্ট নেট। সুতরাং আমাদের কাছে আউটপুট নিয়ন্ত্রণ করার জন্য একটি $o[t]$ আছে , ডোন্ট ফরগেট গেটটি নিয়ন্ত্রণের জন্য একটি $f[t]$ এবং ইনপুট গেটটি নিয়ন্ত্রণের জন্য একটি $i[t]$ রয়েছে। মেমরি এবং গেটগুলির মধ্যে এই সমস্ত কার্যকলাপ হল গুণ মূলক কার্যপ্রণালী  । $i[t]$, $f[t]$ এবং $o[t]$ সবগুলো হল সিগময়েড, ০ থেকে ১-এ চলেছে। অতএব, শূন্য দ্বারা গুণ করলে, আপনার একটি ক্লোজড গেট থাকে। যখন এক দ্বারা গুণ করা হচ্ছে , আপনার একটি ওপেন গেট আছে।
<!--How do we turn off the output? Let’s say we have a purple internal representation $th$ and put a zero in the output gate. Then the output will be zero multiplied by something, and we get a zero. If we put a one in the output gate, we will get the same value as the purple representation.-->
কিভাবে আমরা আউটপুট বন্ধ করব? ধরা যাক আমাদের বেগুনি রঙের অভ্যন্তরীণ উপস্থাপনা রয়েছে $th$ এবং আউটপুট গেটে একটি শূন্য রেখেছি। তারপরে আউটপুট শূন্য হবে কোন কিছুর দ্বারা গুণ করলে এবং আমরা একটি শূন্য পাই। যদি আমরা একটি আউটপুট গেটে রাখি তবে আমরা বেগুনি উপস্থাপনার সমান মান পাব।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/lstm_2.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 23:</b> LSTM Architecture - Output On-->
চিত্র ২৩ঃ এলএসটিএম আর্কিটেকচার - আউটপুট অন
</center>

<center>
<img src="{{site.baseurl}}/images/week06/06-3/lstm_3.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 24:</b> LSTM Architecture - Output Off-->
চিত্র ২৪ঃ এলএসটিএম আর্কিটেকচার - আউটপুট অফ
</center>

<!--Similarly, we can control the memory. For example, we can reset it by having $f[t]$ and $i[t]$ to be zeros. After multiplication and summation, we have a zero inside the memory. Otherwise, we can keep the memory, by still zeroing out the internal representation $th$ but keep a one in $f[t]$. Hence, the sum gets $c[t-1]$ and keeps sending it out. Finally, we can write such that we can get a one in the input gate, the multiplication gets purple, then set a zero in the don’t forget gate so it actually forget.-->
একইভাবে, আমরা মেমরি নিয়ন্ত্রণ করতে পারি। উদাহরণস্বরূপ, আমরা $f[t]$ এবং $i[t]$ জিরো থাকার মাধ্যমে এটি পুনরায় সেট করতে পারি। গুণ এবং যোগের পরে, আমাদের মেমরির ভিতরে একটি শূন্য রয়েছে। অন্যথায়, আমরা এখনও অভ্যন্তরীণ উপস্থাপনা $th$-কে শূন্য করে মেমরিটি রাখতে পারি তবে $f[t]$ এ রাখতে পারি একটি ১ রেখে। সুতরাং, যোগফলটি $c[t-1]$ পায় এবং এটি প্রেরণ করেই চলে। অবশেষে, আমরা এমনটি লিখতে পারি যে আমরা ইনপুট গেটে একটি ১ পেতে পারি, গুণটি বেগুনি হয়ে যায়, তারপর ডোন্ট ফরগেট গেটে একটি শূন্য সেট করুন যাতে এটি আসলে ভুলে যায়।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/memory_cell_vis.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 25:</b> Visualization of the Memory Cell-->
চিত্র ২৫ঃ মেমরি সেল দৃশ্যায়ন
</center>

<center>
<img src="{{site.baseurl}}/images/week06/06-3/lstm_4.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 26:</b> LSTM Architecture - Reset Memory-->
চিত্র ২৬ঃ এলএসটিএম আর্কিটেকচার - রিসেট মেমরি
</center>

<center>
<img src="{{site.baseurl}}/images/week06/06-3/lstm_keep_memory.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 27:</b> LSTM Architecture - Keep Memory-->
চিত্র ২৭ঃ এলএসটিএম আর্কিটেকচার - কিপ মেমরি
</center>

<center>
<img src="{{site.baseurl}}/images/week06/06-3/lstm_write_memory.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 28:</b> LSTM Architecture - Write Memory-->
চিত্র ২৮ঃ এলএসটিএম আর্কিটেকচার - রাইট মেমরি
</center>


<!--## Notebook Examples-->
## নোটবুকের উদাহরণ সমূহ


<!--### Sequence Classification-->
### সিকোয়েন্স শ্রেণিবিন্যাস

<!--The goal is to classify sequences. Elements and targets are represented locally (input vectors with only one non-zero bit). The sequence **b**egins with an `B`, **e**nds with a `E` (the “trigger symbol”), and otherwise consists of randomly chosen symbols from the set `{a, b, c, d}` except for two elements at positions $t_1$ and $t_2$ that are either `X` or `Y`. For the `DifficultyLevel.HARD` case, the sequence length is randomly chosen between 100 and 110, $t_1$ is randomly chosen between 10 and 20, and $t_2$ is randomly chosen between 50 and 60. There are 4 sequence classes `Q`, `R`, `S`, and `U`, which depend on the temporal order of `X` and `Y`. The rules are: `X, X -> Q`; `X, Y -> R`; `Y, X -> S`; `Y, Y -> U`.-->
লক্ষ্যটি হচ্ছে ক্রম শ্রেণিবদ্ধ করা। উপাদান এবং লক্ষ্যগুলি স্থানীয়ভাবে প্রতিনিধিত্ব করা হয় (কেবলমাত্র একটি শূন্য বিটযুক্ত ইনপুট ভেক্টর)। ক্রমটি **শু**রু হয় একটি  `B` দিয়ে , **শে**ষ হয় একটি  `E` দিয়ে ("ট্রিগার প্রতীক") এবং অন্যথায় সেট  `{a, b, c, d}` থেকে এলোমেলোভাবে নির্বাচিত প্রতীকগুলি নিয়ে গঠিত হয়  শুধুমাত্র  $t_1$ এবং  $t_2$ পজিশনে দুটি উপাদান  `X` বা `Y`ছাড়া । `ডিফিকাল্টি  লেভেল.হার্ড` এর ক্ষেত্রে , অনুক্রমের দৈর্ঘ্যটি এলোমেলোভাবে ১০০ এবং ১১০ এর মধ্যে বেছে নেওয়া হয়েছে, $t_1$ এলোমেলোভাবে ১০ এবং ২০ এর মধ্যে এবং $t_2$ এলোমেলোভাবে ৫০ এবং ৬০ এর মধ্যে নির্বাচিত হয়। ৪টি  ক্রম শ্রেণি রয়েছে `Q`,`R`,`S`,এবং `U` যা `X` এবং` Y` এর অস্থায়ী আদেশের উপর নির্ভর করে ।বিধিগুলি হ'ল: `X, X -> Q`; `X, Y -> R`; `Y, X -> S`; `Y, Y -> U`।
<!--1). Dataset Exploration-->
১)। ডেটাসেট এক্সপ্লোরেশন

<!--The return type from a data generator is a tuple with length 2. The first item in the tuple is the batch of sequences with shape $(32, 9, 8)$. This is the data going to be fed into the network. There are eight different symbols in each row (`X`, `Y`, `a`, `b`, `c`, `d`, `B`, `E`). Each row is a one-hot vector. A sequence of rows represents a sequence of symbols. The first all-zero row is padding. We use padding when the length of the sequence is shorter than the maximum length in the batch.  The second item in the tuple is the corresponding batch of class labels with shape $(32, 4)$, since we have 4 classes (`Q`, `R`, `S`, and `U`). The first sequence is: `BbXcXcbE`. Then its decoded class label is $[1, 0, 0, 0]$, corresponding to `Q`.-->
ডেটা জেনারেটর থেকে রিটার্ন টাইপটি  দৈর্ঘ্য ২ এর একটি টাপল। টাপলটির প্রথম আইটেম হচ্ছে $(32,9,8)$ আকারের একাধিক ক্রমের একটি ব্যাচ ।  এই ডেটাটি নেটওয়ার্কে ব্যবহার করা হবে। প্রতিটি সারিতে আটটি পৃথক চিহ্ন রয়েছে (`X`,` Y`, `a`,` b`, `c`,` d`, `B`,` E`)। প্রতিটি সারি একটি ওয়ান-হট ভেক্টর। সারিগুলির একটি ক্রম প্রতীকগুলির ক্রমকে উপস্থাপন করে। প্রথম সমস্ত -শূন্য সারিটি প্যাডিং। যখন সিকোয়েন্সের দৈর্ঘ্য ব্যাচের সর্বোচ্চ দৈর্ঘ্যের চেয়ে কম হয় আমরা প্যাডিং ব্যবহার করি। টাপলে থাকা দ্বিতীয় আইটেমটি হল $(32, 4)$ আকৃতির শ্রেণি লেবেলের সাথে সম্পর্কিত ব্যাচ , যেহেতু আমাদের ৪টি শ্রেণি  (`Q`, `R`, `S`, এবং  `U`) রয়েছে । প্রথম অনুক্রমটি হল: `BbXcXcbE` ।  তারপরে এর ডিকোডেড শ্রেণীর লেবেলটি  হল $[1, 0, 0, 0 ]$ , যা ` Q` এর সাথে সম্পর্কিত।

<center>
<img src="{{site.baseurl}}/images/week06/06-3/dataset.png" style="zoom: 15%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 29:</b> Input Vector Example-->
চিত্র ২৯ঃ ইনপুট ভেক্টর উদাহরণ
</center>


<!--2). Defining the Model and Training-->
২)। সংজ্ঞায়িত মডেল এবং ট্রেইনিং

<!--Let’s create a simple recurrent network, an LSTM, and train for 10 epochs. In the training loop, we should always look for five steps:-->
একটি সাধারণ রিকারেন্ট  নেটওয়ার্ক তৈরি করুন, একটি এলএসটিএম, এবং ১০ ইপোকের জন্য প্রশিক্ষণ দিন। ট্রেইনিং লুপে আমাদের সর্বদা পাঁচটি ধাপ সন্ধান করা উচিত:
<!-- * Perform the forward pass of the model-->
   * মডেলের ফরোয়ার্ড পাস সম্পাদন করুন
<!-- * Compute the loss-->
   * লস গণনা করুন
<!-- * Zero the gradient cache-->
   * গ্র্যাডিয়েন্ট ক্যাশ শূন্য করুন
<!-- * Backpropagate to compute the partial derivative of loss with regard to parameters-->
   * প্যারামিটারগুলির সাথে ক্ষতির আংশিক ডেরাইভেটিভ গণনা করার জন্য ব্যাকপ্রোপাগেট করুন
<!-- * Step in the opposite direction of the gradient-->
   * গ্র্যাডিয়েন্টের বিপরীত দিকে পদক্ষেপ

<center>
<img src="{{site.baseurl}}/images/week06/06-3/train_test_easy.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 30:</b> Simple RNN *vs.* LSTM - 10 Epochs-->
চিত্র ৩০ঃ  সাধারণ RNN *বনাম.* LSTM - ১০ ইপোক
</center>

<!--With an easy level of difficulty, RNN gets 50% accuracy while LSTM gets 100% after 10 epochs. But LSTM has four times more weights than RNN and has two hidden layers, so it is not a fair comparison. After 100 epochs, RNN also gets 100% accuracy, taking longer to train than the LSTM.-->
তুলনামূলক সহজ স্তরের কাঠিন্য মোকাবেলায়, আরএনএন ৫০% নির্ভুলতা পায় যখন এলএসটিএম ১০০% সঠিক ফলাফল পায় ১০ ইপোকের পরে । তবে এলএসটিএমের আরএনএন এর চেয়ে চারগুণ বেশি ওয়েটরয়েছে এবং এর দুটি হিডেন লেয়ার রয়েছে, সুতরাং এটি একটি উপযুক্ত তুলনা নয়। ১০০ ইপোকের পরে, আরএনএনও ১০০% নির্ভুলতা পায়, এলএসটিএম এর চেয়ে বেশি ট্রেইনিং নিতে সময় নেয়।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/train_test_hard.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 31:</b> Simple RNN *vs.* LSTM - 100 Epochs-->
চিত্র ৩১ঃ সাধারণ RNN *বনাম.* LSTM - ১০০ ইপোক
</center>

<!--If we increase the difficulty of the training part (using longer sequences), we will see the RNN fails while LSTM continues to work.-->
যদি আমরা ট্রেইনিং এর অংশটির (দীর্ঘতর ক্রম ব্যবহার করে) কাঠিন্য বাড়িয়ে তুলি তবে আমরা দেখতে পাব যে এলএসটিএম কাজ চালিয়ে যাচ্ছে যখন আরএনএন ব্যর্থ হবে।
<center>
<img src="{{site.baseurl}}/images/week06/06-3/hidden_state_lstm.png" style="zoom: 40%; background-color:#DCDCDC;"/><br>
<!--<b>Figure 32:</b> Visualization of Hidden State Value-->
চিত্র ৩২ঃ হিডেন স্টেটের মানের দৃশ্যায়ন
</center>

<!--The above visualization is drawing the value of hidden state over time in LSTM. We will send the inputs through a hyperbolic tangent, such that if the input is below $-2.5$, it will be mapped to $-1$, and if it is above $2.5$, it will be mapped to $1$. So in this case, we can see the specific hidden layer picked on `X` (fifth row in the picture) and then it became red until we got the other `X`. So, the fifth hidden unit of the cell is triggered by observing the `X` and goes quiet after seeing the other `X`. This allows us to recognize the class of sequence.-->
উপরের দৃশ্যায়নটি এলএসটিএম-এ সময়ের সাথে হিডেন স্টেটের মান অঙ্কন করছে। আমরা হাইপারবোলিক ট্যানজেন্টের মাধ্যমে ইনপুটগুলি প্রেরণ করব, যেমন যদি ইনপুটটি $-2.5$ এর নীচে হয় তবে এটি $-1$ এ ম্যাপ করা হবে এবং এটি যদি $2.5$ এর উপরে হয় তবে এটি ম্যাপ করা হবে $1$ । সুতরাং এই ক্ষেত্রে, আমরা `X`(চিত্রের পঞ্চম সারিতে) বাছাই করা নির্দিষ্ট হিডেন লেয়ারটি দেখতে পাচ্ছি এবং তারপরে এটি অন্য  `X` না পাওয়া পর্যন্ত এটি লাল হয়ে যায় ।  সুতরাং, ঘরের পঞ্চম হিডেন ইউনিটটি  `X`পর্যবেক্ষণ করে ট্রিগার করা হয় এবং অন্য `X'` দেখে স্থির হয়ে যায় এটি আমাদের ক্রম শ্রেণিটি চিনতে সহায়তা করে।

<!--### Signal Echoing-->
###  সিগন্যাল প্রতিধ্বনি
<!--Echoing signal n steps is an example of synchronized many-to-many task. For instance, the 1st input sequence is `"1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ..."`, and the 1st target sequence is `"0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 ..."`. In this case, the output is three steps after. So we need a short-time working memory to keep the information. Whereas in the language model, it says something that hasn't already been said.-->
সিগন্যাল এন-স্টেপগুলি প্রতিধ্বনিত করা বহু-বহু-সংখ্যক কাজের সামঞ্জস্য বিধান করার একটি উদাহরণ। উদাহরণস্বরূপ, প্রথম ইনপুট ক্রমটি হল  `"1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ..."` , এবং প্রথম লক্ষ্য ক্রমটি  `"0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 ... "` ।  এই ক্ষেত্রে, আউটপুট তিন ধাপ পরে হয়। সুতরাং আমাদের তথ্য রাখতে প্রয়োজন একটি স্বল্প সময়ের সক্রিয় মেমরি। ভাষা মডেলটিতে, এটি এমন কিছু বলে যা ইতিমধ্যে বলা হয়নি।

<!--Before we send the whole sequence to the network and force the final target to be something, we need to cut the long sequence into little chunks. While feeding a new chunk, we need to keep track of the hidden state and send it as input to the internal state when adding the next new chunk. In LSTM, you can keep the memory for a long time as long as you have enough capacity. In RNN, after you reach a certain length, it starts to forget about what happened in the past.-->
আমরা পুরো ক্রমটি নেটওয়ার্কে প্রেরণ করার আগে এবং চূড়ান্ত লক্ষ্যটিকে কিছু হতে বাধ্য করার আগে, আমাদের দীর্ঘ ক্রমটি সামান্য খণ্ডে কাটাতে হবে। নতুন খণ্ডকে ব্যবহার করার সময়, আমাদের পরবর্তী গোটা অংশটি যুক্ত করার সময় হিডেন স্টেটের উপর নজর রাখতে হবে এবং এটি অভ্যন্তরীণ অবস্থায় ইনপুট হিসাবে প্রেরণ করতে হবে। এলএসটিএম-এ আপনি যতক্ষণ আপনার পর্যাপ্ত ধারণক্ষমতা রাখেন ততক্ষণ মেমরি দীর্ঘকাল ধরে রাখতে পারেন। আরএনএন-এ, আপনি একটি নির্দিষ্ট দৈর্ঘ্যে পৌঁছানোর পরে, অতীতে কী ঘটেছিল তা ভুলে যেতে শুরু করে।

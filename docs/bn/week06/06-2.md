---
lang-ref: ch.06-2
lecturer: Yann LeCun
title: RNNs, GRUs, LSTMs, Attention, Seq2Seq, এবং Memory Networks
authors: Jiayao Liu, Jialing Xu, Zhengyang Bian, Christina Dominguez
date: 2 March 2020
lang: bn
translation-date: 8 Mar 2021
translator: Mahbuba Tasmin
---


<!--## [Deep Learning Architectures](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=2620s)-->
## [ডিপ লার্নিং আর্কিটেকচার](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=2620s)
<!--In deep learning, there are different modules to realize different functions. Expertise in deep learning involves designing architectures to complete particular tasks.  Similar to writing programs with algorithms to give instructions to a computer in earlier days, deep learning reduces a complex function into a graph of functional modules (possibly dynamic), the functions of which are finalized by learning.-->
ডিপ লার্নিং এর আওতায় , বিভিন্ন কার্যকারিতা উপলব্ধি করার জন্য বিভিন্ন মডিউল রয়েছে। ডিপ লার্নিং এর দক্ষতার মধ্যে নির্দিষ্ট কাজগুলি সম্পন্ন করার জন্য আর্কিটেকচার ডিজাইন করা জড়িত। আগের দিনগুলিতে কম্পিউটারকে নির্দেশনা দেওয়ার জন্য অ্যালগরিদমের সাথে প্রোগ্রাম লেখার অনুরূপ, ডিপ লার্নিং একটি জটিল ক্রিয়াকে ফাংশনাল মডিউলগুলির (সম্ভবত গতিশীল) একটি গ্রাফের মধ্যে হ্রাস করে, যার ক্রিয়াগুলি মডেলের শেখার মাধ্যমে চূড়ান্ত হয়।
<!--As with what we saw with convolutional networks, network architecture is important.-->
কনভলিউশনাল নেটওয়ার্কগুলির সাথে আমরা যা দেখেছি, তার মতো নেটওয়ার্ক আর্কিটেকচার গুরুত্বপূর্ণ।

<!--## Recurrent Networks-->
## রিকারেন্ট নেটওয়ার্ক সমূহ
<!--In a Convolutional Neural Network, the graph or interconnections between the modules cannot have loops. There exists at least a partial order among the modules such that the inputs are available when we compute the outputs.-->
কনভলিউশনাল নিউরাল নেটওয়ার্কে মডিউলগুলির মধ্যে গ্রাফ বা আন্তঃসংযোগগুলির লুপগুলি থাকতে পারে না। মডিউলগুলির মধ্যে কমপক্ষে একটি আংশিক ক্রম বিদ্যমান রয়েছে যেমন আমরা আউটপুটগুলো গণনা করলে ইনপুটগুলি পাওয়া যায়।
<!--As shown in Figure 1, there are loops in Recurrent Neural Networks.-->
চিত্র ১ -এ দেখানো হয়েছে, রিকারেন্ট নিউরাল নেটওয়ার্কগুলিতে লুপ রয়েছে।
<center>
<img src="{{site.baseurl}}/images/week06/06-2/RNN_rolled.png" /><br>
<!--Figure 1. Recurrent Neural Network with roll-->
চিত্র ১: রোল সহকারে রিকারেন্ট নিউরাল নেটওয়ার্ক
</center>

<!-- - $x(t)$ : input that varies across time-->
 - $x(t)$ : ইনপুট যা সময়ের সাথে সাথে পরিবর্তিত হয়
<!--- $\text{Enc}(x(t))$: encoder that generates a representation of input-->
 - $\text{Enc}(x(t))$: এনকোডার যা ইনপুটটির প্রতিনিধিত্ব করে
<!--- $h(t)$: a representation of the input-->
 - $h(t)$: ইনপুটের একটি প্রতিরুপ
<!-- - $w$: trainable parameters-->
 - $w$:  প্রশিক্ষণযোগ্য পরামিতি (প্যারামিটার)
<!-- - $z(t-1)$: previous hidden state, which is the output of the previous time step-->
 - $z(t-1)$: পূর্ববর্তী হিডেন স্টেট, যা পূর্ববর্তী সময়ের ধাপের আউটপুট
<!-- - $z(t)$: current hidden state-->
 - $z(t)$: বর্তমান হিডেন স্টেট
<!-- - $g$: function that can be a complicated neural network; one of the inputs is $z(t-1)$ which is the output of the previous time step-->
 - $g$: একটি ফাংশন যা হতে পারে একটি জটিল নিউরাল নেটওয়ার্ক; ইনপুটগুলির মধ্যে একটি হল $z(t -1)$ যা পূর্ববর্তী সময় ধাপের আউটপুট
<!-- - $\text{Dec}(z(t))$: decoder that generates an output-->
 - $\text{Dec}(z(t))$: ডিকোডার যা একটি আউটপুট উত্পন্ন করে


<!--## Recurrent Networks: Unroll the loop-->
## রিকারেন্ট নেটওয়ার্ক: লুপটি আনরোল করুন

<!--Unroll the loop in time. The input is a sequence $x_1, x_2, \cdots, x_T$.-->
সময়মতো লুপটি আনরোল করুন। ইনপুটটি একটি ক্রম $x_1, x_2, ..., x_T$ অনুসরণ করে।
<center>
 "
<img src="{{site.baseurl}}/images/week06/06-2/RNN_unrolled.png" /><br>
<!--Figure 2. Recurrent Networks with unrolled loop-->
চিত্র ২: আনরোলড  লুপের সাথে রিকারেন্ট নেটওয়ার্কগুলো
</center>

<!--In Figure 2, the input is $x_1, x_2, x_3$.-->
চিত্র ২ -এ ইনপুটটি $x_1, x_2, x_3$ ।

<!--At time t=0, the input $x(0)$ is passed to the encoder and it generates the representation $h(x(0)) = \text{Enc}(x(0))$ and then passes it to G to generate hidden state $z(0) = G(h_0, z', w)$. At $t = 0$, $z'$ in $G$ can be initialized as $0$ or randomly initialized. $z(0)$ is passed to decoder to generate an output and also to the next time step.-->
সময় t = 0 চলাকালীন সময়, ইনপুট $x(0)$ টিকে এনকোডারকে দেওয়া হয় এবং এটি $h(x(0))=\text{Enc}(x(0))$ কে উপস্থাপন করে এবং তারপরে এটি G কে পাস করে হিডেন স্টেট $z(0)= G (h_0, z ', w)$ কে উদ্ভূত করতে।  সময় $t=0$ এ, $G$ এ $z’$ আদ্যক্ষর $0$ হিসেবে কিংবা যথেচ্ছাভাবে আরম্ভ করা যেতে পারে । একটি আউটপুট উৎপন্ন করতে $z(0)$ কে পাস করা হয় ডিকোডারে এবং পরবর্তী সময় ধাপেও  পাস করা হয়।
<!--As there are no loops in this network, and we can implement backpropagation.-->
যেহেতু এই নেটওয়ার্কটিতে কোনও লুপ নেই, এবং আমরা ব্যাক প্রোপাগেশন বাস্তবায়ন করতে পারি।
<!--Figure 2 shows a regular network with one particular characteristic: every block shares the same weights. Three encoders, decoders and G functions have same weights respectively across different time steps.-->
চিত্র-২ এ একটি নির্দিষ্ট বৈশিষ্ট্যযুক্ত একটি নিয়মিত নেটওয়ার্ক দেখায়: প্রতিটি ব্লক একই পরিমাণ গুরুত্ব বহন করে। তিনটি এনকোডার, ডিকোডার এবং G ফাংশনগুলির যথাক্রমে বিভিন্ন সময় ধাপে একই গুরুত্ব বহন করে।
<!--BPTT: Backprop through time.  Unfortunately, BPTT doesn't work so well in the naive form of RNN.-->
BPTT : সময়ের সাথে ব্যাকপ্রপ।  দুর্ভাগ্যক্রমে, BPTT  naive-RNN এর সাথে এত ভাল কাজ করে না।
<!--Problems with RNNs:-->
RNN  এর সাথে সম্পর্কিত সমস্যাগুলোঃ


১. বিলীয়মান গ্র্যাডিয়েন্ট
   - দীর্ঘ ক্রমে, গ্র্যাডিয়েন্টগুলি প্রতি সময়ধাপে ওয়েট ম্যাট্রিক্স (পক্ষান্তরিত) দ্বারা গুণিত হয়। ওয়েট ম্যাট্রিক্সে যদি ছোট মান থাকে তবে গ্র্যাডিয়েন্টের নর্মটি তাৎপর্যপূর্ণভাবে ক্রমান্বয়ে ছোট হতে থাকে।

২. এক্সপ্লোডিং গ্র্যাডিয়েন্ট
   -  যদি একটি বড় ওয়েট ম্যাট্রিক্স থাকে এবং পুনরাবৃত্ত স্তরগুলোর নন-লিনিয়ারিটি  বৈশিষ্ট্য পরিপৃক্ত (স্যাচুরেটিং) না হয় তবে গ্র্যাডিয়েন্টগুলি এক্সপ্লোড হবে। ওয়েটস গুলো আপডেটের ধাপে বিভক্ত হবে। গ্র্যাডিয়েন্ট-ডিসেন্ট কাজের জন্য আমাদের একটি ছোট লার্নিং রেট ব্যবহার করতে হতে পারে।

<!--1. Vanishing gradients-->
<!--   - In a long sequence, the gradients get multiplied by the weight matrix (transpose) at every time step. If there are small values in the weight matrix, the norm of gradients get smaller and smaller exponentially.-->
<!--2. Exploding gradients-->
 <!--  - If we have a large weight matrix and the non-linearity in the recurrent layer is not saturating, the gradients will explode. The weights will diverge at the update step. We may have to use a tiny learning rate for the gradient descent to work.-->

<!--One reason to use RNNs is for the advantage of remembering information in the past. However, it could fail to memorize the information long ago in a simple RNN without tricks.-->
RNN ব্যবহার করার একটি কারণ হচ্ছে অতীতের তথ্য মনে রাখার সুবিধা। যাইহোক, সাধারণ একটি RNN কোন কৌশল ছাড়া অনেক আগের তথ্য মুখস্থ করতে ব্যর্থ হতে পারে।

<!--An example that has vanishing gradient problem:-->
বিলীয়মান গ্র্যাডিয়েন্ট সমস্যা সম্পর্কিত একটি উদাহরণ:
<!--The input is the characters from a C Program. The system will tell whether it is a syntactically correct program. A syntactically correct program should have a valid number of braces and parentheses. Thus, the network should remember how many open parentheses and braces there are to check, and whether we have closed them all. The network has to store such information in hidden states like a counter.  However, because of vanishing gradients, it will fail to preserve such information in a long program.-->

ইনপুটটি হল একটি সি প্রোগ্রাম থেকে নেয়া অংশসমূহ। সিস্টেমটি এটি বলবে যে এটি একটি গঠনগত ভাবে সঠিক প্রোগ্রাম কিনা। একটি সঠিক সংগঠিত প্রোগ্রামে ব্রেসসমূহ (ধনুর্বন্ধনী) এবং বন্ধনীগুলির একটি বৈধ সংখ্যক হওয়া উচিত। সুতরাং, নেটওয়ার্কটিকে মনে রাখতে হবে যে সেখানে পরীক্ষণীয় কতগুলো খালি বন্ধনী এবং ব্রেস আছে এবং আমরা সেসব গুলোকে বন্ধ করেছি কিনা। নেটওয়ার্ককে  একটি কাউন্টারের মতো লুকানো স্টেটে এই জাতীয় তথ্য সংরক্ষণ করতে হয়। তবে  গ্র্যাডিয়েন্টগুলো বিলীয়মান হওয়ার কারণে নেটওয়ার্কটি একটি দীর্ঘ প্রোগ্রামে এ জাতীয় তথ্য সংরক্ষণ করতে ব্যর্থ হবে।


<!--##  RNN Tricks-->
## RNN এর কৌশলগুলোঃ
<!--- clipping gradients:  (avoid exploding gradients)-->
<!--   Squash the gradients when they get too large.-->
- ক্লিপিং গ্র্যাডিয়েন্ট: (এক্সপ্লোডিং গ্র্যাডিয়েন্ট এড়ানো)
  গ্র্যাডিয়েন্টগুলি যখন খুব বড় হয়ে যায় তখন স্কোয়াশ করুন।

- সূচনা ( ডান বলপার্কে সূচনা করে এক্সপ্লোডিং/বিলীন হওয়া এড়ানো)
  মডেলের নর্ম কিছু পরিমাণে অক্ষুণ্ণ রাখতে ওয়েট ম্যাট্রিক্স গুলো সূচনা করুন । উদাহরণস্বরূপ, সমকোণীয়  সূচনাটি ওয়েট ম্যাট্রিক্সকে এলোমেলো  ধরণের   অর্থোগোনাল ম্যাট্রিক্স হিসাবে আরম্ভ করে।



<!--- Initialization (start in right ballpark avoids exploding/vanishing)-->
<!--   Initialize the weight matrices to preserve the norm to some extent. For example, orthogonal initialization initializes the weight matrix as a random orthogonal matrix.-->



<!--## Multiplicative Modules-->
## গুণক মডিউল
<!--In multiplicative modules rather than only computing a weighted sum of inputs, we compute products of inputs and then compute weighted sum of that.-->
গুণগত মডিউলগুলিতে কেবল ইনপুটগুলোর একটি ওয়েটেড সাম গণনা করার পরিবর্তে, আমরা ইনপুটগুলির গুণফল গুলো গণনা করি এবং তারপরে ওয়েটেড সাম গণনা করি।

<!--Suppose $x \in {R}^{n\times1}$, $W \in {R}^{m \times n}$, $U \in {R}^{m \times n \times d}$ and $z \in {R}^{d\times1}$. Here U is a tensor.-->
ধরুন $x \in {R}^{n\times1}$, $W \in {R}^{m \times n}$, $U \in {R}^{m \times n \times d}$ and $z \in {R}^{d\times1}$. এখানে U একটি টেনসর।

$$
w_{ij} = u_{ij}^\top z =
\begin{pmatrix}
u_{ij1} & u_{ij2} & \cdots &u_{ijd}\\
\end{pmatrix}
\begin{pmatrix}
z_1\\
z_2\\
\vdots\\
z_d\\
\end{pmatrix} = \sum_ku_{ijk}z_k
$$

$$
s =
\begin{pmatrix}
s_1\\
s_2\\
\vdots\\
s_m\\
\end{pmatrix} = Wx =  \begin{pmatrix}
w_{11} & w_{12} & \cdots &w_{1n}\\
w_{21} & w_{22} & \cdots &w_{2n}\\
\vdots\\
w_{m1} & w_{m2} & \cdots &w_{mn}
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{pmatrix}
$$

<!--where $s_i = w_{i}^\top x = \sum_j w_{ij}x_j$.-->
যেখানে $s_i = w_{i}^\top x = \sum_j w_{ij}x_j$।

<!--The output of the system is a classic weighted sum of inputs and weights. Weights themselves are also weighted sums of weights and inputs.-->
সিস্টেমের আউটপুট হল ইনপুট এবং ওয়েটগুলোর একটি সর্বোত্তম ওয়েটেড সামের যোগফল। ওয়েট গুলো স্বয়ং ওয়েট এবং ইনপুটের ওয়েটেড সাম।
<!--Hypernetwork architecture: weights are computed by another network.-->
হাইপারনেটওয়ার্ক আর্কিটেকচার: ওয়েটগুলো অন্য একটি নেটওয়ার্ক দ্বারা গণনা করা হয়।

<!--## Attention-->
## অ্যাটেনশন
<!--$x_1$ and $x_2$ are vectors, $w_1$ and $w_2$ are scalars after softmax where $w_1 + w_2 = 1$, and  $w_1$ and $w_2$ are between 0 and 1.-->
$x_1$ এবং $x_2$ ভেক্টর, $w_1$ এবং $w_2$ সফটম্যাক্সের পরে স্কেলারসমূহ যেখানে $w_1 + w_2 = 1$, এবং $w_1$ এবং $w_2$ রয়েছে 0 এবং 1 এর মধ্যে।
<!--$w_1x_1 + w_2x_2$ is a weighted sum of $x_1$ and $x_2$ weighted by coefficients $w_1$ and $w_2$.-->
$w_1x_1 + w_2x_2$ হল $x_1$ এবং $x_2$ এর ওয়েটেড সাম, $w_1$ এবং $w_2$ এর সহগ দ্বারা ওয়েটকৃত।
<!--By changing the relative size of $w_1$ and $w_2$, we can switch the output of $w_1x_1 + w_2x_2$ to $x_1$ or $x_2$ or some linear combinations of $x_1$ and $x_2$.-->
$W_1$ এবং $w_2$ এর আপেক্ষিক আকার পরিবর্তন করে আমরা $w_1x_1 + w_2x_2$ এর আউটপুট $x_1$ বা $x_2$ অথবা $x_1$ এবং $x_2$ এর কিছু  লিনিয়ার সংমিশ্রণে পরিবর্তন করতে পারি।
<!--The inputs can have multiple $x$ vectors (more than $x_1$ and $x_2$). The system will choose an appropriate combination, the choice of which is determined by another variable z. An attention mechanism allows the neural network to focus its attention on particular input(s) and ignore the others.-->
ইনপুটগুলোতে একাধিক $x$ ভেক্টর থাকতে পারে ($x_1$ এবং $x_2$ এর চেয়ে বেশি)। সিস্টেমটি একটি উপযুক্ত সংমিশ্রণটি চয়ন করবে, যার পছন্দটি অন্য ভেরিয়েবল z দ্বারা নির্ধারিত হবে। একটি অ্যাটেনশন পদ্ধতি নিউরাল নেটওয়ার্কটিকে নির্দিষ্ট ইনপুটগুলোতে মনোযোগ নিবদ্ধ করতে এবং অন্যদের উপেক্ষা করার অনুমতি দেয়।

<!--Attention is increasingly important in NLP systems that use transformer architectures or other types of attention.-->
ট্রান্সফরমার আর্কিটেকচার বা অন্যান্য ধরণের অ্যাটেনশন পদ্ধতি ব্যবহার করে এমন NLP প্রক্রিয়াগুলোতে  অ্যাটেনশন ক্রমশ গুরুত্বপূর্ণ হয়ে উঠছে।

<!--The weights are data independent because z is data independent.-->
ওয়েটসমূহ ডেটা থেকে স্বতন্ত্র কারণ Z ডেটা থেকে স্বতন্ত্র।

<!--## [Gated Recurrent Units (GRU)](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=3549s)-->
## [গেটেড রিকারেন্ট ইউনিটসমূহ (GRU)](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=3549s)
<!--As mentioned above, RNN suffers from vanishing/exploding gradients and can’t remember states for very long. GRU, [Cho, 2014](https://arxiv.org/abs/1406.1078), is an application of multiplicative modules that attempts to solve these problems. It's an example of recurrent net with memory (another is LSTM). The structure of A GRU unit is shown below:-->
উপরে উল্লিখিত বর্ণ্নামতে, RNN -এ বিলীয়মান/এক্সপ্লোডিং গ্র্যাডিয়েন্টের সমস্যা বিদ্যমান এবং দীর্ঘ সময় স্টেটসমূহ মনে রাখতে অপারগ। GRU, [Cho, 2014](https://arxiv.org/abs/1406.1078), এই সমস্যাগুলো সমাধান করতে চেষ্টা করা গুণক মডিউলগুলির একটি অ্যাপ্লিকেশন। এটি মেমরির সাথে রিকারেন্ট নেটের একটি উদাহরণ (অন্যটি LSTM)। একটি GRU ইউনিটের কাঠামো নীচে প্রদর্শিত হবে:

<center>
<img src="{{site.baseurl}}/images/week06/06-2/GRU.png" height="300px" style="background-color:#226;"/><br>
<!--Figure 3. Gated Recurrent Unit-->
চিত্র ৩: গেটেড রিকারেন্ট ইউনিট
</center>

$$
\begin{array}{l}
z_t = \sigma_g(W_zx_t + U_zh_{t-1} + b_z)\\
r_t = \sigma_g(W_rx_t + U_rh_{t-1} + b_r)\\
h_t = z_t\odot h_{t-1} + (1- z_t)\odot\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)
\end{array}
$$

<!--where $\odot$ denotes element-wise multiplication(Hadamard product), $x_t$ is the input vector, $h_t$ is the output vector, $z_t$ is the update gate vector, $r_t$ is the reset gate vector, $\phi_h$ is a hyperbolic tanh, and $W$,$U$,$b$ are learnable parameters.-->
যেখানে $\odot$ উপাদান অনুসারে গুণ (Hadamard গুণফল) বোঝায়, $x_t$ ইনপুট ভেক্টর, $h_t$ আউটপুট ভেক্টর, $z_t$ আপডেট গেট ভেক্টর, $r_t$ রিসেট গেট ভেক্টর, $\phi_h$ একটি হাইপারবোলিক tanh এবং $W$, $U$, $b$ নেটওয়ার্কের শেখার প্যারামিটার ।
<!--To be specific, $z_t$ is a gating vector that determines how much of the past information should be passed along to the future. It applies a sigmoid function to the sum of two linear layers and a bias over the input $x_t$ and the previous state $h_{t-1}$.  $z_t$ contains coefficients between 0 and 1 as a result of applying sigmoid. The final output state $h_t$ is a convex combination of $h_{t-1}$ and $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$ via $z_t$. If the coefficient is 1, the current unit output is just a copy of the previous state and ignores the input (which is the default behaviour). If it is less than one, then it takes into account some new information from the input.-->
সুনির্দিষ্টভাবে বলতে গেলে, $z_t$ হল একটি গেটিং ভেক্টর যা নির্ধারণ করে যে অতীতের তথ্যগুলির কতটা ভবিষ্যতে পাঠাতে হবে। এটি দুটি লিনিয়ার স্তর এবং একটি ইনপুট $x_t$ এর উপর আপতিত বায়াসের সমষ্টি এবং পূর্ববর্তী স্টেট $h_{t-1}$ এর জন্য একটি সিগময়েড ফাংশন প্রয়োগ করে। সিগময়েড প্রয়োগের ফলে $z_t$ এর সহগ 0 এবং 1 এর মধ্যে অবস্থিত।  চূড়ান্ত আউটপুট স্টেট $h_t$ হল $h_{t-1}$ এবং $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$ এর উত্তল সংমিশ্রণ $z_t$ এর মাধ্যমে। যদি সহগটি 1 হয়, বর্তমান ইউনিট আউটপুটটি পূর্ববর্তী স্টেটের কেবল একটি অনুলিপি এবং ইনপুটটিকে উপেক্ষা করে (এটি পূর্বনির্ধারিত আচরণ)। যদি এটির চেয়ে কম হয়, তবে এটি ইনপুট থেকে কিছু নতুন তথ্য গ্রহণ করে।
<!--The reset gate $r_t$ is used to decide how much of the past information to forget. In the new memory content $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$, if the coefficient in $r_t$ is 0, then it stores none of the information from the past. If at the same time $z_t$ is 0, then the system is completely reset since $h_t$ would only look at the input.-->
রিসেট গেট $r_t$ অতীতের তথ্যকে কতটা ভুলে যেতে হবে তার সিদ্ধান্ত নিতে ব্যবহৃত হয়। নতুন মেমরি কনটেন্টে $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$, যদি $r_t$ এর সহগ 0 হয় তবে এটি অতীতের কোনও তথ্যই সঞ্চয় করে না। যদি একই সময়ে $z_t$ এর মান 0 হয়, তবে সিস্টেমটি পুরোপুরি পুনরায় সেট করা হয় যেহেতু $h_t$ কেবল ইনপুটকে দেখবে।

<!--## LSTM (Long Short-Term Memory)-->
## LSTM (লং শর্ট - টার্ম মেমরি)
<!--GRU is actually a simplified version of LSTM which came out much earlier, [Hochreiter, Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf). By building up memory cells to preserve past information, LSTMs also aim to solve long term memory loss issues in RNNs. The structure of LSTMs is shown below:-->
GRU আসলে LSTM এর একটি সরলিকৃত সংস্করণ যা অনেক আগে প্রকাশিত হয়েছিল, [Hochreiter, Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf)। অতীত তথ্য সংরক্ষণের জন্য মেমরি সেল তৈরি করার পাশাপাশি, LSTM লক্ষ্য করে  RNN গুলোতে দীর্ঘমেয়াদী মেমরির হ্রাসের সমস্যাগুলি সমাধান করতে। LSTM এর কাঠামো নিচে প্রদর্শিত হয়েছে:

<center>
<img src="{{site.baseurl}}/images/week06/06-2/LSTM.png" height="300px"/><br>
<!--Figure 4. LSTM-->
চিত্র ৪: LSTM
</center>

$$
\begin{array}{l}
f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f)\\
i_t = \sigma_g(W_ix_t + U_ih_{t-1} + b_i)\\
o_t = \sigma_o(W_ox_t + U_oh_{t-1} + b_o)\\
c_t = f_t\odot c_{t-1} + i_t\odot \tanh(W_cx_t + U_ch_{t-1} + b_c)\\
h_t = o_t \odot\tanh(c_t)
\end{array}
$$

<!--where $\odot$ denotes element-wise multiplication, $x_t\in\mathbb{R}^a$ is an input vector to the LSTM unit, $f_t\in\mathbb{R}^h$ is the forget gate's activation vector, $i_t\in\mathbb{R}^h$ is the input/update gate's activation vector, $o_t\in\mathbb{R}^h$ is the output gate's activation vector, $h_t\in\mathbb{R}^h$ is the hidden state vector (also known as output), $c_t\in\mathbb{R}^h$ is the cell state vector.-->
যেখানে $\odot$ উপাদান অনুসারে গুণনকে বোঝায়, $x_t\mathbb{R}^a$ LSTM ইউনিটের একটি ইনপুট ভেক্টর,$f_t\in\mathbb{R}^h$  হল ফরগেট গেটের অ্যাক্টিভেশন ভেক্টর ,$i_t\in\mathbb{R}^h$ হল ইনপুট/আপডেট গেটের অ্যাক্টিভেশন ভেক্টর, $o_t\in\mathbb{R}^h$ আউটপুট গেটের অ্যাক্টিভেশন ভেক্টর, $h_t\in\mathbb{R}^h$ হিডেন স্টেট ভেক্টর (আউটপুট নামেও পরিচিত), $c_t\in\mathbb{R}^h$ হল সেল স্টেট ভেক্টর।

<!--An LSTM unit uses a cell state $c_t$ to convey the information through the unit. It regulates how information is preserved or removed from the cell state through structures called gates. The forget gate $f_t$ decides how much information we want to keep from the previous cell state $c_{t-1}$ by looking at the current input and previous hidden state, and produces a number between 0 and 1 as the coefficient of $c_{t-1}$.  $\tanh(W_cx_t + U_ch_{t-1} + b_c)$ computes a new candidate to update the cell state, and like the forget gate, the input gate $i_t$ decides how much of the update to be applied. Finally, the output $h_t$ will be based on the cell state $c_t$, but will be put through a $\tanh$ then filtered by the output gate $o_t$.-->
একটি LSTM ইউনিট সেল স্টেট $c_t$ ব্যবহার করে ইউনিটের মাধ্যমে তথ্য জানায়। এটি গেটস নামক স্ট্রাকচারের মাধ্যমে কীভাবে সেল স্টেট থেকে তথ্য সংরক্ষণ বা অপসারণ করা হয় তা নিয়ন্ত্রণ করে। ফরগেট গেট $f_t$ বর্তমান ইনপুট এবং পূর্ববর্তী হিডেন স্টেট দেখে নির্ধারণ করে যে আমরা পূর্ববর্তী সেল স্টেটের $c_{t-1}$ থেকে কতটা তথ্য রাখতে চাই এবং $c_{t-1}$ এর সহগ হিসাবে 0 এবং 1 এর মধ্যে একটি সংখ্যা তৈরি করে। $\tanh(W_cx_t + U_ch_{t-1} + b_c)$ একটি নতুন প্রার্থীকে সেল স্টেট আপডেট করার জন্য গণনা করে এবং ফরগেট গেটের মতো ইনপুট গেট $i_t$ সিদ্ধান্ত নেয় যে আপডেটটি কতটা প্রয়োগ করা হবে। শেষ অবধি, আউটপুট $h_t$ সেল স্টেট $c_t$ এর উপর ভিত্তি করে তৈরি করা হবে তবে $\tanh$ এর মাধ্যমে আউটপুট গেট $o_t$ দ্বারা ফিল্টার করে দেওয়া হবে ।

<!--Though LSTMs are widely used in NLP, their popularity is decreasing. For example, speech recognition is moving towards using temporal CNN, and NLP is moving towards using transformers.-->
যদিও LSTM NLPতে ব্যাপকভাবে ব্যবহৃত হয়, তবে তাদের জনপ্রিয়তা হ্রাস পাচ্ছে। উদাহরণস্বরূপ, স্পিচ রিকগনিশন অস্থায়ী সিএনএন ব্যবহারের দিকে এগিয়ে চলেছে, এবং NLP ট্রান্সফর্মারগুলো ব্যবহার করার দিকে এগিয়ে চলেছে।

<!--## Sequence to Sequence Model-->
##  সিকোয়েন্স -টু - সিকোয়েন্স মডেল
<!--The approach proposed by [Sutskever NIPS 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) is the first neural machine translation system to have comparable performance to classic approaches. It uses an encoder-decoder architecture where both the encoder and decoder are multi-layered LSTMs.-->
[Sutskever NIPS 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) দ্বারা প্রস্তাবিত পদ্ধতিটির কর্মক্ষমতা হচ্ছে ক্লাসিক পদ্ধতির সাথে তুলনাযোগ্য প্রথম নিউরাল মেশিন অনুবাদ সিস্টেম। এটি একটি এনকোডার-ডিকোডার আর্কিটেকচার ব্যবহার করে যেখানে এনকোডার এবং ডিকোডার উভয়ই বহু-স্তরযুক্ত LSTM ।
<center>
<img src="{{site.baseurl}}/images/week06/06-2/Seq2Seq.png" height="300px" /><br>
<!--Figure 5. Seq2Seq-->
চিত্র ৫: সিক-টু-সিক
</center>

<!--Each cell in the figure is an LSTM. For the encoder (the part on the left), the number of time steps equals the length of the sentence to be translated. At each step, there is a stack of LSTMs (four layers in the paper) where the hidden state of the previous LSTM is fed into the next one. The last layer of the last time step outputs a vector that represents the meaning of the entire sentence, which is then fed into another multi-layer LSTM (the decoder), that produces words in the target language. In the decoder, the text is generated in a sequential fashion. Each step produces one word, which is fed as an input to the next time step.-->
চিত্রের প্রতিটি ঘর একটি LSTM। এনকোডার (বাম দিকের অংশ) এর জন্য,  সময় পদক্ষেপের সংখ্যা অনুবাদ করা বাক্যটির দৈর্ঘ্যের সমান হয়। প্রতিটি পদক্ষেপে, LSTM এর একটি স্ট্যাক রয়েছে (কাগজের চার স্তর) যেখানে পূর্ববর্তী LSTM এর হিডেন স্টেট পরবর্তী একটিকে দেওয়া হয়। শেষ বারের পদক্ষেপের শেষ স্তরটি একটি ভেক্টর আউটপুট দেয় যা পুরো বাক্যটির অর্থ উপস্থাপন করে, যা পরে অন্য একটি মাল্টি-লেয়ার LSTM(ডিকোডার) এ দেয়া হয়, যা লক্ষ্যকৃত ভাষায় শব্দ তৈরি করে। ডিকোডারে, পাঠ্যটি অনুক্রমিক ফ্যাশনে উত্পন্ন হয়। প্রতিটি পদক্ষেপে একটি শব্দ তৈরি হয়, যা পরের বারের ধাপে ইনপুট হিসাবে দেওয়া হয়।

<!--This architecture is not satisfying in two ways: First, the entire meaning of the sentence has to be squeezed into the hidden state between the encoder and decoder. Second, LSTMs actually do not preserve information for more than about 20 words. The fix for these issues is called a Bi-LSTM, which runs two LSTMs in opposite directions.  In a Bi-LSTM the meaning is encoded in two vectors, one generated by running LSTM from left to right, and another from right to left.  This allows doubling the length of the sentence without losing too much information.-->
এই আর্কিটেকচারটি দুটি উপায়ে পরিপূর্ণ নয়: প্রথমত, বাক্যটির পুরো অর্থ এনকোডার এবং ডিকোডারের মধ্যে হিডেন স্টেটে ছড়িয়ে দিতে হবে। দ্বিতীয়ত, LSTM প্রকৃতপক্ষে প্রায় 20 টিরও বেশি শব্দের জন্য তথ্য সংরক্ষণ করে না। এই সমস্যার সমাধানটিকে Bi-LSTM বলা হয়, যা দুটি LSTM বিপরীত দিকে চালিত করে। Bi-LSTM -এর অর্থ দুটি ভেক্টরগুলোতে এনকোড করা থাকে, একটি বাম থেকে ডানে LSTM চালিয়ে আর অন্যটি ডান থেকে বামে উত্পন্ন হয়। এটি খুব বেশি তথ্য না হারিয়ে বাক্যটির দৈর্ঘ্য দ্বিগুণ করার সুযোগ তৈরি করে।


<!--## Seq2seq with Attention-->
##  অ্যাটেনশনের সাথে সিক-টু-সিক

<!--The success of the approach above was short-lived. Another paper by [Bahdanau, Cho, Bengio](https://arxiv.org/abs/1409.0473)  suggested that instead of having a gigantic network that squeezes the meaning of the entire sentence into one vector, it would make more sense if at every time step we only focus the attention on the relevant locations in the original language with equivalent meaning, *i.e.* the attention mechanism.-->
উপরোক্ত পদ্ধতির সাফল্য স্বল্পস্থায়ী ছিল। [Bahdanau, Cho, Bengio](https://arxiv.org/abs/1409.0473) এর অন্য একটি পেপার  পরামর্শ দিয়েছে যে একটি বিশাল নেটওয়ার্কে পুরো বাক্যটির অর্থ একটি ভেক্টর হিসাবে চেপে রাখার পরিবর্তে এটি আরও অর্থপূর্ণ হবে যদি প্রতিবারের পদক্ষেপে আমরা কেবলমাত্র সমান অর্থের সাথে মূল ভাষার প্রাসঙ্গিক অবস্থানগুলিতে মনোযোগ নিবদ্ধ করি,অর্থাৎ অ্যাটেনশন পদ্ধতি ব্যবহার করি।
<center>
<img src="{{site.baseurl}}/images/week06/06-2/Seq2SeqwAttention.png" height="300px" /><br>
<!--Figure 6. Seq2Seq with Attention-->
চিত্র ৬: অ্যাটেনশনের সাথে সিক-টু-সিক
</center>

<!--In Attention, to produce the current word at each time step, we first need to decide which hidden representations of words in the input sentence to focus on. Essentially, a network will learn to score how well each encoded input matches the current output of the decoder. These scores are normalized by a softmax, then the coefficients are used to compute a weighted sum of the hidden states in the encoder at different time steps. By adjusting the weights, the system can adjust the area of inputs to focus on. The magic of this mechanism is that the network used to compute the coefficients can be trained through backpropagation. There is no need to build them by hand!-->
অ্যাটেনশনে, প্রতিটি ধাপে বর্তমান শব্দটি তৈরি করতে, প্রথমে আমাদের সিদ্ধান্ত নেওয়া দরকার যে ইনপুট বাক্যে শব্দের কী কী হিডেন উপস্থাপনাতে গুরুত্ব দেয়া উচিত। মূলত, একটি নেটওয়ার্ক স্কোর করতে শিখবে যে প্রতিটি এনকোডযুক্ত ইনপুট ডিকোডারের বর্তমান আউটপুটটির সাথে কতটা ভাল মেলে। এই স্কোরগুলি একটি সফটম্যাক্স দ্বারা নর্মালাইয করা হয়, তারপরে সহগগুলি বিভিন্ন সময় পদক্ষেপে এনকোডারে লুকানো রাজ্যের একটি ওয়েটেড সাম গণনা করতে ব্যবহৃত হয়। ওয়েট সামঞ্জস্য করার মাধ্যমে, সিস্টেম ফোকাস করার জন্য ইনপুটগুলির ক্ষেত্রটি সমন্বয় করতে করতে পারে। এই প্রক্রিয়াটির  বিশেষত্বটি হল সহগ গণ্নার জন্য ব্যবহৃত নেটওয়ার্কটি ব্যাক-প্রোপাগেশনের মাধ্যমে প্রশিক্ষণ দেওয়া যেতে পারে। এগুলি হাতে তৈরি করার দরকার নেই!
<!--Attention mechanisms completely transformed neural machine translation. Later, Google published a paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762), and they put forward transformer, where each layer and group of neurons is implementing attention.-->
অ্যাটেনশন পদ্ধতি সম্পূর্ণরূপে নিউরাল মেশিন অনুবাদকে রূপান্তরিত করে। পরবর্তীতে, গুগল একটি প্রবন্ধ প্রকাশ করেছে [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762), এবং তারা ট্রান্সফর্মার রেখেছিল, যেখানে প্রতিটি স্তর এবং নিউরনের সমষ্টি অ্যাটেনশন প্রয়োগ করেছে।

<!--## [Memory network](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=4575s)-->
## [মেমরি নেটওয়ার্ক](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=4575s)

<!--Memory networks stem from work at Facebook that was started by [Antoine Bordes](https://arxiv.org/abs/1410.3916) in 2014 and [Sainbayar Sukhbaatar](https://arxiv.org/abs/1503.08895) in 2015.-->
মেমোরি নেটওয়ার্কগুলি ফেসবুকে এমন কাজ থেকে শুরু হয়েছে যা ২০১৪ সালে [Antoine Bordes](https://arxiv.org/abs/1410.3916) এবং [Sainbayar Sukhbaatar](https://arxiv.org/abs/1503.08895) দ্বারা ২০১৫ সালে শুরু হয়েছিল ।
<!--The idea of a memory network is that there are two important parts in your brain: one is the **cortex**, which is where you have long term memory. There is a separate chunk of neurons called the **hippocampus** which sends wires to nearly everywhere in the cortex. The hippocampus is thought to be used for short term memory, remembering things for a relatively short period of time. The prevalent theory is that when you sleep, there is a lot of information transferred from the hippocampus to the cortex to be solidified in long term memory since the hippocampus has limited capacity.-->
একটি মেমরি নেটওয়ার্কের ধারণাটি হল আপনার মস্তিষ্কে দুটি গুরুত্বপূর্ণ অংশ রয়েছে: একটি হল ** কর্টেক্স **, যেখানে আপনার দীর্ঘমেয়াদী স্মৃতি রয়েছে। ** হিপ্পোক্যাম্পাস ** নামে পরিচিত নিউরনের একটি পৃথক অংশ রয়েছে যা কর্টেক্সের প্রায় সর্বত্র তাড়িত-বার্তাবহ প্রেরণ করে। হিপ্পোক্যাম্পাসটি স্বল্পমেয়াদী মেমরির জন্য ব্যবহার করা হয় বলে মনে করা হয়, তুলনামূলকভাবে স্বল্প সময়ের জন্য জিনিসগুলি স্মরণে রাখে । প্রচলিত তত্ত্বটি হ'ল আপনি যখন ঘুমাবেন, হিপ্পোক্যাম্পাসের সীমিত ক্ষমতা থাকার কারণে হিপ্পোক্যাম্পাস থেকে কর্টেক্সে প্রচুর পরিমাণে স্থানান্তরিত হয় দীর্ঘমেয়াদী স্মৃতিতে দৃঢ়তর হওয়ার জন্য।
<!--For a memory network, there is an input to the network, $x$ (think of it as an address of the memory), and compare this $x$ with vectors $k_1, k_2, k_3, \cdots$ ("keys") through a dot product. Put them through a softmax, what you get are an array of numbers which sum to one. And there are a set of other vectors $v_1, v_2, v_3, \cdots$ ("values"). Multiply these vectors by the scalers from softmax and sum these vectors up (note the resemblance to the attention mechanism) gives you the result.-->
একটি মেমরি নেটওয়ার্কের জন্য, নেটওয়ার্কে একটি ইনপুট রয়েছে, $x$ (এটিকে মেমরির একটি ঠিকানা হিসাবে মনে করুন), এবং এই $x$ টিকে একটি ডট গুণফলের মাধ্যমে $k_1, k_2, k_3, \cdots$ ("keys") ভেক্টরগুলির সাথে তুলনা করুন  । এগুলি একটি সফটম্যাক্সের মারফত রেখে, আপনি যা পান তা হল সংখ্যার অ্যারে (array) বিন্যাস যাদের সমষ্টিগত মান হয় ১। এবং অন্যান্য ভেক্টরগুলির একটি সেট রয়েছে $v_1, v_2, v_3, \cdots$ ("মান")। সফটম্যাক্স থেকে স্কেলারগুলির সাহায্যে এই ভেক্টরগুলিকে গুণ করুন এবং এই ভেক্টরগুলি যোগ করুন (অ্যাটেনশন ব্যবস্থার সাথে সাদৃশ্যটি নোট করুন) , যার পরিপ্রেক্ষিতে আপনাকে ফলাফল দেয়।

<center>
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork1.png" height="300px"/><br>
<!--Figure 7. Memory Network-->
চিত্র ৭:মেমরি নেটওয়ার্ক
</center>

<!--If one of the keys (*e.g.* $k_i$) exactly matches $x$, then the coefficient associated with this key will be very close to one. So the output of the system will essentially be $v_i$.-->
কী গুলির মধ্যে একটি যদি (*e.g.* $k_i$) হুবহু $x$ এর সাথে মিলে যায়, তবে এই কী-টির সাথে যুক্ত সহগ একটির খুব কাছাকাছি থাকবে। সুতরাং সিস্টেমের আউটপুটটি মূলত $v_i$ হবে।
<!--This is **addressable associative memory**. Associative memory is that if your input matches a key, you get *that* value. And this is just a soft differentiable version of it, which allows you to backpropagate and change the vectors through gradient descent.-->
এটি ** অ্যাড্রেসেবল এসোসিয়েটিভ মেমরি **। সহযোগী মেমোরিটি হল যদি আপনার ইনপুট কোনও কী-টির সাথে মেলে, আপনি * সেই * মানটি পাবেন। এবং এটি নিজের একটি সফট ডিফারেন্সিয়াল সংস্করণ, যা আপনাকে গ্র্যাডিয়েন্ট বংশোদ্ভূত মাধ্যমে ভেক্টরগুলিকে ব্যাকপ্রোপাগেট করতে এবং পরিবর্তন করতে দেয়।
<!--What the authors did was tell a story to a system by giving it a sequence of sentences. The sentences are encoded into vectors by running them through a neural net that has not been pretrained. The sentences are returned to the memory of this type. When you ask a question to the system, you encode the question and put it as the input of a neural net, the neural net produces an $x$ to the memory, and the memory returns a value.-->
লেখকরা যা করেছিলেন তা হল একটি সিস্টেমকে বাক্যগুলির ক্রম অনুসারে একটি গল্প বলেছিলেন। বাক্যগুলি ভেক্টরগুলিতে একটি নিউরাল নেট দ্বারা চালিত হয়ে এনকোড করা হয় যা পূর্বে ট্রেইন করা হয়নি। বাক্যগুলি এই ধরণের মেমরিতে ফিরে আসে। আপনি যখন সিস্টেমে কোনও প্রশ্ন জিজ্ঞাসা করেন, আপনি প্রশ্নটি এনকোড করে নিউরাল নেট এর ইনপুট হিসাবে রাখেন, নিউরাল নেট মেমরিতে একটি $x$ উত্পাদন করে এবং মেমরিটি একটি মান দেয়।
<!--This value, together with the previous state of the network, is used to re-access the memory. And you train this entire network to produce an answer to your question. After extensive training, this model actually learns to store stories and answer questions.-->
এই মানটি, নেটওয়ার্কের পূর্বের অবস্থার সাথে একসাথে, মেমরিটিকে পুনরায় অ্যাক্সেস করতে ব্যবহৃত হয়। এবং আপনি আপনার প্রশ্নের উত্তর তৈরি করতে এই পুরো নেটওয়ার্কটিকে প্রশিক্ষণ দিন। ব্যাপক প্রশিক্ষণের পরে, এই মডেলটি আসলে গল্পগুলি সঞ্চয় করতে এবং প্রশ্নের উত্তর দিতে শেখে।
$$
\alpha_i = k_i^\top x \\
c = \text{softmax}(\alpha) \\
s = \sum_i c_i v_i
$$

<!--In memory network, there is a neural net that takes an input and then produces an address for the memory, gets the value back to the network, keeps going, and eventually produces an output. This is very much like computer since there is a CPU and an external memory to read and write.-->
মেমোরি নেটওয়ার্কে, একটি নিউরাল নেট রয়েছে যা একটি ইনপুট নেয় এবং তারপরে মেমরির জন্য একটি ঠিকানা তৈরি করে, নেটওয়ার্কে ফিরে আসার মান পায়, চালিয়ে যায় এবং অবশেষে একটি আউটপুট তৈরি করে। এটি অনেকটা কম্পিউটারের মতো, যেহেতু এখানে একটি সিপিইউ এবং পড়া এবং লেখার জন্য একটি বাহ্যিক মেমরি রয়েছে।
<center>
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork2.png" height="200px" />
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork3.png" height="200px" /> <br>

<!--Figure 8. Comparision between memory network and computer (Photo by <a href='https://www.khanacademy.org/computing/ap-computer-science-principles/computers-101/computer--components/a/computer-memory'>Khan Acadamy</a>)-->
চিত্র ৮: মেমরি নেটওয়ার্ক এবং কম্পিউটারের তুলনা (ছবি কৃতজ্ঞতা <a href='https://www.khanacademy.org/computing/ap-computer-science-principles/computers-101/computer--components/a/computer-memory'>Khan Acadamy</a>)
</center>

<!--There are people who imagine that you can actually build **differentiable computers** out of this. One example is the [Neural Turing Machine](https://arxiv.org/abs/1410.5401) from DeepMind, which was made public three days after Facebook's paper was published on arXiv.-->
এমন লোকেরা আছেন যারা কল্পনা করেন যে আপনি এটির বাইরে ** পার্থক্যযোগ্য কম্পিউটার ** তৈরি করতে পারবেন। এর একটি উদাহরণ হ'ল ডিপমাইন্ডের [Neural Touring Machine](https://arxiv.org/abs/1410.5401), যা ফেসবুকের পেপারটি arXivতে প্রকাশিত হওয়ার তিন দিন পরে প্রকাশ করা হয়েছিল।
<!--The idea is to compare inputs to keys, generate coefficients, and produce values - which is basically what a transformer is.  A transformer is basically a neural net in which every group of neurons is one of these networks.-->
ধারণাটি হল কী-গুলির সাথে ইনপুটগুলি তুলনা করা, সহগ তৈরি করা এবং মানগুলি উত্পাদন করা - যা মূলত ট্রান্সফর্মারের কাজ।  একটি ট্রান্সফর্মার মূলত একটি নিউরাল নেট যেখানে প্রতিটি নিউরনের গ্রুপ এই নেটওয়ার্কগুলির মধ্যে একটি।

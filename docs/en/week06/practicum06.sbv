0:00:00.030,0:00:03.959
so today we are gonna be covering quite

0:00:01.680,0:00:06.060
a lot of materials so I will try not to

0:00:03.959,0:00:08.309
run but then yesterday young scooped me

0:00:06.060,0:00:09.780
completely so young talked about exactly

0:00:08.309,0:00:15.269
the same things I wanted to talk today

0:00:09.780,0:00:17.670
so I'm gonna go a bit faster please slow

0:00:15.269,0:00:18.210
me down if you actually are somehow lost

0:00:17.670,0:00:20.640
okay

0:00:18.210,0:00:21.420
so I will just try to be a little bit

0:00:20.640,0:00:23.369
faster than you sir

0:00:21.420,0:00:26.250
so today we are gonna be talking about

0:00:23.369,0:00:28.680
recurrent neural networks record neural

0:00:26.250,0:00:31.050
networks are one type of architecture we

0:00:28.680,0:00:33.570
can use in order to be to deal with

0:00:31.050,0:00:37.430
sequences of data what are sequences

0:00:33.570,0:00:37.430
what type of signal is a sequence

0:00:39.890,0:00:44.219
temporal is a temporal component but we

0:00:42.149,0:00:46.590
already seen data with temporal

0:00:44.219,0:00:49.350
component how what are they called

0:00:46.590,0:00:53.010
what dimensional what is the dimension

0:00:49.350,0:00:55.320
of that kind of signal so on the

0:00:53.010,0:00:57.660
convolutional net lesson we have seen

0:00:55.320,0:00:59.969
that a signal could be one this signal

0:00:57.660,0:01:01.859
to this signal 3d signal based on the

0:00:59.969,0:01:06.270
domain and the domain is what you map

0:01:01.859,0:01:08.130
from to go to right so temporal handling

0:01:06.270,0:01:10.580
sequential sequences of data is

0:01:08.130,0:01:12.630
basically dealing with one the data

0:01:10.580,0:01:15.119
because the domain is going to be just

0:01:12.630,0:01:17.340
the temporal axis nevertheless you can

0:01:15.119,0:01:18.689
also use RNN to deal with you know two

0:01:17.340,0:01:25.350
dimensional data you have double

0:01:18.689,0:01:28.049
Direction okay okay so this is a

0:01:25.350,0:01:30.329
classical neural network in the diagram

0:01:28.049,0:01:33.299
that is I'm used to draw where I

0:01:30.329,0:01:35.280
represent each in this case bunch of

0:01:33.299,0:01:37.590
neurons like each of those is a vector

0:01:35.280,0:01:40.350
and for example the X is my input vector

0:01:37.590,0:01:42.450
it's in pink as usual then I have my

0:01:40.350,0:01:44.640
hidden layer in a green in the center

0:01:42.450,0:01:46.200
then I have my final blue eared lane

0:01:44.640,0:01:47.790
layer which is the output network so

0:01:46.200,0:01:52.320
this is a three layer neural network in

0:01:47.790,0:01:56.189
my for my notation and so if some of you

0:01:52.320,0:01:57.960
are familiar with digital electronics

0:01:56.189,0:02:00.810
this is like talking about a

0:01:57.960,0:02:03.329
combinatorial logic your current output

0:02:00.810,0:02:05.600
depends only on the current input and

0:02:03.329,0:02:08.420
that's it there is no

0:02:05.600,0:02:10.700
there is no other input instead when we

0:02:08.420,0:02:12.590
are talking about our men we are gonna

0:02:10.700,0:02:15.410
be talking about something that looks

0:02:12.590,0:02:17.420
like this in this case our output here

0:02:15.410,0:02:19.850
on the right hand side depends on the

0:02:17.420,0:02:21.860
current input and on the state of the

0:02:19.850,0:02:24.500
system and again if you're a king of

0:02:21.860,0:02:26.750
digital electronics this is simply

0:02:24.500,0:02:28.640
sequential logic whereas you have an

0:02:26.750,0:02:31.580
internal state the onion is the

0:02:28.640,0:02:33.170
dimension flip-flop if you have no idea

0:02:31.580,0:02:37.040
what a flip-flop you know check it out

0:02:33.170,0:02:39.980
it's just some very basic memory unit in

0:02:37.040,0:02:41.810
digital electronics nevertheless this is

0:02:39.980,0:02:43.550
the only difference right in the first

0:02:41.810,0:02:45.290
case you have an output which is just

0:02:43.550,0:02:46.730
function of the input in the second case

0:02:45.290,0:02:49.580
you have an output which is function of

0:02:46.730,0:02:54.130
the input and the state of the system

0:02:49.580,0:02:54.130
okay that's the big difference yeah

0:02:54.940,0:03:00.640
vanilla is in American term for saying

0:02:58.040,0:03:04.670
it's plane doesn't have a taste that

0:03:00.640,0:03:10.010
American sorry I try to be the most

0:03:04.670,0:03:11.390
American I can in Italy you feel taken

0:03:10.010,0:03:12.799
an ice cream which is doesn't have a

0:03:11.390,0:03:15.950
taste it's gonna be fury laughter which

0:03:12.799,0:03:18.320
is milk taste in here we don't have milk

0:03:15.950,0:03:20.049
tests they have vanilla taste which is

0:03:18.320,0:03:23.310
the plain ice cream

0:03:20.049,0:03:28.360
okay Americans sorry

0:03:23.310,0:03:30.340
all right so oh so let's see what does

0:03:28.360,0:03:32.760
it change this with young representation

0:03:30.340,0:03:35.560
so young draws those kind of little

0:03:32.760,0:03:38.170
funky things here which represent a

0:03:35.560,0:03:39.970
mapping between a TENS tensor to another

0:03:38.170,0:03:41.800
painter from one a vector to another

0:03:39.970,0:03:44.050
vector right so there you have your

0:03:41.800,0:03:46.630
input vector X is gonna be mapped

0:03:44.050,0:03:48.070
through this item here to this hidden

0:03:46.630,0:03:50.620
representation so that actually

0:03:48.070,0:03:52.780
represent my fine transformation so my

0:03:50.620,0:03:54.130
rotation Plus this question then you

0:03:52.780,0:03:56.050
have the heater representation that you

0:03:54.130,0:03:57.850
have another rotation is question then

0:03:56.050,0:04:00.040
you get the final output right similarly

0:03:57.850,0:04:03.220
in the recurrent diagram you can have

0:04:00.040,0:04:04.660
these additional things this is a fine

0:04:03.220,0:04:06.640
transformation squashing that's like a

0:04:04.660,0:04:08.830
delay module with a final transformation

0:04:06.640,0:04:10.900
excursion and now you have the final one

0:04:08.830,0:04:14.620
affine transformation and squashing

0:04:10.900,0:04:18.100
right these things is making noise okay

0:04:14.620,0:04:20.440
sorry all right so what is the first

0:04:18.100,0:04:24.250
case first case is this one is a vector

0:04:20.440,0:04:26.290
to sequence so we input one bubble the

0:04:24.250,0:04:28.270
pink wonder and then you're gonna have

0:04:26.290,0:04:30.730
this evolution of the internal state of

0:04:28.270,0:04:33.070
the system the green one and then as the

0:04:30.730,0:04:35.169
state of the system evolves you can be

0:04:33.070,0:04:38.470
spitting out at every time stamp one

0:04:35.169,0:04:40.750
specific output what can be an example

0:04:38.470,0:04:43.240
of this kind of architecture so this one

0:04:40.750,0:04:45.310
could be the following my input is gonna

0:04:43.240,0:04:46.750
be one of these images and then the

0:04:45.310,0:04:49.240
output is going to be a sequence of

0:04:46.750,0:04:53.140
characters representing the English

0:04:49.240,0:04:55.270
description of whatever this input is so

0:04:53.140,0:04:57.940
for example in the center when we have a

0:04:55.270,0:05:00.220
herd of elephants so the last one herd

0:04:57.940,0:05:03.880
of elephants walking across a dry grass

0:05:00.220,0:05:06.250
field so it's very very very well

0:05:03.880,0:05:09.130
refined right then you have in the

0:05:06.250,0:05:12.880
center here for example two dogs play in

0:05:09.130,0:05:15.640
the in the grass maybe there are three

0:05:12.880,0:05:17.770
but okay they play they're playing in

0:05:15.640,0:05:20.500
the grass right so it's cool in this

0:05:17.770,0:05:21.970
case you know a red motorcycle park on

0:05:20.500,0:05:24.610
the side of the road

0:05:21.970,0:05:26.980
looks more pink or you know a little

0:05:24.610,0:05:30.490
blow a little a little girl in the pink

0:05:26.980,0:05:33.640
that is blowing bubbles that she's not

0:05:30.490,0:05:35.650
blowing right anything there all right

0:05:33.640,0:05:39.010
and then you also have you know even

0:05:35.650,0:05:41.560
more wrong examples right so you have

0:05:39.010,0:05:42.250
like yellow school bus parked in the

0:05:41.560,0:05:44.050
parking lot

0:05:42.250,0:05:46.600
well it's CL um but it's not a school

0:05:44.050,0:05:49.860
bus so it can be failing as well but I

0:05:46.600,0:05:52.810
also can do a very very nice you know

0:05:49.860,0:05:56.470
you can also perform very well so this

0:05:52.810,0:05:59.440
was from one input vector which is B for

0:05:56.470,0:06:01.720
example representation of my image to a

0:05:59.440,0:06:03.790
sequence of symbols which are D for

0:06:01.720,0:06:05.620
example characters or words that are

0:06:03.790,0:06:09.190
making here my English sentence okay

0:06:05.620,0:06:11.440
clear so far yeah okay another kind of

0:06:09.190,0:06:14.200
usage you can have is maybe the

0:06:11.440,0:06:17.560
following so you're gonna have sequence

0:06:14.200,0:06:19.660
two final vector okay so I don't care

0:06:17.560,0:06:22.120
about the intermediate sequences so okay

0:06:19.660,0:06:24.370
the top right is called Auto regressive

0:06:22.120,0:06:26.590
network and outer regressive network is

0:06:24.370,0:06:28.810
a network which is outputting an output

0:06:26.590,0:06:29.950
given that you feel as input the

0:06:28.810,0:06:31.690
previous output okay

0:06:29.950,0:06:33.700
so this is called Auto regressive you

0:06:31.690,0:06:35.260
have this kind of loopy part on the

0:06:33.700,0:06:37.780
network on the left hand side instead

0:06:35.260,0:06:40.140
I'm gonna be providing several sequences

0:06:37.780,0:06:40.140
yeah

0:06:48.930,0:06:53.880
that's gonna be the English translation

0:06:51.509,0:06:55.380
so you have a sequence of words that are

0:06:53.880,0:06:57.660
going to make up your final sentence

0:06:55.380,0:07:00.330
it's it's blue there you can think about

0:06:57.660,0:07:01.889
a index in a dictionary and then each

0:07:00.330,0:07:03.300
blue is going to tell you which word

0:07:01.889,0:07:07.380
you're gonna pick on an indexed

0:07:03.300,0:07:09.780
dictionary right so this is a school bus

0:07:07.380,0:07:12.570
right so oh yeah a yellow school bus you

0:07:09.780,0:07:14.940
go to a index of a then you have second

0:07:12.570,0:07:16.740
index you can figure out that is yellow

0:07:14.940,0:07:17.820
and then school box right so the

0:07:16.740,0:07:19.979
sequence here is going to be

0:07:17.820,0:07:22.590
representing the sequence of words the

0:07:19.979,0:07:24.180
model is out on the other side there on

0:07:22.590,0:07:26.460
the left you're gonna have instead I

0:07:24.180,0:07:28.740
keep feeding a sequence of symbols and

0:07:26.460,0:07:30.750
only at the end I'm gonna look what is

0:07:28.740,0:07:33.539
my final output what can be an

0:07:30.750,0:07:36.150
application of this one so something yun

0:07:33.539,0:07:37.979
also mentioned was different so let's

0:07:36.150,0:07:40.789
see if I can get my network to compile

0:07:37.979,0:07:43.410
Python or to an open pilot own

0:07:40.789,0:07:45.599
interpretation so in this case I have my

0:07:43.410,0:07:49.620
current input which I feed my network

0:07:45.599,0:07:54.979
which is going to be J equal 8580 for

0:07:49.620,0:07:57.960
then for X in range eight some - J 920

0:07:54.979,0:07:59.430
blah blah blah and then print this one

0:07:57.960,0:08:02.010
and then my network is going to be

0:07:59.430,0:08:04.860
tasked with the just you know giving me

0:08:02.010,0:08:06.419
twenty five thousand and eleven okay so

0:08:04.860,0:08:09.210
this is the final output of a program

0:08:06.419,0:08:12.000
and I enforced in the network to be able

0:08:09.210,0:08:13.860
to output me the correct output the

0:08:12.000,0:08:16.229
correct in your solution of this program

0:08:13.860,0:08:18.330
or even more complicated things for

0:08:16.229,0:08:19.740
example I can provide a sequence of

0:08:18.330,0:08:21.900
other symbols which are going to be

0:08:19.740,0:08:23.970
eighty eight thousand eight hundred

0:08:21.900,0:08:26.669
thirty seven then I have C is going to

0:08:23.970,0:08:30.599
be something then I have print this one

0:08:26.669,0:08:33.360
if something that is always true as the

0:08:30.599,0:08:36.450
other one and then you know the output

0:08:33.360,0:08:38.849
should be twelve thousand eight 184

0:08:36.450,0:08:40.890
right so you can train a neural net to

0:08:38.849,0:08:42.690
do these operations so you feed a

0:08:40.890,0:08:44.730
sequence of symbols and then at the

0:08:42.690,0:08:48.870
output you just enforce that the final

0:08:44.730,0:08:54.970
target should be a specific value okay

0:08:48.870,0:08:56.190
and these things making noise okay maybe

0:08:54.970,0:09:00.700
I'm better

0:08:56.190,0:09:02.589
all right so what's next next is going

0:09:00.700,0:09:05.140
to be for example a sequence to vector

0:09:02.589,0:09:07.210
to sequence this used to be the standard

0:09:05.140,0:09:09.490
way of performing length language

0:09:07.210,0:09:13.000
translation so you start with a sequence

0:09:09.490,0:09:14.680
of symbols here shown in pink so you

0:09:13.000,0:09:17.290
have a sequence of inputs then

0:09:14.680,0:09:20.290
everything gets condensed into this kind

0:09:17.290,0:09:23.020
of final age which is this H over here

0:09:20.290,0:09:25.630
which is going to be somehow my concept

0:09:23.020,0:09:27.880
right so I have a sentence I squeeze the

0:09:25.630,0:09:29.800
sentence temporal information into just

0:09:27.880,0:09:31.600
one vector which is representing the

0:09:29.800,0:09:33.520
meaning the message I'd like to send

0:09:31.600,0:09:36.310
across and then I get this meaning in

0:09:33.520,0:09:37.720
whatever representation unrolled back in

0:09:36.310,0:09:41.380
a different language right so I can

0:09:37.720,0:09:43.600
encode I don't know today I'm very happy

0:09:41.380,0:09:47.350
in English as a sequence of word and

0:09:43.600,0:09:48.730
then you know you can get LG Sonoma to

0:09:47.350,0:09:53.170
Felicia and then I speak outside

0:09:48.730,0:09:54.310
Thailand today or whatever now today I'm

0:09:53.170,0:09:58.480
very tired

0:09:54.310,0:10:00.640
Jin Chen walk han lei or whatever ok so

0:09:58.480,0:10:02.020
again you have some kind of encoding

0:10:00.640,0:10:04.540
then you have a compressed

0:10:02.020,0:10:08.110
representation and then you get like the

0:10:04.540,0:10:11.950
decoding given the same compressed

0:10:08.110,0:10:15.040
version ok and so for example I guess

0:10:11.950,0:10:17.890
language translation again recently we

0:10:15.040,0:10:20.709
have seen transformers and a lot of

0:10:17.890,0:10:22.600
things like in the recent time so we're

0:10:20.709,0:10:25.300
going to cover that the next lesson I

0:10:22.600,0:10:28.930
think but this used to be the state of

0:10:25.300,0:10:31.000
the art until few two years ago and here

0:10:28.930,0:10:34.089
you can see that if you actually check

0:10:31.000,0:10:38.950
if you do a PCA over the latent space

0:10:34.089,0:10:42.160
you have that words are grouped by

0:10:38.950,0:10:43.630
semantics ok so if we zoom in that

0:10:42.160,0:10:46.000
region there are we're gonna see that in

0:10:43.630,0:10:48.400
what in the same location you find all

0:10:46.000,0:10:50.529
the amounts december february november

0:10:48.400,0:10:52.750
whatever right if you put a few focus on

0:10:50.529,0:10:53.510
a different region you get that a few

0:10:52.750,0:10:55.250
days

0:10:53.510,0:10:57.350
next few miles and so on right so

0:10:55.250,0:11:00.230
different location will have some

0:10:57.350,0:11:03.380
specific you know common meaning so we

0:11:00.230,0:11:05.780
basically see in this case how by

0:11:03.380,0:11:07.880
training these networks you know just

0:11:05.780,0:11:09.680
with symbols they will pick up on some

0:11:07.880,0:11:12.830
specific semantics

0:11:09.680,0:11:16.130
you know features right in this case you

0:11:12.830,0:11:18.110
can see like there is a vector so the

0:11:16.130,0:11:20.900
vector that is connecting women to men

0:11:18.110,0:11:24.080
is gonna be the same vector that is well

0:11:20.900,0:11:27.590
woman - man which is this one I think is

0:11:24.080,0:11:30.620
gonna be equal to Queen - King right and

0:11:27.590,0:11:32.890
so yeah it's correct and so you're gonna

0:11:30.620,0:11:35.810
have that the same distance in this

0:11:32.890,0:11:37.730
embedding space will be applied to

0:11:35.810,0:11:39.800
things that are female and male for

0:11:37.730,0:11:43.370
example or in the other case you have

0:11:39.800,0:11:45.710
walk-in and walked swimming and swamp so

0:11:43.370,0:11:47.960
you always have this you know specific

0:11:45.710,0:11:51.680
linear transformation you can apply in

0:11:47.960,0:11:53.690
order to go from one type of word to the

0:11:51.680,0:11:55.640
other one or this one you have the

0:11:53.690,0:11:59.180
connection between cities and the

0:11:55.640,0:12:02.240
capitals all right so one more right I

0:11:59.180,0:12:05.210
think what's missing from the big

0:12:02.240,0:12:06.890
picture here it's a big picture because

0:12:05.210,0:12:09.560
it's so large no no it's such a big

0:12:06.890,0:12:12.560
picture because it's the overview okay

0:12:09.560,0:12:18.590
you didn't get the joke it's okay what's

0:12:12.560,0:12:20.540
missing here vector to seek with no okay

0:12:18.590,0:12:23.330
good but no because you can still use

0:12:20.540,0:12:25.700
the other one so you have this one the

0:12:23.330,0:12:27.830
vector is sequence to sequence right so

0:12:25.700,0:12:29.780
this one is you start feeding inside

0:12:27.830,0:12:31.580
inputs you start outputting something

0:12:29.780,0:12:36.320
right what can be an example of this

0:12:31.580,0:12:38.900
stuff so if you had a Nokia phone and

0:12:36.320,0:12:40.760
you use the t9 you know this stuff from

0:12:38.900,0:12:43.100
20 years ago you have basically

0:12:40.760,0:12:45.380
suggestions on what your typing is

0:12:43.100,0:12:47.150
you're typing right so this would be one

0:12:45.380,0:12:49.010
type of these suggestions where like one

0:12:47.150,0:12:50.570
type of this architecture as you getting

0:12:49.010,0:12:54.850
suggestions as you're typing things

0:12:50.570,0:12:57.290
through or you may have like speech to

0:12:54.850,0:12:58.740
captions right I talked and you have the

0:12:57.290,0:13:02.520
things

0:12:58.740,0:13:05.010
below or something very cool which is

0:13:02.520,0:13:08.089
the following so I start writing here

0:13:05.010,0:13:11.089
the rings of Saturn glitter while the

0:13:08.089,0:13:16.260
harsh ice two men look at each other hmm

0:13:11.089,0:13:18.390
okay they were enemies but the server

0:13:16.260,0:13:20.100
robots weren't okay okay hold on

0:13:18.390,0:13:22.770
so this network was trained on some

0:13:20.100,0:13:24.360
sci-fi novels and therefore you can just

0:13:22.770,0:13:26.130
type something then you let the network

0:13:24.360,0:13:28.290
start outputting some suggestions for

0:13:26.130,0:13:31.080
you so you know if you don't know how to

0:13:28.290,0:13:34.620
write a book then you can you know ask

0:13:31.080,0:13:37.290
your computer to help you out okay

0:13:34.620,0:13:39.740
that's so cool or one more that I really

0:13:37.290,0:13:43.560
like it this one is fantastic I think

0:13:39.740,0:13:45.959
you should read read it I think so you

0:13:43.560,0:13:49.320
put some kind of input there like the

0:13:45.959,0:13:51.630
scientist named alone what is it

0:13:49.320,0:13:54.360
or the prompt right so you put in the

0:13:51.630,0:13:56.839
the top prompt and then you get you know

0:13:54.360,0:14:00.149
this network start writing about very

0:13:56.839,0:14:05.690
interesting unicorns with multiple horns

0:14:00.149,0:14:07.980
is called horns say unicorn right okay

0:14:05.690,0:14:09.480
alright let's so cool just check it out

0:14:07.980,0:14:11.820
later and you can take a screenshot of

0:14:09.480,0:14:14.970
the screen anyhow so that was like the

0:14:11.820,0:14:19.920
eye candy such that you get you know

0:14:14.970,0:14:21.089
hungry now let's go into be PTT which is

0:14:19.920,0:14:24.060
the thing that they aren't really like

0:14:21.089,0:14:27.390
yesterday's PTT said okay alright let's

0:14:24.060,0:14:29.190
see how this stuff works okay so on the

0:14:27.390,0:14:31.620
left hand side we see again this vector

0:14:29.190,0:14:34.020
middle in the representation the output

0:14:31.620,0:14:35.520
to a fine transformation and then there

0:14:34.020,0:14:39.020
we have the classical equations right

0:14:35.520,0:14:42.450
all right so let's see how this stuff is

0:14:39.020,0:14:44.940
similar or not similar and you can't see

0:14:42.450,0:14:46.620
anything so for the next two seconds I

0:14:44.940,0:14:48.420
will want one minute I will turn off the

0:14:46.620,0:14:51.300
lights then I turn them on

0:14:48.420,0:14:53.800
[Music]

0:14:51.300,0:14:55.570
okay now you can see something all right

0:14:53.800,0:14:59.290
so let's see what are the questions of

0:14:55.570,0:15:00.490
this new architecture don't stand up

0:14:59.290,0:15:02.200
you're gonna be crushing yourself

0:15:00.490,0:15:04.270
alright so you have here the hidden

0:15:02.200,0:15:07.300
representation now there's gonna be this

0:15:04.270,0:15:10.000
nonlinear function of this rotation of a

0:15:07.300,0:15:12.490
stack version of my input which I

0:15:10.000,0:15:15.520
appended the previous configuration of

0:15:12.490,0:15:18.010
the hidden layer okay and so this is a

0:15:15.520,0:15:19.420
very nice compact notation it's just I

0:15:18.010,0:15:21.730
just put the two vectors one on top of

0:15:19.420,0:15:24.640
each other and then I sign assign I sum

0:15:21.730,0:15:27.520
the bias I also and define initial

0:15:24.640,0:15:29.920
condition my initial H is gonna be 0 so

0:15:27.520,0:15:32.530
at the beginning whenever I have T equal

0:15:29.920,0:15:34.360
1 this stuff is gonna be settle is a

0:15:32.530,0:15:37.840
vector of zeros and then I have this

0:15:34.360,0:15:39.880
matrix WH is gonna be two separate

0:15:37.840,0:15:44.770
matrices so sometimes you see this a

0:15:39.880,0:15:48.130
question is wh x times x plus w HH times

0:15:44.770,0:15:50.440
h t minus 1 but you can also figure out

0:15:48.130,0:15:52.450
that if you stock those two matrices you

0:15:50.440,0:15:53.890
know one attached to the other that you

0:15:52.450,0:15:56.620
just put this two vertical lines

0:15:53.890,0:15:58.360
completely equivalent notation but it

0:15:56.620,0:16:01.360
looked like very similar to whatever we

0:15:58.360,0:16:03.430
had here so hidden layer is affine

0:16:01.360,0:16:05.230
transformation of the input inner layer

0:16:03.430,0:16:08.980
is affine transformation of the input

0:16:05.230,0:16:11.440
and the previous value okay and then you

0:16:08.980,0:16:15.460
have the final output is going to be

0:16:11.440,0:16:20.140
again my final rotation so I'm gonna

0:16:15.460,0:16:24.540
turn on the light so no magic so far

0:16:20.140,0:16:27.690
right you're okay right you're with me

0:16:24.540,0:16:31.720
to shake the heads what about the others

0:16:27.690,0:16:34.930
no yes okay whatever

0:16:31.720,0:16:37.210
so this one is simply on the right hand

0:16:34.930,0:16:40.330
side I simply unroll over time such that

0:16:37.210,0:16:42.670
you can see how things are just not very

0:16:40.330,0:16:43.990
crazy like this loop here is not

0:16:42.670,0:16:45.170
actually a loop this is like a

0:16:43.990,0:16:48.500
connection to

0:16:45.170,0:16:51.019
next time steps right so that around

0:16:48.500,0:16:52.760
arrow means is just this right arrow so

0:16:51.019,0:16:54.529
this is a neural net it's dinkley a

0:16:52.760,0:16:57.950
neural net which is extended in in

0:16:54.529,0:16:59.660
length rather also not only in a in a

0:16:57.950,0:17:01.639
thickness right so you have a network

0:16:59.660,0:17:03.440
that is going this direction input and

0:17:01.639,0:17:05.600
output but as you can think as there's

0:17:03.440,0:17:07.939
been an extended input and this been an

0:17:05.600,0:17:10.220
extended output while all these

0:17:07.939,0:17:11.569
intermediate weights are all share right

0:17:10.220,0:17:14.120
so all of these weights are the same

0:17:11.569,0:17:15.890
weights and then you use this kind of

0:17:14.120,0:17:17.510
shared weights so it's similar to a

0:17:15.890,0:17:19.640
convolutional net in the sense that you

0:17:17.510,0:17:21.410
had this parameter sharing right across

0:17:19.640,0:17:24.939
different time domains because you

0:17:21.410,0:17:28.820
assume there is some kind of you know

0:17:24.939,0:17:30.500
stationarity right of the signal make

0:17:28.820,0:17:32.870
sense so this is a kind of convolution

0:17:30.500,0:17:37.960
right you can see how this is kind of a

0:17:32.870,0:17:40.130
convolution alright so that was kind of

0:17:37.960,0:17:42.650
you know a little bit of the theory we

0:17:40.130,0:17:46.160
already seen that so let's see how this

0:17:42.650,0:17:49.490
works for a practical example so in this

0:17:46.160,0:17:51.830
case we we are just reading this code

0:17:49.490,0:17:55.340
here so this is world language model you

0:17:51.830,0:17:57.770
can find it at the PI torch examples so

0:17:55.340,0:17:59.450
you have a sequence of symbols I have

0:17:57.770,0:18:01.910
just represented there every symbol is

0:17:59.450,0:18:03.290
like a letter in the alphabet and then

0:18:01.910,0:18:05.419
the first part is gonna be basically

0:18:03.290,0:18:08.299
splitting this one in this way right

0:18:05.419,0:18:10.309
so you preserve vertically in the time

0:18:08.299,0:18:13.070
domain but then I split the long long

0:18:10.309,0:18:16.640
long sequence such that I can now chop I

0:18:13.070,0:18:19.549
can use best bets bets how do you say

0:18:16.640,0:18:21.980
computation so the first thing you have

0:18:19.549,0:18:24.980
the best size is gonna be 4 in this case

0:18:21.980,0:18:27.410
and then I'm gonna be getting in my

0:18:24.980,0:18:30.679
first batch and then I will force the

0:18:27.410,0:18:33.650
network to be able to so this will be my

0:18:30.679,0:18:36.020
best back propagation through time

0:18:33.650,0:18:38.270
period and I will force the network to

0:18:36.020,0:18:41.990
output the next sequence of characters

0:18:38.270,0:18:44.510
ok so given that I have ABC I will force

0:18:41.990,0:18:46.910
my network to say D given that I have

0:18:44.510,0:18:50.000
GHI I will force the network to

0:18:46.910,0:18:52.310
come up with J givin em an orbit for

0:18:50.000,0:18:54.980
tonight poke at P and s tu for the

0:18:52.310,0:18:57.170
network 20 right so how can you actually

0:18:54.980,0:18:59.660
make sure you understand what I'm saying

0:18:57.170,0:19:02.360
whenever you are able to predict my next

0:18:59.660,0:19:04.010
world you're actually able to you know

0:19:02.360,0:19:09.710
you basically know in already what I'm

0:19:04.010,0:19:11.720
saying right yeah so by trying to

0:19:09.710,0:19:13.460
predict an upcoming word you're going to

0:19:11.720,0:19:15.170
be showing some kind of comprehension of

0:19:13.460,0:19:19.670
whatever is going to be this temporal

0:19:15.170,0:19:22.700
information in the data all right so

0:19:19.670,0:19:24.590
after we get the beds we have so how

0:19:22.700,0:19:26.510
does it work let's actually see you know

0:19:24.590,0:19:27.920
and about a bit of a detail this is

0:19:26.510,0:19:30.650
gonna be my first output is going to be

0:19:27.920,0:19:32.510
a batch with four items I feed this

0:19:30.650,0:19:34.220
inside the near corner all night and

0:19:32.510,0:19:36.680
then my neural net we come up with a

0:19:34.220,0:19:39.740
prediction of the upcoming sample right

0:19:36.680,0:19:44.120
and I will force that one to be my B H

0:19:39.740,0:19:47.450
and T okay then I'm gonna be having my

0:19:44.120,0:19:51.110
second input I will provide the previous

0:19:47.450,0:19:53.420
hidden state to the current RNN I will

0:19:51.110,0:19:56.240
feel these inside and then I expect to

0:19:53.420,0:19:58.670
get the second line of the output the

0:19:56.240,0:20:01.160
target right and then so on right I get

0:19:58.670,0:20:03.410
the next state and sorry the next input

0:20:01.160,0:20:05.690
I get the next state and then I'm gonna

0:20:03.410,0:20:07.700
get inside the neural net the RN n I

0:20:05.690,0:20:13.840
which I will try to force to get the

0:20:07.700,0:20:13.840
final target okay so far yeah

0:20:14.260,0:20:21.260
each one is gonna be the output of the

0:20:18.730,0:20:28.280
internet recurrent neural net right I'll

0:20:21.260,0:20:41.510
show you the equation before you have h1

0:20:28.280,0:20:43.460
comes out from this one right second the

0:20:41.510,0:20:46.550
output I'm gonna be forcing the output

0:20:43.460,0:20:48.170
actually to be my target my next word in

0:20:46.550,0:20:50.570
the sequence of letters right so I have

0:20:48.170,0:20:52.610
a sequence of words force my network to

0:20:50.570,0:20:58.670
predict what's the next word given the

0:20:52.610,0:21:02.480
previous word know h1 is going to be fed

0:20:58.670,0:21:05.060
inside here and you stuck the next word

0:21:02.480,0:21:07.880
the next word together with the previous

0:21:05.060,0:21:10.610
state and then you'll do a rotation of

0:21:07.880,0:21:13.670
the previous word with a previous sorry

0:21:10.610,0:21:15.080
the new word with the next state the new

0:21:13.670,0:21:17.720
word with the previous state you'll do

0:21:15.080,0:21:18.620
our rotation here find transformation

0:21:17.720,0:21:21.230
right and then you apply the

0:21:18.620,0:21:23.750
non-linearity so you always get a new

0:21:21.230,0:21:25.610
word that is the current X and then you

0:21:23.750,0:21:27.230
get the previous state just to see in

0:21:25.610,0:21:30.650
what state the system once and then you

0:21:27.230,0:21:32.810
output a new output right and so we are

0:21:30.650,0:21:35.000
in this situation here we have a bunch

0:21:32.810,0:21:36.860
of inputs I have my first input and then

0:21:35.000,0:21:39.200
I get the first output I have this

0:21:36.860,0:21:41.720
internal memory that is sent forward and

0:21:39.200,0:21:44.240
then this network will now be aware of

0:21:41.720,0:21:46.910
what happened here and then I input the

0:21:44.240,0:21:49.450
next input and so on I get the next

0:21:46.910,0:21:52.460
output and I force the output to be the

0:21:49.450,0:21:57.040
output here the value inside the batch

0:21:52.460,0:21:57.040
ok alright what's missing now

0:21:57.070,0:22:00.160
[Music]

0:22:00.490,0:22:05.830
this is for PowerPoint drawing

0:22:02.890,0:22:08.370
constraint all right what's happening

0:22:05.830,0:22:11.230
now so here I'm gonna be sending the

0:22:08.370,0:22:13.300
here I just drawn an arrow with the

0:22:11.230,0:22:13.660
final HT but there is a slash on the

0:22:13.300,0:22:16.780
arrow

0:22:13.660,0:22:25.660
what is the slash on the arrow who can

0:22:16.780,0:22:27.100
understand what the slash mean of course

0:22:25.660,0:22:28.780
there will be there is gonna be the next

0:22:27.100,0:22:31.570
batch they're gonna be starting from

0:22:28.780,0:22:44.110
here D and so on this is gonna be my

0:22:31.570,0:22:46.690
next batch d j pv e KQ v + FL rx right

0:22:44.110,0:22:49.300
so this slash here means do not back

0:22:46.690,0:22:51.550
propagate through okay so that one is

0:22:49.300,0:22:53.710
gonna be calling dot detach in Porsche

0:22:51.550,0:22:56.560
which is gonna be stopping the gradient

0:22:53.710,0:22:59.050
to be you know propagated back to

0:22:56.560,0:23:01.450
forever okay so this one say know that

0:22:59.050,0:23:03.220
and so whenever I get the sorry no no

0:23:01.450,0:23:06.970
gradient such that when I input the next

0:23:03.220,0:23:09.580
gradient the first input here it's gonna

0:23:06.970,0:23:11.530
be this guy over here and also of course

0:23:09.580,0:23:15.780
without gradient such that we don't have

0:23:11.530,0:23:17.170
an infinite length RN n okay make sense

0:23:15.780,0:23:20.760
yes

0:23:17.170,0:23:24.640
no I assume it's a yes

0:23:20.760,0:23:27.430
okay so vanishing and exploding

0:23:24.640,0:23:30.730
gradients we touch them upon these also

0:23:27.430,0:23:33.850
yesterday so again I'm kind of going a

0:23:30.730,0:23:35.620
little bit faster to the intent user so

0:23:33.850,0:23:38.260
let's see how this works

0:23:35.620,0:23:40.390
so usually for our recurrent neural

0:23:38.260,0:23:41.970
network you have an input you have a

0:23:40.390,0:23:45.160
hidden layer and then you have an output

0:23:41.970,0:23:48.010
then this value of here how do you get

0:23:45.160,0:23:50.680
this information through here what what

0:23:48.010,0:23:52.120
what does this R represent do you

0:23:50.680,0:23:55.840
remember the equation of the hidden

0:23:52.120,0:23:58.570
layer so the new hidden layer is gonna

0:23:55.840,0:24:01.050
be the previous hidden layer which we

0:23:58.570,0:24:01.050
rotate

0:24:03.100,0:24:08.030
alright so we rotate the previous hidden

0:24:05.480,0:24:11.780
layer and so how do you rotate hidden

0:24:08.030,0:24:15.220
layers matrices right and so every time

0:24:11.780,0:24:17.300
you see all ads on tile arrow there is a

0:24:15.220,0:24:21.920
rotation there is a matrix

0:24:17.300,0:24:24.770
now if the you know this matrix can

0:24:21.920,0:24:26.900
change the sizing of your final output

0:24:24.770,0:24:29.030
right so if you think about perhaps

0:24:26.900,0:24:31.190
let's say the determinant right if the

0:24:29.030,0:24:32.870
terminal is unitary it's a mapping the

0:24:31.190,0:24:34.610
same areas for the same area if it's

0:24:32.870,0:24:37.400
larger than one they're going to be

0:24:34.610,0:24:39.560
getting you know this radians to getting

0:24:37.400,0:24:41.360
larger and larger or if it's smaller

0:24:39.560,0:24:44.660
than I'm gonna get these gradients to go

0:24:41.360,0:24:47.300
to zero whenever you perform the back

0:24:44.660,0:24:48.920
propagation in this direction okay so

0:24:47.300,0:24:51.200
the problem here is that whenever we do

0:24:48.920,0:24:53.390
is send gradients back so the gains are

0:24:51.200,0:24:55.520
going to be going down like that are

0:24:53.390,0:24:57.800
gonna be going like down like this then

0:24:55.520,0:25:00.260
down like this way and down like this

0:24:57.800,0:25:01.610
way and also all down this way and so on

0:25:00.260,0:25:03.620
right so the gradients are going to be

0:25:01.610,0:25:06.380
always going against the direction of

0:25:03.620,0:25:08.870
the arrow in H ro has a matrix inside

0:25:06.380,0:25:11.510
right and again this matrix will affect

0:25:08.870,0:25:14.090
how these gradients propagate and that's

0:25:11.510,0:25:18.590
why you can see here although we have a

0:25:14.090,0:25:21.770
very bright input that one like gets

0:25:18.590,0:25:23.720
lost through oh well if you have like a

0:25:21.770,0:25:27.410
gradient coming down here the gradient

0:25:23.720,0:25:30.410
gets you know kill over time okay so how

0:25:27.410,0:25:34.370
do we fix that to fix this one we simply

0:25:30.410,0:25:40.420
remove the matrices in this horizontal

0:25:34.370,0:25:43.610
operation does it make sense no yes no

0:25:40.420,0:25:47.630
the problem is that the next hidden

0:25:43.610,0:25:49.610
state will have you know its own input

0:25:47.630,0:25:52.910
memory coming from the previous step

0:25:49.610,0:25:55.400
through a matrix multiplication now this

0:25:52.910,0:25:58.760
matrix multiplication will affect what's

0:25:55.400,0:26:00.140
gonna be the gradient that comes in the

0:25:58.760,0:26:02.630
other direction okay

0:26:00.140,0:26:04.790
so whenever you have an output here you

0:26:02.630,0:26:06.740
have a final loss now you have the grade

0:26:04.790,0:26:10.520
that are gonna be going against the

0:26:06.740,0:26:12.050
arrows up to the input the problem is

0:26:10.520,0:26:14.690
that this gradient which is going

0:26:12.050,0:26:16.910
through the in the opposite direction of

0:26:14.690,0:26:19.510
these arrows will be multiplied by the

0:26:16.910,0:26:22.460
matrix right the transpose of the matrix

0:26:19.510,0:26:24.410
and again these matrices will affect

0:26:22.460,0:26:26.030
what is the overall norm of this

0:26:24.410,0:26:26.810
gradient right and it will be all

0:26:26.030,0:26:28.310
killing it

0:26:26.810,0:26:30.380
you have vanishing gradient or you're

0:26:28.310,0:26:32.690
gonna have exploding the gradient which

0:26:30.380,0:26:35.120
is going to be whenever is going to be

0:26:32.690,0:26:37.880
getting amplified right so in order to

0:26:35.120,0:26:39.830
be avoiding that we have to avoid so you

0:26:37.880,0:26:41.960
can see this is a very deep network so

0:26:39.830,0:26:43.700
recurrently our network where the first

0:26:41.960,0:26:45.320
deep networks back in the night is

0:26:43.700,0:26:48.200
actually and the word

0:26:45.320,0:26:49.850
depth was actually in time which and of

0:26:48.200,0:26:52.550
course they were facing the same issues

0:26:49.850,0:26:54.350
we face with deep learning in modern day

0:26:52.550,0:26:56.270
days where ever we were still like

0:26:54.350,0:26:58.450
stacking several layers we were

0:26:56.270,0:27:02.920
observing that the gradients get lost as

0:26:58.450,0:27:05.750
depth right so how do we solve gradient

0:27:02.920,0:27:08.770
getting lost through the depth in a

0:27:05.750,0:27:08.770
current days

0:27:09.310,0:27:13.670
skipping constant connection right the

0:27:11.270,0:27:15.530
receiver connections we use and

0:27:13.670,0:27:18.410
similarly here we can use skip

0:27:15.530,0:27:21.860
connections as well when we go down well

0:27:18.410,0:27:22.330
up in in time okay so let's see how this

0:27:21.860,0:27:30.500
works

0:27:22.330,0:27:32.480
yeah so the problem is that the

0:27:30.500,0:27:34.250
gradients are only going in the backward

0:27:32.480,0:27:35.500
paths right back

0:27:34.250,0:27:38.990
[Music]

0:27:35.500,0:27:41.120
well the gradient has to go the same way

0:27:38.990,0:27:42.680
it went forward by the opposite

0:27:41.120,0:27:45.050
direction right I mean you're computing

0:27:42.680,0:27:46.970
chain rule so if you have a function of

0:27:45.050,0:27:49.310
a function of a function then you just

0:27:46.970,0:27:52.220
use those functions to go back right the

0:27:49.310,0:27:53.810
point is that whenever you have these

0:27:52.220,0:27:55.790
gradients coming back they will not have

0:27:53.810,0:27:58.190
to go through matrices therefore also

0:27:55.790,0:28:01.250
the forward part has not doesn't have to

0:27:58.190,0:28:03.650
go through the matrices meaning that the

0:28:01.250,0:28:07.310
memory cannot go through matrix

0:28:03.650,0:28:08.900
multiplication if you don't want to have

0:28:07.310,0:28:11.770
this effect when you perform back

0:28:08.900,0:28:11.770
propagation okay

0:28:14.050,0:28:19.420
yeah it's gonna be worth much better

0:28:16.100,0:28:22.539
working I show you in the next slide

0:28:19.420,0:28:22.539
[Music]

0:28:25.960,0:28:30.560
show you next slide

0:28:27.740,0:28:32.270
so how do we fix this problem well

0:28:30.560,0:28:34.670
instead of using one recurrent neural

0:28:32.270,0:28:36.650
network we're gonna using for recurrent

0:28:34.670,0:28:39.650
neural network okay so the first

0:28:36.650,0:28:41.510
recurrent Network alcohol on the first

0:28:39.650,0:28:43.580
network is gonna be the one that goes

0:28:41.510,0:28:46.370
from the input to this intermediate

0:28:43.580,0:28:48.830
state then I have other three networks

0:28:46.370,0:28:51.410
and each of those are represented by

0:28:48.830,0:28:53.390
these three symbols 1 2 & 3

0:28:51.410,0:28:56.870
okay think about this as our open mouth

0:28:53.390,0:29:02.120
and it's like a closed mouth okay like

0:28:56.870,0:29:04.580
the emoji okay so if you use this kind

0:29:02.120,0:29:07.520
of for net for recurrent neural network

0:29:04.580,0:29:09.740
be regular Network you gotta have for

0:29:07.520,0:29:11.600
example from the input I send things

0:29:09.740,0:29:14.390
through in the open mouth therefore it

0:29:11.600,0:29:16.940
gets here I have a closed mouth here so

0:29:14.390,0:29:18.920
nothing goes forward then I'm gonna have

0:29:16.940,0:29:21.500
this open mouth here such that the

0:29:18.920,0:29:23.600
history goes forward so the history gets

0:29:21.500,0:29:26.360
sent forward without going through a

0:29:23.600,0:29:29.120
neural network matrix multiplication

0:29:26.360,0:29:31.400
it just gets through our open mouth and

0:29:29.120,0:29:34.670
all the other inputs find a closed mouth

0:29:31.400,0:29:38.480
so the hidden state will not change upon

0:29:34.670,0:29:40.820
new inputs okay and then here you're

0:29:38.480,0:29:43.010
gonna have a open mouth here such that

0:29:40.820,0:29:44.960
you can get the final output here then

0:29:43.010,0:29:46.550
the open mouth keeps going here such

0:29:44.960,0:29:48.560
that you have another output there and

0:29:46.550,0:29:52.670
then finally you get the last closed

0:29:48.560,0:29:54.620
mouth at the last one now if you perform

0:29:52.670,0:29:57.560
back prop you will have the gradients

0:29:54.620,0:29:58.880
flowing through the open mouth and you

0:29:57.560,0:30:01.250
don't get any kind of matrix

0:29:58.880,0:30:04.400
multiplication so now let's figure out

0:30:01.250,0:30:07.880
how these open mouths are represented

0:30:04.400,0:30:10.010
how are they instantiated in like in in

0:30:07.880,0:30:11.540
terms of mathematics is it clear design

0:30:10.010,0:30:13.130
right so now we are using open and

0:30:11.540,0:30:15.950
closed mouths and each of those mouths

0:30:13.130,0:30:17.880
is plus the the first guy here that

0:30:15.950,0:30:20.630
connects the input to the hidden are

0:30:17.880,0:30:25.580
brn ends so these on here that is a

0:30:20.630,0:30:28.560
gated recurrent network it's simply for

0:30:25.580,0:30:32.060
normal recurrent neural network combined

0:30:28.560,0:30:34.290
in a clever way such that you have

0:30:32.060,0:30:37.920
multiplicative interaction and not

0:30:34.290,0:30:39.480
matrix interaction is it clear so far

0:30:37.920,0:30:42.000
this is like intuition I haven't shown

0:30:39.480,0:30:45.200
you how all right so let's figure out

0:30:42.000,0:30:48.570
who made this and how it works okay so

0:30:45.200,0:30:50.820
we're gonna see now those long short

0:30:48.570,0:30:55.530
term memory or gated recurrent neural

0:30:50.820,0:30:57.270
networks so I'm sorry okay that was the

0:30:55.530,0:30:59.730
dude okay this is the guy who actually

0:30:57.270,0:31:03.780
invented this stuff actually him and his

0:30:59.730,0:31:07.620
students back some in 1997 and we were

0:31:03.780,0:31:10.650
drinking here together okay all right so

0:31:07.620,0:31:14.010
that is the question of a recurrent

0:31:10.650,0:31:15.690
neural network and on the top left are

0:31:14.010,0:31:18.000
you gonna see in the diagram so I just

0:31:15.690,0:31:21.330
make a very compact version of this

0:31:18.000,0:31:23.310
recurrent neural network here is going

0:31:21.330,0:31:25.110
to be the collection of equations that

0:31:23.310,0:31:27.840
are expressed in a long short term

0:31:25.110,0:31:31.170
memory they look a little bit dense so I

0:31:27.840,0:31:32.970
just draw it for you here okay let's

0:31:31.170,0:31:33.420
actually goes through how this stuff

0:31:32.970,0:31:36.320
works

0:31:33.420,0:31:38.640
so I'm gonna be drawing an interactive

0:31:36.320,0:31:40.500
animation here so you have your input

0:31:38.640,0:31:41.730
gate here which is going to be an affine

0:31:40.500,0:31:43.380
transformation so all of these are

0:31:41.730,0:31:45.990
recurrent Network write the same

0:31:43.380,0:31:49.920
equation I show you here so this input

0:31:45.990,0:31:53.190
transformation will be multiplying my C

0:31:49.920,0:31:55.440
tilde which is my candidate gate here I

0:31:53.190,0:31:59.820
have a don't forget gate which is

0:31:55.440,0:32:01.920
multiplying my previous value of my cell

0:31:59.820,0:32:05.520
memory and then my Poppa stylist maybe

0:32:01.920,0:32:08.100
don't forget previous plus input ii i'm

0:32:05.520,0:32:10.560
gonna show you now how it works then i

0:32:08.100,0:32:12.600
have my final hidden representations to

0:32:10.560,0:32:14.820
be multiplication element wise

0:32:12.600,0:32:17.850
multiplication between my output gate

0:32:14.820,0:32:20.610
and my you know whatever hyperbolic

0:32:17.850,0:32:22.740
tangent version of the cell such that

0:32:20.610,0:32:25.350
things are bounded and then I have

0:32:22.740,0:32:26.880
finally my C tilde which is my candidate

0:32:25.350,0:32:29.160
gate is simply

0:32:26.880,0:32:31.110
Anette right so you have one recurrent

0:32:29.160,0:32:33.810
network one that modulates the output

0:32:31.110,0:32:35.730
one that modulates this is don't forget

0:32:33.810,0:32:37.830
gate and this is the input gate

0:32:35.730,0:32:40.050
so all this interaction between the

0:32:37.830,0:32:42.690
memory and the gates is a multiplicative

0:32:40.050,0:32:44.490
interaction and this forget input and

0:32:42.690,0:32:46.470
don't forget the input and output are

0:32:44.490,0:32:48.780
all sigmoids and therefore they are

0:32:46.470,0:32:50.700
going from 0 to 1 so I can multiply by a

0:32:48.780,0:32:53.340
0 you have a closed mouth or you can

0:32:50.700,0:32:56.040
multiply by 1 if it's open mouth right

0:32:53.340,0:33:00.120
if you think about being having our

0:32:56.040,0:33:03.780
internal linear volume which is below

0:33:00.120,0:33:06.120
minus 5 or above plus 5 okay such that

0:33:03.780,0:33:09.030
you using the you use the gate in the

0:33:06.120,0:33:11.940
saturated area or 0 or 1 right you know

0:33:09.030,0:33:13.490
the sigmoid so let's see how this stuff

0:33:11.940,0:33:16.260
works

0:33:13.490,0:33:17.400
this is the output let's turn off the

0:33:16.260,0:33:20.450
output how do I do

0:33:17.400,0:33:23.220
turn off the output I simply put a 0

0:33:20.450,0:33:26.310
inside so let's say I have a purple

0:33:23.220,0:33:28.410
internal representation see I put a 0

0:33:26.310,0:33:29.730
there in the output gate the output is

0:33:28.410,0:33:33.690
going to be multiplying a 0 with

0:33:29.730,0:33:36.300
something you get 0 okay then let's say

0:33:33.690,0:33:39.090
I have a green one I have one then I

0:33:36.300,0:33:40.830
multiply one with the purple I get

0:33:39.090,0:33:44.010
purple and then finally I get the same

0:33:40.830,0:33:46.170
value similarly I can control the memory

0:33:44.010,0:33:48.150
and I can for example we set it in this

0:33:46.170,0:33:51.240
case I'm gonna be I have my internal

0:33:48.150,0:33:53.730
memory see this is purple and then I

0:33:51.240,0:33:57.450
have here my previous guy which is gonna

0:33:53.730,0:33:59.010
be blue I guess I have a zero here and

0:33:57.450,0:34:01.500
therefore the multiplication gives me a

0:33:59.010,0:34:03.000
zero there I have here a zero so

0:34:01.500,0:34:05.190
multiplication is gonna be giving a zero

0:34:03.000,0:34:07.020
at some two zeros and I get a zero

0:34:05.190,0:34:09.690
inside of memory so I just erase the

0:34:07.020,0:34:12.750
memory and you get the zero there

0:34:09.690,0:34:15.210
otherwise I can keep the memory I still

0:34:12.750,0:34:17.550
do the internal thing I did a new one

0:34:15.210,0:34:19.919
but I keep a wonder such that the

0:34:17.550,0:34:21.929
multiplication gets blue the Sun gets

0:34:19.919,0:34:25.649
blue and then I keep sending out my

0:34:21.929,0:34:28.950
bloom finally I can write such that I

0:34:25.649,0:34:31.110
can get a 1 in the input gate the

0:34:28.950,0:34:34.350
multiplication gets purple then the I

0:34:31.110,0:34:35.010
set a zero in the don't forget such that

0:34:34.350,0:34:38.550
the

0:34:35.010,0:34:40.679
we forget and then multiplication gives

0:34:38.550,0:34:42.960
me zero I some do I get purple and then

0:34:40.679,0:34:45.780
I get the final purple output okay so

0:34:42.960,0:34:48.240
here we control how to send how to write

0:34:45.780,0:34:50.850
in memory how to reset the memory and

0:34:48.240,0:34:52.440
how to output something okay so we have

0:34:50.850,0:35:04.770
all different operation this looks like

0:34:52.440,0:35:06.510
a computer - and in an yeah it is

0:35:04.770,0:35:08.700
assumed in this case to show you like

0:35:06.510,0:35:11.130
how the logic works as we are like

0:35:08.700,0:35:14.250
having a value inside the sigmoid has

0:35:11.130,0:35:15.900
been or below minus 5 or being above

0:35:14.250,0:35:27.780
plus 5 such that we are working as a

0:35:15.900,0:35:30.780
switch 0 1 switch okay the network can

0:35:27.780,0:35:32.790
choose to use this kind of operation to

0:35:30.780,0:35:34.830
me make sense I believe this is the

0:35:32.790,0:35:37.110
rationale behind how this network has

0:35:34.830,0:35:40.800
been put together the network can decide

0:35:37.110,0:35:42.690
to do anything it wants usually they do

0:35:40.800,0:35:44.850
whatever they want but this seems like

0:35:42.690,0:35:46.800
they can work at least if they've had to

0:35:44.850,0:35:49.140
saturate the gates it looks like things

0:35:46.800,0:35:51.930
can work pretty well so in the remaining

0:35:49.140,0:35:55.350
15 minutes of kind of I'm gonna be

0:35:51.930,0:35:56.880
showing you two notebooks I kind of went

0:35:55.350,0:35:58.770
a little bit faster because again there

0:35:56.880,0:36:04.220
is much more to be seen here in the

0:35:58.770,0:36:04.220
notebooks so yeah

0:36:10.140,0:36:17.440
so this the the actual weight the actual

0:36:14.880,0:36:19.359
gradient you care here is gonna be the

0:36:17.440,0:36:21.970
gradient with respect to previous C's

0:36:19.359,0:36:23.619
right the thing you care is gonna be

0:36:21.970,0:36:25.000
basically the partial derivative of the

0:36:23.619,0:36:27.880
current seen with respect to previous

0:36:25.000,0:36:30.160
C's such that you if you have the

0:36:27.880,0:36:33.040
original initial C here and you have

0:36:30.160,0:36:35.140
multiple C over time you want to change

0:36:33.040,0:36:36.760
something in the original C you still

0:36:35.140,0:36:39.130
have the gradient coming down all the

0:36:36.760,0:36:43.740
way until the first C which comes down

0:36:39.130,0:36:43.740
to getting gradients through that matrix

0:36:44.160,0:36:48.430
WC here right so if you want to change

0:36:46.660,0:36:52.089
those weights here you just go through

0:36:48.430,0:36:55.300
the chain of multiplications that are

0:36:52.089,0:36:56.890
not involving any matrix multiplication

0:36:55.300,0:36:58.690
as such that you when you get the

0:36:56.890,0:37:00.490
gradient it still gets multiplied by one

0:36:58.690,0:37:03.460
all the time and it gets down to

0:37:00.490,0:37:05.760
whatever we want to do okay did I answer

0:37:03.460,0:37:05.760
your question

0:37:09.150,0:37:16.660
so the matrices will change the

0:37:13.150,0:37:19.210
amplitude of your gradient right so if

0:37:16.660,0:37:22.000
you have like these largest eigenvalue

0:37:19.210,0:37:23.490
being you know 0.0001 every time you

0:37:22.000,0:37:26.079
multiply you get the norm of this vector

0:37:23.490,0:37:29.319
getting killed right so you have like an

0:37:26.079,0:37:31.569
exponential decay in this case if my

0:37:29.319,0:37:35.400
forget gate is actually always equal to

0:37:31.569,0:37:37.510
1 then you get C equal C minus team

0:37:35.400,0:37:41.859
derivative what is the partial

0:37:37.510,0:37:43.299
derivative of C T with respect to what

0:37:41.859,0:37:46.900
is the partial derivative of C T with

0:37:43.299,0:37:48.579
respect to C minus t minus 1 1 right so

0:37:46.900,0:37:50.020
the parts of the relative that is the

0:37:48.579,0:37:52.390
thing that you actually multiply every

0:37:50.020,0:37:54.609
time there's gonna be 1 so output

0:37:52.390,0:37:57.609
gradient output gradients can be input

0:37:54.609,0:37:58.809
gradients right yeah i'll pavillions

0:37:57.609,0:38:01.510
gonna be implicit because it would apply

0:37:58.809,0:38:03.109
the output gradient by the derivative of

0:38:01.510,0:38:05.599
this module right if the

0:38:03.109,0:38:08.749
this module is e1 then the thing that is

0:38:05.599,0:38:14.660
here keeps going that is the rationale

0:38:08.749,0:38:20.599
behind this now this is just for drawing

0:38:14.660,0:38:24.710
purposes I assumed it's like a switch

0:38:20.599,0:38:27.019
okay such that I can make things you

0:38:24.710,0:38:29.089
know you have a switch on and off to

0:38:27.019,0:38:30.380
show like how it should be working maybe

0:38:29.089,0:38:46.579
doesn't work like that but still it

0:38:30.380,0:38:48.049
works it can work this way right yeah so

0:38:46.579,0:38:50.089
that's the implementation of pro

0:38:48.049,0:38:52.400
question is gonna be simply you just pad

0:38:50.089,0:38:55.069
all the other sync when sees with zeros

0:38:52.400,0:38:57.980
before the sequence so if you have

0:38:55.069,0:38:59.920
several several sequences yes several

0:38:57.980,0:39:03.049
sequences that are of a different length

0:38:59.920,0:39:03.619
you just put them all aligned to the

0:39:03.049,0:39:07.099
right

0:39:03.619,0:39:08.960
and then you put some zeros here okay

0:39:07.099,0:39:12.049
such that you always have in the last

0:39:08.960,0:39:14.599
column the latest element if you put two

0:39:12.049,0:39:16.039
zeros here it's gonna be a mess in right

0:39:14.599,0:39:17.299
in the code if you put the zeros in the

0:39:16.039,0:39:19.279
in the beginning you just stop doing

0:39:17.299,0:39:21.319
back propagation when you hit the last

0:39:19.279,0:39:23.480
symbol right so you start from here you

0:39:21.319,0:39:25.460
go back here so you go forward then you

0:39:23.480,0:39:27.289
go back prop and stop whenever you

0:39:25.460,0:39:29.599
actually reach the end of your sequence

0:39:27.289,0:39:33.289
if you pad on the other side you get a

0:39:29.599,0:39:34.730
bunch of drop there in the next ten

0:39:33.289,0:39:37.369
minutes so you're gonna be seen two

0:39:34.730,0:39:45.049
notebooks if you don't have other

0:39:37.369,0:39:47.509
questions okay wow you're so quiet okay

0:39:45.049,0:39:49.970
so we're gonna be going now for sequence

0:39:47.509,0:39:52.730
classification alright so in this case

0:39:49.970,0:39:54.589
I'm gonna be I just really stuff loud

0:39:52.730,0:39:57.499
out loud the goal is to classify a

0:39:54.589,0:40:00.259
sequence of elements sequence elements

0:39:57.499,0:40:03.710
and targets are represented locally

0:40:00.259,0:40:05.660
input vectors with only one nonzero bit

0:40:03.710,0:40:07.530
so it's a one hot encoding the sequence

0:40:05.660,0:40:10.770
starts with a B for beginning and end

0:40:07.530,0:40:13.380
efore end and otherwise consists of a

0:40:10.770,0:40:16.370
randomly chosen symbols from a set a b c

0:40:13.380,0:40:19.350
and d which are some kind of noise

0:40:16.370,0:40:22.380
expect for two elements in position t1

0:40:19.350,0:40:27.120
and t2 this position can be either or X

0:40:22.380,0:40:29.460
or Y in for the hard difficulty level

0:40:27.120,0:40:31.770
you have for example that the sequence

0:40:29.460,0:40:35.220
length length is chose randomly between

0:40:31.770,0:40:37.680
100 and 110 10 t1 is randomly chosen

0:40:35.220,0:40:40.530
between 10 and 20 Tinto is randomly

0:40:37.680,0:40:44.640
chosen between 50 and 60 there are four

0:40:40.530,0:40:47.010
sequences classes QRS nu which depends

0:40:44.640,0:40:50.880
on the temporal order of x and y so if

0:40:47.010,0:40:53.520
you have X X you can be getting a Q x

0:40:50.880,0:40:55.350
and y you get an R Y and X you get an S

0:40:53.520,0:40:57.750
and YY with you so we're going to be

0:40:55.350,0:41:01.590
doing a sequence classification based on

0:40:57.750,0:41:03.720
the X and y or whatever those to import

0:41:01.590,0:41:05.010
to these kind of triggers okay

0:41:03.720,0:41:08.370
and in the middle in the middle you can

0:41:05.010,0:41:10.650
have ABCD in random positions like you

0:41:08.370,0:41:12.810
know randomly generated is it clear so

0:41:10.650,0:41:15.240
far so we do cast a classification of

0:41:12.810,0:41:23.180
sequences where you may have these x and

0:41:15.240,0:41:23.180
y's or X X or Y or Y X so in this case

0:41:23.210,0:41:29.460
I'm showing you first the first input so

0:41:27.180,0:41:32.670
the return type is a tuple of sequence

0:41:29.460,0:41:36.780
of two which is going to be what is the

0:41:32.670,0:41:38.670
output of this example generator and so

0:41:36.780,0:41:43.050
let's see what is what is this thing

0:41:38.670,0:41:45.750
here so this is my data I'm going to be

0:41:43.050,0:41:48.030
feeding to the network so I have one two

0:41:45.750,0:41:51.270
three four five six seven eight there

0:41:48.030,0:41:54.180
are eight different symbols here in a

0:41:51.270,0:41:59.850
row every time why there are eight we

0:41:54.180,0:42:02.970
have x and y a b c and d beginning and

0:41:59.850,0:42:05.520
end right so we have one hot out of you

0:42:02.970,0:42:08.400
know eight characters and then i have a

0:42:05.520,0:42:11.520
sequence of rows which are my sequence

0:42:08.400,0:42:12.980
of symbols okay in this case you can see

0:42:11.520,0:42:17.700
here i have a beginning with all zeros

0:42:12.980,0:42:19.260
why is all zeros padding right so in

0:42:17.700,0:42:21.090
this case the sequence was shorter than

0:42:19.260,0:42:21.329
the expect the maximum sequence in the

0:42:21.090,0:42:24.839
bed

0:42:21.329,0:42:29.279
and then the first first sequence has an

0:42:24.839,0:42:32.640
extra zero item at the beginning in them

0:42:29.279,0:42:34.859
you're gonna have like in this case the

0:42:32.640,0:42:38.099
second item is of the two a pole to pole

0:42:34.859,0:42:41.160
is the corresponding best class for

0:42:38.099,0:42:43.829
example I have a batch size of 32 and

0:42:41.160,0:42:51.930
then I'm gonna have an output size of 4

0:42:43.829,0:42:55.619
y 4 Q R whatever right the QRS you okay

0:42:51.930,0:42:57.450
so I have 4 a 4 dimensional target

0:42:55.619,0:43:02.160
vector and I have a sequence of 8

0:42:57.450,0:43:04.499
dimensional vectors as input okay so

0:43:02.160,0:43:06.049
let's see how this sequence looks like

0:43:04.499,0:43:12.779
in this case is gonna be the beginning

0:43:06.049,0:43:15.059
BX CX CBE so X X let's see X X X X is Q

0:43:12.779,0:43:18.569
right so we have our Q sequence and

0:43:15.059,0:43:22.079
that's why the final target is a Q the 1

0:43:18.569,0:43:25.019
0 0 0 and then you're gonna see B B X C

0:43:22.079,0:43:27.660
so the second item and the second last

0:43:25.019,0:43:30.390
is gonna be B lowercase B you can see

0:43:27.660,0:43:33.239
here the second item and the second last

0:43:30.390,0:43:36.390
item is going to be a be okay all right

0:43:33.239,0:43:39.180
so let's now create a recurrent Network

0:43:36.390,0:43:41.249
in a very quick way so here I can simply

0:43:39.180,0:43:44.910
say my recurrent network is going to be

0:43:41.249,0:43:47.369
torch and an RNN and I'm gonna be using

0:43:44.910,0:43:51.180
a reader network really non-linearity

0:43:47.369,0:43:52.709
and then I have my final linear layer in

0:43:51.180,0:43:55.349
the other case I'm gonna be using a led

0:43:52.709,0:43:57.119
STM and then I'm gonna have a final

0:43:55.349,0:44:01.170
inner layer so I just execute these guys

0:43:57.119,0:44:07.920
I have my training loop and I'm gonna be

0:44:01.170,0:44:09.869
training for 10 books so in the training

0:44:07.920,0:44:13.259
group you can be always looking for

0:44:09.869,0:44:16.769
those five different steps first step is

0:44:13.259,0:44:18.900
gonna be get the data inside the model

0:44:16.769,0:44:25.019
right so that's step number one what is

0:44:18.900,0:44:30.669
step number two there are five steps we

0:44:25.019,0:44:33.319
remember hello

0:44:30.669,0:44:35.089
you feel that you feed the network if

0:44:33.319,0:44:38.779
you feed the network with some data then

0:44:35.089,0:44:41.539
what do you do you compute the loss okay

0:44:38.779,0:44:47.539
then we have compute step to compute the

0:44:41.539,0:44:52.549
loss fantastic number three is zero the

0:44:47.539,0:45:05.809
cash right then number four which is

0:44:52.549,0:45:09.699
computing the off yes lost dog backwards

0:45:05.809,0:45:13.099
lost not backward don't compute the

0:45:09.699,0:45:16.449
partial derivative of the loss with

0:45:13.099,0:45:20.959
respect to the network's parameters yeah

0:45:16.449,0:45:27.380
here backward finally number five which

0:45:20.959,0:45:29.809
is step in opposite direction of the

0:45:27.380,0:45:31.819
gradient okay all right those are the

0:45:29.809,0:45:33.619
five steps you always want to see in any

0:45:31.819,0:45:37.909
training blueprint if someone is missing

0:45:33.619,0:45:40.459
then you're [ __ ] up okay so we try now

0:45:37.909,0:45:42.469
the RNN and the lsdm and you get

0:45:40.459,0:45:46.699
something looks like this

0:45:42.469,0:45:55.929
so our NN goes up to 50% in accuracy and

0:45:46.699,0:45:55.929
then the lsdm got 100% okay oh okay

0:45:56.439,0:46:06.019
first of all how many weights does this

0:45:59.689,0:46:07.909
lsdm have compared to the RN n four

0:46:06.019,0:46:11.059
times more weights right so it's not a

0:46:07.909,0:46:13.849
fair comparison I would say because lsdm

0:46:11.059,0:46:16.819
is simply for rnns combined somehow

0:46:13.849,0:46:18.589
right so this is a two layer neural

0:46:16.819,0:46:20.659
network whereas the other one is at one

0:46:18.589,0:46:22.549
layer right always both ever like it has

0:46:20.659,0:46:25.009
one hidden layer they are an end if

0:46:22.549,0:46:29.949
Alice TM we can think about having two

0:46:25.009,0:46:33.199
hidden so again one layer two layers

0:46:29.949,0:46:35.299
well one hidden to lead in one set of

0:46:33.199,0:46:37.610
parameters four sets of the same numbers

0:46:35.299,0:46:40.730
like okay not fair okay anyway

0:46:37.610,0:46:43.610
let's go with hundred iterations okay so

0:46:40.730,0:46:47.420
now I just go with 100 iterations and I

0:46:43.610,0:46:49.490
show you how if they work or not and

0:46:47.420,0:46:53.860
also when I be just clicking things such

0:46:49.490,0:46:56.000
that we have time to go through stuff

0:46:53.860,0:47:00.860
okay now my computer's going to be

0:46:56.000,0:47:02.990
complaining all right so again what are

0:47:00.860,0:47:04.220
the five types of operations like five

0:47:02.990,0:47:06.860
okay now is already done

0:47:04.220,0:47:11.090
sorry I was going to do okay so this is

0:47:06.860,0:47:16.280
the RNN right RNN and finally actually

0:47:11.090,0:47:18.080
gave to 100% okay so iron and it just

0:47:16.280,0:47:20.030
let it more time like a little bit more

0:47:18.080,0:47:22.790
longer training actually works the other

0:47:20.030,0:47:26.060
one okay and here you can see that we

0:47:22.790,0:47:28.070
got 100% in twenty eight bucks okay the

0:47:26.060,0:47:30.650
other case we got 2,100 percent in

0:47:28.070,0:47:32.750
roughly twice as long

0:47:30.650,0:47:35.690
twice longer at a time okay so let's

0:47:32.750,0:47:39.800
first see how they perform here so I

0:47:35.690,0:47:42.200
have this sequence BC y dy Dae which is

0:47:39.800,0:47:43.820
au sequence and then we ask the network

0:47:42.200,0:47:46.760
and he actually meant for actually like

0:47:43.820,0:47:49.190
labels it as you okay so below we're

0:47:46.760,0:47:51.140
gonna be seeing something very cute so

0:47:49.190,0:47:54.110
in this case we were using sequences

0:47:51.140,0:47:56.870
that are very very very very small right

0:47:54.110,0:47:59.930
so even the RNN is able to train on

0:47:56.870,0:48:02.390
these small sequences so what is the

0:47:59.930,0:48:06.230
point of using a lsdm well we can first

0:48:02.390,0:48:07.430
of all increase the difficulty of the

0:48:06.230,0:48:09.350
training part and we're gonna see that

0:48:07.430,0:48:13.280
the RN ends can be miserably failing

0:48:09.350,0:48:16.700
whereas the lsdm keeps working in this

0:48:13.280,0:48:19.790
visualization part below okay I train a

0:48:16.700,0:48:22.820
network now Alice and lsdm now with the

0:48:19.790,0:48:26.000
moderate level which has eighty symbols

0:48:22.820,0:48:28.490
rather than eight or ten ten symbols so

0:48:26.000,0:48:31.430
you can see here how this model actually

0:48:28.490,0:48:35.480
managed to succeed at the end although

0:48:31.430,0:48:38.870
there is like a very big spike and I'm

0:48:35.480,0:48:42.020
gonna be now drawing the value of the

0:48:38.870,0:48:43.970
cell state over time okay so I'm going

0:48:42.020,0:48:46.580
to be input in our sequence of eighty

0:48:43.970,0:48:49.090
symbols and I'm gonna be showing you

0:48:46.580,0:48:50.910
what is the value of the hidden state

0:48:49.090,0:48:53.330
hidden State

0:48:50.910,0:48:54.829
so in this case I'm gonna be showing you

0:48:53.330,0:48:56.910
[Music]

0:48:54.829,0:48:58.259
hidden hold on

0:48:56.910,0:49:01.140
yeah I'm gonna be showing I'm gonna send

0:48:58.259,0:49:04.079
my input through a hyperbolic tangent

0:49:01.140,0:49:06.029
such that if you're below minus 2.5 I'm

0:49:04.079,0:49:09.809
gonna be mapping to minus 1 if you're

0:49:06.029,0:49:12.329
above 2.5 you get mapped to plus 1 more

0:49:09.809,0:49:14.579
or less and so let's see how this stuff

0:49:12.329,0:49:18.029
looks so in this case here you can see

0:49:14.579,0:49:25.289
that this specific hidden layer picked

0:49:18.029,0:49:27.720
on the X here and then it became red

0:49:25.289,0:49:30.180
until you got the other X right so this

0:49:27.720,0:49:33.710
is visualizing the internal state of the

0:49:30.180,0:49:36.980
LSD and so you can see that in specific

0:49:33.710,0:49:39.599
unit because in this case I use hidden

0:49:36.980,0:49:44.400
representation like hidden dimension of

0:49:39.599,0:49:47.700
10 and so in this case the 1 2 3 4 5 the

0:49:44.400,0:49:50.250
fifth hidden unit of the cell lay the

0:49:47.700,0:49:52.829
5th cell actually is trigger by

0:49:50.250,0:49:55.319
observing the first X and then it goes

0:49:52.829,0:49:58.410
quiet after seen the other acts this

0:49:55.319,0:50:01.440
allows me to basically you know take

0:49:58.410,0:50:07.440
care of I mean recognize if the sequence

0:50:01.440,0:50:12.150
is ru OPRS okay does it make sense okay

0:50:07.440,0:50:14.519
oh this one more notebook I'm gonna be

0:50:12.150,0:50:19.200
showing just quickly which is the Eco

0:50:14.519,0:50:22.410
data in this case I'm gonna be in South

0:50:19.200,0:50:24.839
corner I'm gonna have a network echo in

0:50:22.410,0:50:27.059
whatever I'm saying so if I say

0:50:24.839,0:50:28.769
something I asked a network to say if I

0:50:27.059,0:50:30.960
say something I asked my neighbor to say

0:50:28.769,0:50:34.710
if I say something I ask ok Anderson

0:50:30.960,0:50:42.150
right ok so in this case here and I'll

0:50:34.710,0:50:46.829
be inputting this is the first sequence

0:50:42.150,0:50:50.579
is going to be 0 1 1 1 1 0 and you'll

0:50:46.829,0:50:55.170
have the same one here 0 1 1 1 1 0 and I

0:50:50.579,0:50:57.259
have 1 0 1 1 0 1 etc right so in this

0:50:55.170,0:50:58.890
case if you want to output something

0:50:57.259,0:51:00.900
after some

0:50:58.890,0:51:03.589
right this in this case is three time

0:51:00.900,0:51:06.809
step after you have to have some kind of

0:51:03.589,0:51:08.849
short-term memory where you keep in mind

0:51:06.809,0:51:11.780
what I just said where you keep in mind

0:51:08.849,0:51:13.220
what I just said where you keep in mind

0:51:11.780,0:51:16.890
[Music]

0:51:13.220,0:51:19.140
what I just said yeah that's correct so

0:51:16.890,0:51:22.099
you know pirating actually requires

0:51:19.140,0:51:24.269
having some kind of working memory

0:51:22.099,0:51:27.569
whereas the other one the language model

0:51:24.269,0:51:31.829
which it was prompted prompted to say

0:51:27.569,0:51:33.539
something that I haven't already said

0:51:31.829,0:51:35.190
right so that was a different kind of

0:51:33.539,0:51:38.700
task you actually had to predict what is

0:51:35.190,0:51:40.890
the most likely next word in keynote you

0:51:38.700,0:51:42.329
cannot be always right right but this

0:51:40.890,0:51:46.440
one you can always be right you know

0:51:42.329,0:51:49.079
this is there is no random stuff anyhow

0:51:46.440,0:51:51.000
so I have my first batch here and then

0:51:49.079,0:51:53.549
the sec the white patch which is the

0:51:51.000,0:51:57.269
same similar thing which is shifted over

0:51:53.549,0:52:01.319
time and then we have we have to chunk

0:51:57.269,0:52:03.059
this long long long sequence so before I

0:52:01.319,0:52:05.250
was sending a whole sequence inside the

0:52:03.059,0:52:07.230
network and I was enforcing the final

0:52:05.250,0:52:09.569
target to be something right in this

0:52:07.230,0:52:11.309
case I had to chunk if the sequence goes

0:52:09.569,0:52:13.319
this direction I had to chunk my long

0:52:11.309,0:52:15.990
sequence in little chunks and then you

0:52:13.319,0:52:18.869
have to fill the first chunk keep trace

0:52:15.990,0:52:21.480
of whatever is the hidden state send a

0:52:18.869,0:52:23.549
new chunk where you feed and initially

0:52:21.480,0:52:25.109
as the initial hidden state the output

0:52:23.549,0:52:28.319
of this chant right so you feed this

0:52:25.109,0:52:31.920
chunk you have a final hidden state then

0:52:28.319,0:52:33.960
you feed this chunk and as you put you

0:52:31.920,0:52:36.180
have to put these two as input to the

0:52:33.960,0:52:38.430
internal memory right now you feed the

0:52:36.180,0:52:42.269
next chunk where you put this one as

0:52:38.430,0:52:44.670
input as to the internal state and you

0:52:42.269,0:52:51.450
we are going to be comparing here RNN

0:52:44.670,0:52:57.059
with analyst TMS I think so at the end

0:52:51.450,0:53:00.210
here you can see that okay we managed to

0:52:57.059,0:53:02.789
actually get we are an n/a accuracy that

0:53:00.210,0:53:05.190
goes 100 100 percent then if you are

0:53:02.789,0:53:08.220
starting now to mess with the size of

0:53:05.190,0:53:09.079
the memory chunk with a memory interval

0:53:08.220,0:53:11.619
you can be seen

0:53:09.079,0:53:13.880
with the lsdm you can keep this memory

0:53:11.619,0:53:16.399
for a long time as long as you have

0:53:13.880,0:53:18.380
enough capacity the RNN after you reach

0:53:16.399,0:53:22.880
some kind of length you start forgetting

0:53:18.380,0:53:26.599
what happened in the past and it was

0:53:22.880,0:53:29.809
pretty much everything for today so stay

0:53:26.599,0:53:30.980
warm wash your hands and I'll see you

0:53:29.809,0:53:34.929
next week bye bye

0:53:30.980,0:53:34.929
[Music]


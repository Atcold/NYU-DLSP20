---
lang-ref: ch.12-3
title: The Transformer
authors: Francesca Guiso, Annika Brundyn, Noah Kasmanoff, and Luke Martin
date: 26 April 2020
---


# Attention
We introduce the concept of attention before talking about the Transformer architecture. There are two main types of attention: self attention vs. cross attention, within those categories, we can have "hard" vs. soft attention.

As we will later see, transformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs.


### Self Attention (I)

Consider a set of $t$ input $\boldsymbol{x}$'s:

$\{\boldsymbol{x}_i\}_{i=1}^t = \{\boldsymbol{x}_1,...,\boldsymbol{x}_t\}$

where each $\boldsymbol{x}_i$ is an $n$-dimensional vector. Since the set has $t$ elements, each of which belongs to $\mathbb{R}^n$, we can represent the set as a matrix $\boldsymbol{X}\in \mathbb{R}^{n \times t}$.

With self-attention, the hidden representation $h$ is a linear combination of the inputs:

$\boldsymbol{h} = \boldsymbol{\alpha}_1 \boldsymbol{x}_1 + \boldsymbol{\alpha}_2 \boldsymbol{x}_2 + ... +  \boldsymbol{\alpha}_t \boldsymbol{x}_t + \boldsymbol{\alpha}_t \boldsymbol{x}_t$

Using the matrix representation described above, we can write the hidden layer as the matrix product:

$\boldsymbol{h} = \boldsymbol{X} \boldsymbol{\alpha}$

where $\boldsymbol{\alpha} \in \mathbb{R}^n$ is a column vector of $\boldsymbol{\alpha}_i$'s.

Note that this differs from the hidden representation we have seen so far, where the inputs are multiplied by a matrix of weights.

Depending on the constraints we impose on the $\alpha$ vector, we can achieve hard or soft attention.


#### Hard Attention

With hard-attention, we impose the following constraint on the alphas: $||\boldsymbol{\alpha}||_0 = 1$. This means $\alpha$ is a one-hot vector. Therefore, all but one of the coefficients in the linear combination of the inputs equals zero, and the hidden representation reduces to the input $\boldsymbol{x}_i$ corresponding to the element $\alpha_i=1$.


#### Soft Attention

With soft attention, we impose that $||\boldsymbol{\alpha}||_1 = 1$. The hidden representations is a linear combination of the inputs where the coefficients sum up to 1.


### Self Attention (II)

Where do the $\alpha$'s come from?

We obtain the vector $\boldsymbol{\alpha} \in \mathbb{R}^t$ in the following way:

$$\boldsymbol{\alpha} = \[\text{soft}\](\arg)\max_{\beta} (\boldsymbol{X}^{\top}\boldsymbol{x})$$

Where $\beta$ represents the temperature parameter of the soft(arg)max. $\boldsymbol{X}^{\top} \in \mathbb{R}^{t \times n}$ is the transposed matrix representation of the set $\{\boldsymbol{x}_i\}_{i=1}^t$, and $\boldsymbol{x}$ represents a generic $\boldsymbol{x}_i$ from the set. Note that the $j$th row of $X^{\top}$ corresponds to an element $\boldsymbol{x}_j \in  \mathbb{R}^n$, so the $j$th row of $\boldsymbol{X}^{\top}\boldsymbol{x}$ is the scalar product of $\boldsymbol{x}_j$ with each $\boldsymbol{x}_i$ in $\{\boldsymbol{x}_i\}_{i=1}^t$.

The components of the $\alpha$ vector are also called "scores" because the scalar product between two vectors tells us how aligned or similar two vectors are. Therefore, the elements of $\alpha$ provide information about the similarity of the overall set to a particular $\boldsymbol{x}_i$.

The square brackets represent an optional argument. Note that if argmax is used, we get a one-hot vector of alphas, resulting in hard attention. On the other hand, soft(arg)max leads to soft attention. In each case, the components of the resulting vector $\boldsymbol{\alpha}$ sum to 1.

Generating $\alpha$ this way gives a set of alphas, one for each $\boldsymbol{x}_i$. Moreover, each $\boldsymbol{\alpha}_i \in \mathbb{R}^t$ so we can stack the alphas in a matrix $\boldsymbol{A}\in \mathbb{R}^{t \times t}$.

Since each hidden state is a linear combination of the inputs $\boldsymbol{X}$ and a vector $\boldsymbol{\alpha}$, we obtain a set of $t$ hidden states, which we can stack into a matrix $\boldsymbol{H}\in \mathbb{R}^{n \times t}$.

$\boldsymbol{H}=\boldsymbol{XA}$

# Key-value store

A key-value store is a paradigm designed for storing (saving), retrieving (querying) and managing associative arrays (dictionaries/hash tables).

For example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for "lasagne" - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.

Basically, the query is the question. Given one query, we check this query against every key and retrieve all matching content.


### Queries, keys and values
$$
\begin{align}q &= W_q x \\k &= W_k x \\v &= W_v x\end{align}
$$

Each of the vectors $q, k, v$ can simply be viewed as rotations of the specific input $x$. Where $q$ is just $x$ rotated by $W_q$, $k$ is just $x$ rotated by $W_k$ and similarly for $v$. Note that this is the first time we are introducing "learnable" parameters. We also do not include any non-linearities since attention is completely based on orientation.

In order to compare the query against all possible keys, $q$ and $k$ must have the same dimensionality, i.e. $q, k \in \mathbb{R}^{d'}$.

However, $v$ can be of any dimension. If we continue with our lasagna recipe example - we need the query to have the dimension as the keys, i.e. the titles of the different recipes that we're searching through. The dimension of the corresponding recipe retrieved, $v$, can be arbitrarily long though. So we have that $v \in \mathbb{R}^{d''}$.

For simplicity, here we will make the assumption that everything has dimension $d$, i.e.
$$
d^{'} = d^{''} = d
$$
So now we have a set of x's, a set of queries, a set of keys and a set of values. We can stack these sets into matrices each with $t$ columns since we stacked $t$ vectors; each vector has height $d$.
$$
\{ x_i \}_{i=1}^t \rightsquigarrow \{ q_i \}_{i=1}^t, \, \{ k_i \}_{i=1}^t, \, \, \{ v_i \}_{i=1}^t \rightsquigarrow Q, K, V \in \mathbb{R}^{d \times t}
$$
We compare one query $q$ against the matrix of all keys $K$:
$$
a = [\text{soft}](\arg) \max_{\beta} (K^{\T} q) \in \mathbb{R}^t
$$
Then the hidden layer is going to be the linear combination of the columns of $V$ weighted by the coefficients in $a$:
$$
h = V a \in \mathbb{R}^d
$$
Since we have $t$ queries, we'll get $t$ corresponding $a$ weights and therefore a matrix $A$ of dimension $t \times t$.
$$
\{ q_i \}_{i=1}^t \rightsquigarrow \{ a_i \}_{i=1}^t, \rightsquigarrow A \in \mathbb{R}^{t \times t}
$$
Therefore in matrix notation we have:
$$
H = VA \in \mathbb{R}^{d \times t}
$$
As an aside, we typically set $\beta$ to:
$$
\beta = \frac{1}{\sqrt{d}}
$$
This is done to keep the temperature constant and so we divide by the square root of the number of dimensions $d$.



# The Transformer

Expanding on our knowledge of attention in particular, we now interpret the fundamental building blocks of the transformer. In particular, we will take a forward pass through a basic transformer, and see how attention is used in the standard encoder-decoder paradigm compares to the sequential architectures of RNNs. 


## Encoder-Decoder Architecture

We should be familiar with this terminology. It is shown most prominently during autoencoder demonstrations, and is prerequisite understanding up to this point. To summarize, an input is fed through an encoder and decoder which impose some sort of bottleneck on the data, forcing only the most important information through. This information is stored in the output of the encoder block, and can be used for a variety of unrelated tasks. 

<center>
<img src="{{site.baseurl}}/images/week-12/12-3/figure1.png" style="zoom: 20%; background-color:#DCDCDC;" /><br>
<b>Figure 1:</b> Two example diagrams of an autoencoder. The model on the left shows how an autoencoder can be design with two affine transformations + activations, where the image on the right replaces this single "layer" with an arbitrary module of operations.
</center>



Our "attention" is drawn to the autoencoder layout as shown in the model on the right and will now take a look inside, in the context of transformers. 

### Encoder Module


<center>
<img src="{{site.baseurl}}/images/week-12/12-3/figure2.png" style="zoom: 20%; background-color:#DCDCDC;" /><br>
<b>Figure 2:</b> The transformer encoder, which accepts at set of inputs $x$, and outputs a of hidden representations $h^{Enc}$.
</center>

The encoder module accepts an input, which is simultaneously fed through the self attention block and bypasses it to reach the Add,Norm block. At which point, it is again simultaneously passed through the 1D-Convolution and another Add,Norm block, and resultingly outputted as the hidden representation. This hidden representation is then either sent through an arbitrary number of encoder modules (ie: more layers), or to the decoder. We shall now discuss these blocks in more detail. 

Typically, multiple $x's$ are sent through the encoder module, and concatenated at the end. We will soon see why, but for now, it is sufficient to understand that the encoder of the transformer applies self attention to $x$ and outputs a hidden representation $h^{Enc}$.

#### Self-attention
The self-attention model is a normal attention model. The query, key, and value are copied from the same item of the sequential input. In tasks that try to model sequential data, positional encodings are added prior to this input. The output of this block is the attention-weighted values. The self-attention block accepts a set of inputs, from $1, .. , t-1$, and outputs $1, ..., t-1$ attention weighted values which are fed through the rest of the encoder. 



<center>
<img src="{{site.baseurl}}/images/week-12/12-3/figure3.png" style="zoom: 20%; background-color:#DCDCDC;" /><br>
<b>Figure 3:</b> The self-attention block. The sequence of inputs is shown as a set along the 3rd dimension, and concatenated.
</center>

#### Add,Norm
The add norm block has two components. First is the add block, which is a residual connection, and layer normalization. 

#### 1D-convolution
Following this step, a 1D-convolution (aka a position-wise feed forward network) is applied. This block consists of two dense layers. Depending on what values are set, this block allows you to adjust the dimensions of the output $h^{Enc}$


### Decoder Module

The transformer decoder follows a similar procedure as the encoder. However, there is one additional sub-block to take into account. Additionally, the inputs to this module are different. 


<center>
<img src="{{site.baseurl}}/images/week-12/12-3/figure4.png" style="zoom: 20%; background-color:#DCDCDC;" /><br>
<b>Figure 4:</b> The transformer model https://arxiv.org/pdf/1706.03762.pdf
</center>

<center>
<img src="{{site.baseurl}}/images/week-12/12-3/figure5.png" style="zoom: 20%; background-color:#DCDCDC;" /><br>
<b>Figure 5:</b> A friendlier explanation of the decoder.
</center>


#### Cross-attention
  
The cross attention follows the query, key, and value setup used in all prior attention blocks.  However, the inputs are a little more complicated. Since the input to this data is a sequence, the input to the decoder is a value $x_t$, which is then passed through the self attention and add norm blocks, and finally ends up at the cross-attention block. This serves as the query for cross-attention, where the key and value pairs are the output $h^{Enc}$, where this output was calculated wth all past inputs $x_1, ..., x_{t-1}$. We can also specify how long this sequence can be. 


## Summary 

A sequence, $x_t$ to $x_{t-1}$ is fed through the encoder. Using self-attention and some more blocks, an output representation, $h^{Enc}$ is obtained. The final part of this sequence, $x_t$, is fed through the decoder. After applying self-attention to it, cross attention is applied. In this block, the query corresponds to a representation of $x_t$, and the key and values are for all prior words in the sequence ($x_1$ to $x_{t-1}}$. Intuitively, cross attention finds which values in the sequence are most relevant to constructing $x_t$, and therefore deserve the highest attention coefficients. The output of this cross attention is then fed through another 1D-convolution sub-block, and resultingly, we have $h_{Dec}$. For the specified training task, it is straightforward from here to see how training will commence, by comparing $h^{Dec}$ to some target data. 

 
### Word Language Models

There are a few important facts we left out before to explain the most important modules of a transformer, but will need to discuss them now to understand how transformers can achieve state-of-the-art results in language tasks. 

#### Positional encoding

Attention mechanisms allow us to parallelize the operations and greatly accelerate a model's training time,  but loses sequential information. The positional encoding feature enables allows us to capture this context.



#### Semantic Representations

Throughout the training of a transformer, many hidden representations are generated. To create an embedding space similar to the one used by the word-language model example in PyTorch, the output of the cross-attention, will provide a semantic representation of the word $x_i$, at which point further experimentation can be performed over this dataset. 

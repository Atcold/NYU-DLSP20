0:00:00.560,0:00:05.200
we have today Mike Lewis he's a research scientist at Facebook AI Research

0:00:05.200,0:00:08.639
working on natural language processing so previously he was a postdoc at the

0:00:08.639,0:00:13.360
University of Washington and working with Luke Zettlemoyer

0:00:13.360,0:00:16.400
and on the search based structure prediction

0:00:16.400,0:00:19.600
he completed his phd at the University of Edinburgh

0:00:19.600,0:00:23.519
and based on combining distributional and logical approaches

0:00:23.519,0:00:27.920
to semantics he has a mastery degree from the University of Oxford and won

0:00:27.920,0:00:32.640
the best paper award on EMNLP in 2016.

0:00:32.640,0:00:37.200
so uh without further ado let's get started with today's presentation

0:00:37.200,0:00:40.480
okay well thank you very much for the introduction Alfredo and inviting me

0:00:40.480,0:00:43.440
to talk um so yeah this lecture just wants to

0:00:43.440,0:00:47.280
try giving a high level overview of how deep learning

0:00:47.280,0:00:51.680
is used for natural languages processing these days

0:00:51.680,0:00:55.280
so i think it's [inaudible] it's been really dramatic progress in nlp in the last few

0:00:55.280,0:01:00.160
years with the playing deep learning um so

0:01:00.160,0:01:03.760
these days i mean you can get machine translation systems which will produce

0:01:03.760,0:01:07.680
translations which blind human narrators prefer the ones

0:01:07.680,0:01:12.560
produced by professional translators um you can make some question answering

0:01:12.560,0:01:15.360
systems that you ask them a question give me wikipedia

0:01:15.360,0:01:18.880
and they'll give you answers that are [inaudible] more accurate than the ones that

0:01:18.880,0:01:22.799
people will give you i could like have language models that

0:01:22.799,0:01:28.799
can like generate uh several paragraphs of fluent text

0:01:28.799,0:01:31.520
and for all of these things if you'd asked me like maybe five years ago i'd

0:01:31.520,0:01:33.920
have told you there's absolutely no way any of this is

0:01:33.920,0:01:37.600
possible in 2020 but um there seems to be a few

0:01:37.600,0:01:40.159
techniques that have been introduced that have really

0:01:40.159,0:01:44.880
made dramatic differences and one really nice properties actually

0:01:44.880,0:01:49.119
you can achieve all of these things with fairly generic models so

0:01:49.119,0:01:53.040
all the models for all these tasks actually look very similar

0:01:53.040,0:01:57.520
uh there's just a few high-level principles which are really useful

0:01:57.520,0:02:00.799
um so i'm gonna be covering quite a lot ground on this quite quickly

0:02:00.799,0:02:05.920
so please interrupt me often with questions

0:02:05.920,0:02:10.319
um right so yeah um also one thing i should say is

0:02:13.520,0:02:15.440
that with all the progress we've been seeing

0:02:15.440,0:02:19.040
in nlp uh probably a lot of stuff i'm going to tell you right now is going to

0:02:19.040,0:02:21.280
be out to date in a year or two i mean i hope

0:02:21.280,0:02:24.560
we continue to make progress and stuff continues to

0:02:24.560,0:02:28.000
be out today so as well as like explaining some of the models we're

0:02:28.000,0:02:31.519
using i want to try and sort of give you some intuitions about what kinds of

0:02:31.519,0:02:36.239
principles are working out well which will hopefully have a bit more of

0:02:36.239,0:02:38.879
a shelf life okay so um the first

0:02:42.319,0:02:46.319
topic i want to cover this lecture is language modeling

0:02:46.319,0:02:49.920
language modeling isn't necessarily a useful task in itself but it seems to be

0:02:49.920,0:02:53.760
a really good building block for introducing all the

0:02:53.760,0:02:57.440
techniques that we'll need later on um so uh i don't know any of you seen

0:03:00.879,0:03:06.000
this before this is an example from a language model called gpt-2

0:03:06.000,0:03:10.959
which came out in 2019. um so what's going on here is that we've

0:03:10.959,0:03:15.519
got some humans played some uh introduction

0:03:15.519,0:03:18.720
here about um scientists finally heard of unicorns of

0:03:18.720,0:03:23.280
the andes that apparently speak english and then given that text they've asked

0:03:23.280,0:03:27.440
this language model to write some more texts and stats of that

0:03:27.440,0:03:30.000
and the text we get here is actually quite

0:03:30.000,0:03:32.879
impressive like everyone was really shocked that language models could work

0:03:32.879,0:03:40.159
this well uh last year so you can see um

0:03:40.159,0:03:43.519
the convenient text seems actually quite plausible for a news article about this

0:03:43.519,0:03:48.080
it's um talk to me talk about unicorns um

0:03:48.080,0:03:52.720
the text is very fluent grammatical it's not really any flaws there and

0:03:52.720,0:03:55.840
it seems to like invent quite a lot of details like the name of the scientists

0:03:55.840,0:03:58.720
who discovered them um obviously all this is complete

0:04:01.200,0:04:05.760
nonsense nothing here is true but um also

0:04:05.760,0:04:08.159
none of this will look like anything the model was ever trained on like i'm

0:04:08.159,0:04:12.080
pretty sure this paragraph over his unicorn has like

0:04:12.080,0:04:15.120
no neighbors anywhere on the internet this

0:04:15.120,0:04:17.040
is all completely new language but it's all

0:04:17.040,0:04:22.240
actually quite high quality text um i'm not going to read all this out

0:04:22.240,0:04:25.440
but like if you read the rest of the article that wrote then

0:04:25.440,0:04:30.000
um there are some flaws but they're quite hard to spot and generally

0:04:30.000,0:04:33.520
uh this seems to be quite a good language model

0:04:33.520,0:04:36.320
so i'm gonna try and show you like the kind of techniques you need to actually

0:04:36.320,0:04:41.930
build the lines model that works as well all right so um

0:04:41.930,0:04:45.120
[Silence] very briefly what is language model so

0:04:45.120,0:04:48.800
language model is just a basic density estimation for text

0:04:48.800,0:04:52.320
so we're going to assign a probability to

0:04:52.320,0:04:58.080
uh every possible string and hopefully when puts more probability on like

0:04:58.080,0:05:02.160
strings which are fluent english than other strings

0:05:02.160,0:05:06.240
um so how do we model this density well um obviously

0:05:06.240,0:05:10.320
there are quite a lot of possible sentences

0:05:10.320,0:05:14.880
exponentially many so we can't just predict classified suits directly

0:05:14.880,0:05:18.000
um there are different techniques you can do for this but the one i'm going to

0:05:18.000,0:05:21.039
talk about is the one that's most popularly used which is

0:05:21.039,0:05:25.440
basically to factorize this distribution using the chain rule

0:05:25.440,0:05:32.800
um so here all we're going to do is um just [inaudible] to say it's going

0:05:32.800,0:05:37.120
to predict the first word then predict the second word given the first then

0:05:37.120,0:05:40.639
the third given the previous two um this is an exact characterization it

0:05:44.800,0:05:50.720
doesn't cost us anything to do like this um so really what we turned is

0:05:50.720,0:05:54.000
the density estimation problem into a series of classification problems

0:05:54.000,0:05:58.080
these classification problems are the form given a bunch of texts predict the

0:05:58.080,0:06:00.400
next word and that's going to be a theme through a

0:06:00.400,0:06:03.840
lot of techniques we have in this talk so more concretely we have this uh from

0:06:06.880,0:06:10.400
this example i showed you before we got this

0:06:10.400,0:06:12.880
um string the model output so like the scientists

0:06:12.880,0:06:16.240
name the population after the distinct detail on orbits and you've got to

0:06:16.240,0:06:20.479
predict the next word and uh the correct word in this case is

0:06:20.479,0:06:22.880
unicorn okay so um

0:06:27.440,0:06:30.560
at high level all these language models look something like this basically we

0:06:30.560,0:06:34.240
input this text into a neural network somehow

0:06:34.240,0:06:38.080
the neural network will map all this context onto a vector

0:06:38.080,0:06:41.440
this vector represents the next word and then we're going to

0:06:41.440,0:06:49.599
have some big word about matrix so our um word embedding matrix will

0:06:49.599,0:06:54.960
basically contain a vector for every possible word model knows how to output

0:06:54.960,0:06:58.000
and then all we need to do is compute the similarity by just doing a dot

0:06:58.000,0:07:00.720
product between the context vector and each of these

0:07:00.720,0:07:05.120
word vectors and uh we'll get a likelihood of

0:07:05.120,0:07:08.319
predicting the next word then we'll just train this model by

0:07:08.319,0:07:12.080
maximum likelihood in the obvious way okay so um

0:07:17.759,0:07:21.520
i mean the detail here is often we don't deal with words directly we deal with

0:07:21.520,0:07:24.960
things called subwords or even characters but um all

0:07:24.960,0:07:28.319
the modeling techniques remain the same all right so um

0:07:31.919,0:07:34.960
how all skills here is in this context encoder

0:07:34.960,0:07:40.319
how do we build this um so kind of the first approach people

0:07:40.319,0:07:44.479
took for this is um basically convolutional models

0:07:44.479,0:07:48.560
so these convolution models kind of encode this

0:07:48.560,0:07:52.720
uh inductive bias that language can has this translation of variance

0:07:52.720,0:07:56.240
property we should interpret a phrase the same

0:07:56.240,0:08:03.840
way no matter what position it occurs in um so a typical model might look this

0:08:03.840,0:08:06.960
way basically first of all for every word we'll just map it to some

0:08:06.960,0:08:10.240
vector um which is just a lookup table into an

0:08:10.240,0:08:13.199
embedding matrix so the word will get the same vector no

0:08:13.199,0:08:19.280
matter what context it appears in and then we'll apply a bunch of layers

0:08:19.280,0:08:22.720
of the 1d convolutions followed by non-linearities

0:08:22.720,0:08:26.000
until eventually we end up with some kind of vector

0:08:26.000,0:08:30.879
representing that context which really really [inaudible] the vector means what should

0:08:30.879,0:08:33.599
the next word be um and these models were first

0:08:38.080,0:08:41.599
i think this is actually maybe the first language model from bengio in 2003 world's

0:08:41.599,0:08:46.080
first neural language model um and uh these kind of convolutional

0:08:46.080,0:08:49.360
approaches were actually shared if to work quite well by Yann LeCun

0:08:49.360,0:08:52.880
in 2016 if you applied in a modern deep learning

0:08:52.880,0:08:58.959
techniques um kind of the these models very fast

0:08:58.959,0:09:03.040
which is great um actually speed is very impossible

0:09:03.040,0:09:05.680
language modeling because typically we use huge amounts of

0:09:05.680,0:09:11.200
training data um there's one come downside which is

0:09:11.200,0:09:15.120
that they're only able to really condition on the certain reception field

0:09:15.120,0:09:20.480
um so in this kind of toy example uh this word unicorn could only condition

0:09:20.480,0:09:26.320
uh the previous uh five words uh because of the sort of kernel width

0:09:26.320,0:09:30.399
the number of layers we're using here um obviously like realistic

0:09:30.399,0:09:34.240
convolutional models i have a much bigger receptor field like this but

0:09:34.240,0:09:40.800
um natural language tends to have extremely long range dependencies i mean

0:09:41.120,0:09:43.680
uh for an extreme example you can imagine if you're like trying to build a

0:09:43.680,0:09:47.519
language model of a complete book it might actually help you to be able to

0:09:47.519,0:09:50.880
condition on the title of the book at all time steps and

0:09:50.880,0:09:54.480
obviously the title will be hundreds of thousands of words

0:09:54.480,0:09:58.560
previously and it's quite hard to kind of build this into

0:09:58.560,0:10:02.880
build a convolutional receptor field big enough to do this

0:10:03.440,0:10:07.680
okay so how do we condition our model context

0:10:07.680,0:10:12.160
i guess um the most popular approach until a couple years ago

0:10:12.160,0:10:17.360
was uh what's called the current neural networks

0:10:17.360,0:10:24.399
um this is kind of a conceptually quite straightforward idea that basically at

0:10:24.399,0:10:27.360
every time set we're going to maintain some state so have some state coming in

0:10:27.360,0:10:29.760
from the previous time step which represents

0:10:29.760,0:10:33.680
what we've read so far we'll combine the state with uh

0:10:33.680,0:10:37.120
the recurrent word we've read and we'll use that to update our states

0:10:37.120,0:10:40.800
and then we'll just iterate this for as many time steps

0:10:40.800,0:10:47.279
as we need um so i think this seems like quite a

0:10:47.279,0:10:50.640
natural model of reading i mean i think for the most part people kind of

0:10:50.640,0:10:53.440
read left or right and maintain some kind of states

0:10:53.440,0:10:58.959
as they go at least in principle as well you can model unbounded contexts like

0:10:58.959,0:11:04.320
this  um at least in principle like the

0:11:04.320,0:11:07.920
say the title of a book would affect the hidden states

0:11:07.920,0:11:11.040
of the last word of the book um in practice so there are some fairly

0:11:14.560,0:11:17.920
significant issues with this model firstly um there's kind of

0:11:22.320,0:11:26.720
no free lunch here so by the time the effect of trying to maintain

0:11:26.720,0:11:29.120
mistakes is we're going to compress the whole

0:11:29.120,0:11:34.959
history of the document reading into a single vector at each time step

0:11:34.959,0:11:38.079
and then you can't once you've read a word you can never look at it again you

0:11:38.079,0:11:41.519
have to memorize that and that means you have to

0:11:41.519,0:11:45.440
actually have to cram a huge amount of information into a

0:11:45.440,0:11:49.120
single vector and this is firstly it's a kind of a

0:11:49.120,0:11:53.200
bottleneck involved also i mean there's a question about how much

0:11:53.200,0:11:57.120
information you can really store in one vector but

0:11:57.120,0:12:02.079
also um it's kind of a practical learning problem to you

0:12:02.079,0:12:05.920
that's the the um you get an issue called the vanishing gradient problem

0:12:05.920,0:12:08.160
where it means that you know every time you go

0:12:10.399,0:12:15.120
through one of these steps then you you'll have some kind of

0:12:15.120,0:12:18.320
non-linearity which will mean that the effective words in the past

0:12:18.320,0:12:22.000
will kind of get exponentially smaller each time step

0:12:22.000,0:12:27.120
that means that once you have uh no gradient for a particular word in the

0:12:27.120,0:12:29.600
past it's very hard to actually suddenly learn later on

0:12:29.600,0:12:32.639
that word was impossible. um one final issue i want to mention

0:12:35.760,0:12:41.920
with [inaudible] is they're actually quite slow

0:12:41.920,0:12:47.120
um the reason for this is particularly for training

0:12:47.600,0:12:53.519
um so the reason is that in order to like um

0:12:53.519,0:12:56.720
build your state for a particular work you actually have to

0:12:56.720,0:13:00.880
build your state for every previous word first

0:13:00.880,0:13:04.720
let me essentially have a big for loop that's going over your entire document

0:13:04.720,0:13:09.120
and the longer your document is the bigger this for loop is

0:13:09.120,0:13:11.920
and [inaudible] most of these operations you can't actually compute in

0:13:11.920,0:13:15.839
parallel you actually have to do it sequentially and one gpu

0:13:15.839,0:13:18.880
hardware is really based around being able to do

0:13:18.880,0:13:25.200
operations in parallel um okay

0:13:25.839,0:13:28.880
so the convolutional network didn't have this problem it's everything's in

0:13:28.880,0:13:32.639
parallel but on the other hand you get this boundary

0:13:32.639,0:13:38.480
receptor field and the recurrent models you have

0:13:38.480,0:13:42.000
in principle an infinite amount of receptor field

0:13:42.000,0:13:45.519
but it's quite slow to train so the solution to this is now what's

0:13:47.839,0:13:51.360
called the transformer um which is the model that's used in all

0:13:51.360,0:13:57.279
state-of-the-art nlp systems these days um so i'm going to go through

0:13:57.279,0:13:59.279
transformer in quite a lot more detail when i did

0:13:59.279,0:14:05.440
the rnns or cnns um transformers were introduced in 2017

0:14:05.440,0:14:09.360
by Ashish Vaswani and famous paper called attention is all

0:14:09.360,0:14:13.360
you need and they really revolutionize lots of

0:14:13.360,0:14:19.199
NLP so i included a figure here from the

0:14:19.199,0:14:22.880
original transformer paper um i don't know how confusing is this to you

0:14:24.240,0:14:26.959
certainly when i first saw this figure in 2017

0:14:26.959,0:14:30.079
it took me quite a while to get my head around it and there's

0:14:30.079,0:14:33.120
quite a lot of details going on in these boxes so i'm going to

0:14:33.120,0:14:36.720
just try to slowly drill into them all right so what's going on um

0:14:42.480,0:14:46.560
basically you see we have this input state this um

0:14:46.560,0:14:50.560
end times this transformer block and then um an output phase

0:14:50.560,0:14:56.800
so this end times block thing just means we're going to unroll

0:14:56.839,0:15:00.320
uh the same block with different parameters

0:15:00.320,0:15:04.160
a certain number of times there so the example has uh

0:15:04.160,0:15:06.959
six layers which i think had the original transform paper which seems

0:15:06.959,0:15:10.639
quite cute these days um these days people are training models

0:15:10.639,0:15:16.560
with billions of parameters and uh many many dozens of layers

0:15:17.360,0:15:20.399
all right so i'm just going to drill into this box

0:15:20.399,0:15:26.399
more detail so this can be the core of the transformer the transformer block

0:15:26.399,0:15:29.930
you see it's actually incorporated to two different sub layers

0:15:29.930,0:15:35.759
[Notification Sound] um which are both very important um

0:15:35.759,0:15:40.560
sub layer 2 is maybe the more obvious one this is just a big

0:15:40.560,0:15:46.320
feedforward network um it could be any mlp but it's

0:15:46.320,0:15:52.560
important it's actually quite expressive um

0:15:52.839,0:15:56.000
and beneath we have this multi-headed attention module

0:15:56.000,0:16:00.639
the multi-head attention is kind of the key building block behind transformers

0:16:00.639,0:16:06.320
and why they work um so

0:16:06.320,0:16:10.800
this sublayer is also connected by these um boxes labeled add and norm

0:16:10.800,0:16:15.199
so the add part means this is a residual connection

0:16:15.199,0:16:19.440
um which helps stopping the gradients vanishing in large models

0:16:19.440,0:16:23.360
uh the norm here means uh layer normalization

0:16:23.360,0:16:26.480
i'm not going to the layer norm in detail here but

0:16:26.480,0:16:29.759
it's actually very important to make these models work and there's actually

0:16:29.759,0:16:32.800
some subtleties about how exactly you do the layer normalization that

0:16:32.800,0:16:37.680
makes a big difference in practice uh hey excuse me i have a question sure

0:16:37.680,0:16:41.199
uh so this isn't immediately clear but

0:16:41.199,0:16:45.279
could you talk a little bit more about just the intuition behind using

0:16:45.279,0:16:48.240
multi-headed attention as opposed to a single head

0:16:48.240,0:16:51.680
i mean presumably each head learns something different

0:16:51.680,0:16:54.959
and attends over its input differently but

0:16:54.959,0:16:58.160
what's what was sort of the intuition behind that

0:16:58.160,0:17:01.279
um i'll ask that question a bit i'm going to go through

0:17:01.279,0:17:04.880
uh exactly what multi-headed attention is a bit first and then i'll try and get some

0:17:04.880,0:17:07.760
iterations as to why this is a good thing to do

0:17:07.760,0:17:11.280
uh if we don't answer your question then please

0:17:11.280,0:17:15.679
follow me up in a few slides time thank you

0:17:16.880,0:17:20.559
any other questions at this stage by the way

0:17:21.520,0:17:28.319
uh i have a question uh used that the the transformer module uses layer

0:17:28.319,0:17:33.280
normalization uh why can you provide some intuition into why that works

0:17:33.280,0:17:38.559
better than group normalization or uh old-fashioned normalization

0:17:40.480,0:17:42.880
um i don't think i can actually give a very

0:17:44.960,0:17:48.160
satisfying technical answer to this and i think a lot of this is quite empirical

0:17:48.160,0:17:52.480
as to why in nlp we layer norm works great and

0:17:52.480,0:17:58.720
in computer vision batch norm works great um a nice prosperous layer norm

0:17:58.720,0:18:02.559
is that it doesn't depend on the batch dimension uh which

0:18:02.559,0:18:05.679
batch norm does so uh in practice that's quite a big

0:18:08.480,0:18:12.480
advantage because it's quite hard to train with large

0:18:12.480,0:18:16.880
batches with them very large models yeah you can see people written lots of

0:18:22.160,0:18:27.440
papers and actually why things like that even work for computer version i think

0:18:27.679,0:18:30.640
at least that's how i read it's there's still some debate as to actually what

0:18:30.640,0:18:33.520
it's doing um maybe the intuition is the original

0:18:33.520,0:18:35.840
paper why even batch norms are not great

0:18:35.840,0:18:40.559
um so personally i would say this is one of the slightly unsatisfying

0:18:40.559,0:18:44.240
things in deep learning where it works but it's a little bit unclear

0:18:44.240,0:18:46.400
why okay thank you yeah maybe i was waiting

0:18:49.760,0:18:57.840
with a more satisfying answer than that [Drinking]

0:18:59.760,0:19:03.280
another question is coming here is do transformers share weights across

0:19:03.280,0:19:07.760
time steps like rnn lstms uh yeah great question i should have

0:19:07.760,0:19:11.200
made that clear um yeah so all these weights can be shared across

0:19:11.200,0:19:15.360
timesteps um so it's kind of convolutional nonsense

0:19:15.360,0:19:18.640
um so you have one block and you'll play it

0:19:18.640,0:19:22.000
every time step uh you can actually also apply them use

0:19:22.000,0:19:24.160
the same weights and every layer and that works quite well too

0:19:24.160,0:19:28.080
but it's not what people normally do thank you

0:19:33.679,0:19:36.880
any other questions so far i think those were the questions i read

0:19:39.440,0:19:46.000
so far on the chat okay so uh what is this mysterious

0:19:46.000,0:19:50.400
multi-headed attention thing um so here's another figure i don't know

0:19:50.400,0:19:53.840
if this helps um uh so basically you can

0:19:57.840,0:20:04.159
compute these three quantities called um b k and q here they stand for a query

0:20:04.159,0:20:09.039
key and value respectively um

0:20:09.039,0:20:12.000
do the scale dot product attention operation and then concatenate the

0:20:12.000,0:20:15.520
outputs all right so drilling into this scale

0:20:15.520,0:20:19.520
dot product attention um eventually we will run out of boxes to

0:20:19.520,0:20:25.520
expand um it looks something like this um

0:20:28.080,0:20:31.679
so we're going to do compute this query and key

0:20:31.679,0:20:35.600
do a dot products and softmax and use this

0:20:35.600,0:20:39.840
as a way to some of the values i don't worry that it makes sense i will

0:20:39.840,0:20:45.039
do more detail so let's look at this example um

0:20:45.039,0:20:48.480
where the context here is let's say these horned silver-whites

0:20:48.480,0:20:52.480
and we're trying to create the next word which in the example before was uh

0:20:52.480,0:20:58.559
unicorn so um

0:20:58.799,0:21:01.840
for the word we're trying to predict we're going to like compute this value

0:21:01.840,0:21:05.440
called the query and for all the previous words we're

0:21:05.440,0:21:09.039
going to compute the quantity called the key and

0:21:09.039,0:21:14.159
these are linear layers based on the current states of this layer

0:21:14.559,0:21:18.240
um tomorrow we're going to be coding this in in practice so we're going to be

0:21:18.240,0:21:21.760
seeing this in like only the small details in the in

0:21:21.760,0:21:24.640
the code as well okay so

0:21:28.559,0:21:34.400
um you can think of this query as the model asking a question over here it's

0:21:34.400,0:21:39.200
context so far but it's going to help you predict what the next word should be

0:21:39.520,0:21:43.760
so the query could be something like um tell me what previous adjective is or

0:21:43.760,0:21:47.840
tell me what the uh previous determiner is

0:21:47.840,0:21:55.280
um and a disabler's word like these here

0:21:55.840,0:21:58.880
and then for the keys they're going to be things that sort of

0:21:58.880,0:22:02.000
label the current word with telling you some information about it so they could

0:22:02.000,0:22:04.640
be saying this word is an adjective this word is a

0:22:04.640,0:22:08.880
determiner this word is a verb something like that

0:22:08.880,0:22:12.320
or can be something more complex like it could be like a

0:22:12.320,0:22:16.640
any [inaudible] relation like co-reference or something

0:22:17.039,0:22:22.559
so [inaudible] completely as a questions query and then it's going to

0:22:22.559,0:22:25.600
compute just do a dot product with all of the

0:22:25.600,0:22:30.720
keys and use this to compute and then you do softmax as well and this

0:22:30.720,0:22:33.840
is going to induce a distribution over all the

0:22:33.840,0:22:36.559
previous words so here you can imagine a query

0:22:39.200,0:22:43.039
something like telling me what previous adjective is and the

0:22:43.039,0:22:46.159
attention will produce this kind of distribution over

0:22:46.159,0:22:50.159
these three previous words it's going to put most probability mass on either horn

0:22:50.159,0:22:56.320
or silver white um

0:22:57.600,0:23:00.559
we're also going to compute this other value this is the quantity called the

0:23:00.559,0:23:03.280
value and we'll do that for all the previous

0:23:03.280,0:23:06.640
words as well and maybe the value will tell you

0:23:06.640,0:23:10.640
something slightly more about what the contents of the word is

0:23:11.919,0:23:16.000
and then i'm going to compute this hidden state by basically marginalizing

0:23:16.000,0:23:20.240
out the attention distribution so here this

0:23:20.240,0:23:24.320
hidden state is going to be a weighted sum of the values

0:23:24.320,0:23:28.080
of all the previous words that's going to be

0:23:28.320,0:23:31.840
weight by the probability of that word um so

0:23:38.320,0:23:41.919
that's basically what's going on the left side of this figure here

0:23:41.919,0:23:45.200
i left out this detail about the scaling escape that's just a

0:23:45.200,0:23:51.279
hack to make the gradients more stable okay um but there's another detail here

0:23:51.279,0:23:56.240
which is that uh that's kind of single-headed

0:23:56.240,0:23:59.520
attention i've described so far but we're actually gonna do this thing

0:23:59.520,0:24:02.840
called multi-headed attention and that basically just means we're gonna

0:24:02.840,0:24:07.039
compute the same thing with different queries keys and values

0:24:07.039,0:24:10.480
multiple times in parallel so this question before about like what

0:24:12.880,0:24:18.559
the intuition behind that is um and really it's like you actually

0:24:18.559,0:24:20.640
want let's create the next word you need to

0:24:20.640,0:24:24.400
know a lot of different things so just this example before

0:24:24.400,0:24:26.640
um uh let's see so let's say the next word

0:24:29.840,0:24:36.080
here should be unicorns plural um to know it should be unicorns i mean

0:24:36.080,0:24:39.760
you probably want to know both that it's horned and silver white because

0:24:39.760,0:24:43.200
uh the conjunction of those makes it more likely to be unicorn

0:24:43.200,0:24:46.960
but you also want to know that the uh the determinant here was these

0:24:46.960,0:24:50.159
not a if it was like a horn silver white it'd be

0:24:50.159,0:24:54.559
unicorn singular the fact is these means it should be unicorn plural so you can

0:24:54.559,0:24:59.120
plural it [inaudible] so you actually need to like look at all

0:24:59.120,0:25:02.000
these three words at once to have a good idea

0:25:02.000,0:25:05.360
what the next word should be a multi-attention is a way of like letting

0:25:05.360,0:25:10.000
each word look at multiple previous words simultaneously

0:25:10.720,0:25:16.480
a question here is why are we actually uh in need of using the softmax

0:25:16.480,0:25:19.840
why do we use this yeah it's a good question um

0:25:25.600,0:25:31.760
i think firstly i mean [inaudible] on normalization

0:25:31.760,0:25:35.120
effect is probably good i mean otherwise when you got longer

0:25:35.120,0:25:40.240
sequences this uh summation would get kind of bigger and

0:25:40.240,0:25:43.760
bigger the further you went through so having a normalization

0:25:43.760,0:25:47.919
is probably good the normalization also kind of lets the model um

0:25:47.919,0:25:52.000
discard information to you so it can say this word

0:25:52.000,0:25:58.080
just isn't relevant um which is good i think i have seen people

0:25:58.080,0:26:00.159
experiment with using things like ReLu instead

0:26:00.159,0:26:04.559
so which kind of different different way of discarding information but

0:26:04.559,0:26:11.840
i think the evidence is softmaxes work the best

0:26:14.240,0:26:17.840
uh another question sorry i may have missed this um

0:26:17.840,0:26:24.960
the mask there in pink don't know what that is briefly right mask is actually

0:26:24.960,0:26:28.480
important um so

0:26:29.520,0:26:33.919
i'm going to say make a point first um so one of the really big wins about this

0:26:33.919,0:26:37.200
whole [inaudible] multi-headed attention is

0:26:37.200,0:26:41.520
that it's extremely paralyzable um so now this

0:26:41.520,0:26:45.200
computation of queries keys and values a given time step depends on

0:26:45.200,0:26:48.880
what you're doing at any of the other time steps so

0:26:48.880,0:26:51.679
unlike your current network you can actually compute all of these

0:26:51.679,0:26:54.559
simultaneously um which plays very well with um the

0:26:57.200,0:27:00.720
kind of hardware we have these days so not only are we going to put all the

0:27:00.720,0:27:03.440
different heads at once we're going to compute all the time steps at once in a

0:27:03.440,0:27:08.400
single forward pass um

0:27:08.400,0:27:13.200
so that's great except that if you're computing all the time steps

0:27:13.200,0:27:17.039
at once there's nothing to actually stop you

0:27:17.039,0:27:23.200
um words looking at the future since like 

0:27:23.200,0:27:26.559
auto-regressive factorization we're dealing with here then

0:27:26.559,0:27:30.640
we only want words conditioned on previous words

0:27:30.640,0:27:35.520
um but as i've spread them all so far the words could look at future words too

0:27:35.520,0:27:38.240
which is the problem because then they can cheat

0:27:38.240,0:27:42.880
and use themselves on future context to predict themselves

0:27:42.880,0:27:48.799
so the solutions that here is um uh so it's what we call self-attention

0:27:48.799,0:27:54.480
masking so a mask is just a upper triangular matrix that sort of uh

0:27:54.480,0:27:58.960
has zeros and lower triangle and like negative infinity in the upper triangle

0:27:58.960,0:28:02.799
and we're going to just add this to the attention scores

0:28:02.799,0:28:06.880
and the effect of that is going to be that um every word to the left has a

0:28:06.880,0:28:10.000
much higher attention score than everywhere to the right so the model

0:28:10.000,0:28:14.880
will only end up practicing using words to the left but this is deterministic

0:28:14.880,0:28:20.240
mask without trainable weights um just values either zero or negative

0:28:20.240,0:28:24.559
or infinity uh so you'd only mask in case of an

0:28:24.559,0:28:28.000
application-specific training task correct if you had to just

0:28:28.000,0:28:31.120
build representations for example you wouldn't need to mask because it wouldn't

0:28:31.120,0:28:33.840
matter uh yeah that's a great question and so

0:28:33.840,0:28:37.039
we'll go to more general representation learning later so

0:28:37.039,0:28:41.679
uh most of the time you just want a text encoder you don't need to mask them

0:28:41.679,0:28:45.360
bidirectional context is absolutely helpful um

0:28:45.360,0:28:48.799
in this case language modeling which we're working through so far then

0:28:48.799,0:28:52.720
the mask is sort of crucial to make them all mathematically correct and compute

0:28:52.720,0:28:54.960
the correct factorization but yeah great

0:28:54.960,0:28:59.840
question thanks okay um

0:29:07.440,0:29:12.720
okay so uh one other detail we need to do to make all this work

0:29:12.720,0:29:17.760
is add something should be input over positional writing

0:29:17.760,0:29:20.000
so as i describe small so far it's actually

0:29:25.360,0:29:29.919
um it's doing a low bias model it knows very little about language the inputs

0:29:29.919,0:29:36.080
could be anything and it would work so um

0:29:37.440,0:29:40.399
in particular it's like i mean you can model a set or a graph or anything like

0:29:40.399,0:29:45.039
this it should be fine um but we know

0:29:45.279,0:29:47.840
i mean in language there are some properties which are useful like for

0:29:47.840,0:29:52.159
example um there's an ordering to words which is

0:29:52.159,0:29:55.840
really important to how you interpret them

0:29:55.919,0:29:58.559
and this one doesn't actually know anything about that and that's in

0:29:58.559,0:30:01.360
contrast to the convolutional models and the parallel models i showed you earlier

0:30:01.360,0:30:05.279
which both have different ways of encoding the

0:30:05.279,0:30:10.159
order of text um so one of the techniques that was

0:30:10.159,0:30:15.679
introduced in this paper was called positional embedding um the

0:30:15.679,0:30:18.240
different ways you can do this they describe something in the paper

0:30:18.240,0:30:20.640
which is slightly weird actually i'm not going to describe

0:30:20.640,0:30:25.840
but it works just as well to essentially learn separate embedding for every time

0:30:27.440,0:30:31.919
step so for every positioning document zero one two three four five

0:30:31.919,0:30:35.760
you just learn separate embedding and then you add this to your input

0:30:35.760,0:30:39.520
so your input now is a summation of the word vector and some kind of positional

0:30:39.520,0:30:41.919
vector and it's very simple but it gives the

0:30:44.720,0:30:46.880
model kind of the order information it needs and it

0:30:46.880,0:30:54.399
works great okay so why are these models so good why

0:30:54.399,0:30:57.600
does everyone use them um i think the really powerful thing about

0:31:00.880,0:31:03.200
the model is that it kind of gives you direct connections

0:31:03.200,0:31:09.039
between every pair of words so each word can directly access the

0:31:09.039,0:31:14.960
hidden state of every previous word um and

0:31:16.000,0:31:19.840
that's a contrast to convolutional model could barely get the state of all the

0:31:19.840,0:31:23.840
words in this receptor field but nothing further back in time than that

0:31:23.840,0:31:28.000
and our current model um the state had to go through his

0:31:28.000,0:31:32.720
bottleneck of each time at each time step so

0:31:32.720,0:31:36.480
um you can actually directly access the previous words

0:31:36.480,0:31:39.600
beyond the like literally one previous word anything further than the past

0:31:39.600,0:31:43.760
had to get somehow compressed and you have lost information about this

0:31:43.760,0:31:47.279
for self attention you can in principle put 100% attention on

0:31:47.279,0:31:52.480
any word in the distant past and see exactly what was there

0:31:52.480,0:31:57.120
and this just makes it a really powerful model it like avoids things like uh

0:31:57.120,0:32:02.960
issues like vanishing gradients quite effectively

0:32:04.159,0:32:09.039
um and amazingly can just learn very expressive functions very easily

0:32:09.279,0:32:13.039
the other great thing about this is how paralyzable it is i mean

0:32:13.039,0:32:17.440
so the one hand this model's doing quite a lot of computation in that it's doing

0:32:17.440,0:32:22.159
uh this um so the self-attention operation is quadratic

0:32:22.159,0:32:26.000
basically because every word can look at every other word

0:32:26.000,0:32:29.279
and that sounds quite slow but the really nice thing is you can do it in

0:32:29.279,0:32:31.600
parallel so because all these operations are

0:32:31.600,0:32:36.799
independent to each other you just do it as one big matrix multiplication so

0:32:36.799,0:32:40.159
even though sometimes you probably do kind of more

0:32:40.159,0:32:43.840
add multiply operations and you would do with the equivalent RNN

0:32:43.840,0:32:47.360
you can do all these operations um much faster because you do them all at once

0:32:47.360,0:32:52.080
rather sequentially so this is a really good trade-off

0:32:53.440,0:32:59.440
um i also want to quickly talk about some other things that's great

0:32:59.440,0:33:04.320
the stuff like um multi-headed attention and positional embeddings and stuff so

0:33:04.320,0:33:07.279
i've got all the attention when transformers were first released

0:33:07.279,0:33:09.919
but transformers also came along with a whole bag of

0:33:09.919,0:33:13.840
other tricks as well and these tricks were all

0:33:13.840,0:33:17.840
actually really important to make this stuff work

0:33:17.840,0:33:22.960
um and sometimes this paper really kind of modernized NLP i think

0:33:22.960,0:33:26.640
um so for example i've mentioned about this useful layer normalization before

0:33:26.640,0:33:33.039
[inaudible] it's really helpful um they also have doing these things

0:33:33.039,0:33:37.360
like um these learning rate schedules so uh for

0:33:37.360,0:33:39.679
whatever reason to make transformers work well

0:33:39.679,0:33:44.399
uh you have to sort of linearly warm up your learning rate from

0:33:44.399,0:33:48.799
some from zero to your goal learning rate over

0:33:48.799,0:33:56.159
several thousand steps um and people do do warm learning rates

0:33:56.159,0:33:59.440
warm-up learning rates in other settings but transformers really really need this

0:33:59.440,0:34:03.919
to work um also the things that the

0:34:03.919,0:34:06.720
initialization actually really do matter with these and

0:34:06.720,0:34:11.119
some initializations don't work um and they're throwing these other

0:34:11.119,0:34:15.679
tricks like a label smoothing at the output which

0:34:15.760,0:34:18.879
uh again wasn't invented in this paper but it turns out

0:34:18.879,0:34:23.200
quite helpful for the task like machine translation

0:34:23.679,0:34:29.280
all right um so to give you some idea how well these things

0:34:29.280,0:34:34.000
are working these models described so far um

0:34:34.000,0:34:38.560
here are some results on a language modeling depth benchmark um

0:34:38.560,0:34:44.240
so the number on the right is what's called perplexity which is a

0:34:44.560,0:34:48.960
measure of the likelihood of held out data and here lower is better

0:34:48.960,0:34:54.800
um so let's see you see that an lcm for 2016 gets perplexity of

0:34:54.800,0:34:58.879
uh 48 um you see that Yann Dauphin's convolutional

0:35:00.640,0:35:04.880
models uh from 2016 uh doing quite a bit better at

0:35:04.880,0:35:09.760
about 20 about 37. uh also people played around with a

0:35:09.760,0:35:13.599
whole bunch of RNN variants like you speak dozens of

0:35:13.599,0:35:16.960
papers on how you make variations in LSTMs and some of these

0:35:16.960,0:35:20.640
getting low 30s as well but then when you introduce transformers

0:35:20.640,0:35:26.720
you get a really big jump down to go to 18 and 20.

0:35:26.720,0:35:32.640
and in terms of language modeling that's a really enormous geometry performance

0:35:35.359,0:35:38.640
um i should say these gains we saw were particularly

0:35:38.640,0:35:43.200
large on uh kind of long context language modeling so language modeling

0:35:43.200,0:35:46.240
where so there's some benchmarks where you

0:35:46.240,0:35:49.680
just get a single sentence to predict uh on this task you actually get a whole

0:35:49.680,0:35:54.960
wikipedia article so uh potentially thousands of words and

0:35:54.960,0:35:56.640
the transformers really shine on this what

0:35:56.640,0:36:00.400
you've got thousands of words context model and you need to

0:36:00.400,0:36:04.079
retain information across all of them okay um

0:36:15.359,0:36:19.760
uh yeah so there's a quick comparison just to visualize how transformers and LSTMs

0:36:19.760,0:36:22.160
look which it's going to indicate some points

0:36:22.160,0:36:25.440
of shame before so in the LSTM you have a lot fewer connections between

0:36:25.440,0:36:28.560
the words like everything's kind of very sequential left to right

0:36:28.560,0:36:32.320
and transform but you don't have any of this um

0:36:32.320,0:36:35.760
so every word is directly connected to every other word

0:36:35.760,0:36:42.240
um i guess you should say as well in some senses maybe slightly unnatural

0:36:42.240,0:36:46.560
well for reading so it kind of suggests that however as well as reading text and

0:36:46.560,0:36:49.119
every time it reads a word it goes and really reads

0:36:49.119,0:36:56.880
every other word very quickly um but it's very effective

0:36:57.680,0:37:00.720
okay one other good thing about transformers is they do scale up

0:37:00.720,0:37:04.560
extremely well so um with tasks like language modeling

0:37:04.560,0:37:06.960
you get essentially infinite amounts of data because

0:37:06.960,0:37:11.280
it's just hundreds of billions of words out there it's far more than you'd ever

0:37:11.280,0:37:16.320
need uh that means to actually fit this kind

0:37:16.320,0:37:20.640
of distribution you need very big models and you just keep on

0:37:20.640,0:37:23.200
adding transform parameters transformers they keep on

0:37:23.200,0:37:26.880
just working better and better uh the examples i showed you forward

0:37:26.880,0:37:30.800
from this gpt-2 model with two billion parameters which was

0:37:30.800,0:37:34.800
uh quite big for 2019 but apparently 2020 or up to

0:37:34.800,0:37:39.599
17 billion and there are rumors 100 billion parameter models will be coming

0:37:39.599,0:37:43.760
along soon uh excuse me i have a question

0:37:43.760,0:37:47.440
aha you said transform is a really good for scaling up

0:37:47.440,0:37:51.119
i was just wondering in a language modeling task if we have like

0:37:51.119,0:37:54.320
say a ten thousand twenty thousand word document in an

0:37:54.320,0:37:57.520
RNN we can just insert a word step by step and we

0:37:59.680,0:38:04.400
wouldn't need uh a lot of memory per se like

0:38:04.400,0:38:07.920
for for a transformer we need to have a batch size of 10000

0:38:07.920,0:38:12.720
wouldn't we like uh like the the length of the sequence but if we have a

0:38:12.720,0:38:16.839
really long sequence can we model these long term

0:38:16.839,0:38:19.839
dependencies yeah that's a really great question um i

0:38:24.240,0:38:32.960
actually meant to mention this point so um

0:38:32.960,0:38:36.240
two things they want to say so firstly you're absolutely right the

0:38:36.240,0:38:42.000
self-attention is because of quadratic the expense obviously grows super

0:38:42.000,0:38:46.800
linearly and that is a problem in practice it means

0:38:46.800,0:38:53.440
um so mostly transformers do 512 token context

0:38:53.440,0:38:57.680
windows which is fairly affordable for gpus

0:38:58.160,0:39:00.800
i love these language models to do more than that let me about a few thousand

0:39:00.800,0:39:03.520
tokens um which is kind of limits what we can

0:39:03.520,0:39:08.320
do um but it's definitely the case that a

0:39:08.320,0:39:14.000
vanilla transformer can't model say a 50000 word book at all

0:39:14.000,0:39:20.079
um there it's like this whole cutting-edge industry recently building

0:39:20.079,0:39:25.440
various transformers which can do long sequences um it's very hot

0:39:25.440,0:39:29.520
topic right now um there's most things you can do but

0:39:29.520,0:39:31.520
one kind of thing you do is like if you replace

0:39:31.520,0:39:35.119
self-attention with something like nearest neighbor search

0:39:35.119,0:39:42.000
uh you can do the self-attention uh subquadratic time and that makes it

0:39:42.000,0:39:45.920
faster um

0:39:47.040,0:39:50.640
there are also versions that try to do kind of sparse attention where like you

0:39:50.640,0:39:53.040
can't attend to every previous word directly but

0:39:53.040,0:39:58.960
you kind of have some dilated uh set of previous words you can look at

0:39:58.960,0:40:02.320
and you don't get quite a direct connection to every word but you can

0:40:02.320,0:40:06.000
sort of guarantee that you have short paths across every word

0:40:06.000,0:40:10.800
um there are also things like compressed transformers we try to um

0:40:10.800,0:40:17.440
[inaudible] and to sort of uh compress the pre little distant passes

0:40:17.440,0:40:21.839
into shorter representations um

0:40:21.920,0:40:26.560
okay so you brought up the question of RNN so at impress time absolutely a RNN

0:40:26.560,0:40:30.160
can model infinite context where there absolutely

0:40:30.160,0:40:33.359
no additional cost which is great i mean they can put

0:40:33.359,0:40:38.400
like a million words and output really first just fine

0:40:38.400,0:40:43.839
um the regression actually is using this context and um

0:40:46.079,0:40:49.520
that's probably it's not so at training time

0:40:49.520,0:40:52.319
um you can't do this at training time you actually have to back propagate

0:40:52.319,0:40:54.160
through what's what's called back propogate

0:40:54.160,0:40:58.960
through time where the LSTM would um

0:41:00.160,0:41:03.599
have to give model one context like gradient will have to propagate all the

0:41:03.599,0:41:07.599
way back through the sort of all the recurrent steps to make

0:41:07.599,0:41:11.200
a difference to the distant pass there uh in practice firstly the gradient will

0:41:13.280,0:41:18.839
vanish like well before you hit a thousand steps and

0:41:18.839,0:41:23.040
also this is very expensive so this is at

0:41:23.040,0:41:26.640
training time system for free and this back propagation operation will

0:41:26.640,0:41:28.960
get more and more expensive the longer sequences

0:41:28.960,0:41:34.480
you're modeling so that means you can't actually really learn to

0:41:34.480,0:41:38.319
uh well this is the pass even interesting it's not expensive you just

0:41:38.319,0:41:41.280
wouldn't know what to do with this any practices

0:41:41.280,0:41:44.319
you probably just forget it anyway because the

0:41:44.319,0:41:48.960
uh you can't remember that much data once

0:41:48.960,0:41:52.640
um all right one more quick point in that um

0:41:52.640,0:41:56.160
this is interesting uh i guess one case where they'll

0:41:56.160,0:42:00.720
where the RNNs do have an advantage is on certain algorithmic tests

0:42:00.720,0:42:03.760
um so if you aren't modeling language let's say you're i'm doing something

0:42:03.760,0:42:08.560
like addition or trying to like model a parity

0:42:08.560,0:42:11.920
of a string  um

0:42:11.920,0:42:15.520
so you use string of like zeros and ones and ask you either an even number

0:42:15.520,0:42:20.880
of ones or not um in those cases

0:42:20.880,0:42:24.000
um you basically do actually want to apply literally the same operation every

0:42:24.000,0:42:27.920
time step and you can you don't actually have to

0:42:27.920,0:42:30.720
have much memory because i mean your state really just needs to be a

0:42:30.720,0:42:35.119
zero or one in this case the uh the RNNs actually do

0:42:35.119,0:42:37.839
work very well because you can train them on short sequences and they'll

0:42:37.839,0:42:40.079
show like great generalizations for long

0:42:40.079,0:42:42.960
sequences all these kinds of toy problems

0:42:42.960,0:42:46.960
and actually on tray but i imagine the transformer would actually find it much

0:42:46.960,0:42:49.920
harder to get that kind of generalization

0:42:49.920,0:42:53.520
uh that only really applies to these kind of algorithmic problems in terms of

0:42:53.520,0:43:00.000
modeling natural language then uh yeah it seems like various transformers

0:43:00.000,0:43:04.319
are going to be much more effective than recurrent nets

0:43:04.560,0:43:08.240
thank you that was really helpful um any other questions on transformers?

0:43:12.400,0:43:15.920
i've been addressing the questions i could uh via text

0:43:15.920,0:43:19.119
so i think it's we are all good right now

0:43:19.119,0:43:25.680
okay all right so um next thing we want to cover is uh

0:43:25.680,0:43:29.440
what's called decoding or inference for language models so

0:43:29.440,0:43:32.640
uh we train this language model this language model puts a

0:43:32.640,0:43:36.240
hopefully probability maths on things which good english and

0:43:36.240,0:43:42.800
uh their probability on grammatical and nonsensical things um

0:43:42.800,0:43:45.680
but if you want to like create these samples like i showed you before and how

0:43:45.680,0:43:49.920
do we actually generate stacks

0:43:50.160,0:43:56.160
um so often we think about for inference with the graphical model what we care

0:43:56.160,0:43:58.880
what we'd like to do is find the max so by creating sentences which

0:43:58.880,0:44:03.200
maximizes large world's probability um

0:44:03.200,0:44:07.520
unfortunately there's um as i mentioned before there's quite a lot of english

0:44:07.520,0:44:09.680
sentences that are possible and we can't just like

0:44:09.680,0:44:16.800
score them all to kind of max um also these models don't really

0:44:17.280,0:44:20.960
uh there's no dynamic programming here so

0:44:20.960,0:44:26.240
sometimes you can't find the max of overexponential structures when you

0:44:26.240,0:44:30.800
have a model that factorizes somewhere that lets you build a dynamic program

0:44:30.800,0:44:35.359
which lets you kind of share state across different hypotheses um

0:44:35.359,0:44:40.319
but these models don't be composed in a friendly way so

0:44:40.480,0:44:44.000
kind of whatever choice make the first word could affect all the other

0:44:44.000,0:44:46.800
other decisions all right so must given that

0:44:50.319,0:44:54.000
uh one thing to do is do greedy decoding um this is where we're just

0:44:54.000,0:44:58.079
gonna take the most likely the first word and then given

0:44:58.079,0:45:05.680
that word pretty much like second word uh guess was like a third word

0:45:05.680,0:45:11.040
um that's okay but it's um there's no guarantee that's

0:45:11.040,0:45:15.200
actually gonna meet the most likely sequence you want to output because if

0:45:15.200,0:45:19.520
you have happen to make a bad step at some point then you've got

0:45:19.520,0:45:25.760
no way of backtracking your search to undo any previous decisions

0:45:25.760,0:45:30.839
um this ledger says that exhaustive search is impossible

0:45:30.839,0:45:36.640
um and kind of hits middle ground of what's called beam search

0:45:36.640,0:45:40.160
um so beam searched is a way of trying to keep track of

0:45:40.160,0:45:43.599
an n-best list of hypotheses and then we're going to

0:45:48.560,0:45:53.200
uh just every time step try and keep track update this list

0:45:53.200,0:45:58.560
with uh you as we've added uh this may be easy to show you an

0:45:58.560,0:46:04.720
example um this slides from Abigail See at Stanford um

0:46:04.720,0:46:06.960
so uh we start we output let's say

0:46:11.760,0:46:15.040
we have the size of two well i'll put these two possible words these are

0:46:15.040,0:46:19.359
these are two most likely words now for each of these words we're going

0:46:19.359,0:46:23.599
to generate um work out what our [inaudible] like the next

0:46:23.599,0:46:26.160
words that so obviously with I or he that does affect

0:46:29.520,0:46:32.160
what the next word should be and we'll come with different hypotheses

0:46:32.160,0:46:36.560
for each of these um

0:46:39.200,0:46:43.680
and then every time we're going to kind of compress these down to

0:46:43.680,0:46:48.079
a list of uh two that we're going to continue [inaudible]

0:46:48.079,0:46:51.119
so we're looking for the lowest total sum is it correct

0:46:51.119,0:46:54.240
uh yeah so these are log likelihoods so

0:46:58.160,0:47:02.480
um we actually want the highest sum if that makes sense which should be

0:47:02.480,0:47:06.319
where zero would be a probability one oh yeah

0:47:12.880,0:47:19.440
so every score is going to be less than zero but we're going to find the

0:47:19.440,0:47:22.960
secrets with the whether some of the scores is

0:47:22.960,0:47:25.839
the highest did I make sense it is so sorry yeah

0:47:31.599,0:47:34.400
i got the the sign flipped i was many like in

0:47:34.400,0:47:38.079
magnitude we tried to get the smallest number in magnitude uh

0:47:38.079,0:47:43.520
as well as the magnitude yeah sure sorry sorry maybe this is not the most best

0:47:43.520,0:47:49.359
way to show this okay how deep does the um does this like tree

0:47:49.359,0:47:56.400
go into the beam search when do you stop looking for um like

0:47:56.400,0:48:03.839
candidate sequences i guess um all right so we basically

0:48:03.839,0:48:07.680
one detail i haven't told you is we have  this like end of sentence token

0:48:07.680,0:48:11.119
and the end of sentence token means once you once you output that then you

0:48:11.119,0:48:16.800
this hypothesis is finished um and the aim is to find complete

0:48:16.800,0:48:20.160
hypotheses so from from a start to the end token

0:48:20.160,0:48:27.599
that have the highest possible score um so

0:48:29.040,0:48:33.680
um we'll just keep on generating you hypotheses

0:48:33.680,0:48:35.920
until uh there's no possible new words for

0:48:41.040,0:48:46.800
that that would be the k complete hypothesis we have still

0:48:46.800,0:48:53.040
okay cool there's another question here why do you think in NMT very very large

0:48:53.040,0:48:56.319
beam size will most often result in empty

0:48:56.319,0:49:01.680
translations ooh uh great question um i have opinions

0:49:01.680,0:49:05.440
on this um so

0:49:06.720,0:49:09.040
um the research is good and since it's

0:49:12.640,0:49:18.079
guaranteed to give you a higher scoring hypothesis than

0:49:18.079,0:49:24.880
the uh greedy search i mentioned in the previous slide

0:49:25.040,0:49:30.160
but it's kind of a catch here which is that we're not actually

0:49:30.160,0:49:35.280
um at training time we're typically not using a beam

0:49:35.440,0:49:37.680
so um at training time we normally just use

0:49:42.880,0:49:45.760
kind of the auto regressive factorization i showed you before

0:49:45.760,0:49:49.440
where like given the sort of n previous correct

0:49:49.440,0:49:57.520
outputs predict the n plus first word um what we're not doing is

0:49:57.520,0:50:02.240
exposing the model to its own mistakes in it so when you do beam search

0:50:02.240,0:50:06.559
um you can get all kinds of nonsense showing up in your beam for whatever

0:50:06.559,0:50:11.119
reason because if you have a very big beam then

0:50:11.119,0:50:15.920
probably some of it will be garbage and in these garbage states the model has no

0:50:15.920,0:50:20.319
idea what to do because there's never training the situation

0:50:20.319,0:50:23.760
um so it's kind of reasonable to expect your model to generalize words like

0:50:23.760,0:50:28.319
making great predictions for some completely nonsensical series

0:50:28.319,0:50:32.480
of words which is like a very part of its training distribution and

0:50:32.480,0:50:37.280
in this case the model can do all kinds of weird and uncerebral

0:50:37.280,0:50:41.440
things like maybe they put a very high probability on something else

0:50:41.440,0:50:43.760
um very kind of a classic example this i

0:50:47.200,0:50:49.359
don't know if this i don't have one here but

0:50:49.359,0:50:51.920
you may have seen this language models get stuck in these feedback loops where they

0:50:51.920,0:50:56.319
end up just like repeating the same word or phrase infinitely many times i

0:50:56.319,0:50:59.359
think it's kind of example this where once the mods like start going into this

0:50:59.359,0:51:02.240
kind of loop then it doesn't really know what to do and

0:51:02.240,0:51:04.319
the easiest thing for it to carry on doing is

0:51:04.319,0:51:13.920
just keep on looping um so yeah i think

0:51:13.920,0:51:17.839
the issue [inaudible] beam search is one of uh

0:51:17.839,0:51:22.559
not exposing the model to its own mistakes at training time

0:51:22.559,0:51:26.319
so it just it put actually puts a bunch of probability amounts and all kinds of

0:51:26.319,0:51:30.000
things it shouldn't do um kind of the obvious solution to that

0:51:30.000,0:51:33.220
is like why don't you have a beam at training time um

0:51:33.220,0:51:39.599
[Distant Noises] there's uh and the short answer that is

0:51:39.599,0:51:43.359
because it's expensive uh if you guys go around this whole

0:51:43.359,0:51:46.640
inference procedure a training time that firstly gets rid of

0:51:46.640,0:51:50.800
all the nice parallelism but we have the transformer and introduces all

0:51:50.800,0:51:54.880
the search and also gives you many more things to

0:51:54.880,0:51:57.359
score for every training example

0:52:00.640,0:52:09.040
so in practice people tend to just ignore this problem and train

0:52:10.160,0:52:13.119
create a big model for as long as they can like exposing like nice fast

0:52:13.119,0:52:16.240
parallels and you get from this autoregressive version and then at test

0:52:19.359,0:52:22.079
time people will often tune the size of their beam to get the

0:52:22.079,0:52:27.440
best performance so uh i think that translation like

0:52:27.440,0:52:30.880
increasing the beam normally helps up to a point then makes

0:52:30.880,0:52:33.040
basically worse again we started covering

0:52:33.040,0:52:37.040
uh these weird degene-degenerate outputs

0:52:37.040,0:52:42.240
um but yeah that's just the unsatisfying thing people

0:52:42.240,0:52:46.240
have to do sorry that's quite a long answer that answer your question

0:52:46.240,0:52:51.839
i think uh i think this student i will see now what the student says

0:52:51.839,0:52:55.440
like uh that was a question from a student

0:52:56.079,0:52:59.680
and there is another question a small one on this current slide

0:52:59.680,0:53:04.160
why the why is the a and one in green on the right um

0:53:07.680,0:53:15.839
um i could admit i don't know i stole this slide from abby um

0:53:15.839,0:53:19.839
i'm not quite sure what point she was trying to make there okay

0:53:19.839,0:53:22.240
it's okay oh okay the point actually someone

0:53:25.280,0:53:30.400
answer because they are interchangeable regardless of the the the the one you

0:53:30.400,0:53:35.440
pick you get both outputs pie and tart

0:53:38.319,0:53:43.040
like either or you go for either you go for a or one both of them will

0:53:43.040,0:53:50.559
tell you pie tart or pie tart uh i'm not sure um i mean

0:53:50.559,0:53:54.000
even if you were to do that then you you can't compress these it's not that you

0:53:54.000,0:53:58.240
could get a dynamic program where you can sort of collapse these hypotheses

0:53:58.240,0:54:02.880
because the hidden states both these those two hypotheses would still be

0:54:02.880,0:54:06.000
different um depending on which path you took

0:54:06.000,0:54:10.480
together uh so i don't know but hopefully this

0:54:10.480,0:54:14.559
illustrates might beam search is okay okay

0:54:16.800,0:54:24.820
okay any more questions on this um

0:54:27.920,0:54:32.640
okay this is a bit more description of how the algorithm looks um

0:54:32.720,0:54:37.440
basically all you do is every time step you generate

0:54:37.440,0:54:40.480
a distribution of next words through thr hypotheses you have

0:54:40.480,0:54:45.920
and then you take the top-k hypothesis next words across all your hypotheses

0:54:45.920,0:54:50.000
and pull these before you go to the next word

0:54:50.799,0:54:54.160
all right so um all right so beam search sometimes is the

0:54:58.559,0:55:02.880
right thing to do but um often it actually isn't

0:55:02.880,0:55:06.240
uh so this is the result of applying beam search

0:55:06.240,0:55:09.599
to the example i showed you before so this example from gpt-2 about like

0:55:09.599,0:55:15.119
scientists finding unicorns in the andes um

0:55:15.119,0:55:18.400
you can see here that the model is actually

0:55:18.400,0:55:21.599
it starts outputting some good stuff and gets stuck in this weird feedback

0:55:21.599,0:55:24.480
loop um

0:55:26.720,0:55:31.599
so i think yes we're just gonna repeat the same phrase over and

0:55:31.599,0:55:34.000
over and over again i'll probably just keep on repeating this phrase

0:55:34.000,0:55:38.720
forever i mean i guess kind of what's going on here like once you said this

0:55:38.720,0:55:42.079
phrase twice maybe it's just saying the third

0:55:42.079,0:55:45.200
time is actually the most likely thing to do and

0:55:45.200,0:55:48.000
then all these other hypotheses get very high prob

0:55:48.000,0:55:52.480
transitions get right have probability even if they're not good

0:55:52.880,0:55:57.359
but there's also kind of a slightly different problem which is like maybe

0:55:57.440,0:56:02.160
in some cases we don't actually want the most likely sequence after all um

0:56:02.160,0:56:07.119
maybe what we want is something interesting so

0:56:07.520,0:56:10.720
you see this problem also a lot in like things like dialogue response generation

0:56:10.720,0:56:12.960
so you're trying to build a model to hold

0:56:12.960,0:56:16.640
conversation with someone and if you do this kind of beam search often what

0:56:16.640,0:56:20.799
you'll get is it will just give you the most generic response to

0:56:20.799,0:56:24.079
anything you say so whatever you say it'll be like oh

0:56:24.079,0:56:28.480
that's interesting thanks and maybe that actually genuinely

0:56:28.480,0:56:32.160
is the most likely thing to do because these responses are good

0:56:32.160,0:56:34.240
in most situations um however that's not

0:56:38.240,0:56:40.400
actually a very good experience or a very good

0:56:40.400,0:56:42.720
system so how about if instead of taking the um

0:56:48.079,0:56:53.200
max we're going to sample from the model distribution instead

0:56:53.280,0:56:58.559
um so this is a conceptually kind of appealing but it

0:56:58.559,0:57:02.079
doesn't actually give you very good output so

0:57:02.079,0:57:07.440
uh this is again the result sampling on that same input

0:57:07.440,0:57:09.680
um i mean so this is kind of good but then

0:57:12.880,0:57:17.359
it goes kind of uh it gets more weird and degenerate as

0:57:17.359,0:57:20.319
it goes on um

0:57:21.440,0:57:26.079
and again you get to now problem where like once you sample the bad choice

0:57:26.079,0:57:29.520
then the models in state was never in training

0:57:29.520,0:57:33.119
um and then once it's not stages in during training it's more likely to give

0:57:33.119,0:57:35.680
you support bad output and you'll get stuck in these

0:57:35.680,0:57:38.799
horrible feedback loops okay um

0:57:43.520,0:57:46.480
all right so here's something that actually does work because it was used

0:57:46.480,0:57:50.240
to get you those uh beautiful outputs we had before

0:57:50.240,0:57:54.079
um unfortunately it's not very satisfying technique but

0:57:54.079,0:57:58.240
it's a disclosure um so it's called top-k sampling it's

0:57:58.240,0:58:02.400
used introduced by uh Angela Fan a couple years ago

0:58:02.400,0:58:05.599
um basically top-k sampling what we're going to do

0:58:05.599,0:58:09.040
is truncate our distribution to just taking the k best

0:58:09.040,0:58:12.880
and then sample from that um so this advantage of giving us kind

0:58:15.599,0:58:22.319
of diversity like choosing randomly amongst like good options but tries to

0:58:22.319,0:58:27.760
stop us kind of uh falling off this kind of manifold and

0:58:27.760,0:58:32.480
actually good language but when we sample something bad um so the

0:58:32.480,0:58:35.839
[inaudible] basis is just chop off the long tail and just sample from the head of the

0:58:35.839,0:58:38.720
distribution and this is the sampling for the beam search

0:58:38.720,0:58:43.520
is it uh sorry this isn't beam search this is just um generation so

0:58:43.520,0:58:48.799
we're gonna um there's not gonna be beam here this

0:58:48.799,0:58:54.160
could be one hypothesis i guess you could it creates beam search

0:58:54.160,0:58:57.839
too but uh this is actually pure sampling so

0:58:57.839,0:59:05.680
i can generate a word in this method then use that to generate the next word

0:59:05.680,0:59:10.799
okay um so when you uh do all that then this is finally the

0:59:16.079,0:59:19.040
technique that was used during this nice sample

0:59:19.040,0:59:22.880
obviously this top-k i'm playing it's a bit of a hack it's not very satisfying

0:59:22.880,0:59:27.839
um i was um an author on that paper so i can mean about the method

0:59:27.839,0:59:36.000
but um it does seem to work quite well um i guess one thing to be aware of like

0:59:36.000,0:59:38.160
when you see these great samples and things like this which

0:59:38.160,0:59:42.000
like openly i am very happy to put in their publicity

0:59:42.000,0:59:45.040
uh it's kind of useful to know how it's actually made

0:59:45.040,0:59:48.000
and this is like this is not like a real sample from all distribution the model

0:59:48.000,0:59:54.319
is like not putting uh and it must use probability mass on

0:59:54.319,0:59:59.839
this this is generated by uh

0:59:59.839,1:00:04.160
doing this slightly weird inference procedure with the model

1:00:05.119,1:00:09.520
um i just want to quickly cover like all right so give us some text like this

1:00:09.520,1:00:13.040
how do we know if it's any good or not like how do you evaluate this

1:00:13.040,1:00:18.240
um so like it's not like evaluating language model it's quite

1:00:18.240,1:00:22.799
easy i mean it's language modeling it's task density estimation so you just

1:00:22.799,1:00:26.720
look at the log likelihood of that data

1:00:26.839,1:00:31.680
um if you want to do instead like uh take some text to model and say if

1:00:31.680,1:00:34.720
it's any good or not then uh this is not trivial and

1:00:38.079,1:00:42.880
uh it's kind of people tend to use like these automated metrics like BLEU and

1:00:42.880,1:00:45.920
ROUGE which measure like and grammar overlap with the reference

1:00:45.920,1:00:50.799
but um they're not very satisfying

1:00:50.799,1:00:55.200
and this is recent research and trying to do awesome asymmetrics

1:00:55.599,1:01:02.160
okay i should place me down for this um all right so this is um that's

1:01:02.160,1:01:08.160
unconditional language models um so they're generating samples of text

1:01:08.160,1:01:12.720
uh this actually isn't a very useful thing to do what's much more useful is

1:01:12.720,1:01:16.640
uh conditional language models so models which will give us a text

1:01:16.640,1:01:20.000
given some input generate use some output so

1:01:20.000,1:01:24.880
for example you can think about things like um

1:01:24.880,1:01:29.760
given a french sentence like translate it into english i've given a document

1:01:29.760,1:01:34.240
generate a summary given a dialogue generate the

1:01:34.240,1:01:39.280
next response or you can give it a question i'll put the answer

1:01:39.280,1:01:42.720
so this is called sequence sequence models

1:01:42.720,1:01:46.319
um because you get given some input sequence and then you have to

1:01:46.319,1:01:51.119
generate some output sequence um because the first models these are

1:01:51.119,1:01:55.359
introduced by Ilya Sutskever um look kind of like this with recurrent

1:01:55.359,1:01:58.880
neural networks where basically you'd have some encoded network which

1:01:58.880,1:02:02.160
would read your inputs produce some vector which you

1:02:02.160,1:02:05.839
modestly called a thought vector and then you'd use this

1:02:05.839,1:02:08.400
to initialize your decoder which would generate tokens word

1:02:08.400,1:02:12.240
by word again hopefully you get your theme here

1:02:12.240,1:02:16.240
like carrying these kind of bottlenecks and recurrence is not a good thing to do

1:02:16.240,1:02:21.039
uh you want to have expressive models which you might see everything

1:02:21.039,1:02:25.599
uh so there's a variation transformer for sequence sequence models

1:02:25.599,1:02:31.520
um here we're going to have a two stacks an encoder stack and decoder stack

1:02:31.520,1:02:35.039
uh basically the encoder stack is the same as what i showed you before

1:02:35.039,1:02:39.119
apart from the self attention isn't masked so every token the input can look at

1:02:39.119,1:02:42.559
every other token in the input the decoder stack will be similar except

1:02:46.559,1:02:49.200
that as well as doing the self attention over itself

1:02:49.200,1:02:57.599
it's also going to do attention over um the

1:02:58.319,1:03:02.160
complete inputs so this means that every token in output

1:03:02.160,1:03:05.520
has direct connection to every previous token in the output

1:03:05.520,1:03:10.880
um and also to every word in the inputs which makes these models very expressive

1:03:13.680,1:03:17.839
and powerful translation scores over

1:03:25.520,1:03:30.720
the previous recurrent convolutional models

1:03:30.720,1:03:33.839
so what we're training these models typically what we do is we rely on labeled

1:03:33.839,1:03:37.680
text so we uh it's a string translation system for

1:03:37.680,1:03:41.359
example you try and get lots of manually translated

1:03:41.359,1:03:45.280
text it turns out one of the best ways to get this is things like parliament

1:03:45.280,1:03:48.079
proceedings because they always translate

1:03:48.079,1:03:51.520
the european parliament or write proceedings lots of different languages

1:03:51.520,1:03:58.640
um and then you just use those as inputs and now however like

1:03:58.640,1:04:00.880
not all languages are represented in the european

1:04:00.880,1:04:07.280
parliament um so and these transformers are very data

1:04:07.280,1:04:11.680
hungry and like more texts can throw at them the best they will do

1:04:11.680,1:04:19.200
um so this question like how we use monolingual text to improve these so

1:04:19.200,1:04:23.760
this is actually saying it can we learn without just having

1:04:23.760,1:04:29.520
input output pairs uh the way we could do this is a

1:04:29.520,1:04:33.119
technique called back translation which is

1:04:33.119,1:04:37.760
quite simple conceptually that's our goal is to train a translation system

1:04:37.760,1:04:42.400
that inputs german and we'll output english

1:04:42.880,1:04:46.160
um first of all we're going to actually do the opposite we're going to train a

1:04:46.160,1:04:53.039
reverse translation model which will give an english output german

1:04:53.839,1:04:59.359
then we can then run this model over all the english texts we can find and we

1:04:59.359,1:05:02.240
can uh find a lot of english text on the

1:05:02.240,1:05:06.480
internet and we'll translate it all in german

1:05:08.079,1:05:11.359
um and that gives us like lots more pairs of

1:05:11.359,1:05:14.480
english and german text and then we're going to train a forward model or try

1:05:14.480,1:05:22.240
and translate this german into english um the next thing to see here is

1:05:22.240,1:05:28.640
that it doesn't actually matter how good the

1:05:28.640,1:05:32.000
initial model is right it doesn't matter if the your reverse model is making

1:05:32.000,1:05:35.039
mistakes um so if your reverse model makes

1:05:35.039,1:05:38.240
mistakes then your final train data will contain kind of

1:05:38.240,1:05:42.000
noisy german translates to clean english um

1:05:42.000,1:05:45.680
which might even help regularize your model but probably

1:05:45.680,1:05:48.880
shouldn't pass its performance when you show it

1:05:48.880,1:05:56.559
a clean german data what's a bitext oh i'm sorry um a bitex just means a

1:05:56.559,1:06:01.119
parallel text so the same sentences

1:06:01.119,1:06:06.559
in two different languages okay thanks

1:06:08.319,1:06:12.640
okay um and the nice thing about back translation is that

1:06:12.640,1:06:16.400
the outputs you get are actually always uh you know

1:06:16.400,1:06:20.400
high quality because these are the outputs of the system

1:06:20.400,1:06:23.039
i think to you uh sentences you found in the wild on

1:06:26.799,1:06:29.440
the internet you're just going to create some noisy

1:06:29.440,1:06:34.240
inputs that you can use for these pair of these outputs

1:06:34.799,1:06:41.119
um could you go back a slide the third point translate billions of

1:06:41.119,1:06:45.440
words of english to german is that through the reverse translation model

1:06:45.440,1:06:50.880
exactly yeah okay cool uh and you're saying back translation

1:06:50.880,1:06:54.400
helps uh generate higher quality translations

1:06:54.400,1:06:57.839
uh because of regularization is that it ah it's not just

1:06:57.839,1:07:01.119
regularization it also the really useful thing is it gives you lots

1:07:01.119,1:07:06.400
of clean output data so let's say you ought to be a good

1:07:06.400,1:07:11.599
german to english translation model you kind of need both to understand

1:07:11.599,1:07:14.960
german we'll also be able to write lots of

1:07:14.960,1:07:18.559
fluent english as well and then the english grammar too and the back

1:07:18.559,1:07:23.119
translation gives you a way of incorporating tons of

1:07:23.359,1:07:29.520
additional language data beyond what you have translations for

1:07:29.520,1:07:34.960
um okay so kind of combines translation [inaudible] language

1:07:34.960,1:07:39.119
model and you can also um iterate this procedure too so you can

1:07:43.280,1:07:49.119
uh use it use that whole [inaudible] we described before to train a better model

1:07:49.119,1:07:52.799
and then uh

1:07:54.400,1:07:57.440
do this to help you generate better translate better back translations which

1:07:57.440,1:08:01.280
you can use to train again and this can be really helpful i mean it

1:08:01.280,1:08:04.960
helps even in english german but it's particularly helpful in cases

1:08:04.960,1:08:11.359
where you don't have much data so this is on a [inaudible] to english translation

1:08:11.359,1:08:14.559
where there isn't a lot of parallel data but you can

1:08:14.559,1:08:22.560
uh get really large improvements by just iterating back translation uh this

1:08:22.560,1:08:27.279
was recent work from FAIR which i forgot to reference for

1:08:28.239,1:08:33.600
um and again here are some results on uh uh i think this is english german

1:08:35.600,1:08:40.480
showing some good improvements uh one of the direction um in machine

1:08:40.480,1:08:42.880
translation people are exploring now is a

1:08:42.880,1:08:46.080
massively multilingual mt so people are trying to

1:08:46.080,1:08:49.920
not just translate between two languages but trying to

1:08:49.920,1:08:55.120
take dozens of 100 languages and try and train a single neural network that could

1:08:55.120,1:09:02.480
translate between all of these um and if you do this you start see um

1:09:02.880,1:09:05.600
a big improvement particularly in languages where you don't have much text

1:09:05.600,1:09:08.480
presumably the model's learning some kind of

1:09:08.480,1:09:13.279
more general language independent information

1:09:13.600,1:09:18.719
okay so the last topic i want to cover in this is a really important one which

1:09:18.719,1:09:24.480
is that self-supervised learning um all right so um young is just seeing

1:09:24.480,1:09:29.140
this cake now but i think it's actually a good image for this so the idea is

1:09:31.759,1:09:36.080
um really that to learn stuff like most of the information

1:09:36.080,1:09:40.400
we need is going to be most learning we do has to be unsupervised so

1:09:40.400,1:09:43.440
we have huge amounts of text without any labels on it

1:09:43.440,1:09:48.640
and we just have a little bit of um sort of supervised training data that's

1:09:48.640,1:09:52.960
represented by the cake here being the unsupervised learning and the

1:09:52.960,1:09:56.320
supervised learning just being a little bit of icing on top of the cake

1:09:56.320,1:10:00.800
i think actually um the recent self-supervised learning from nlp has

1:10:00.800,1:10:07.920
really proved this metaphor to work okay so

1:10:09.440,1:10:12.480
um i'm going to describe quite a few methods for how you do self-supervised

1:10:12.480,1:10:15.920
learning for nlp um just so you can try and get some ideas

1:10:15.920,1:10:22.400
as to what's actually working uh so

1:10:22.400,1:10:27.520
uh the first one to describe is word2vec so the area word2vec was trying to

1:10:30.480,1:10:33.920
i think it's really the first paper that showed

1:10:33.920,1:10:38.560
uh got people excited about self-supervised learning in NLP

1:10:38.560,1:10:42.000
uh but have his previous work from uh [inaudible] which had

1:10:42.000,1:10:47.520
also shown some good gains um so the goal here is going to be trying

1:10:47.520,1:10:52.400
to learn what's called word embedding so vector space representations for words

1:10:52.400,1:10:58.800
and the hope is by that by looking at unlabeled english

1:10:58.800,1:11:02.239
text we can learn something about what these words mean

1:11:02.239,1:11:05.840
that's what the intuition behind all this is that if two words are

1:11:05.840,1:11:09.520
ok close together in the text then they're likely to have some kind of

1:11:09.520,1:11:14.560
relationship between each other um so the pre-training task we're going

1:11:14.560,1:11:17.199
to do is going to be a filling in the blanks task

1:11:17.199,1:11:22.159
so um in this sentence i'm going to mask out this word in the middle which

1:11:22.159,1:11:24.640
is unicorns and try and predict what this word should be

1:11:24.640,1:11:28.560
based on the surrounding context and hope would be that

1:11:28.560,1:11:33.520
words like unknown silver-haired and horned will somehow

1:11:33.520,1:11:37.679
are more likely to occur in the context of a unicorn than they are

1:11:37.679,1:11:41.840
i know a word like some other animal

1:11:46.640,1:11:49.280
um so basically this is gonna be a very simple model where basically we're gonna

1:11:49.280,1:11:52.880
take all these context words we're gonna apply some linear projection

1:11:52.880,1:11:57.120
to these and map all that to a fixed size context

1:11:57.120,1:12:01.760
and then just do a softmax over our vocabulary

1:12:01.760,1:12:05.120
uh so it looks a little bit like a convolutional language model the only

1:12:05.120,1:12:10.000
difference is we're predicting the word in the middle not the word at the end um

1:12:10.000,1:12:14.000
and in practice this model was um just a shallow linear projection and didn't

1:12:14.000,1:12:17.120
was not a very deep model okay um so

1:12:22.480,1:12:25.360
one of the things we find interesting about this was these um word weddings

1:12:25.360,1:12:29.920
to show some kind of surprising stretches to them uh

1:12:29.920,1:12:31.360
i'll show you how it was fun this is like

1:12:31.360,1:12:34.560
people debated about how meaningful is this um

1:12:34.560,1:12:39.040
but basically the claim was that if you took you're embedding of the word

1:12:39.040,1:12:42.000
king which you train like this and you subtract you're ready for word man then

1:12:42.000,1:12:45.920
you add the embedding of the word woman you'll get something that's fairly close

1:12:45.920,1:12:53.120
to the embedding for the word queen so somehow um

1:12:53.199,1:12:57.760
it's just this kind of unsupervised fill in the blanks learning task

1:12:57.760,1:13:01.440
isn't using this kind of linear structure with kind of meaningful

1:13:01.440,1:13:05.840
differences between vectors okay so i mean this

1:13:10.159,1:13:13.760
was great and the really good thing about this was that it's a really really

1:13:13.760,1:13:15.920
fast thing to do so you could train this on

1:13:15.920,1:13:19.600
billions of words texts you back in 2013.

1:13:19.600,1:13:22.800
um but there's a big limitation which is these word

1:13:22.800,1:13:26.640
embeddings are independent of the context so

1:13:26.640,1:13:31.040
um you get like one vector per word in your vocabulary

1:13:31.040,1:13:35.520
um but it doesn't know anything about how that word relational works

1:13:35.520,1:13:42.480
and we know that to in language like um a sentence is involved in just by

1:13:42.480,1:13:45.280
words it depends each word interacts with other words

1:13:45.280,1:13:48.719
somehow and these interactions are in some way

1:13:48.719,1:13:54.320
it's a really powerful thing so in one example

1:13:54.320,1:13:57.760
obvious example is like ambiguous words so lots of words can have many different

1:13:57.760,1:14:00.880
meanings and these word vectors won't capture

1:14:00.880,1:14:03.679
that or the best [inaudible] of all

1:14:03.679,1:14:06.080
the meanings so how do we add context to these well

1:14:09.600,1:14:15.679
um see the most obvious way is to

1:14:15.679,1:14:20.960
do a language model i think i'm missing a slide here uh basically

1:14:21.040,1:14:24.400
what we do is train a conditional language model sorry an unconditional

1:14:24.400,1:14:27.840
language model this is going to be exactly the kind of model i described

1:14:27.840,1:14:34.640
earlier in the talk and then um

1:14:34.640,1:14:39.120
given this language model the language model will be outputting hidden states every

1:14:39.120,1:14:44.560
time step predicting the next word and instead when we want to self-supervised

1:14:44.560,1:14:50.000
learning what we're going to do is replace these outputs with um

1:14:50.000,1:14:54.400
some other output that depends on our task so the pre-training phase is just

1:14:54.400,1:14:59.520
going to be predict the next word but then kind of the icing on the cake

1:14:59.520,1:15:04.239
or supervised learning will be the predicts some other property so i'll

1:15:04.239,1:15:06.159
show you an example here for a task called

1:15:06.159,1:15:10.080
speech tagging which is trying to say put some labels in every word here

1:15:10.080,1:15:15.520
so turn light label scientists as noun and distinctive as

1:15:15.520,1:15:18.719
an adjective um but you can actually fit all kinds of

1:15:18.719,1:15:24.880
tasks into this kind of framework um so for example maybe you can fit

1:15:24.880,1:15:27.920
something like this is a sentiment analysis task where

1:15:27.920,1:15:32.400
you're given some text to predict like from an amazon review and predict

1:15:32.400,1:15:36.719
the uh rating so

1:15:36.800,1:15:40.080
um this is a review that says what can i say about its banana slicer that hasn't

1:15:40.080,1:15:44.880
already been said about the wheel penicillin or the iphone and this review

1:15:44.880,1:15:47.679
got five stars so here was you're going to predict one

1:15:50.560,1:15:53.520
output from this language model which is going to be the

1:15:53.520,1:15:59.120
uh a token at the end which you can some kind of test specific

1:15:59.120,1:16:01.440
label so one of the really nice things about

1:16:04.840,1:16:11.679
this approach called gpt was that it kind of eliminated tasks

1:16:11.679,1:16:16.080
specific modeling so now suddenly we have

1:16:16.320,1:16:20.560
one model which you can pre-train and we can that fine tune that model to do

1:16:20.560,1:16:25.280
basically any task we want to do like

1:16:25.280,1:16:31.280
classification um so before this

1:16:31.280,1:16:34.320
um there's actually a few years when people were building all these crazy

1:16:34.320,1:16:37.600
architectures that you build different architectures to do build a

1:16:37.600,1:16:41.679
question answering model let's do a sentiment analysis model or

1:16:41.679,1:16:46.320
something um now you train pre-train one big model and then it's

1:16:46.320,1:16:49.199
really easy to find units do whatever you like

1:16:49.199,1:16:56.239
so that was a really big step forward um unfortunately model has kind of the

1:16:56.239,1:17:00.159
obvious limitation i said the important thing to do was kind of contextualize

1:17:00.159,1:17:04.560
words so like let words know build word representations depend on the

1:17:04.560,1:17:09.199
context but if you pre-train this language model

1:17:09.199,1:17:12.400
you can only really condition on your left word context

1:17:12.400,1:17:16.400
so your expectations for each word necessarily can't depend on the

1:17:16.400,1:17:21.440
representation for any future words

1:17:21.679,1:17:23.920
um and that kind of limits what the model

1:17:26.560,1:17:30.000
can do quite a lot um there's one kind of obvious fix to

1:17:30.000,1:17:34.719
that which is um the approach taken by elmo

1:17:34.719,1:17:38.800
um so elmo run the training one left to right language model

1:17:38.800,1:17:42.400
um also train the second language model which operates

1:17:42.400,1:17:46.400
in the reverse direction so this is like um

1:17:46.400,1:17:51.040
so it's the last word in document and it keeps trying to create the previous ones

1:17:51.040,1:17:55.840
and then you get word representations by concatenating the output layers of your

1:17:55.840,1:18:00.239
left right model and your right left model

1:18:00.560,1:18:05.199
uh so this model is in some ways better in that like now your word

1:18:05.199,1:18:09.280
representations can't condition on local context and are

1:18:09.280,1:18:13.840
right with context and that's really helpful for lots of

1:18:13.840,1:18:16.239
tasks but it's still kind of limited and that

1:18:18.560,1:18:22.320
you don't really much like interactions in these contexts so these

1:18:22.320,1:18:25.440
um you get this shallow kind of concatenation of left

1:18:25.440,1:18:29.199
representation the right representation and what you really do is having like

1:18:29.199,1:18:34.000
rich interactions between the left context and the right context

1:18:34.239,1:18:38.159
right um and all this brings me to BERT which is

1:18:38.159,1:18:44.320
uh maybe you've heard of which has made a very big difference in lp

1:18:45.520,1:18:49.679
um so bert actually looks quite a lot like word2vec

1:18:52.480,1:18:56.640
it is basically a fill in the blanks task so you take some text

1:18:56.640,1:19:00.480
you hide some tokens by masking them out and then you just try and fill it in the

1:19:00.480,1:19:04.159
mask so here's text like something is a golden

1:19:04.159,1:19:09.840
something muppets and you fill in there yeah

1:19:20.480,1:19:23.600
the thing i want you to notice is firstly it actually looks quite a lot

1:19:23.600,1:19:27.440
like word2vec um word2vec was also given some text

1:19:27.440,1:19:33.360
fill in the blanks um the reasons works much better is that

1:19:33.360,1:19:37.840
in word2vec you would just had this linear projection like

1:19:37.840,1:19:41.280
including the context words whereas you have them

1:19:41.280,1:19:46.239
a very large transformer uh which look in much more context and well much

1:19:46.239,1:19:52.560
more interactions in that context so there is a question here how are

1:19:52.560,1:19:58.800
contextual representations maintained when fine-tuning for a specific task

1:19:58.800,1:20:05.840
um how are they maintained um i guess um it's not clear they are

1:20:10.480,1:20:16.480
maintained um so when you find your particular task

1:20:16.840,1:20:21.360
you kind of let the model to learn enough general stuff that language

1:20:21.360,1:20:25.199
the pre-trained task and then doing the fine tuning probably i guess it's

1:20:25.199,1:20:29.679
forgetting a lot of stuff but it doesn't need to solve this

1:20:29.679,1:20:34.960
particular task so if you're fine-tuning on a sentiment analysis

1:20:34.960,1:20:38.400
or something it could probably probably kind of lose a lot of this

1:20:38.400,1:20:47.679
information during fine tuning that seems fine thanks

1:20:47.679,1:20:50.719
okay um so BERT it worked very well um they gave

1:20:55.600,1:20:57.600
like quite large improvements on a bunch of tasks

1:20:57.600,1:21:01.440
um it was actually achieving the performance of humans or at least

1:21:01.440,1:21:05.360
humans approximated by Amazon Mechanical Turk

1:21:05.360,1:21:10.719
on a bunch of important uh question answering benchmarks

1:21:10.719,1:21:14.560
but um BERT was definitely not the end of the story here um

1:21:14.560,1:21:18.719
but i've lots of people very excited about self supervised training

1:21:18.719,1:21:25.360
um so just to quickly summarize details there so

1:21:25.760,1:21:30.400
it's very simple model is just going to mask out 15 of the tokens

1:21:30.400,1:21:33.760
and try and fill in the masks um to build on that um doesn't work at

1:21:39.840,1:21:48.480
Facebook Yinhan Liu which looked at scaling up bert um

1:21:49.040,1:21:53.679
so can you tell the combination actually actually has second pre-training

1:21:53.679,1:21:56.719
objective which we showed didn't actually help

1:21:56.719,1:22:00.000
so can i read can i ask a question for the slide

1:22:00.000,1:22:03.280
so there there were three bars i think i missed one point

1:22:03.280,1:22:08.239
the the dark blue what is the dark blue so we have amazon turk there

1:22:08.239,1:22:12.880
thank you sorry i said that um dark blue hair is previously that so

1:22:12.880,1:22:18.000
okay which was um these models were probably elmo which is

1:22:18.000,1:22:22.159
oh yeah okay so previously i was definitely using

1:22:22.159,1:22:27.920
self-supervised training but uh but improved relevant by having

1:22:27.920,1:22:32.719
this kind of like i see and GLUE actually it's a benchmark

1:22:32.719,1:22:37.840
that we've been uh creating here at nyu exactly some students were involved

1:22:37.840,1:22:40.880
yeah so yeah glue is a big benchmark it's very

1:22:40.880,1:22:45.199
important okay um so to beat BERT it turned out all

1:22:45.199,1:22:49.679
you had to do was firstly simplify training objective then just

1:22:49.679,1:22:54.480
scale it up so scaling up here means bigger batch sizes

1:22:54.480,1:22:58.400
uh huge numbers of gpus more free training text and

1:23:01.679,1:23:06.639
then um you get very large gains on top of

1:23:10.880,1:23:16.800
work so yellow here is then this new roberta model

1:23:17.280,1:23:22.320
and uh actually roberta the question answering is like

1:23:22.320,1:23:26.880
superhuman by quite a few points and also on this GLUE benchmark from nyu is

1:23:26.880,1:23:31.199
also uh outperformed people

1:23:31.679,1:23:34.719
and this isn't really about doing anything smart it's just

1:23:34.719,1:23:39.040
taking self supervised training and doing it more

1:23:39.040,1:23:44.159
alright um how why why do you say like if you go on this slide here so there is

1:23:44.159,1:23:48.560
a very large improvement between uh bert and then roberta in the GLUE

1:23:48.560,1:23:52.639
but not such a huge uh change maybe in the squad or is

1:23:52.639,1:23:58.239
it just a zooming factor i'm like um

1:23:58.239,1:24:03.440
maybe it's just a zooming factor right those bars on the left are taller maybe

1:24:03.440,1:24:06.000
yeah i think maybe the scales distorting i think the

1:24:09.600,1:24:14.000
point is if compared to human performance um

1:24:14.880,1:24:19.760
i know um bert was 0.6 points best of the people whereas

1:24:19.760,1:24:23.520
roberta is uh three and a half points better

1:24:23.520,1:24:28.880
so by that metric is actually quite a big jump

1:24:31.040,1:24:36.400
okay uh so let's quickly discuss a few of the other

1:24:36.400,1:24:41.280
things people have been doing in self-supervised training and

1:24:41.280,1:24:47.040
so there's one called XLNet now um basically so input when you predict all

1:24:47.040,1:24:50.159
the mask tokens you predict all the masks conditionally

1:24:50.159,1:24:54.000
independently um XLNet has a trick lets you

1:24:54.000,1:24:59.280
predict these mass tokens auto regressively but in random order

1:24:59.280,1:25:02.400
and they claim some improvements from doing that

1:25:02.400,1:25:07.280
it's also uh SpanBERT which uh rather than masking out words you're gonna mask

1:25:07.280,1:25:11.280
out a sequence of consecutive words um there

1:25:11.280,1:25:16.080
is ELECTRA which um rather than

1:25:16.080,1:25:19.360
masking words we're going to substitute words with

1:25:19.360,1:25:23.120
sort of similar ones and then have a binary classification problem to tell

1:25:23.120,1:25:27.280
you which word's changed or not uh

1:25:27.280,1:25:34.639
it's albert which is um bert but you tie the weights across layers um also

1:25:34.639,1:25:39.040
um XLM and XLMR which looking to do this multilingually it turns out

1:25:39.040,1:25:42.560
basically you run the BERT pre-training objective but rather's

1:25:42.560,1:25:46.320
feeding in english text you feed in text in every language you can

1:25:46.320,1:25:51.600
find it does a great job learning cross-lingual structure

1:25:52.840,1:25:58.480
um the key point what you take from this is

1:25:58.480,1:26:02.560
really like these are all kind of variation

1:26:02.560,1:26:09.920
um there's but in the end like lots of different

1:26:09.920,1:26:12.560
things work and the important thing is you have

1:26:12.560,1:26:15.920
big models you have bi-directional interactions between the words

1:26:15.920,1:26:19.520
and you uh if anything the scale you do this

1:26:19.520,1:26:23.840
is the most important thing um so

1:26:27.600,1:26:31.760
one limitation these models i described is they're only doing kind of text

1:26:31.760,1:26:37.840
classification problems but often we'll want to do

1:26:38.960,1:26:41.280
problems where the output isn't the classification problem is actually

1:26:41.280,1:26:45.199
output some more text uh pre-training for sequence sequence

1:26:45.199,1:26:49.840
modeling uh two papers came out about the same

1:26:49.840,1:26:52.960
time for this one which i was involved in um

1:26:52.960,1:26:57.600
called BART and T5 um so these models basically are going to

1:27:00.080,1:27:04.800
pre-train sequence sequence models by denoising text

1:27:04.800,1:27:08.159
um the tree training objective looks kind of like that basically all you're

1:27:08.159,1:27:14.560
going to do is take some text corrupt it somehow by

1:27:14.560,1:27:17.760
applying some kind of masking scheme and then

1:27:17.760,1:27:19.920
rather than predict fill in the blanks you're going

1:27:19.920,1:27:25.280
to feed the corrupted text into a seq2seq model and try and predict

1:27:25.280,1:27:28.480
the complete outputs and um

1:27:32.560,1:27:36.080
and this is kind of nice because then you can actually go beyond just masking

1:27:36.080,1:27:41.040
text you can apply like any random uh corruption thing that you want

1:27:41.040,1:27:43.679
so for example you can shuffle sentences or delete whole

1:27:43.679,1:27:48.400
phrases or it's a new phrases and the seq2seq framework

1:27:48.400,1:27:52.639
is very flexible um but it turns out just doing simple

1:27:52.639,1:27:56.880
masking actually works about as well as anything else

1:27:57.920,1:28:01.760
um and then if you do this and as well as like it

1:28:01.760,1:28:05.679
doing well things benchmarks like squad and GLUE which are classification benchmarks

1:28:05.679,1:28:10.639
you can also get [inaudible] results on um

1:28:10.639,1:28:17.440
tasks like summarization so um where the output is a text so

1:28:19.840,1:28:22.480
on the left here we've got some lengthy document fed in we ask them all

1:28:22.480,1:28:27.600
to produce some kind of summary of this and uh it is a great job like actually

1:28:27.600,1:28:30.239
uses context from across the whole document and solves things like

1:28:30.239,1:28:33.920
co-reference and generally as far as you can tell seems

1:28:33.920,1:28:37.360
to show some understanding of the inputs um okay we're running out of time

1:28:42.800,1:28:48.880
um so briefly um i don't think it's the end story like

1:28:48.880,1:28:52.960
the nlp is now solved um a few open questions which i think

1:28:52.960,1:28:56.639
are interesting including how do we integrate like

1:28:56.639,1:28:58.719
background knowledge into this do we just want

1:28:58.719,1:29:02.800
these models to try and memorize the whole internet or should we

1:29:02.800,1:29:06.320
build memory mechanisms somehow uh as someone brought up earlier how do

1:29:08.239,1:29:12.080
we model long documents we're typically doing 512 tokens here

1:29:12.080,1:29:17.199
how can we do say a whole book at once

1:29:17.199,1:29:21.600
um one unsatisfying thing about this is like we have like the same model

1:29:21.600,1:29:24.960
architecture can solve all kinds of problems but it tends to not be able to

1:29:24.960,1:29:27.280
solve all these problems at once and typically

1:29:27.280,1:29:30.000
you fine-tune a separate model for each task

1:29:30.000,1:29:34.480
it would be great if you actually have like one model solves everything

1:29:34.719,1:29:41.520
um and um it's kind of related to that like we

1:29:41.520,1:29:44.960
basically get human level performance the main task where you have say a hundred

1:29:44.960,1:29:49.120
thousand uh labeled examples you're learning from

1:29:49.120,1:29:51.440
but can we build models will do well with

1:29:51.440,1:29:56.880
like 1000 or 10 or 1 labeled examples

1:29:57.280,1:29:59.760
and finally that people bring these questions that's where these models are

1:29:59.760,1:30:02.560
just actually understanding language i was

1:30:02.560,1:30:09.760
really good at breaking benchmarks okay so to wrap up this

1:30:09.760,1:30:16.180
uh lecture um i think the main takeaways from this uh um

1:30:18.480,1:30:21.760
these kind of low bias models like transformers work great we shouldn't try

1:30:21.760,1:30:24.000
to explicitly model linguistic structure we

1:30:24.000,1:30:26.719
should have very expressive models and show them lots

1:30:26.719,1:30:32.560
of text and let them learn whatever structure they need um

1:30:32.560,1:30:39.199
predicting words and texts is a great uh unsupervised learning objective um

1:30:39.199,1:30:42.239
but if you want to understand what's crucial to like incorporate words in

1:30:42.239,1:30:46.239
context in particular bi-directional context

1:30:46.239,1:30:50.719
okay so that's all i have to thank you very much for listening

1:30:50.800,1:30:54.480
let's see if we have some questions now i think there will be

1:30:54.480,1:30:59.600
several thank you mike yeah there's a whole bunch of

1:31:01.840,1:31:06.560
discussions while we while you're talking okay links to various papers

1:31:06.560,1:31:10.080
and explanations of various concepts in the background

1:31:10.080,1:31:16.239
okay um any more questions uh yeah i had one um on one of the open questions

1:31:16.239,1:31:18.719
it's like understanding whether or not these

1:31:18.719,1:31:24.560
models are actually uh understanding the language

1:31:24.560,1:31:28.639
what are some ways that exist right now to quantify and measure that

1:31:28.639,1:31:37.199
um and how like how how would we do that yeah okay so

1:31:38.960,1:31:43.199
what typically happens is someone says i know

1:31:43.760,1:31:47.520
these models aren't understanding language if they could they'd be able to solve this

1:31:47.520,1:31:49.600
new task that they're introducing then they

1:31:49.600,1:31:54.560
introduce some new tasks which they can't do and then

1:31:54.560,1:31:57.199
say bert isn't doing it then next week

1:31:57.199,1:32:00.000
someone trains a bigger model and then actually gets human performance

1:32:00.000,1:32:05.440
on this task and i think really

1:32:05.920,1:32:10.000
what's happening is like some people just have intuitions that

1:32:10.639,1:32:15.840
this kind of these neural networks can't be understanding language

1:32:15.840,1:32:19.040
and they must just be gaming data sets somehow

1:32:19.040,1:32:23.120
and the extent that these models do well there must be kind of weird biases in

1:32:23.120,1:32:26.080
our data sets and make the moles can exploit without really

1:32:26.080,1:32:30.639
understanding anything um and it's definitely true that like a

1:32:30.639,1:32:35.760
lot of data sets do have biases in them and like it's

1:32:37.679,1:32:42.639
kind of hard to make ones that don't do it takes a lot of skill but

1:32:42.639,1:32:46.080
on the other hand it's like people are failing to produce good counter examples

1:32:46.080,1:32:51.679
so what these models can't do like basically

1:32:51.679,1:32:55.280
sorry it's a good example in recent times was the

1:32:55.280,1:32:59.840
winograd schema results where you know winograd schemas are those

1:32:59.840,1:33:03.280
sentences that are ambiguous and there is a reference there is a

1:33:03.280,1:33:06.960
pronoun that refers to one of the words and you can tell

1:33:06.960,1:33:10.239
which word the pronoun refers to unless you know something about how the word

1:33:10.239,1:33:14.719
works right so the standard example is uh the trophy

1:33:14.719,1:33:17.120
doesn't fit in the suitcase because it's too large

1:33:17.120,1:33:20.560
or the trophy doesn't fit in the suitcase because it's too small

1:33:20.560,1:33:24.000
and you know in in one case the the trophy

1:33:24.000,1:33:27.520
refers to the the the pronoun refers to the trophy in the other case it refers

1:33:27.520,1:33:30.639
to the the suitcase and there was a list of

1:33:30.639,1:33:34.800
those and people created a data set of those things and

1:33:34.800,1:33:39.520
until about two years ago the best results were around 60

1:33:39.520,1:33:46.000
for computers humans do 95 or something but um the best uh uh

1:33:46.000,1:33:49.040
the best you know artificial systems were getting about 60

1:33:49.040,1:33:52.400
and now i think it's about 90 percent yeah

1:33:52.400,1:33:55.280
right yeah something like that right you don't even get any training dates for

1:33:55.280,1:33:59.199
these this is just a purely unsupervised

1:33:59.199,1:34:04.320
problem right and so so the question is you know it's clear

1:34:04.320,1:34:07.360
that those systems have learned something about

1:34:07.360,1:34:11.679
uh the role of objects and you know a little bit about how the world works

1:34:11.679,1:34:17.840
by just observing statistics about text but it's relatively superficial it's not

1:34:17.840,1:34:21.280
um i mean you know as it's pretty obvious

1:34:21.280,1:34:25.440
when you look at text that's generated uh you know we're talking about unicorn

1:34:25.440,1:34:29.280
and then the first sentence is uh unicorns has four horns right which of

1:34:29.280,1:34:31.679
course doesn't make sense because unicorns have only one

1:34:31.679,1:34:34.800
that's kind of the point of being a unicorn so um

1:34:34.800,1:34:39.199
so the whole idea of i mean the whole con problem of learning common sense

1:34:39.199,1:34:42.719
uh has not been solved very far from it but those systems but they work

1:34:42.719,1:34:46.239
surprisingly well i mean it's surprisingly it's surprising how far you

1:34:46.239,1:34:49.490
can go with just you know looking at text

1:34:49.490,1:34:52.239
uhum yeah learaning common sense is super

1:34:52.239,1:34:55.199
hard because in some sense the things you want to learn

1:34:55.199,1:34:58.639
are things that aren't written down like no one ever writes down

1:34:58.639,1:35:02.639
common sense knowledge yeah probably no one ever writes down like a unicorn has

1:35:02.639,1:35:08.560
exactly one corn and not for i mean it's just because everyone knows that um

1:35:08.560,1:35:12.080
so probably there are limitations to what you can learn from just

1:35:12.080,1:35:15.679
looking at text but can you can you tell us something about the

1:35:15.679,1:35:18.560
the word i mean about some of the work you're doing on sort of grounded

1:35:18.560,1:35:24.880
language uh sure yeah so i mean um

1:35:24.880,1:35:28.800
i put nothing back grounding in this um but there's a whole

1:35:28.800,1:35:34.000
interesting topic and so i think it's really interesting trying

1:35:34.000,1:35:37.280
[inaudible] people don't use text because

1:35:37.280,1:35:41.119
it's like it's a representation of the world um

1:35:41.119,1:35:46.080
so one type of guy's really interested in like trying to like kind of

1:35:46.080,1:35:50.800
ground dialogue usage and goals so um run some people like chit chat systems

1:35:50.800,1:35:54.080
that talk to each other or talk to people to have a conversation

1:35:54.080,1:35:59.600
like can you build systems where you try to achieve some goal um we

1:35:59.600,1:36:01.840
had some work a couple years ago and doing a

1:36:01.840,1:36:05.679
this in the context of negotiations so trying to

1:36:05.679,1:36:10.080
um there's two of you which are trying to have a conversation just kind

1:36:10.080,1:36:12.560
of crank on some agreement with each other

1:36:12.560,1:36:16.159
um and the only way you can come to agreement is by

1:36:16.159,1:36:19.199
having a dialogue in natural language and there's like some outcomes which are

1:36:19.199,1:36:22.880
good for use and what's good for them you have to find some compromise

1:36:22.880,1:36:26.800
um and then yeah i'm kind of interested in the setting

1:36:26.800,1:36:29.119
because um yeah it seems like

1:36:33.520,1:36:36.400
you actually using language for a purpose is gonna be really crucial to a really

1:36:36.400,1:36:41.199
understanding things like i think

1:36:41.199,1:36:43.440
that's going to get hints like there's limits to what we'll learn from just

1:36:43.440,1:36:47.760
like purely observational use of language and so just purely

1:36:47.760,1:36:51.119
seeing text out there in the world that other people wrote like really

1:36:51.119,1:36:54.960
to understand things you want to be agents which

1:36:54.960,1:36:57.840
are using language to try and choose some goal and interact with people and

1:36:57.840,1:37:00.960
seeing what works and learning from that kind of signal as

1:37:00.960,1:37:05.280
well um maybe that's it's just the cherry on the

1:37:05.280,1:37:08.800
icing on the cake or whatever but i think that's

1:37:08.800,1:37:13.760
it is that you need a cherry more questions from the audience come on

1:37:19.360,1:37:26.080
guys don't be too shy uh i have a question uh

1:37:26.080,1:37:29.199
the first point on the open questions how should we

1:37:29.199,1:37:33.760
integrate world knowledge so uh the way i i was thinking about is that

1:37:33.760,1:37:37.280
these billion parameter transformers have

1:37:37.280,1:37:40.719
so much information about the world in them and then if we

1:37:40.719,1:37:47.119
try to fine-tune or train this model on some new data could we like forget

1:37:47.119,1:37:51.600
some of the earlier concepts that uh this model had learnt and how would

1:37:51.600,1:37:56.400
we like quantify what concepts like

1:37:56.400,1:38:00.719
has the model forgotten apart from the validation set

1:38:00.719,1:38:05.360
does that make sense um yeah so probably you will forget i mean

1:38:05.360,1:38:07.840
probably if you fine-tune this model to do

1:38:07.840,1:38:11.280
something that doesn't need well knowledgeable happily forget

1:38:11.280,1:38:15.600
all the knowledge you told it um i mean there's some evidence these models

1:38:15.600,1:38:19.920
are like memorizing quite a lot of facts so there's a remarkable

1:38:19.920,1:38:24.080
uh result recently from this paper this google system called T5 which i think is

1:38:24.080,1:38:27.280
12 billion parameters and it's just trained in a

1:38:27.280,1:38:29.679
self-supervised way and if you have people

1:38:29.679,1:38:34.320
and then you um just fine tune to answer questions

1:38:34.320,1:38:37.760
which could be questions about anything but you don't show it any

1:38:37.760,1:38:40.560
you don't show up wikipedia or something to let you know you just like see what's

1:38:40.560,1:38:43.360
memorized and you can test how much knowledge is in

1:38:43.360,1:38:48.320
the model from that and um it's not safety after that but it

1:38:48.320,1:38:52.320
like it's kind of scarily good it's like

1:38:52.320,1:38:55.360
it turns out if you have 12 billion parameters you train for a long time you

1:38:55.360,1:38:58.800
can just fit huge numbers of facts into these

1:38:58.800,1:39:01.520
levels i'm not sure it's like the most

1:39:01.520,1:39:05.199
desirable way necessarily to memorize knowledge but

1:39:05.199,1:39:13.840
um seems somewhat effective

1:39:14.080,1:39:17.679
okay that was it thank you everyone for attending

1:39:17.679,1:39:22.320
thank you so much mike for uh for giving a guest lecture

1:39:22.320,1:39:26.560
um it's good to hear that stuff from the horse's mouth

1:39:26.560,1:39:33.600
and we'll see see everyone again next uh next week

1:39:33.600,1:39:35.440
we're gonna we'll actually tomorrow right tomorrow we're gonna be

1:39:35.440,1:39:39.040
implementing everything from scratch don't forget so tomorrow you get the

1:39:39.040,1:39:41.520
gory details from the code side of all of this

1:39:41.520,1:39:45.199
and then uh monday you'll hear from you'll hear about

1:39:45.199,1:39:50.400
graph neural nets from Xavier Bresson actually about graph neural net and

1:39:50.400,1:39:53.920
graph knowledge is that is graph knowledge used for language as

1:39:53.920,1:39:57.440
well somehow because right i i don't know i have not really

1:39:57.440,1:40:00.320
uh knowledgeable about this part of the

1:40:00.320,1:40:03.520
field well so you can view to some extent you can view

1:40:03.520,1:40:06.080
all the those supervised training in text

1:40:06.080,1:40:10.800
uh like word2vec uh bert et cetera they use a graph and the graph is

1:40:10.800,1:40:15.119
how uh you know how often two words appear next to each other

1:40:15.119,1:40:18.320
or some distance away from each other in the text

1:40:18.320,1:40:22.480
that's you know the graph of similarity between words basically is determined by

1:40:22.480,1:40:26.080
how far you know how often they're they appear nearby

1:40:26.080,1:40:30.000
yeah that's that's when you decide to put them in the input of a neural net

1:40:30.000,1:40:34.159
because they appear you know within the sequence so you can think of

1:40:34.159,1:40:37.760
those those those are supervised learning systems as

1:40:37.760,1:40:40.960
basically a very simple form of graph neural net

1:40:40.960,1:40:44.239
well the graph always has the same structure it's always linear but and it

1:40:44.239,1:40:47.280
always uh indicates your your neighbors in a

1:40:47.280,1:40:49.600
text all right thank you so much everyone

1:40:52.719,1:40:58.000
have a good evening bye-bye

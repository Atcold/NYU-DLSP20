0:00:01.480,0:00:10.040
okay okay okay so today we're gonna be sharing the screen okay sure screen sure

0:00:10.040,0:00:17.900
boom and here we go alright so foundations of deep learning that's me

0:00:17.900,0:00:23.990
follow me on Twitter yay of course okay so today we're going

0:00:23.990,0:00:28.460
to be talking about attention and specifically there are two types of

0:00:28.460,0:00:33.440
attention there is self attention or cross attention and there's going to be

0:00:33.440,0:00:41.390
also heart attention or soft attention okay but in generally generally when we

0:00:41.390,0:00:45.469
talk about attention we're gonna be talking about dealing with sets

0:00:45.469,0:00:51.199
so since I'm gonna give you a small preview Transformers are made of

0:00:51.199,0:00:57.590
attention modules transformers are going to be something that Maps sets to sets

0:00:57.590,0:01:01.219
they don't really deal with sequences they can write a sequence can be thought

0:01:01.219,0:01:08.150
as a order set but they don't necessarily need to have order sequences

0:01:08.150,0:01:14.570
that's the super cool part all right so let's get started let's think let's

0:01:14.570,0:01:21.530
start first with self attention so we have like a set of X's this my pink axis

0:01:21.530,0:01:28.990
which are X subscript I with I goes from 1 to t right so you have t different

0:01:28.990,0:01:37.310
elements in this set x1, x2, …, xt So now we can think about each

0:01:37.310,0:01:42.710
of these X is is belonging to ℝⁿ right that's our classical representation we

0:01:42.710,0:01:49.550
have seen so far right so it's like n dimensional vector so all there is to

0:01:49.550,0:01:56.030
this self attention is that my H my hidden representation is going to be a

0:01:56.030,0:02:03.560
linear combination of these vectors right and so I guess if you remember

0:02:03.560,0:02:09.500
from one of the classes early on like I think the fourth class I show you how

0:02:09.500,0:02:14.000
you can do like a quick way like a quick notation for a right in this

0:02:14.000,0:02:21.110
linear combination of vectors rain and it's simply by using matrix

0:02:21.110,0:02:28.220
multiplication so this set of x's which are t x in dimension n can be

0:02:28.220,0:02:34.790
thought as a matrix capital X in pink that has n rows no because the height of

0:02:34.790,0:02:41.240
these guys are n and then has t columns right so you can think about that is

0:02:41.240,0:02:47.650
basically the stack the horizontal stack of these vectors right so this is my set

0:02:47.650,0:02:50.780
in which again you have an order right now

0:02:50.780,0:02:57.140
and so now this hidden representation can be written as what the matrix X

0:02:57.140,0:03:04.610
times the vector of those alphas which I'm going to be calling a in a bold bold

0:03:04.610,0:03:13.100
typeset okay so in this case my H is the X matrix times a it's a bit funky right

0:03:13.100,0:03:18.320
usually when we think about the hidden representation we have a rotation of my

0:03:18.320,0:03:27.230
input right with the weight matrix but in this case is like huh what is this is

0:03:27.230,0:03:31.640
the rotation of the attention so it doesn't kind of I can't really think

0:03:31.640,0:03:35.450
that way so I prefer to think about this as you know the linear combination of

0:03:35.450,0:03:42.500
these columns of X range so the linear combination of those X is cool so we can

0:03:42.500,0:03:48.650
write it there top right just to keep it in in memory right now we can think

0:03:48.650,0:03:55.579
about hard attention if we impose that the zero norm of these a vector it's

0:03:55.579,0:04:01.640
equal one and the nonzero term equal one as well so which means a is a one hot

0:04:01.640,0:04:07.220
encoded vector and therefore what has happen if you multiply these x times a

0:04:07.220,0:04:13.340
one hot vector you can select the you know the specific where the one is

0:04:13.340,0:04:18.470
there's going to be that specific column right so if the second element of a is

0:04:18.470,0:04:25.070
equal one and all the others are zero when you multiply a capital X time

0:04:25.070,0:04:30.320
which is 0-1 or later or the others are zero you can adjust retrieve the column

0:04:30.320,0:04:37.160
second column right so you can see now how this can work out right so this

0:04:37.160,0:04:44.720
attention just pays attention to one element of your set and one element only

0:04:44.720,0:04:51.020
in the case that we are when we are talking about heart attention but we can

0:04:51.020,0:04:55.400
also have a different attention which is the soft attention in the soft attention

0:04:55.400,0:05:01.700
instead the constraint is that the summation of these elements all by a the

0:05:01.700,0:05:05.360
alphas they have to sum to one okay so that's the difference

0:05:05.360,0:05:10.180
and in this case H is going to be just you know a dinner combination of these

0:05:10.180,0:05:17.150
the columns of the matrix capital X right so far could rain there is no

0:05:17.150,0:05:25.040
we're stuff right okay so second part attention number two we're gonna figure

0:05:25.040,0:05:30.440
out where these a is coming from right and so in this case a as I as you can

0:05:30.440,0:05:37.340
figure out is going to be the arc max or the soft dark max of these capital X

0:05:37.340,0:05:43.640
where I transpose it so that you mean every row is one sample and then I

0:05:43.640,0:05:48.770
compute the scalar product between the the row and the vector let every element

0:05:48.770,0:05:53.750
in the final product every element is going to be the scalar product of each

0:05:53.750,0:06:01.490
and every you know a vector X I that was from 1 to T against these specific X

0:06:01.490,0:06:08.990
okay but so is it is it clear so far so before we said what's beta beta is

0:06:08.990,0:06:13.730
the parameter for the soft arc max write the inverse of the temperature whenever

0:06:13.730,0:06:18.260
you know soft dark mass right the expo of the argument divided by the sum of

0:06:18.260,0:06:24.290
the X and then inside the X we have beta right where does beta come from yeah and

0:06:24.290,0:06:28.880
that's the whenever you have these soft arc max or what people call soft max

0:06:28.880,0:06:33.530
there is always a beta parameter usually it's set to 1 so you don't see it

0:06:33.530,0:06:43.639
so before we saw that my capital X okay I didn't okay I didn't

0:06:43.639,0:06:50.120
talk about yet so the big eggs right big X is this set of columns Tata Tata Tata

0:06:50.120,0:06:54.320
and then the H is gonna be a linear combination of these columns but where

0:06:54.320,0:07:00.230
are these alphas coming from right and so my given alpha my first vector a is

0:07:00.230,0:07:08.030
going to be you know each and every basically column of X so which means

0:07:08.030,0:07:11.870
every row in the X transpose u multiplied by X right and you get the

0:07:11.870,0:07:17.900
scalar product of these two guys so you get how much my X what is the value of

0:07:17.900,0:07:25.510
the scalar product of my X against every other value every other X in my set okay

0:07:25.510,0:07:33.169
is X a column of vector yeah X is one vector size n okay this

0:07:33.169,0:07:40.820
like it was written before X is a generic X right so my square bracket

0:07:40.820,0:07:45.860
represents a optional argument so you can have either the Arg max and

0:07:45.860,0:07:53.090
therefore you get a one hot a or if you have this soft arcamax which is the the

0:07:53.090,0:07:56.120
one that we've been using all the course so far you are going to get the

0:07:56.120,0:07:59.930
exponential divided by the summation of the old exponential right that's the

0:07:59.930,0:08:05.570
classical one that the one that people call softmax usually all right so but

0:08:05.570,0:08:11.510
then we said that we have a set of X's right so if you have a set of X's these

0:08:11.510,0:08:15.560
will imply that you have a set of AIDS right because for every X you're gonna

0:08:15.560,0:08:21.680
have an A and so if you have these many aids that are vectors right you can

0:08:21.680,0:08:26.780
stack them one after each other tan-tan tan tan-tan and you get capital A and so

0:08:26.780,0:08:34.430
capital a it's gonna have the height it's gonna be of size t right because

0:08:34.430,0:08:41.180
the size t so you know the lowercase a has size T just because you're gonna

0:08:41.180,0:08:48.200
have t rose in x transpose right and then you stuck t's of them right because

0:08:48.200,0:08:53.200
you have t x's I hope it's clear and so far all right

0:08:53.200,0:08:58.790
cool know what's missing right now so what's missing is the following so given

0:08:58.790,0:09:04.280
it now we have a set of the ai therefore you're gonna have a set of

0:09:04.280,0:09:09.740
ai right if you see from the top right side of this slide you have H was this

0:09:09.740,0:09:16.520
you know matrix multiplication between the capital X and the a right so H was

0:09:16.520,0:09:24.470
the linear combination of the columns of my X matrix right but given that we have

0:09:24.470,0:09:31.010
many aids we are going to end up with many ages right so how many H is the

0:09:31.010,0:09:36.440
same number of A's right and so you're gonna have these capital X matrix which

0:09:36.440,0:09:42.740
is gonna have many many columns right hey sorry so small X in the soft max

0:09:42.740,0:09:47.240
equation that belongs to ℝⁿ right that's one of the one of the columns of

0:09:47.240,0:09:52.670
X yeah yeah so I would you can call that xi and I would have a would be ai

0:09:52.670,0:09:57.710
right about I remove the index so it becomes a bit less cluttered okay but so

0:09:57.710,0:10:04.880
X is T by n right so X transpose should be n by T so X is going to be n by T

0:10:04.880,0:10:11.360
grain Oh X is n by T also X is gonna be the the stack of all these columns one

0:10:11.360,0:10:18.170
of three at each other right okay yeah I saw there are two options you can think

0:10:18.170,0:10:22.820
about X being the the set of rows and this I think what's done usually in code

0:10:22.820,0:10:27.830
but I think this is much easier to write down in this way if you write the math I

0:10:27.830,0:10:33.020
mean I prefer okay and so again given that we have many AIDS you're gonna have

0:10:33.020,0:10:40.190
many columns of the H matrix right and so we can simply write that my capital H

0:10:40.190,0:10:45.830
which is the subset of these H's is going to be the linear combination of

0:10:45.830,0:10:51.600
this element of X using the factors like in the first

0:10:51.600,0:10:57.600
column of a and the second column of a third column right so that was pretty

0:10:57.600,0:11:05.100
much it about attention so you basically mix the components of this set of X's

0:11:05.100,0:11:09.300
which we can represent as a matrix by using these coefficients that are

0:11:09.300,0:11:15.150
computed by using the Arg max of the soft argh max where each component

0:11:15.150,0:11:21.510
inside we can call this stuff inside here a score it's simply the scalar

0:11:21.510,0:11:30.540
product of 1 given X against all the sets of my exes ok so this is the first

0:11:30.540,0:11:36.980
part of the lecture it should be clear until here otherwise we can move forward

0:11:36.980,0:11:43.950
so is it clear so far or is not explained the first equation again for

0:11:43.950,0:11:51.180
one what is soft Arg max and value a multiplying big eggs with eggs right so

0:11:51.180,0:11:56.550
in the in the previous slide we simply said there is one line only here

0:11:56.550,0:12:01.800
basically and we say that the hidden layer it's gonna be a linear combination

0:12:01.800,0:12:10.200
of these X's right yes and this in a combination is using these alphas which

0:12:10.200,0:12:15.090
is contained in this vector a yes and so I wrote this one here right so H is

0:12:15.090,0:12:20.580
gonna be the linear combination of the columns of the X where the column of X

0:12:20.580,0:12:26.790
is gonna be the elements in my set yes ok so if we got this one we go to the

0:12:26.790,0:12:33.320
second slide and I tell you how to compute one a ok so to compute one a

0:12:33.320,0:12:40.770
here this one a is going to be for example the soft dark max which is again

0:12:40.770,0:12:47.850
people call the soft max of what so you have a specific X here right when you

0:12:47.850,0:12:52.950
compute the product between these X transpose X transpose is gonna have all

0:12:52.950,0:12:59.220
those X's in rows right so let me draw this one right so X transpose gonna have

0:12:59.220,0:13:04.589
like first simple second guy third guy right and then I do this one

0:13:04.589,0:13:09.600
against my guy here right so if you do matrix vector multiplication the first

0:13:09.600,0:13:14.209
item is going to be this color scalar product of the first guy against myself

0:13:14.209,0:13:20.939
then the second guy against myself and the third guy against myself right then

0:13:20.939,0:13:24.689
I'm you get here for example three scores right you're gonna see how close

0:13:24.689,0:13:31.170
how aligned basically your vector is with respect to the three items in my

0:13:31.170,0:13:36.749
set and then I around the soft Arg max right so I get these three values you

0:13:36.749,0:13:42.629
get at the end one value two second value third value at the end is gonna

0:13:42.629,0:13:50.970
sum to one right yes I'll talk about it it's not sure and you can have either

0:13:50.970,0:13:54.989
one you can have the soft calc max you have the exponents of blah blah blah or

0:13:54.989,0:13:59.699
you can get the arc max right and you get which is basically sending the beta

0:13:59.699,0:14:05.040
to very large values right so you can just simply writes off target max and

0:14:05.040,0:14:15.119
have large beta I'm still confused why is this a vector in ℝᵗ which on line

0:14:15.119,0:14:24.829
defining a yes army okay so X transpose is gonna be my axis right

0:14:25.110,0:14:31.519
direction right so this guy here is my X transpose

0:14:34.760,0:14:37.760
okay and this length here this is gonna be N

0:14:42.460,0:15:00.510
and this is gonna be T so if you do T times this guy which is also n right

0:15:02.640,0:15:07.430
you're gonna get a vector of steep right

0:15:08.480,0:15:15.060
does it make spell though is a are one hot batter a can be a one hot if you use

0:15:15.060,0:15:20.310
arc Max or is going to be you know a softer version if you use this soft dark

0:15:20.310,0:15:29.819
max right which is the X divided by the sum of the X prime well when you

0:15:29.819,0:15:34.680
multiply a matrix by a vector you get a vector so if you're taking an arm R max

0:15:34.680,0:15:40.319
over that you should get a scaler no no the Arg max is gonna give you the index

0:15:40.319,0:15:47.220
ring like the one hot vector corresponding to the the vector you can

0:15:47.220,0:15:51.870
think about art mass art max is giving you like a one where you have the

0:15:51.870,0:16:02.699
maximum and all the rest are going to be 0 okay yeah does it make sense oh yeah

0:16:02.699,0:16:11.129
so if you have like 3 7 9 to a vector of for the arc max you can give you 0 0 1 0

0:16:11.129,0:16:18.389
no it's gonna give you the one at the position where the maximum is okay all

0:16:18.389,0:16:24.329
right well it's beta Skyler right now you can think is that being equal to 1

0:16:24.329,0:16:28.910
there is no need here oh this one right now okay for a little bit of additional

0:16:28.910,0:16:36.029
certification is our the xi terms are those are those one hot vectors that

0:16:36.029,0:16:45.420
represent our like our input xi are sorry xi are just my input doesn't

0:16:45.420,0:16:50.660
have to be one hot but they could be if they represent

0:16:50.660,0:16:55.610
words or no I think they are usually they are embeddings so they are actually

0:16:55.610,0:17:03.860
dense so I don't think they're okay then the X transpose by X is that kind of

0:17:03.860,0:17:10.459
multiplying that's that's determining the similarity between okay so this is

0:17:10.459,0:17:17.689
determining how similar each element in my set of X's it's similar to my X right

0:17:17.689,0:17:24.079
so this a tells me how these all of these guys here are similar with respect

0:17:24.079,0:17:29.450
to this guy over here okay I think cool cool cool

0:17:29.450,0:17:33.260
all right so this was the first part let's move on and see how these can be

0:17:33.260,0:17:37.720
improved and like expanded so here we have a definition the key

0:17:46.450,0:17:50.590
value store this is about data structure just to give you a little bit of

0:17:50.590,0:17:56.110
definition right so this is a paradigm for storing saving retrieving Aquarion

0:17:56.110,0:18:01.870
or managing an associative array or a dictionary or a hash table what does it

0:18:01.870,0:18:08.350
mean so for example you want to type some you know question on like let's say

0:18:08.350,0:18:15.040
you want to check a video about how to make a lasagna on YouTube okay so you're

0:18:15.040,0:18:20.230
going YouTube you write lasagna lasagna whatever you press enter so you have a

0:18:20.230,0:18:25.810
query and then the query is gonna be checking against all possible keys in

0:18:25.810,0:18:30.160
your data set and the keys could be like the title of the video or the

0:18:30.160,0:18:35.500
description right and so you check how a line is your query against all those

0:18:35.500,0:18:41.770
titles that are available in the YouTube data set right when you find the maximum

0:18:41.770,0:18:46.870
matching score you can retrieve that one right so if you do the arc max you just

0:18:46.870,0:18:51.940
retrieve one video otherwise if you do these soft arc max you can get basically

0:18:51.940,0:18:56.560
a probability distribution right and then you can do retrieve in order right

0:18:56.560,0:19:01.240
you can retrieve first the most aligned video that you have you know you can

0:19:01.240,0:19:07.060
have a sequence of less and less relevant videos right so is it clear so

0:19:07.060,0:19:13.840
far about what is this key value store paradigm right so you just have a query

0:19:13.840,0:19:19.540
which is your question given one query you check all the keys and you can find

0:19:19.540,0:19:25.450
how matching the key is with respect to your query and then you retrieve all

0:19:25.450,0:19:32.410
those videos all those values right all those content so we're gonna do exactly

0:19:32.410,0:19:37.150
the same here in in this case right so we're gonna be

0:19:37.150,0:19:42.730
specializing a little bit what we have seen so far which was pretty you know

0:19:42.730,0:19:48.640
trivial the key here would be the title of the video yeah that's correct right

0:19:48.640,0:19:53.130
so the keys are the title of all the videos in YouTube you have one query

0:19:53.130,0:19:58.010
lasagna okay how to cook lasagna is I've and I you

0:19:58.010,0:20:05.240
check this question against all the keys and then when you find you know the art

0:20:05.240,0:20:09.380
max you find the index of the highest you retrieve that one right or again if

0:20:09.380,0:20:13.550
you do the soft dark max you're gonna get like probabilities now you can sort

0:20:13.550,0:20:21.710
by probability for example right so we have now queries keys and values so what

0:20:21.710,0:20:31.010
are these well these are simply rotation all my specific input X right so my Q is

0:20:31.010,0:20:38.000
going to be getting my X here I rotate it by Wq then my key is going to be

0:20:38.000,0:20:44.240
again my x I rotated by Wk and then I have my value of v sorry I'm gonna be

0:20:44.240,0:20:52.310
rotating the X by a Wv okay so so far we just added three matrices more and at it

0:20:52.310,0:20:58.040
so this is like how you can you know add so much more well we finally add some

0:20:58.040,0:21:02.720
training parameters right so so far we didn't have any training board

0:21:02.720,0:21:11.690
parameters so far what would be D would be X in the lasagna metaphor X is me

0:21:11.690,0:21:19.190
very hungry and so me very hungry and goes and try to write some question

0:21:19.190,0:21:27.020
about how to get the food done okay but then given that I also know how to cook

0:21:27.020,0:21:30.110
I can also check against all okay there you go

0:21:30.110,0:21:34.760
right so the the about cooking lasagna all right so I'm hungry and it would be

0:21:34.760,0:21:39.980
my hacks X so my query would be what is the best recipe I can find and then I

0:21:39.980,0:21:46.310
can check in my memory right in my own head all the possible lasagna recipes I

0:21:46.310,0:21:50.240
have all my mother's cookbook right so I can check check check check check and I

0:21:50.240,0:21:55.670
saw my grandmother lasagna and then I retrieved the recipe from my granny

0:21:55.670,0:22:02.620
which is you know amazing makes sense right I'm hungry

0:22:02.620,0:22:07.940
there a reason why we don't add nonlinearities here yeah yeah so this is

0:22:07.940,0:22:15.620
just these attention thing it's completely based on orientation right

0:22:15.620,0:22:19.940
we're gonna just check in what is the orientation of these vectors so this is

0:22:19.940,0:22:24.290
how attention works okay we don't like nonlinearities the only non-linearity is

0:22:24.290,0:22:29.330
whenever you try to get the probability you know distribution right the the soft

0:22:29.330,0:22:34.550
dark max make sense okay cool all right so you're on board so again we

0:22:34.550,0:22:38.780
first now introduce these learning parameters such that we can train

0:22:38.780,0:22:46.370
something right machine learning yay okay so Q and K have to have the same

0:22:46.370,0:22:51.140
length right the same dimensionality because you're gonna check one query one

0:22:51.140,0:22:55.820
question how to make lasagna against all the possible representation of the

0:22:55.820,0:22:59.600
titles right so they have to have the same length because otherwise you can't

0:22:59.600,0:23:06.680
check the orientation right it'd have to be in the same space V that's V is the

0:23:06.680,0:23:11.090
content of my recipe right I don't care about the length it can be you know five

0:23:11.090,0:23:17.540
pages of my of the recipe right so it's just you know that's the whole recipe

0:23:17.540,0:23:22.460
right so V in my case is huge and the key is the representation of the title

0:23:22.460,0:23:26.500
which match the size of the representation of the question right

0:23:26.500,0:23:32.780
cool let's make things you know simple and down bend down everything we just

0:23:32.780,0:23:40.520
said D prime equal D second equal to D right so everything is just D all right

0:23:40.520,0:23:45.230
so we said we have a sequence we know sequence right and if I'm wrong we have

0:23:45.230,0:23:50.960
a set of X's right so given that we have a set of X's therefore you're gonna get

0:23:50.960,0:23:57.830
a set of queries you get a set of keys and a set a set of values named and yes

0:23:57.830,0:24:02.780
as you can imagine you're gonna get a matrix which is you know stuck in all

0:24:02.780,0:24:07.490
those columns of all the cues that are down all the case started at a time and

0:24:07.490,0:24:11.540
all the values down Bam Bam Bam Bam so how many columns you have you have T

0:24:11.540,0:24:15.740
columns right because you stuck T vectors right what is the height of the

0:24:15.740,0:24:19.619
vectors well d right because you we just said

0:24:19.619,0:24:26.320
mmm quickly so what next so before we said that my a was these

0:24:26.320,0:24:33.580
soft dark max over the arc max of ha now you're gonna check one query against

0:24:33.580,0:24:40.749
all these keys right so we tilt first the K such that I have my Rose right tan

0:24:40.749,0:24:45.519
tan tan tan and then I multiply my first row my first key four times my query

0:24:45.519,0:24:50.529
second key times the K query third key blah blah blah right how many

0:24:50.529,0:24:56.739
rows I have I have T lowercase T right so at the end you have T scores and now

0:24:56.739,0:25:01.769
you compute the soft dark max and therefore we get the probability right

0:25:01.769,0:25:07.529
hmm I think it makes sense I mean it makes sense to me I spend a bit of time

0:25:07.529,0:25:17.100
but yeah does it make sense to you know yes what's the difference in Q and K

0:25:17.100,0:25:24.279
okay he represents the key which is the title in the recipe recipe book right q

0:25:24.279,0:25:29.619
is gonna be my question I want to make lasagna and then I check all the titles

0:25:29.619,0:25:35.200
of my recipe book like how to make pizza how to make pasta how to make ravioli

0:25:35.200,0:25:39.789
how to make tortellini how to make polpettone how to make lasagna hey

0:25:39.789,0:25:47.409
there you go you get a high score there you retrieve that one okay cool so I

0:25:47.409,0:25:52.239
understand why the query is being derived the biotransformation of X but I

0:25:52.239,0:26:01.269
don't understand why K and V are also being derived from X there is a value so

0:26:01.269,0:26:07.149
in your analog II V would be a video so why do we derive it from yes it says

0:26:07.149,0:26:12.100
you're completely right and so that's gonna be the next slide but this one is

0:26:12.100,0:26:17.350
called self attention so you are actually doing a retrospective work

0:26:17.350,0:26:21.700
you're actually thinking in your head I want to make lasagna so let me think

0:26:21.700,0:26:28.389
even harder what are the recipes I can make and aha found it then I have my

0:26:28.389,0:26:32.740
recipe so everything is my head and given that my head is X I just have

0:26:32.740,0:26:37.540
the three things coming from my head and this is called self attention right

0:26:37.540,0:26:40.840
otherwise if you're a bit more normal than me

0:26:40.840,0:26:45.910
and you just go I get a recipe book you're gonna get that the question comes

0:26:45.910,0:26:49.390
from your ex or your brain right but then the keys and the values come

0:26:49.390,0:26:53.230
from the book right so keys and values you're gonna get them somewhere else and

0:26:53.230,0:26:58.000
there's gonna be the cross attention right and yeah you check your query

0:26:58.000,0:27:04.240
against all those kids and they you're three of the final thing all right so

0:27:04.240,0:27:10.810
what's next next is going to be the fact that yeah we said a hidden layer is

0:27:10.810,0:27:16.480
gonna be my linear combination of these V's known these V columns that are

0:27:16.480,0:27:22.860
making my matrix again weighted by these coefficients alphas that are inside my a

0:27:22.860,0:27:29.130
okay so this is exactly the same as we have seen before but I just specified

0:27:29.130,0:27:36.190
specialized yet specialized those instead of using all the time X and you

0:27:36.190,0:27:40.240
know X transpose I have now keys Aquarians and values right queries keys

0:27:40.240,0:27:46.410
queries that one side and then keys and by just on the other side um Alfredo

0:27:46.410,0:27:52.210
yesterday and lecture you and professor Lacan says there is only one maybe like

0:27:52.210,0:27:57.700
one Curie but multiple you know one curium multiple keys and value so here

0:27:57.700,0:28:05.170
one means there's only one Curie for 1x I like 1qi current 1x I but it has to

0:28:05.170,0:28:11.020
interact all the other keys right right so exactly that was okay thank you for

0:28:11.020,0:28:15.010
reminding me that's so the point here in this line the last line I show you here

0:28:15.010,0:28:20.440
you have one Q in one Q this is one question how to make lasagna you're

0:28:20.440,0:28:25.270
gonna check how these matches all the titles in the book right so one question

0:28:25.270,0:28:30.520
is gonna be going through all the titles Tata Tata ton in order to find where the

0:28:30.520,0:28:34.690
correct title is right it's a one question how to make lasagna and you

0:28:34.690,0:28:38.260
check how to make pizza how to make pasta how to make tortellini how to make

0:28:38.260,0:28:43.000
polpettone a name you retrieve the one that the one that actually matches right

0:28:43.000,0:28:46.720
so you have one you check that our many keys and then

0:28:46.720,0:28:52.750
you retrieve the value of the one that matches or if you actually have two high

0:28:52.750,0:28:59.770
scores you can make a mixture of recipes right and then I don't know how well

0:28:59.770,0:29:05.310
that interpolate does it make sense someone was talking before by two

0:29:09.940,0:29:17.610
recipes right so if I check check how to make lasagna and then in my book I have

0:29:17.610,0:29:22.450
lasagna with the e at the end and then I don't know some other thing that sounds

0:29:22.450,0:29:27.160
very similar so maybe if the the word you're looking for or let me think

0:29:27.160,0:29:31.210
another word like let's say I want to make pizza but then there is another is

0:29:31.210,0:29:36.460
a recipe which is called chicken pizza Allah with Allah is like like with pizza

0:29:36.460,0:29:40.570
but like similar with pizza but if I look for pizza also Pizza Allah is gonna

0:29:40.570,0:29:46.900
be having like a higher matching score right and so if I make pizza if I take

0:29:46.900,0:29:50.860
the art max it's gonna work fine if I take the soft dark max I'm gonna get a

0:29:50.860,0:29:55.360
combination between pizza and pizza Allah because they are similar so you're

0:29:55.360,0:30:00.250
gonna get some you know mass probability mass cameo so in the other guy and then

0:30:00.250,0:30:03.130
when you retrieve the values the values are going to be again the linear

0:30:03.130,0:30:08.800
combination of those columns using multiplied by these coefficients and so

0:30:08.800,0:30:13.420
if you have a one hot you're gonna get just one value but then if there are

0:30:13.420,0:30:18.370
multiple values which are you know known like if you have like a soft dark max

0:30:18.370,0:30:23.680
you may have you know several values that are gonna be mixed together and you

0:30:23.680,0:30:29.380
say we got two very close candidates that has high a value say then it's

0:30:29.380,0:30:35.560
pizza and Yola but here my Curie the Q is only one right so Q is gonna be pizza

0:30:35.560,0:30:41.170
but then inside the key there are two recipes which is pizza and pizza Yola

0:30:41.170,0:30:46.350
and these two are very similar so both of them the score will be somehow

0:30:46.350,0:30:51.460
similar right and so whenever you do the soft arc max you don't exactly get only

0:30:51.460,0:30:56.020
one very high score maybe gonna get a high score and then another high score

0:30:56.020,0:30:58.980
so H is gonna be the linear combination of

0:30:58.980,0:31:04.160
these recipes are gonna be like the average of the pizza and the pizza Ola

0:31:04.160,0:31:07.560
got it okay yeah I truly make sense thank you

0:31:07.560,0:31:13.980
of course there was another question no okay maybe we should move on because we

0:31:13.980,0:31:22.530
have 15 minutes left okay again how many queues do we have we have T Q's right we

0:31:22.530,0:31:25.320
have many kills and therefore all hold on

0:31:25.320,0:31:33.320
so what's beta beta we like to set that to 1 over square root of D why is that

0:31:33.320,0:31:39.750
because if you have a vector of all ones in one dimension the length of a vector

0:31:39.750,0:31:45.360
of one has is one in two dimensions the vector which is all come coordinates one

0:31:45.360,0:31:50.160
is going to be square root of 2 a vector that has all components one in the three

0:31:50.160,0:31:55.800
dimension is going to have square root of three right if you have four

0:31:55.800,0:32:00.150
components can be square root of that so you a vector in D dimension you're gonna

0:32:00.150,0:32:04.710
have that the magnitude grows with the square root of the number of dimensions

0:32:04.710,0:32:10.260
and so in order to keep the temperature constant of these soft arc max we want

0:32:10.260,0:32:14.400
to divide by the square root of the number of dimensions again technicality

0:32:14.400,0:32:20.730
doesn't matter if you don't get it again how many queries we have T and therefore

0:32:20.730,0:32:28.140
we can get T AIDS right and therefore we get a matrix capital a so finally you're

0:32:28.140,0:32:33.420
gonna get a big H which is simply the multiplication of these values by this

0:32:33.420,0:32:40.200
matrix of you know where the columns are going to be the mixing components and it

0:32:40.200,0:32:47.600
was pretty much it about cross cross attention could you just say exactly why

0:32:47.600,0:32:53.280
queue and care of the same dimension but V is their friend I would expect key and

0:32:53.280,0:32:58.130
V K and V to be of the same dimension but Q to be something else

0:32:58.130,0:33:03.960
V is gonna be my pizza recipe right it's gonna be 10 pages long oh no can make

0:33:03.960,0:33:10.050
one page long instead Q and K is the question and the title and they have to

0:33:10.050,0:33:13.200
match because I'm gonna comparing like I'm gonna compare what is

0:33:13.200,0:33:20.250
the matching you know degree right how these two are aligned this comes from

0:33:20.250,0:33:27.270
this thing over here right so whenever I check my query against all keys

0:33:27.270,0:33:32.640
I do the roll times vector and they have to be having the same size the same

0:33:32.640,0:33:37.200
length right otherwise I can't multiply so you check your question against all

0:33:37.200,0:33:42.390
these keys you know you get a sports yeah I think that matrix and then you

0:33:42.390,0:33:46.920
get the recipe which can be the whole YouTube video or meat cooking whatever

0:33:46.920,0:33:53.210
right right okay okay thank you sure anytime

0:33:53.210,0:33:59.520
I'm hungry okay okay so one implementation detail known for example

0:33:59.520,0:34:04.980
to make things faster and we can stack all those doubles in one whole W right

0:34:04.980,0:34:10.650
and then BOOM you compare you can you compute all Q and K and D in one one one

0:34:10.650,0:34:16.230
second right in one iteration nothing fancy Ryan we have done the same

0:34:16.230,0:34:21.330
before for the RNN right remember we were stuck in the X and the H and

0:34:21.330,0:34:25.620
computing these with the stock version of the W reign on the top right hand

0:34:25.620,0:34:33.300
side okay so nothing fancy well then there is something else called heads so

0:34:33.300,0:34:38.940
this can represent one head but we may have multiple heads for example H heads

0:34:38.940,0:34:47.610
so if I have h heads I'm gonna have h qs and I have age case I'm gonna have H B's

0:34:47.610,0:34:51.600
and so you're going to end up with a thing that is going to be H times taller

0:34:51.600,0:34:56.460
right but then you can still get it back to whatever dimension if you multiply at

0:34:56.460,0:35:01.620
this big guy at the end now this final big vector which is gonna be the vector

0:35:01.620,0:35:05.880
of the B's right at the end you can multiply by a matrix to make it back to

0:35:05.880,0:35:12.600
this size of D this is a possible way of implementing this stuff right but again

0:35:12.600,0:35:17.580
details not important so let's get finally to this transformer what the

0:35:17.580,0:35:22.170
heck is this transformer so the originary this transformer is made by

0:35:22.170,0:35:26.710
two blocks is a made of encoder and a decoder okay so where have

0:35:26.710,0:35:31.540
we seen before is this encoder decoder architecture

0:35:31.540,0:35:36.680
right in the janitors out encoders there you go so recap from out encoders right

0:35:36.680,0:35:40.490
so out encoders we had the diagram on the left today we're going to be

0:35:40.490,0:35:43.880
focusing the diagram on the right hand side you have two blocks you have an

0:35:43.880,0:35:49.700
encoder and I have a decoder the encoder Maps the X to the hidden representation

0:35:49.700,0:35:54.290
and then the decoder mapped the hidden representation to these again

0:35:54.290,0:35:59.540
input again but we don't have to I mean we have these two main major components

0:35:59.540,0:36:03.710
right in the app out encoder and so in this case we're gonna have something

0:36:03.710,0:36:09.140
similar more or less so this is the transformer encoder which is the purple

0:36:09.140,0:36:14.270
block right so in these guys in this guy we're gonna have the self attention cool

0:36:14.270,0:36:19.700
we already know what it is in the upper part we're going to be running a

0:36:19.700,0:36:25.790
basically a linear layer here every component so if you think about a

0:36:25.790,0:36:31.220
convolutional a convolution with the kernel size of one you basically apply

0:36:31.220,0:36:37.609
the same linear layer to every element in the in the set right sometimes they

0:36:37.609,0:36:41.510
call it this you know feed-forward but it's going to be a feed-forward applied

0:36:41.510,0:36:44.630
to every element in the set so it's actually a convolution where the

0:36:44.630,0:36:50.630
convolutional kernel is equal one then we apply some module which we can call

0:36:50.630,0:36:54.200
this ad and norm both after both of these guys

0:36:54.200,0:36:59.270
what is this module here so this guy is basically a box here which has two

0:36:59.270,0:37:05.000
components has a addition component and then has a layer normalization okay and

0:37:05.000,0:37:09.680
so if we connect this guy here on the right hand side you're gonna get that

0:37:09.680,0:37:15.260
the self attention basically has a residual connection but like that is

0:37:15.260,0:37:19.400
bypassing it and then the layer normalization and the same happens for

0:37:19.400,0:37:24.530
the other guy on top so also the the convolutional part has these residual

0:37:24.530,0:37:29.900
connection and be a layer normalization so how does it work over all this stuff

0:37:29.900,0:37:35.570
you wanna basically put the set of inputs at the bottom and then you make

0:37:35.570,0:37:40.880
it blah blah blah bubble up right and you get the hidden representation at the

0:37:40.880,0:37:44.750
output of the encoder right so this hEnc
0:37:44.750,0:37:50.300
thing fancy right I just put two blocks we have seen the self attention just

0:37:50.300,0:37:55.220
before the one deconvolution us actually know how it works right is a applying

0:37:55.220,0:37:59.900
just you know the single multi-layer perception to every component in this

0:37:59.900,0:38:04.550
set and then normalization helps you know getting those gradients coming back

0:38:04.550,0:38:09.100
later on and the receiver connection connection makes everything smooth

0:38:09.100,0:38:15.830
alright so this was the encoder how do we how what is the decoder in this case

0:38:15.830,0:38:21.590
so let me clean up a little bit so let me remove first the encoder box let me

0:38:21.590,0:38:25.790
remove that old zone Teledyne let me actually even remove these X on the

0:38:25.790,0:38:30.860
bottom and the final out the drain so this was the encoder but now I'm gonna

0:38:30.860,0:38:36.560
delete the connection there in the center poof okay and so now we're going

0:38:36.560,0:38:42.020
to be talking about the decoder so the decoder is exactly like the encoder but

0:38:42.020,0:38:47.390
I'm gonna have a cross attention like some one of you was mentioning before of

0:38:47.390,0:38:51.350
course right they were you were asking me why the heck are you checking these

0:38:51.350,0:38:56.960
keys from yourself so this cross attention gets connected right after

0:38:56.960,0:39:02.930
these normalization module right and of course cross attention gets these hidden

0:39:02.930,0:39:08.330
representation from the last layer of the encoder and then what else you're

0:39:08.330,0:39:12.350
gonna have the same stuff so the addition and normalization you connect

0:39:12.350,0:39:16.070
this stuff and then finally you plug it back right so we have one extra module

0:39:16.070,0:39:20.930
and this is gonna be my decoder right so the decoder is like the encoder but has

0:39:20.930,0:39:27.440
this additional module sandwiched between these previous between it could

0:39:27.440,0:39:31.190
you say more about the cross attention cross attention is exactly the self

0:39:31.190,0:39:38.819
attention but you have the fact that my keys like this

0:39:38.819,0:39:47.699
X here and this guy here are no longer X but these are gonna be my age from the

0:39:47.699,0:39:57.089
encoder right that's it and coder and this was the set right so

0:39:57.089,0:40:05.950
I still like the set of hi all the same and whatever

0:40:05.950,0:40:09.970
try it with the mouse okay so this is exactly the same where I just replaced

0:40:09.970,0:40:16.869
the X's with the final hidden representation from the encode array so

0:40:16.869,0:40:23.109
that's it this guy here is gonna be providing me the values and the keys so

0:40:23.109,0:40:27.520
we still use the original axis to compute the query and then you use the

0:40:27.520,0:40:35.859
HS to compute the so the this one you're gonna get these the X to compute the the

0:40:35.859,0:40:40.630
the D queries here here I compute the queries and this one allows me to

0:40:40.630,0:40:47.349
compute the K keys and then the values okay so how does it work how do we train

0:40:47.349,0:40:51.400
this stuff right so what we get there on the bottom from the bottom there we're

0:40:51.400,0:40:58.210
gonna have the output these ŷ from the previous iteration and so you're

0:40:58.210,0:41:02.320
gonna get a output there at the output of the of the system maybe there is like

0:41:02.320,0:41:06.820
some additional layer missing there and then in an autoregressive fashion you're

0:41:06.820,0:41:11.290
gonna get this output you feed it back you know there's some additional layer

0:41:11.290,0:41:15.640
on top of it it doesn't matter I mean it's not important and then you put it

0:41:15.640,0:41:20.050
back and then you you know you out aggressively output a sequence of

0:41:20.050,0:41:24.640
outputs right and then every time you have a new input this new input will ask

0:41:24.640,0:41:28.599
for a different query and a different query is going to be asking about

0:41:28.599,0:41:34.900
different values from the encoder right so the encoder basically summarized what

0:41:34.900,0:41:43.869
is the content of my input set right so before we saw that this guy here yeah so

0:41:43.869,0:41:48.940
here we have a input set and then the output of this guy is going to be a set

0:41:48.940,0:41:53.230
of hidden representation and then you have the decoder is going to be querying

0:41:53.230,0:42:02.050
what is required through this cue from this set of representations from the

0:42:02.050,0:42:07.930
encoder okay and we really have to go to the notebook because otherwise we have

0:42:07.930,0:42:14.200
no time left so import stuff whatever so here we have the multi-head attention

0:42:14.200,0:42:19.869
right how does this multi-head attention work so in the init

0:42:19.869,0:42:26.950
part we're gonna have these three matrices Wq, Wk, Wv that are allowing

0:42:26.950,0:42:34.869
me to rotate my current input right and then we have this one that is allowing

0:42:34.869,0:42:42.190
me to merge together the heads at the end and so how does this forward works

0:42:42.190,0:42:47.920
right so in the forward you're gonna get a input X for the query and input X for

0:42:47.920,0:42:53.049
the keys and an input for the values and now you have the Q the K and the V are

0:42:53.049,0:43:01.119
simply you know the multiplication of my input for the specific item multiplied

0:43:01.119,0:43:05.380
by this matrix Wq right so this is the rotation of my X and so here you have

0:43:05.380,0:43:13.089
the Q K and V then you're going to compute this scale dot product so we can

0:43:13.089,0:43:17.019
go and see this on scale dot product which are basically the dot product

0:43:17.019,0:43:24.190
between the one question against all the keys so if we go up here I can't even

0:43:24.190,0:43:33.999
scroll sorry let me zoom a little one second okay so

0:43:33.999,0:43:39.849
here you get these arrays so here you get basically the discourse okay first

0:43:39.849,0:43:42.880
we divide by the square root of the dimension because we said before

0:43:42.880,0:43:47.589
otherwise stuff starts exploding then we have a matrix multiplication between one

0:43:47.589,0:43:53.140
query against all these keys right and then at the end we apply the soft dark

0:43:53.140,0:43:58.150
max right such that we can compute the squares they I'm sorry did we can

0:43:58.150,0:44:04.720
compute compute the mixing mixing coefficient right and then finally you

0:44:04.720,0:44:12.099
multiply these mixing coefficients with the V matrix you get basically the final

0:44:12.099,0:44:18.910
output right and that was pretty much it this was the self attention right and

0:44:18.910,0:44:22.420
then finally since you have multiple heads we're going to be squashing

0:44:22.420,0:44:27.789
everything together by using this final Wh so that was the first part they

0:44:27.789,0:44:32.710
dubbed the attention are there questions on these attention I mean if you follow

0:44:32.710,0:44:37.000
this light this is exactly the same okay just let me know

0:44:37.000,0:44:44.560
if you need me to go slower on this part then on the bottom part so what do we

0:44:44.560,0:44:50.560
need we have the attention part so this is the self attention right so what what

0:44:50.560,0:44:55.240
I what are we trying to do right this is a multi-headed attention so what are we

0:44:55.240,0:44:59.020
trying to do here we are going to be just using an encoder to classify some

0:44:59.020,0:45:04.119
sentences which are sentences described in some movies has been a positive

0:45:04.119,0:45:08.590
review or a negative review so I'm just gonna be using the encoder and then

0:45:08.590,0:45:12.880
train this encoder to perform a classification task so what do we need

0:45:12.880,0:45:18.790
for the encoder the encoder if we check from these slides so if we check here

0:45:18.790,0:45:23.619
what do we need for the encoder the encoder has two components right there

0:45:23.619,0:45:27.760
is the self attention which we just saw the code and then we had this

0:45:27.760,0:45:32.260
convolution right this MLP multi-layer perceptron applied to every element in

0:45:32.260,0:45:38.140
the set so let's figure out where this convolutional layer is some sanity check

0:45:38.140,0:45:45.190
hold on this stuff is gonna be online by the end of today so there is this

0:45:45.190,0:45:48.520
encoder layer and the encoder layer has this much attention plus the

0:45:48.520,0:45:52.180
convolutional net and the convolutional net you pretty know pretty much know how

0:45:52.180,0:45:56.109
it works right so you can actually figure out if you check the PyTorch

0:45:56.109,0:46:01.260
documentation that the linear and then linear acts is a one-dimensional

0:46:01.260,0:46:05.830
convolution so you can use you know nn linear here so this is like a

0:46:05.830,0:46:10.480
convolutional net you have a convolution the real o and then you have the final

0:46:10.480,0:46:16.570
convolution okay I think I'm not running because I think you already know how to

0:46:16.570,0:46:21.460
make convolutional net and then we have the tool layer normalization right so

0:46:21.460,0:46:26.770
first you have multi-head attention it's a self attention so we provide X X and X

0:46:26.770,0:46:33.130
for all the inputs right then use a stupid question but why call it a

0:46:33.130,0:46:38.859
convolution at all is it's just a no okay it's not it's not stupid it's very

0:46:38.859,0:46:45.820
important because a linear layer would be mapping you know some representation

0:46:45.820,0:46:51.490
into some other representation is applied to every component in the set

0:46:51.490,0:46:57.550
right so I have a set here right yeah a set of inputs and over here I'm gonna

0:46:57.550,0:47:07.210
have a set of representation right so I have a set here yeah then I apply the

0:47:07.210,0:47:13.570
linear layer to every element separately okay mm-hmm so if you apply the same

0:47:13.570,0:47:18.150
linear layer to every element in a sequence Oh

0:47:18.150,0:47:24.850
Bhushan right okay so everyone could each each each hidden representation is

0:47:24.850,0:47:29.650
going to be M going through the one contribution separately yeah so in the

0:47:29.650,0:47:35.500
in the original paper they call it a linear layer but it's not because it's

0:47:35.500,0:47:39.220
actually a convolution right so again every every time you're gonna see the

0:47:39.220,0:47:42.850
every implementation is gonna use a linear layer all of them are going to be

0:47:42.850,0:47:48.490
calling a feed-forward but are these are convolution it does some broadcasting

0:47:48.490,0:47:52.300
but I can eat a convolution the same way we call soft arcamax

0:47:52.300,0:47:56.320
and we called with I don't call it soft max because you know it's it's wrong

0:47:56.320,0:48:05.460
right okay but it's very good question I'm almost done I understand okay so

0:48:05.460,0:48:11.290
this is my convolutional net and then you have multi-layer attention and this

0:48:11.290,0:48:17.880
CN n right and again this is simply a one dimensional one dimensional like

0:48:17.880,0:48:23.650
it's a one dimensional convolution where the kernel size is also one right and so

0:48:23.650,0:48:28.990
it can be implemented with a linear but I would write here and then one

0:48:28.990,0:48:33.700
deconvolution right with the kernel of size one actually they are implemented

0:48:33.700,0:48:38.440
in exactly the same way in in if you check the code on PyTorch but this

0:48:38.440,0:48:42.190
would be the correct like is a convolution way all right so you have

0:48:42.190,0:48:46.720
the multi lytic Multi multi head attention you get the first output then

0:48:46.720,0:48:52.000
you send this output you sum this to the input right because we had the residual

0:48:52.000,0:48:56.020
connection you send it through the layer normalization and then you have an

0:48:56.020,0:49:00.440
output so you have an output oh one you send

0:49:00.440,0:49:04.220
the output inside the convolution you get this guy which you're gonna be

0:49:04.220,0:49:07.790
bypassing now with a residual connection and you feed it to the layer

0:49:07.790,0:49:13.580
normalization so that's the encoder right so the encoder puts all that was

0:49:13.580,0:49:20.420
the encoder finnaly fit finish something I didn't mention was that let me go back

0:49:20.420,0:49:26.990
here so I'm using this encoder to do some sentence classification right

0:49:26.990,0:49:32.930
and so each there is there is actually a order now in the in the words if you put

0:49:32.930,0:49:39.050
a bag of word this is basically like acting and working on a bag of words but

0:49:39.050,0:49:42.980
if you actually would like to make sense you'd like to also send an index right

0:49:42.980,0:49:48.740
so the first item should also in the set should have or this was first item so

0:49:48.740,0:49:55.250
your house should send information about what position that item an item takes

0:49:55.250,0:50:01.130
okay so so far this encoder in this transformer and this attention is

0:50:01.130,0:50:06.950
completely permutation equivalent right because we don't have any information

0:50:06.950,0:50:11.930
about order but if I'd like to do classification of sentences maybe make

0:50:11.930,0:50:16.970
sense to take an account the order of words right because you know order my

0:50:16.970,0:50:22.070
matter so we can add some kind of position information but again it's not

0:50:22.070,0:50:29.570
important all right so I have my encoder which is gonna be just having the

0:50:29.570,0:50:35.450
embeddings for the input and then it has several layers of the encoder so over

0:50:35.450,0:50:41.960
here I just show you where the become okay okay so this is just one encoder

0:50:41.960,0:50:47.870
but since we are doing deep networks you can you know you have multiple encoders

0:50:47.870,0:50:55.870
after you know deep networks each of these is an encoder right and coder so

0:50:55.870,0:51:02.060
you can stack multiple of these encoders to make your network more powerful and

0:51:02.060,0:51:07.160
so here you have like a list for a number of layers you attend several

0:51:07.160,0:51:12.020
encoders together then we train this stuff on these IMDB

0:51:12.020,0:51:17.090
data set which is basically giving me the reviews of the of the movie and then

0:51:17.090,0:51:22.970
we have to figure out whether it was a good or a bad movie and pretty much

0:51:22.970,0:51:30.110
that's it so we train these big guy and I just

0:51:30.110,0:51:33.260
keep the training loop because it's gonna be exactly the same train loop we

0:51:33.260,0:51:39.260
have seen so many times you get some you know accuracy at the beginning it's 50

0:51:39.260,0:51:43.670
percent because it doesn't know better and then as you keep training we get up

0:51:43.670,0:51:50.000
to some kind of 1992 and we start maybe overfitting a little bit and this is the

0:51:50.000,0:51:55.730
test accuracy which is 83 percent okay so something you want to really pay

0:51:55.730,0:52:03.920
attention is the fact that when you do sentence classification with a RNN you

0:52:03.920,0:52:08.150
have to send it multiple times right you have to send the first word inside

0:52:08.150,0:52:13.700
they've sent the second word then it's a sequential set of operations instead in

0:52:13.700,0:52:18.200
this case these attention mechanism I show you right now there is no

0:52:18.200,0:52:23.060
sequential operation everything is computed in one go right so in this case

0:52:23.060,0:52:29.810
here you get these final age matrix which is the representational of all the

0:52:29.810,0:52:35.290
element in my sentence it's computing one goal right so there is no more

0:52:35.290,0:52:40.930
temporal loop there is no more waiting time right it's like boom immediately

0:52:40.930,0:52:47.000
immediately done right so this allow you to paralyze so much because this is just

0:52:47.000,0:52:52.400
matrix multiplication right so this is like stupid to paralyze one more thing

0:52:52.400,0:53:01.190
to pay attention is that where is it there is a matrix the a matrix okay this

0:53:01.190,0:53:07.820
guy here this is very dangerous right T is the number of T is the number of

0:53:07.820,0:53:13.130
titles right okay and there you go so my recipe book has thousand recipes

0:53:13.130,0:53:16.880
because it's the one thousand I'll free the recipe book

0:53:16.880,0:53:21.260
what's the size of these metrics is a thousand times a thousand right

0:53:21.260,0:53:28.340
1 million dimension is huge and so you can can see clearly here the

0:53:28.340,0:53:34.850
fact that if you have many indexes many many many keys right this stuff starts

0:53:34.850,0:53:39.230
blowing up quickly pretty quickly right so you have to pay attention about that

0:53:39.230,0:53:44.390
and you know there are different ways to handle this as well for example you can

0:53:44.390,0:53:50.990
split it in half and do something up but again implementation detail so right now

0:53:50.990,0:53:54.920
we are 10 minutes after class I am here for you and answer every kind of

0:53:54.920,0:53:59.270
question but I think we managed to go through the notebook in a ok

0:53:59.270,0:54:13.100
manner questions as to questions yes so in one head in one attention head you

0:54:13.100,0:54:21.830
will only use one matrix one weight matrix each for the query the key and

0:54:21.830,0:54:27.500
the value yes yeah I know you stuck all the final v's together and then you can

0:54:27.500,0:54:36.560
squash them back so at the end I may have H V then I can use this matrix here

0:54:36.560,0:54:43.040
to squash down everything to D dimension this is a way of doing this ok ok so

0:54:43.040,0:54:48.710
multi descent multi-headed attention essentially is just using multiple

0:54:48.710,0:54:54.890
weight matrices that don't share that yeah something happened in the between

0:54:54.890,0:55:01.100
by yes the the multi-headed attention means you have multiple queries for the

0:55:01.100,0:55:06.410
same input right and then allows you to have multiple questions about the same

0:55:06.410,0:55:11.870
wheel like you're hungry so one question would be how can I make lasagna but you

0:55:11.870,0:55:14.300
know that you don't have ground beef at home

0:55:14.300,0:55:19.640
so a second question would be mmm can I do a vegetarian dish and so you know

0:55:19.640,0:55:24.020
given that you're still hungry you may have different questions your mind

0:55:24.020,0:55:29.700
okay and can you go to the slide with the encoder/decoder structure that used

0:55:29.700,0:55:38.100
cross attention yeah sure here okay so the input to the first self attention

0:55:38.100,0:55:44.520
layer would be the way you calculate the query in that case would be Wq times xi

0:55:44.520,0:55:50.100
Say qi will be called attack so in this in the self attention right yeah so

0:55:50.100,0:55:56.820
in the self attention all those are hold on those q,k and v are coming from the

0:55:56.820,0:56:04.130
ŷ instead of x. You want to replace this x with ŷ right.

0:56:04.130,0:56:09.030
Sorry what was the ŷ again ? ŷ is gonna be my so

0:56:09.030,0:56:13.560
whenever you train this system here you're gonna be predicting the first

0:56:13.560,0:56:16.710
word okay aha I didn't even tell you you're you're right

0:56:16.710,0:56:22.530
so this system is trained to do translation you put a sentence in input

0:56:22.530,0:56:27.510
in a one language I'm happy hungry I'm hungry and then you put the other

0:56:27.510,0:56:33.180
language like in Italian for example or famine and you want to make you know you

0:56:33.180,0:56:37.320
feed here and I'm hangry and it's gonna be the representation of

0:56:37.320,0:56:42.150
the I'm hungry hungry not hungry I'm hungry in English and then after you

0:56:42.150,0:56:47.160
feed you put here I'm hungry the first word you're gonna be outputting is gonna

0:56:47.160,0:56:53.070
be here in Inked in Italian oh and if I you put all down here you enforce the

0:56:53.070,0:57:00.030
system to put family as output which is hungry in Italian so if I say okay let

0:57:00.030,0:57:07.680
me let me put a minute write down maybe if I want to say okay so it's so it's a

0:57:07.680,0:57:14.520
cut yeah so let's say a cut in English and then you have in Italian oh god oh

0:57:14.520,0:57:25.350
god got oh okay so first you have a cut that goes inside the encoder and then

0:57:25.350,0:57:30.330
the encoder spits this guy here and there is one H associated to each of

0:57:30.330,0:57:35.910
these inputs and then these if you put this stuff inside here at the beginning

0:57:35.910,0:57:39.430
you're going to a big zero here and this stuff is gonna

0:57:39.430,0:57:47.650
spit out a one which is the a now you put the one down here and then this guy

0:57:47.650,0:57:51.640
is gonna be spitting out got oh they put got to inciting us in this guy this

0:57:51.640,0:58:00.579
guy's gonna say finish that's it it will go to the end right II yes okay every

0:58:00.579,0:58:06.280
time you get a different input here at the bottom the decoder can decide to

0:58:06.280,0:58:11.470
look at different components of these eight encoder okay that doesn't make

0:58:11.470,0:58:18.280
sense yes and what's going to the class attention module in that case the cross

0:58:18.280,0:58:23.829
attention module is getting the output this wire here is the output of this add

0:58:23.829,0:58:31.930
norm so the output of this add norm goes inside the heater for the Q the the

0:58:31.930,0:58:38.890
query in the cross attention and then is getting this guy here the the values and

0:58:38.890,0:58:47.470
the the keys from the you know the encoder okay thanks okay there were take

0:58:47.470,0:58:56.730
questions you said the first was about how many matrices you'd use for a single

0:58:56.730,0:59:01.150
thank you sure of course I hope it was a bit more clear than

0:59:01.150,0:59:05.670
yesterday but again I noticed there is a lot of it was pretty dense today class I

0:59:05.670,0:59:10.599
hope like I was just confused about what the cells refer to and what the cross

0:59:10.599,0:59:22.329
referred yeah yeah yeah more questions in this example of the cat on gato mum

0:59:22.329,0:59:29.050
so you said out of the decoder you get some representation and you pass that

0:59:29.050,0:59:40.119
back into as a why so to me that looks like like some kind of recurrence is it

0:59:40.119,0:59:42.730
not this is this is called auto regressive

0:59:42.730,0:59:47.890
right so this is for generating text and so to generate text you have to generate

0:59:47.890,0:59:52.329
the first output now you feed the and that output inside you're gonna get

0:59:52.329,0:59:56.859
the second guy right so the encoder doesn't have any auto regressive thing

0:59:56.859,1:00:03.999
yeah the encoder just generates this hEnc then the decoder is gonna be

1:00:03.999,1:00:09.849
generating one word at a time in an autoregressive fashion so the right guy

1:00:09.849,1:00:15.099
is a generative model right but d-train the encoder and decoder at

1:00:15.099,1:00:21.160
all at once when you're training this model yeah so how does it how do you

1:00:21.160,1:00:28.059
train it if it's auto regressive and one step depends on the previous step your

1:00:28.059,1:00:33.609
sentence when you're doing inference is auto regressive oh so inference is auto

1:00:33.609,1:00:37.349
regressive but training is yeah because you have the wrong ones

1:00:37.349,1:00:42.099
okay you just mask the future time step so that you don't show it if you're

1:00:42.099,1:00:45.969
trying to do it for the first word it receives only the first not everything

1:00:45.969,1:00:51.180
else and so on let's call like a look ahead mask okay

1:00:51.180,1:01:00.160
Jesus thanks I'm just going with your examples from earlier with this the cat

1:01:00.160,1:01:03.940
and we were saying to cue in this case would be like an Italian word right I

1:01:03.940,1:01:09.759
like pudding the second time around and one top right right right and then it

1:01:09.759,1:01:15.130
comes down and then we feed it through and it the cue goes into the cross

1:01:15.130,1:01:23.829
attention and at that point K is gonna be the key for the English the encoded

1:01:23.829,1:01:32.349
English word but then the value is also gonna be from the English representation

1:01:32.349,1:01:38.309
right so how does that end up spitting out like an Italian word good question

1:01:38.309,1:01:46.109
you stuck multiple of these modules and somehow the magic happens I don't know

1:01:46.289,1:01:52.599
the values and the keys are coming from the English but again this okay so

1:01:52.599,1:01:57.190
whenever you train this system you will end up with representations that are

1:01:57.190,1:02:02.019
basically language agnostic I would assume therefore you have on one side

1:02:02.019,1:02:06.910
English the other side Italian but in the middle part whenever you have

1:02:06.910,1:02:11.470
these kind of embeddings we can assume this to be like language agnostic right

1:02:11.470,1:02:16.510
and so the question is gonna just figure out hey this Italian word is looking for

1:02:16.510,1:02:21.190
something that looks like this what are the encoding what are the embeddings

1:02:21.190,1:02:26.920
here that are you know matching my specific question right now okay so I

1:02:26.920,1:02:30.370
think that could be like an interpretation I guess so you have

1:02:30.370,1:02:35.710
Italian down English down here and as you bubble up the encoder you remove the

1:02:35.710,1:02:41.740
language specificity and then you kind of reuse this kind of you know encoders

1:02:41.740,1:02:47.560
I guess I mean this is similar to how it works in the encoder decoder recurrent

1:02:47.560,1:02:53.200
neural network you have an encoder which is encoding one whole sentence and then

1:02:53.200,1:02:56.920
you can have like a representation of that sentence that doesn't depend on the

1:02:56.920,1:03:00.640
language anymore right and then actually after having the

1:03:00.640,1:03:06.190
recurrent Network you used to have a dick like a decoder which is or just

1:03:06.190,1:03:09.520
using that final representation or you can also have an attention which is

1:03:09.520,1:03:17.200
looking at specific I think time steps in the in the past so it's it's part of

1:03:17.200,1:03:26.530
the language the natural neural language translation NTM brain neural language

1:03:26.530,1:03:32.320
and nltc machine machine translation and it that yeah that's part of that kind of

1:03:32.320,1:03:38.920
stuff sorry so just last question somewhat did I answer your question

1:03:38.920,1:03:44.680
before I mean this is my guess right it's like that the embedding is gonna be

1:03:44.680,1:03:49.930
the hEnc are like they are stripping off the the language specific

1:03:49.930,1:03:55.480
information they are just concepts right they are just the the representation of

1:03:55.480,1:04:00.540
the concept without the language attached there you go that would be my

1:04:00.540,1:04:07.180
it sounds like thank you in a sense it's gonna be an embedding that's in itself

1:04:07.180,1:04:11.050
so it's and then that's going to be compared with the K that's correct and

1:04:11.050,1:04:16.390
the Q in this case comes from your language that is gonna be the target

1:04:16.390,1:04:21.590
language right right okay make sense

1:04:21.590,1:04:28.210
thank you I would really recommend having a look to the blog from my friend

1:04:28.210,1:04:35.360
which is called The Illustrated Transformer it's very very very nicely written and

1:04:35.360,1:04:40.250
it's a bit maybe it has a bit more context about the the language part I

1:04:40.250,1:04:45.350
try to do you know I try not to have the language inside this presentation

1:04:45.350,1:04:49.070
because you know you can use this transformer for any kind of data right

1:04:49.070,1:04:54.860
and basically these are mapping sets to sets but again maybe this example here

1:04:54.860,1:05:01.310
was just you know very tailored to the translation part but translation you can

1:05:01.310,1:05:07.640
also have like transformers for making generative models pixel by pixel so you

1:05:07.640,1:05:13.220
can actually draw things with this thing with this architecture. Is

1:05:13.220,1:05:17.620
The Illustrated Transformer by Jay Alammar? yeah yeah him okay

1:05:17.620,1:05:23.840
yeah I really I really like the way he sees things but here's all my matrices

1:05:23.840,1:05:29.930
transposed so that is like bugging me I think I'm the one who transposed the

1:05:29.930,1:05:36.710
matrices everyone has them horizontal I I think it was the math was nicer with a

1:05:36.710,1:05:43.940
vertical okay more questions you said that the encoded representations would

1:05:43.940,1:05:50.210
be language agnostic doesn't that assume some sort of similarity in the way that

1:05:50.210,1:05:56.810
I'm in both the languages you're translating to and strong like as

1:05:56.810,1:06:00.140
language is like for example something that case for the representations to be

1:06:00.140,1:06:03.530
language agnostic you have the same between English and English in French

1:06:03.530,1:06:08.000
there's more of a similarity then say English and Chinese not just with

1:06:08.000,1:06:12.400
respect to the kind of data that's available but also the combat experience

1:06:12.400,1:06:17.060
reports of the languages so does this work as well across languages languages

1:06:17.060,1:06:22.700
that aren't as similar or does it how much worse does it perform does it work

1:06:22.700,1:06:26.180
as well you know any sorts languages or like languages which are not very

1:06:26.180,1:06:30.060
similar is a problem for any any model like and of course on something

1:06:30.060,1:06:34.290
I should that can solve it so I don't think it's it's going to attack that

1:06:34.290,1:06:40.980
problem what all classes of models have issues with like when the target and

1:06:40.980,1:06:45.360
source language is a very different his Libyan work surrounding trying to bridge

1:06:45.360,1:06:51.950
that gap just out of curiosity and yes but like it's an open problem for sure

1:06:51.950,1:06:55.950
okay more questions for me or for Aishwarya?

1:06:55.950,1:07:03.750
language question for hair and content maybe questions for me um Carol again

1:07:03.750,1:07:11.280
asked about encoder and decoder that could we go back to the page yeah thank

1:07:11.280,1:07:17.580
you so I understand cross attention is we have attention with the encoder you

1:07:17.580,1:07:22.620
know States I'm a bit confused about the self attention part so here self

1:07:22.620,1:07:28.440
attention only happens in why because we couldn't see the words in the future

1:07:28.440,1:07:34.770
right so when I input like when I'm in time T then the sava would the self

1:07:34.770,1:07:40.910
attention to does it do attention amount order why that hat although why that is

1:07:40.910,1:07:46.620
before the time sake so we had to specify actually I think I'd done a poor

1:07:46.620,1:07:51.150
job explaining here so there are two parts of this the first is training and

1:07:51.150,1:07:57.810
in training you have the whole sequence but then of course you can't look at the

1:07:57.810,1:08:02.910
future output right so the first why here at the bottom you cannot look at

1:08:02.910,1:08:07.020
the second why right and the second why cannot look at the third and so like the

1:08:07.020,1:08:11.160
first one cannot look at the whole future wise but the future wise can look

1:08:11.160,1:08:15.570
at the previous wise right because you can always know what you are gonna be

1:08:15.570,1:08:19.350
outputting but you cannot know what you might output in the future so whenever

1:08:19.350,1:08:24.630
you train this system you also need to somehow mask the future information that

1:08:24.630,1:08:28.080
you're providing to the system and so here you're gonna have the whole set

1:08:28.080,1:08:34.290
going in this basically first module of the end of the decoder generates the

1:08:34.290,1:08:39.000
questions and the questions come down here are going to be retrieving the

1:08:39.000,1:08:43.560
information from the encoded sentence and

1:08:43.560,1:08:49.020
and is encoded sentence somehow gets converted into the other language and

1:08:49.020,1:08:54.120
then this is done all in one pass boom whenever you actually do inference in

1:08:54.120,1:08:58.200
this case you're gonna have you basically start with a specific

1:08:58.200,1:09:03.359
representation from the encoder you get no initial value so you get like a zero

1:09:03.359,1:09:08.040
maybe and then you you're gonna ask an old question what is gonna be the first

1:09:08.040,1:09:11.430
word I should start with and then this one's gonna tell you all you should

1:09:11.430,1:09:16.819
start with the translation of a maybe right so you go end up with a una una

1:09:16.819,1:09:23.760
and and then you go you place this on down to the input and so given that now

1:09:23.760,1:09:28.710
the network knows that oh I already outputted one then what is gonna be the

1:09:28.710,1:09:33.839
next question is going to be all alone listen what's gonna be my next word

1:09:33.839,1:09:39.150
after I have inputted une and this is my second question right the second

1:09:39.150,1:09:42.839
question is going to retrieve or you should talk about the cut and the cut is

1:09:42.839,1:09:47.640
this representation and is this cut representation gets therefore converted

1:09:47.640,1:09:54.690
here into the corresponding gato that is you know cat in Italian and then you put

1:09:54.690,1:09:59.700
cut gato back here and then same process is gonna say oh you reach the end of the

1:09:59.700,1:10:06.720
sentence you have like period at the end we get all have attention with own when

1:10:06.720,1:10:14.130
Godot is the second word you can see everything at all time steps before so

1:10:14.130,1:10:19.560
when they see it it means they can have like attention yes yes it is overall

1:10:19.560,1:10:25.680
time steps before the like a builder current actor internship okay okay thank

1:10:25.680,1:10:27.780
you and I would recommend to watch those

1:10:27.780,1:10:33.300
animations from the Distilled dot pub they have like an article about the

1:10:33.300,1:10:39.420
attention and they show you how each word looks at different specific other

1:10:39.420,1:10:48.470
words like if you want to say I couldn't fit my I couldn't fit my trophy in my

1:10:48.470,1:10:55.890
suitcase because it was too large I guess is the trophy was too large so it

1:10:55.890,1:10:59.760
wasn't fitting but if you say my trophy wasn't couldn't fit in my

1:10:59.760,1:11:04.620
luggage because it was too small then the small is gonna be attending now to

1:11:04.620,1:11:09.510
the luggage right because it was too small then you actually can fit it and

1:11:09.510,1:11:14.100
so if you check for example like the sentence is actually the same the small

1:11:14.100,1:11:19.380
or large are both adjectives but one objective we look at the trophy and the

1:11:19.380,1:11:23.760
other objective we look at the luggage or the suitcase and so again I would

1:11:23.760,1:11:27.420
recommend watching the check-in this distilled out pub article where they

1:11:27.420,1:11:31.850
actually give you some nice visuals about how these attention checks

1:11:31.850,1:11:37.410
different parts of a sentence and these are called something fancy maybe Aishwarya 

1:11:37.410,1:11:41.850
knows these sentences I forgot Winograd Schema Challenge ? yeah yeah there you

1:11:41.850,1:11:45.090
go Winograd schemes and these are like

1:11:45.090,1:11:48.989
do you mind sending that link or at least directing us what is it called

1:11:48.989,1:11:54.890
distill.pub ? Distill.pub that's from my friend Chris, Christopher

1:11:54.890,1:12:01.920
Olah he used to be a Google brain now he's at the OpenAI. He's

1:12:01.920,1:12:06.090
basically sponsoring himself this website which is actually basically a

1:12:06.090,1:12:13.739
online journal where you have very cute visualizations right so I make videos

1:12:13.739,1:12:19.770
and presentation he makes you know interactive articles and yeah I really

1:12:19.770,1:12:24.960
recommend reading everything from there I really like it yeah I have nice

1:12:24.960,1:12:34.950
friends on the internet more questions I think this should have been spread

1:12:34.950,1:12:40.380
across two lessons to split the dance I'm sorry do you have any idea about

1:12:40.380,1:12:44.910
what the reformer network does yeah so you can actually check the reformer

1:12:44.910,1:12:53.489
Network on author blog post I forgot them deal with longer sequences because

1:12:53.489,1:12:58.140
like the current ones were are not able to deal with like more than 512 for

1:12:58.140,1:13:05.190
example but but yeah they do some fancy LSH attention I don't know like it was

1:13:05.190,1:13:09.690
the back of my hand and the problem with having those long sequences is this

1:13:09.690,1:13:17.040
alright so this one blows right there is a again there is a blog

1:13:17.040,1:13:25.020
post from a girl I forgot her name Lilian [Weng] ! Lilian has a nice blog

1:13:25.020,1:13:31.100
post from two days ago or three days ago which is called a transformer family

1:13:31.100,1:13:41.850
okay there are some errors in the blog post but ok I think it's good more

1:13:41.850,1:13:48.150
questions or I'm gonna be cooking dinner sorry I could you just say one more time

1:13:48.150,1:13:52.860
that they titled the article from this table pub so the Bastille a pub let me

1:13:52.860,1:14:01.370
check I actually don't know exactly okay this is Jay supercool guy and the

1:14:01.370,1:14:07.260
article he actually I didn't write so this should be coming from the Distilled

1:14:07.260,1:14:15.090
dot pub if I'm not mistaken so the thing I was referring is this one okay this

1:14:15.090,1:14:20.820
this illustration these pictures from this Illustrated transformer and I

1:14:20.820,1:14:25.890
believe this is coming from the distilled of pub from a Christopher Olah

1:14:25.890,1:14:34.520
so the other website is distill.pub yeah there we go so here you have

1:14:34.520,1:14:47.090
the attention sequence modeling [Music]

1:14:47.090,1:14:53.290
attention they're gone I think this I think it's coming from here

1:14:53.290,1:14:57.189
[Music] it talks about the heart attention and

1:15:04.100,1:15:13.400
soft attention I think okay maybe I lied I was talking about these pictures here

1:15:13.400,1:15:17.060
okay so I thought this was coming from distill.pub that maybe I was mistaken

1:15:17.060,1:15:23.090
these are the pictures that was talking like the their name they attend to

1:15:23.090,1:15:28.100
different words okay okay the animal didn't cross the street because it was

1:15:28.100,1:15:33.199
too tired and then you have here that it was the only more atom anima right but

1:15:33.199,1:15:38.120
then it was - maybe the animal didn't cross the street because it was too wide

1:15:38.120,1:15:43.250
in this case white instead of tired should be you know attending to street

1:15:43.250,1:15:47.840
in this case right both it can be street and animal but you know you have higher

1:15:47.840,1:15:54.320
scores here right so the scalar product is a has a higher score in this region

1:15:54.320,1:15:58.739
here right you need anything from me more

1:15:58.739,1:16:06.059
no nope okay right what why nice seeing you and nice

1:16:06.059,1:16:12.960
talking to you so we are done now it was quite a substantial lesson right so

1:16:12.960,1:16:15.719
again how can you get more out of these lessons

1:16:15.719,1:16:20.789
so again comprehension something was not clear I've done a poor job just then ask

1:16:20.789,1:16:26.309
me anything in the comment section below news you can find everything about what

1:16:26.309,1:16:32.820
I'm doing and what I'm teaching on Twitter at alfcnz said handle updates

1:16:32.820,1:16:36.539
again if you subscribe to this YouTube channel you're gonna have the latest

1:16:36.539,1:16:41.730
videos as soon as I upload them online if you like my work and this video in

1:16:41.730,1:16:46.050
particular just press the like button this video has an English transcript you

1:16:46.050,1:16:52.860
can find in the course website where all the titles are linked to the sections of

1:16:52.860,1:16:57.179
this video. Parli italiano? ¿Hablas español? 你說普通話嗎?

1:16:57.179,1:17:01.590
you speak Korean you speak Turkish we have all these translations now

1:17:01.590,1:17:05.429
available on the course website so go out there and check it out

1:17:05.429,1:17:09.239
if you'd like to have your own language available as well feel free to contact

1:17:09.239,1:17:14.670
me such that we can get started with the translation part and finally you should

1:17:14.670,1:17:19.440
try to go over the piperj notebook we have core in this class and

1:17:19.440,1:17:24.749
make yourself familiar with all the methods and classes and all the little

1:17:24.749,1:17:29.010
things you should try to train this notebook change parameters such that you

1:17:29.010,1:17:34.440
can get you know some good understanding of what it was all about okay it was

1:17:34.440,1:17:38.880
quite a lot this time so you'd better check out this notebook right and

1:17:38.880,1:17:44.130
finally if you find errors typos and you know everything you think I can do

1:17:44.130,1:17:48.690
better we can do better we can improve the content with your help if you

1:17:48.690,1:17:53.400
contribute to the github repository where the website is hosted and that was

1:17:53.400,1:18:00.110
pretty much it again thank you so much for sticking around with us and bye-bye

0:00:00.000,0:00:05.339
last time we have seen that a matrix can

0:00:02.550,0:00:10.559
be written basically let me draw here

0:00:05.339,0:00:12.719
the matrix so we had similar roles right

0:00:10.559,0:00:15.990
and then we multiplied usually design by

0:00:12.719,0:00:18.210
one one column all right and so whenever

0:00:15.990,0:00:20.730
we multiply these guys you can see these

0:00:18.210,0:00:23.340
and as two types two different

0:00:20.730,0:00:25.590
equivalent types of representation it

0:00:23.340,0:00:28.980
can you see right you don't is it

0:00:25.590,0:00:32.250
legible okay so you can see basically as

0:00:28.980,0:00:35.430
the output of this product has been a

0:00:32.250,0:00:38.879
sequence of like the first row times

0:00:35.430,0:00:40.469
this column vector and then again I'm

0:00:38.879,0:00:42.570
just okay shrinking them this should be

0:00:40.469,0:00:46.170
the same size right right because

0:00:42.570,0:00:48.600
otherwise you can't multiply them so you

0:00:46.170,0:00:52.170
have this one and so on right until the

0:00:48.600,0:00:54.329
last one and this is gonna be my final

0:00:52.170,0:01:00.960
vector and we have seen that each of

0:00:54.329,0:01:02.870
these bodies here what are these I talk

0:01:00.960,0:01:05.339
to me please

0:01:02.870,0:01:07.170
there's a scalar products right but what

0:01:05.339,0:01:08.820
do they represent what is it how can we

0:01:07.170,0:01:11.790
call it what's another name for calling

0:01:08.820,0:01:13.290
a scalar product I show you last time a

0:01:11.790,0:01:15.500
demonstration with some Chi government

0:01:13.290,0:01:18.119
trigonometry right what is it

0:01:15.500,0:01:20.340
so this is all the projection if you

0:01:18.119,0:01:22.619
talk about geometry or you can think

0:01:20.340,0:01:25.259
about this as a nun normalized cosine

0:01:22.619,0:01:29.310
value right so this one is going to be

0:01:25.259,0:01:31.320
my projection basically of one kernel or

0:01:29.310,0:01:36.030
my input signal onto the kernel right so

0:01:31.320,0:01:38.340
these are projections projection alright

0:01:36.030,0:01:40.619
and so then there was also a another

0:01:38.340,0:01:42.720
interpretation of this like there is

0:01:40.619,0:01:45.390
another way of seeing this which was

0:01:42.720,0:01:49.140
what basically we had the first column

0:01:45.390,0:01:53.579
of the matrix a multiplied by the first

0:01:49.140,0:01:56.340
element of the X of these of this vector

0:01:53.579,0:01:58.260
right so back element number one then

0:01:56.340,0:02:01.500
you had a second call

0:01:58.260,0:02:04.020
time's the second element of the X

0:02:01.500,0:02:07.440
vector until you get to the last column

0:02:04.020,0:02:11.100
right times the last an element right

0:02:07.440,0:02:14.580
suppose that this is long N and this is

0:02:11.100,0:02:16.110
M times n right so the height again is

0:02:14.580,0:02:18.210
going to be the dimension towards we

0:02:16.110,0:02:19.550
should - and the width of a matrix is

0:02:18.210,0:02:23.100
dimension where we're coming from

0:02:19.550,0:02:24.810
second part was the following so we said

0:02:23.100,0:02:26.910
instead of using this matrix here

0:02:24.810,0:02:29.450
instead since we are doing convolutions

0:02:26.910,0:02:32.070
because we'd like to exploit sparsity a

0:02:29.450,0:02:35.400
stationarity and compositionality of the

0:02:32.070,0:02:39.030
data we still use the same matrix here

0:02:35.400,0:02:41.370
perhaps right we use the same guy here

0:02:39.030,0:02:43.710
but then those kernels we are going to

0:02:41.370,0:02:45.510
be using them over and over again the

0:02:43.710,0:02:48.690
same current across the whole signal

0:02:45.510,0:02:51.360
right so in this case the width of this

0:02:48.690,0:02:54.390
matrix is no longer be it's no longer n

0:02:51.360,0:02:56.820
as it was here is going to be K which is

0:02:54.390,0:02:59.720
gonna be the kernel size right so here

0:02:56.820,0:03:03.090
I'm gonna be drawing my thinner matrix

0:02:59.720,0:03:05.730
and this one is gonna be K lowercase K

0:03:03.090,0:03:10.140
and the height maybe we can still call

0:03:05.730,0:03:13.110
it n okay all right so let's say here I

0:03:10.140,0:03:18.230
have several kernels for example let me

0:03:13.110,0:03:21.959
have my tsiyon carnal then I may have my

0:03:18.230,0:03:25.080
other non green let me change

0:03:21.959,0:03:28.769
let's put pink so you have this one and

0:03:25.080,0:03:33.180
then you may have green one right and so

0:03:28.769,0:03:36.000
on so how do we use these kernels right

0:03:33.180,0:03:38.280
now so we basically can use these

0:03:36.000,0:03:40.800
kernels by stacking them and shifted

0:03:38.280,0:03:43.650
them a little bit right so we get the

0:03:40.800,0:03:47.489
first kernel out of here and then you're

0:03:43.650,0:03:50.519
gonna get basically you get the first

0:03:47.489,0:03:53.250
guy here then you shift it shift it

0:03:50.519,0:03:58.290
shift it and so on right until you get

0:03:53.250,0:03:59.970
the whole matrix and we were putting a 0

0:03:58.290,0:04:02.100
here and a 0 here right this is just

0:03:59.970,0:04:05.940
recap and then you have this one for the

0:04:02.100,0:04:11.379
blue color now you do magic here and

0:04:05.940,0:04:15.650
just do copy copy and I you do paste

0:04:11.379,0:04:19.370
and now you can also do color see

0:04:15.650,0:04:22.280
fantastic magic and we have pink one and

0:04:19.370,0:04:25.360
then you have the last one right can I

0:04:22.280,0:04:27.770
do the same copy yes I can do fantastic

0:04:25.360,0:04:29.080
so you cannot do copy and paste on the

0:04:27.770,0:04:33.680
paper

0:04:29.080,0:04:38.419
all right color and the last one light

0:04:33.680,0:04:40.580
green okay all right so we just

0:04:38.419,0:04:44.479
duplicate how many matrices do we have

0:04:40.580,0:04:46.490
now how many layers no don't count the

0:04:44.479,0:04:50.600
number like there are letters on the on

0:04:46.490,0:04:59.180
the screen and K or M what is it what is

0:04:50.600,0:05:00.620
K the side usually you're just guessing

0:04:59.180,0:05:02.930
you shouldn't be guessing you should

0:05:00.620,0:05:07.120
tell me the correct answer I think about

0:05:02.930,0:05:11.600
this as a job interview I'm training you

0:05:07.120,0:05:14.990
so how many maps we have and right so

0:05:11.600,0:05:17.620
this one here are as many as my M which

0:05:14.990,0:05:21.470
is the number of rows of this initial

0:05:17.620,0:05:24.560
thing over here right all right so what

0:05:21.470,0:05:30.289
is instead the width of this little

0:05:24.560,0:05:38.750
kernel here okay right okay what is the

0:05:30.289,0:05:41.349
height of this matrix what is the height

0:05:38.750,0:05:41.349
of the matrix

0:05:42.340,0:05:45.480
you sure

0:05:48.200,0:05:59.780
try again

0:05:49.220,0:06:04.310
I can't hear and minus k plus one okay

0:05:59.780,0:06:06.230
and the final what is the output of this

0:06:04.310,0:06:08.660
thing right so the output is going to be

0:06:06.230,0:06:12.370
one vector which is gonna be of height

0:06:08.660,0:06:19.430
the same right and minus k plus 1 and

0:06:12.370,0:06:21.320
then it should be correct yeah but then

0:06:19.430,0:06:27.890
how many what is the thickness of this

0:06:21.320,0:06:33.260
final vector M right so this stuff here

0:06:27.890,0:06:35.600
and goes as thick as M right so this is

0:06:33.260,0:06:37.700
where we left last time right but then

0:06:35.600,0:06:39.770
someone asked me now then I realized so

0:06:37.700,0:06:41.990
we have here as many as the different

0:06:39.770,0:06:45.170
colors right so for example in this case

0:06:41.990,0:06:46.790
if I just draw to make sure we

0:06:45.170,0:06:49.730
understand what's going on you have the

0:06:46.790,0:06:55.600
first thing here now you have the second

0:06:49.730,0:06:55.600
one here and I have the third one right

0:06:56.020,0:07:01.100
in this case all right so last time they

0:06:59.750,0:07:03.650
asked me if someone asked me at the end

0:07:01.100,0:07:06.890
of the class so how do we do convolution

0:07:03.650,0:07:09.760
when we end up in this situation over

0:07:06.890,0:07:12.590
here because here we assume that my

0:07:09.760,0:07:14.990
corners are just you know whatever K

0:07:12.590,0:07:18.920
long let's say three long but then they

0:07:14.990,0:07:21.380
are just one little vector right and so

0:07:18.920,0:07:22.730
somebody told me no then what do you do

0:07:21.380,0:07:24.950
from here like how do we keep going

0:07:22.730,0:07:29.300
because now we have a thickness before

0:07:24.950,0:07:32.510
we started with a something here this

0:07:29.300,0:07:34.400
vector which had just n elements right

0:07:32.510,0:07:35.690
are you following so far I'm going

0:07:34.400,0:07:39.260
faster because we already seen these

0:07:35.690,0:07:44.030
things I'm just reviewing but are you

0:07:39.260,0:07:45.650
with me until now yes no yes okay

0:07:44.030,0:07:47.720
fantastic so let's see how we actually

0:07:45.650,0:07:49.580
keep going so the thing is

0:07:47.720,0:07:51.680
show you right now is actually assuming

0:07:49.580,0:07:56.440
that we start with that long vector

0:07:51.680,0:08:01.400
which was of height what was the height

0:07:56.440,0:08:03.440
and right but in this case also this one

0:08:01.400,0:08:13.060
means that we have something that looks

0:08:03.440,0:08:16.520
like this and so you have basically here

0:08:13.060,0:08:20.720
this is 1 this is also 1 so we only have

0:08:16.520,0:08:24.500
a monophonic signal for example and this

0:08:20.720,0:08:26.300
was n the height right all right so

0:08:24.500,0:08:29.660
let's assume now we're using a

0:08:26.300,0:08:33.950
stereophonic system so what is gonna be

0:08:29.660,0:08:35.510
my domain here so you know my X can be

0:08:33.950,0:08:39.740
thought as a function that goes from the

0:08:35.510,0:08:45.890
domain to the are number of channels so

0:08:39.740,0:08:47.840
what is this guy here yeah x is one

0:08:45.890,0:08:52.190
dimension and somewhere so what is this

0:08:47.840,0:08:59.930
Omega we have seen this slide last slide

0:08:52.190,0:09:02.210
of Tuesday lesson right second Omega is

0:08:59.930,0:09:11.720
not set of real numbers no someone else

0:09:02.210,0:09:14.480
tries we are using computers it's time

0:09:11.720,0:09:16.520
line yes and how many samples you you

0:09:14.480,0:09:18.020
have one sample number sample number two

0:09:16.520,0:09:21.710
or sample number three so you have

0:09:18.020,0:09:23.630
basically a subset of the natural space

0:09:21.710,0:09:30.860
right so this one is going to be

0:09:23.630,0:09:34.400
something like 0 1 2 so on set which is

0:09:30.860,0:09:36.410
gonna be subset of n right so it's not

0:09:34.400,0:09:40.130
our R is gonna be if you have time

0:09:36.410,0:09:45.850
continuous domain what you see in this

0:09:40.130,0:09:51.320
case the in the case I just showed you

0:09:45.850,0:09:55.160
so far what is seen in this case now

0:09:51.320,0:09:57.050
number of input channels because this is

0:09:55.160,0:10:00.740
going to be my X right this is my input

0:09:57.050,0:10:04.550
so in this case we show so far in this

0:10:00.740,0:10:07.220
case here we were just using one so it

0:10:04.550,0:10:09.260
means we have a monophonic audio let's

0:10:07.220,0:10:10.880
seven now the assumption make the

0:10:09.260,0:10:13.040
assumption that this guy is that it's

0:10:10.880,0:10:22.780
gonna be two such that you're gonna be

0:10:13.040,0:10:22.780
talking about stereo phonic signal right

0:10:23.200,0:10:27.380
okay

0:10:24.410,0:10:36.560
so let's see how this stuff changes so

0:10:27.380,0:10:38.450
in this case my let me think yeah so how

0:10:36.560,0:10:40.459
do I draw I'm gonna just draw right

0:10:38.450,0:10:43.400
little complain if you don't follow

0:10:40.459,0:10:45.140
are you following so far yes because if

0:10:43.400,0:10:46.550
i watch my tablet I don't see you right

0:10:45.140,0:10:48.589
so you should be complaining if

0:10:46.550,0:10:50.750
something doesn't make sense right

0:10:48.589,0:10:52.190
otherwise becomes boring from waiting

0:10:50.750,0:10:56.390
and watching you all the time

0:10:52.190,0:10:56.900
right yes no yes okay I'm boring okay

0:10:56.390,0:11:00.080
thank you

0:10:56.900,0:11:02.900
all right so we have here this signal

0:11:00.080,0:11:07.280
right and then now we have some

0:11:02.900,0:11:12.530
thickness in this case what is the

0:11:07.280,0:11:14.660
thickness of this guy see right so in

0:11:12.530,0:11:16.850
this case this one is going to be C and

0:11:14.660,0:11:18.589
in the case of the stereophonic signal

0:11:16.850,0:11:23.240
you're gonna just have two channels left

0:11:18.589,0:11:30.170
and right and this one keeps going down

0:11:23.240,0:11:32.030
right all right so our kernels if I'd

0:11:30.170,0:11:35.030
like to perform a convolution over this

0:11:32.030,0:11:42.290
signal right so you have different same

0:11:35.030,0:11:44.150
pussy right and so on right if I'd like

0:11:42.290,0:11:45.830
to perform a convolution one big

0:11:44.150,0:11:47.089
convolution I'm not talking about two

0:11:45.830,0:11:50.990
deconvolution right because they are

0:11:47.089,0:11:52.670
still using domain which is here number

0:11:50.990,0:11:57.140
one right so this is actually important

0:11:52.670,0:11:58.510
so if I ask you what type of signal this

0:11:57.140,0:12:00.430
is you're gonna be basically

0:11:58.510,0:12:02.890
you have to look at this number over

0:12:00.430,0:12:07.300
here right so we are talking about one

0:12:02.890,0:12:12.490
dimensional signal which is one

0:12:07.300,0:12:15.280
dimensional domain right 1d domain okay

0:12:12.490,0:12:17.710
so we are still using a 1d signal but in

0:12:15.280,0:12:21.160
this case it has you know you have two

0:12:17.710,0:12:25.750
values per point so what kind of kernels

0:12:21.160,0:12:29.080
are we gonna be using so I'm gonna just

0:12:25.750,0:12:31.450
draw it in this case we're gonna be

0:12:29.080,0:12:34.480
using something similar like this so I'm

0:12:31.450,0:12:37.990
gonna be drawing this guy let's say I

0:12:34.480,0:12:40.870
have K here which is gonna be my width

0:12:37.990,0:12:42.700
of the kernel but in this case I'm gonna

0:12:40.870,0:12:53.320
be also have some thickness in this case

0:12:42.700,0:12:56.230
here right so basically you apply this

0:12:53.320,0:13:00.940
thing here okay and then you can go

0:12:56.230,0:13:04.060
second line and third line and so on

0:13:00.940,0:13:06.520
right so you may still have like here m

0:13:04.060,0:13:11.590
kernels but in this case you also have

0:13:06.520,0:13:13.660
some thickness which has to match the

0:13:11.590,0:13:17.680
other thickness right so this thickness

0:13:13.660,0:13:21.940
here has to match the thickness of the

0:13:17.680,0:13:23.980
input size so let me show you how to

0:13:21.940,0:13:30.880
apply the convolution so you're gonna

0:13:23.980,0:13:37.980
get one of these slices here and then

0:13:30.880,0:13:37.980
you're gonna be applying this over here

0:13:39.320,0:13:46.190
okay and then you simply go down this

0:13:45.180,0:13:50.880
way

0:13:46.190,0:13:53.870
alright so whenever you apply these you

0:13:50.880,0:14:02.130
perform this guy here the inner product

0:13:53.870,0:14:04.410
with these over here what you get it's

0:14:02.130,0:14:07.650
actually a one by one is a scalar so

0:14:04.410,0:14:09.540
whenever I use this orange thingy here

0:14:07.650,0:14:11.880
on the left hand side and I do a dot

0:14:09.540,0:14:14.190
product scalar product with this one I

0:14:11.880,0:14:17.370
just get a scalar so this is actually my

0:14:14.190,0:14:19.620
convolution in 1d the convolution in 1d

0:14:17.370,0:14:21.660
means that it goes down this way and

0:14:19.620,0:14:27.480
only in one way that's why it's called

0:14:21.660,0:14:30.870
1d but we multiply each element of this

0:14:27.480,0:14:36.290
mask times this guy here now a second

0:14:30.870,0:14:38.940
row and this guy here okay

0:14:36.290,0:14:41.090
you saw you multiply all of them you sum

0:14:38.940,0:14:45.120
all of them and then you get your first

0:14:41.090,0:14:47.250
output here okay so whenever I make this

0:14:45.120,0:14:50.760
multiplication I get my first output

0:14:47.250,0:14:52.050
here then I keep sliding this kernel

0:14:50.760,0:14:55.050
down and then you're gonna get the

0:14:52.050,0:14:58.380
second output third out fourth and so on

0:14:55.050,0:15:00.570
until you go down at the end then what

0:14:58.380,0:15:03.780
happens then happens that I'm gonna be

0:15:00.570,0:15:05.370
picking up different kernel I'm gonna

0:15:03.780,0:15:07.950
back it let's say I get the third one

0:15:05.370,0:15:11.940
okay let's get the second one I get a

0:15:07.950,0:15:19.050
second one and I perform the same

0:15:11.940,0:15:20.790
operation you're gonna get here this one

0:15:19.050,0:15:23.240
actually let's actually make it like a

0:15:20.790,0:15:23.240
matrix

0:15:26.940,0:15:33.790
you go down okay until you go with the

0:15:29.830,0:15:41.050
last one which is gonna be the end right

0:15:33.790,0:15:45.450
the empty kernel which is gonna be going

0:15:41.050,0:15:45.450
down this way you get the last one here

0:15:51.680,0:15:58.790
okay yes no confusing clearing so this

0:15:57.230,0:16:06.769
was the question I got at the end of the

0:15:58.790,0:16:10.339
class yeah Suzy yeah because it's a dot

0:16:06.769,0:16:13.939
product of all those values between so

0:16:10.339,0:16:18.259
basically do the projection of this part

0:16:13.939,0:16:20.259
of the signal onto this kernel so you'd

0:16:18.259,0:16:22.879
like to see what is the contribution

0:16:20.259,0:16:24.920
like what is the alignment of this part

0:16:22.879,0:16:27.350
of the signal on to this specific

0:16:24.920,0:16:30.230
subspace okay this is how a convolution

0:16:27.350,0:16:31.850
works when you have multiple channels so

0:16:30.230,0:16:35.319
far I'll show you just with single

0:16:31.850,0:16:35.319
channel now we have multiple channels

0:16:36.220,0:16:56.329
okay so oh yeah yeah in one second one

0:16:54.259,0:16:59.509
and one one at the top one at the bottom

0:16:56.329,0:17:02.540
so you actually lose the first row here

0:16:59.509,0:17:04.850
and you lose the last row here so at the

0:17:02.540,0:17:07.850
end in this case the output is going to

0:17:04.850,0:17:10.490
be n minus three plus one so you lose

0:17:07.850,0:17:12.589
two one on top okay in this case you

0:17:10.490,0:17:15.140
lose two at the bottom if you actually

0:17:12.589,0:17:17.059
do a Center at the center the

0:17:15.140,0:17:20.390
convolution usually you lose one at the

0:17:17.059,0:17:22.579
beginning one at the end every time you

0:17:20.390,0:17:24.409
perform a convolution you lose the

0:17:22.579,0:17:26.809
number of the dimension of the kernel

0:17:24.409,0:17:28.789
minus one you can try if you put your

0:17:26.809,0:17:30.830
hand like this you have a kernel of

0:17:28.789,0:17:34.340
three you get the first one here and it

0:17:30.830,0:17:37.159
is matching then you switch one and then

0:17:34.340,0:17:39.440
you switch to right so okay with fight

0:17:37.159,0:17:41.720
let's tell a parent of two right so you

0:17:39.440,0:17:44.149
have your signal of five you have your

0:17:41.720,0:17:47.720
kernel with two you have one two three

0:17:44.149,0:17:49.070
and four so we started with five and you

0:17:47.720,0:17:51.080
end up with four because you use a

0:17:49.070,0:17:54.500
kernel size of two if you use a kernel

0:17:51.080,0:17:56.480
size of three you get one two and three

0:17:54.500,0:17:57.289
so you goes to if you use a kernel size

0:17:56.480,0:17:59.030
of three okay

0:17:57.289,0:18:01.010
so you can always try to do this

0:17:59.030,0:18:02.720
alright so I'm gonna show you now the

0:18:01.010,0:18:07.040
dimensions of these kernels and the

0:18:02.720,0:18:18.080
outputs with PI torch okay Yes No

0:18:07.040,0:18:18.500
all right good okay mister can you see

0:18:18.080,0:18:22.120
anything

0:18:18.500,0:18:25.520
yes right I mean zoom a little bit more

0:18:22.120,0:18:30.640
okay so now we can go we do conv

0:18:25.520,0:18:33.770
activate activate and then we have

0:18:30.640,0:18:37.390
debate yeah PDL right pythor sleep

0:18:33.770,0:18:40.520
learning so here we can just run ipython

0:18:37.390,0:18:44.270
if i press ctrl L I clear the screen and

0:18:40.520,0:18:49.820
we can do import torch then I can do

0:18:44.270,0:18:51.770
from torch import nn so now we can see

0:18:49.820,0:18:54.500
for example called let's set my

0:18:51.770,0:18:56.330
convolutional convolutional layer it's

0:18:54.500,0:18:59.930
going to be equal to NN conf

0:18:56.330,0:19:02.150
and then I can keep going until I get

0:18:59.930,0:19:04.220
this one let's say yeah let's say I have

0:19:02.150,0:19:06.110
no idea how to use this function I just

0:19:04.220,0:19:08.750
put a question mark I press ENTER and

0:19:06.110,0:19:11.390
I'm gonna see here now the documentation

0:19:08.750,0:19:13.460
okay so in this case you're gonna have

0:19:11.390,0:19:15.320
the first item is going to be the input

0:19:13.460,0:19:19.820
channel then I have the output channels

0:19:15.320,0:19:21.380
then I have the corner sighs alright so

0:19:19.820,0:19:24.290
for example we are going to be putting

0:19:21.380,0:19:28.190
here input channels we have a stereo

0:19:24.290,0:19:30.530
signal so we put two channels the number

0:19:28.190,0:19:32.720
of corners we said that was M and let's

0:19:30.530,0:19:36.650
say we have 16 kernels so this is the

0:19:32.720,0:19:39.200
number of kernels I'm gonna be using and

0:19:36.650,0:19:41.810
then let's have our kernel size of what

0:19:39.200,0:19:45.080
the same I use here so let's have K or

0:19:41.810,0:19:47.570
the kernel size equal 3 okay in so here

0:19:45.080,0:19:50.690
I'm going to define my first convolution

0:19:47.570,0:19:52.910
object so if I print this one comes

0:19:50.690,0:19:57.580
you're gonna see we have a convolution a

0:19:52.910,0:19:57.580
2d combo sorry 1 deconvolution made that

0:19:57.820,0:20:05.149
okay so we have a 1d convolution

0:20:02.149,0:20:08.869
which is going from two channels so a

0:20:05.149,0:20:12.019
stereophonic to a sixteen channels means

0:20:08.869,0:20:16.039
I use sixteen kernels the skirmish size

0:20:12.019,0:20:18.889
is 3 and then the stride is also 1 ok so

0:20:16.039,0:20:23.859
in this case I'm gonna be checking what

0:20:18.889,0:20:23.859
is gonna be my convolutional weights

0:20:27.429,0:20:33.379
what is the size of the weights

0:20:29.829,0:20:36.279
how many weights do we have how many how

0:20:33.379,0:20:40.069
many planes do we have for the weights

0:20:36.279,0:20:44.479
16 right so we have 16 weights what is

0:20:40.069,0:20:53.649
the length of the the day of the key of

0:20:44.479,0:20:53.649
D of the kernel okay Oh what is this -

0:20:54.549,0:21:00.349
Janis right so I have 16 of these

0:20:57.679,0:21:04.339
scanners which have thickness - and then

0:21:00.349,0:21:05.539
length of 3 ok makes sense right because

0:21:04.339,0:21:09.799
you're gonna be applying each of these

0:21:05.539,0:21:11.629
16 across the whole signal so let's have

0:21:09.799,0:21:15.639
my signal now you're gonna be is gonna

0:21:11.629,0:21:20.599
be equal toage dot R and and and oh

0:21:15.639,0:21:23.359
sighs I don't know let's say 64 I also

0:21:20.599,0:21:25.129
have to say I have a batch of size 1 so

0:21:23.359,0:21:28.009
I have a virtual site one so I just have

0:21:25.129,0:21:31.879
one signal and then this is gonna be 64

0:21:28.009,0:21:35.059
how many channels we said this has two

0:21:31.879,0:21:37.819
right so I have one signal one example

0:21:35.059,0:21:42.439
which has two channels and has 64

0:21:37.819,0:21:46.689
samples so this is my X hold on what is

0:21:42.439,0:21:46.689
the convolutional bias size

0:21:48.320,0:21:54.380
a 16 right because you have one bias /

0:21:51.440,0:22:05.110
plain / / / way ok so what's gonna be in

0:21:54.380,0:22:07.539
our my convolution of X the output hello

0:22:05.110,0:22:12.470
so I'm gonna still have one sample right

0:22:07.539,0:22:15.919
how many channels 16 what is gonna be

0:22:12.470,0:22:19.759
the length of the signal okay that's

0:22:15.919,0:22:22.700
good 6 fix it okay fantastic

0:22:19.759,0:22:27.399
all right so what if I'm gonna be using

0:22:22.700,0:22:32.240
a convolution with size of the kernel 5

0:22:27.399,0:22:32.919
what do I get now yet to shout I can't

0:22:32.240,0:22:36.320
hear you

0:22:32.919,0:22:41.899
60 okay you're following fantastic okay

0:22:36.320,0:22:44.059
so let's try now instead to use a hyper

0:22:41.899,0:22:46.580
spectral image with a 2d convolution

0:22:44.059,0:22:49.100
okay so I'm going to be coding now my

0:22:46.580,0:22:51.139
convolution here is going to be my in

0:22:49.100,0:22:55.490
this case is correct or is going to be a

0:22:51.139,0:22:57.230
conf come to D again I don't know how to

0:22:55.490,0:22:59.059
use it so I put a question mark and then

0:22:57.230,0:23:03.259
I have here input channel output channel

0:22:59.059,0:23:05.450
criticize strident padding okay so I'm

0:23:03.259,0:23:07.730
going to be putting inputs tried input

0:23:05.450,0:23:10.429
channel so it's a hyper spectral image

0:23:07.730,0:23:12.710
with 20 planes so what's gonna be the

0:23:10.429,0:23:16.149
input in this case 20 right because you

0:23:12.710,0:23:18.740
have you start from 20 spectral bands

0:23:16.149,0:23:20.419
then we're gonna be inputting the output

0:23:18.740,0:23:23.090
number of channels we let's say we're

0:23:20.419,0:23:25.330
gonna be using again 16 in this case I'm

0:23:23.090,0:23:28.549
going to be inputting the kernel size

0:23:25.330,0:23:33.440
since I'm planning to use okay let's

0:23:28.549,0:23:36.350
actually define let's actually define my

0:23:33.440,0:23:40.120
signal first so my X is gonna be a torch

0:23:36.350,0:23:46.779
dot R and and let's say one sample with

0:23:40.120,0:23:52.820
20 channels of height for example I

0:23:46.779,0:23:57.110
guess 6128 well hold on 64 and then with

0:23:52.820,0:23:58.820
128 okay so this is gonna be my my input

0:23:57.110,0:24:00.950
my eople data okay

0:23:58.820,0:24:04.370
so my convolution now it can be

0:24:00.950,0:24:06.710
something like this so I have 20

0:24:04.370,0:24:09.110
channels from input 16 our Mike Ernest

0:24:06.710,0:24:12.140
I'm gonna be using then I'm gonna be

0:24:09.110,0:24:15.050
specifying the kernel size in this case

0:24:12.140,0:24:19.670
let's use something that is like three

0:24:15.050,0:24:24.580
times five okay so what is going to be

0:24:19.670,0:24:24.580
the output what are the kernel size

0:24:29.170,0:24:47.630
anyone yes no what no 20 Janice is the

0:24:44.500,0:24:50.750
channels of the input data right so you

0:24:47.630,0:24:51.680
have how many kernels here 16 right

0:24:50.750,0:24:53.720
there you go

0:24:51.680,0:24:56.420
we have 16 kernels which have 20

0:24:53.720,0:24:59.720
channels such that they can lay over the

0:24:56.420,0:25:03.410
input 3 by 5 right teeny like a short

0:24:59.720,0:25:08.140
like yeah short but large ok so what is

0:25:03.410,0:25:08.140
gonna be my convolution on eggs sized

0:25:11.920,0:25:20.270
one 1660 224 let's say I'd like to

0:25:16.310,0:25:22.190
actually add back the I'd like to head

0:25:20.270,0:25:24.350
the sing dimensionality I can add some

0:25:22.190,0:25:25.730
padding right so here there is going to

0:25:24.350,0:25:28.430
be the stride I'm gonna have a stride of

0:25:25.730,0:25:29.930
1 again if you don't remember the the

0:25:28.430,0:25:31.910
syntax you can just put the question

0:25:29.930,0:25:35.120
mark can you figure out and then how

0:25:31.910,0:25:40.580
much strive should I add now how much

0:25:35.120,0:25:41.870
stride in the y-direction sorry yes how

0:25:40.580,0:25:44.990
much padding should I add in the

0:25:41.870,0:25:46.490
y-direction one because it's gonna be

0:25:44.990,0:25:49.430
one on top one on the bottom but then

0:25:46.490,0:25:51.890
then on the x-direction okay you know

0:25:49.430,0:25:54.020
you're following fantastic and so now if

0:25:51.890,0:25:57.320
I just run this one you wanna get the

0:25:54.020,0:26:00.530
initial size okay so now you have both

0:25:57.320,0:26:05.500
1d and 2d the point is that what is the

0:26:00.530,0:26:10.690
dimension of a convolutional kernel and

0:26:05.500,0:26:12.470
symbol for to the dimensional signal

0:26:10.690,0:26:14.809
again I repeat what is the

0:26:12.470,0:26:20.049
dimensionality of the collection of

0:26:14.809,0:26:20.049
careness use for two-dimensional data

0:26:20.860,0:26:27.679
again for right so four is gonna be the

0:26:24.710,0:26:30.470
number of dimensions that are required

0:26:27.679,0:26:35.659
to store the collection of kernels when

0:26:30.470,0:26:38.929
you perform 2d convolutions the one is

0:26:35.659,0:26:40.370
going to be the stride so if you don't

0:26:38.929,0:26:42.320
know how this works you just put a

0:26:40.370,0:26:44.000
question mark and gonna tell you here so

0:26:42.320,0:26:46.250
stride is gonna be telling you you

0:26:44.000,0:26:50.929
stride off you move every time the

0:26:46.250,0:26:53.269
kernel by one if you are the first one

0:26:50.929,0:26:55.460
means you only is the batch size so

0:26:53.269,0:26:57.380
torch expects you to always use batches

0:26:55.460,0:27:00.110
meaning how many signals you're using

0:26:57.380,0:27:02.450
just one right so that our expectation

0:27:00.110,0:27:04.549
if you send an input vector which is

0:27:02.450,0:27:06.350
going to be input tensor which has

0:27:04.549,0:27:12.289
dimension three is gonna be breaking and

0:27:06.350,0:27:14.809
complain okay so we have still some time

0:27:12.289,0:27:18.049
to go in the second part all right

0:27:14.809,0:27:20.269
second part is going to be so you've

0:27:18.049,0:27:23.779
been computing some derivatives right

0:27:20.269,0:27:26.860
for the first homework right so the

0:27:23.779,0:27:31.909
following homework maybe you have to do

0:27:26.860,0:27:35.000
you have to compute this one okay you're

0:27:31.909,0:27:35.510
supposed to be laughing it's a joke okay

0:27:35.000,0:27:39.529
there you go

0:27:35.510,0:27:43.340
fantastic so this is what you can wrote

0:27:39.529,0:27:47.840
back in the 90s for the computation of

0:27:43.340,0:27:50.029
the gradients of the of the lsdm which

0:27:47.840,0:27:53.210
are gonna be covered I guess in next

0:27:50.029,0:27:54.950
next lesson so how somehow so they had

0:27:53.210,0:27:58.070
to still do these things right it's kind

0:27:54.950,0:28:00.769
of crazy nevertheless we can use Python

0:27:58.070,0:28:03.500
to have automatic computation of these

0:28:00.769,0:28:06.500
gradients so we can go and check out how

0:28:03.500,0:28:12.159
these automatic gradient works

0:28:06.500,0:28:12.159
okay all right so

0:28:19.960,0:28:26.539
all right so we are going to be going

0:28:23.090,0:28:28.490
now to the notebook number three which

0:28:26.539,0:28:30.980
is the yeah

0:28:28.490,0:28:33.590
invisible let me see if I can highlight

0:28:30.980,0:28:39.200
it now it's even worse okay number three

0:28:33.590,0:28:41.619
Auto gratitute Oriole okay let me go

0:28:39.200,0:28:45.499
fullscreen

0:28:41.619,0:28:53.029
okay so out of our tutorial was gonna be

0:28:45.499,0:28:55.100
here here just create my tensor which

0:28:53.029,0:28:57.499
has as well these required gradients

0:28:55.100,0:28:59.990
equal true in this case I mean asking

0:28:57.499,0:29:02.539
torch please track all the gradient

0:28:59.990,0:29:05.169
computations did it got the competition

0:29:02.539,0:29:07.749
over the tensor such that we can perform

0:29:05.169,0:29:10.940
computation of partial derivatives okay

0:29:07.749,0:29:13.279
in this case I'm gonna have my Y is

0:29:10.940,0:29:16.659
going to be so X is simply gonna be one

0:29:13.279,0:29:20.419
two three four the Y is going to be X

0:29:16.659,0:29:22.879
subtracted number two okay alright so

0:29:20.419,0:29:26.869
now we can notice that there is this

0:29:22.879,0:29:29.720
grad F n grad f NN FN function here so

0:29:26.869,0:29:32.059
let's see what this stuff is we go sit

0:29:29.720,0:29:35.299
there and see oh this is a sub backward

0:29:32.059,0:29:37.629
what is it meaning that the Y has been

0:29:35.299,0:29:41.090
generated by a module which performs the

0:29:37.629,0:29:43.669
subtraction between X and and - right so

0:29:41.090,0:29:47.110
you have X minus 2 therefore if you

0:29:43.669,0:29:51.860
check who generated Y well there's a sub

0:29:47.110,0:29:58.249
a subtraction module ok so what's gonna

0:29:51.860,0:30:01.009
be now the God function of X you're

0:29:58.249,0:30:03.580
supposed to answer oh okay

0:30:01.009,0:30:03.580
why is none

0:30:05.409,0:30:10.460
because they should have written there

0:30:07.580,0:30:12.020
Alfredo generated that right okay all

0:30:10.460,0:30:14.360
right none is fine as well

0:30:12.020,0:30:17.000
okay so let's actually put our nose

0:30:14.360,0:30:19.190
inside we were here we can actually

0:30:17.000,0:30:23.770
access the first element you have the

0:30:19.190,0:30:23.770
accumulation why is the accumulation I

0:30:25.090,0:30:29.830
don't know I forgot but then if you go

0:30:27.710,0:30:32.450
inside there you're gonna see the

0:30:29.830,0:30:34.760
initial vector the initial tensor we are

0:30:32.450,0:30:36.830
using is the one two three four okay so

0:30:34.760,0:30:41.390
inside this computational graph you can

0:30:36.830,0:30:44.539
also find the original tensor okay all

0:30:41.390,0:30:46.880
right so let's now get the Z and inside

0:30:44.539,0:30:48.830
is gonna be my Y square times three and

0:30:46.880,0:30:51.620
then I compute my average a it's gonna

0:30:48.830,0:30:54.559
be the mean of Z right so if I compute

0:30:51.620,0:30:56.330
the square of this thing here and I

0:30:54.559,0:30:59.000
multiply by three and I take the average

0:30:56.330,0:31:00.500
so this is the square part times 3 and

0:30:59.000,0:31:03.289
then this is the average okay so you can

0:31:00.500,0:31:06.200
try if you don't believe me all right so

0:31:03.289,0:31:08.299
let's see how this thing looks like so

0:31:06.200,0:31:10.549
I'm gonna be promoting here all these

0:31:08.299,0:31:13.010
sequence of computations so we started

0:31:10.549,0:31:16.669
by from a two by two matrix what was

0:31:13.010,0:31:18.679
this guy here to buy - who is this X

0:31:16.669,0:31:22.399
okay you're following it cool then we

0:31:18.679,0:31:25.970
subtracted - right and then we

0:31:22.399,0:31:27.440
multiplied by Y twice right that's why

0:31:25.970,0:31:29.120
you have to ro so you get the same

0:31:27.440,0:31:31.669
subtraction that is the whyatt

0:31:29.120,0:31:33.559
the X minus 2 multiplied by itself then

0:31:31.669,0:31:36.649
you have another multiplication what is

0:31:33.559,0:31:39.830
this okay multiply by three and then you

0:31:36.649,0:31:42.980
have the final the mean backward because

0:31:39.830,0:31:48.649
this Y is green because it's mean no

0:31:42.980,0:31:51.140
okay yeah thank you for laughing okay so

0:31:48.649,0:31:56.899
I compute back prop right

0:31:51.140,0:31:59.409
what does backdrop do what does this

0:31:56.899,0:31:59.409
line do

0:32:00.360,0:32:08.610
I want to hear everyone you know already

0:32:04.580,0:32:09.840
we compute what radians right so black

0:32:08.610,0:32:11.580
propagation is how you compute the

0:32:09.840,0:32:16.740
gradients how do we train your networks

0:32:11.580,0:32:20.730
with gradients ain't right

0:32:16.740,0:32:24.320
or whatever Aaron said yesterday back

0:32:20.730,0:32:27.000
propagation is that is used for

0:32:24.320,0:32:28.289
computing the gradient completely

0:32:27.000,0:32:29.970
different things okay

0:32:28.289,0:32:32.250
please keep them separate don't merge

0:32:29.970,0:32:34.559
them everyone after a bit that don't

0:32:32.250,0:32:39.630
they don't see me those two things keep

0:32:34.559,0:32:43.740
colliding into one mushy thought don't

0:32:39.630,0:32:47.700
it's painful okay she'll compute the

0:32:43.740,0:32:51.659
gradients right so guess what we are

0:32:47.700,0:32:58.399
computing some gradients now okay so we

0:32:51.659,0:33:02.580
go on your page it's going to be what

0:32:58.399,0:33:06.899
what was a it was the average right so

0:33:02.580,0:33:10.529
this is 1/4 right the summation of all

0:33:06.899,0:33:13.799
those red eyes okay

0:33:10.529,0:33:17.460
what so I goes from 1 to 4

0:33:13.799,0:33:24.090
okay so what is that I said I is going

0:33:17.460,0:33:27.539
to be equal to 3y I square right yeah no

0:33:24.090,0:33:31.139
questions no okay all right and then

0:33:27.539,0:33:36.840
this one is was equal to 3 times X minus

0:33:31.139,0:33:38.899
2 square right so a what does it belong

0:33:36.840,0:33:38.899
to

0:33:40.490,0:33:45.740
where does a belong to what is the R

0:33:44.279,0:33:51.200
right so it's a scaler

0:33:45.740,0:33:55.500
okay all right so now we can compute da

0:33:51.200,0:33:58.110
over DX so how much is this stuff you're

0:33:55.500,0:34:00.570
gonna have 1/4 comes out forum here and

0:33:58.110,0:34:03.090
then you have you know let's have this

0:34:00.570,0:34:07.080
one with respect to the X ice element

0:34:03.090,0:34:09.179
okay so we're gonna have this one I

0:34:07.080,0:34:12.929
inside is that I have the three why I

0:34:09.179,0:34:15.899
Square and it's gonna be three X minus

0:34:12.929,0:34:18.780
two square right so these three comes

0:34:15.899,0:34:22.080
out here the two comes down as well and

0:34:18.780,0:34:25.589
then you multiply by X I minus 2 right

0:34:22.080,0:34:33.260
so far should be correct okay fantastic

0:34:25.589,0:34:33.260
all right so my X was this element here

0:34:33.589,0:34:38.190
actually let me compute as well this one

0:34:36.450,0:34:47.690
so this one goes away this one becomes

0:34:38.190,0:34:47.690
true this is 1.5 times X I minus 3 right

0:34:49.010,0:34:52.550
- 2 - 3

0:34:55.159,0:35:06.780
ok mathematics okay okay thank you

0:34:58.920,0:35:08.849
all right so what's gonna be my da over

0:35:06.780,0:35:11.339
the X I'm actually writing the transpose

0:35:08.849,0:35:15.300
directly here so for the first element

0:35:11.339,0:35:18.859
you have one you have one times 1.5 so

0:35:15.300,0:35:21.300
1.5 minus 3 you get 1 minus 1.5 right

0:35:18.859,0:35:23.670
second one is going to be 3 minus 3 you

0:35:21.300,0:35:25.619
get 0 Ryan this is 3 minus 3

0:35:23.670,0:35:27.420
maybe I should write everything right so

0:35:25.619,0:35:33.180
you're actually following so you have

0:35:27.420,0:35:37.589
1.5 minus 3 now you have 3 minus 3 below

0:35:33.180,0:35:43.650
you have 4 point 5 minus 3 and then the

0:35:37.589,0:35:47.160
last one is going to be 6 minus 3 which

0:35:43.650,0:35:54.410
is going to be equal to minus 1 point 5

0:35:47.160,0:35:59.789
0 1 point 5 and then 3 right you agree

0:35:54.410,0:36:04.259
ok let me just write this on here

0:35:59.789,0:36:06.149
okay just remember so we have you be

0:36:04.259,0:36:07.679
computed the backpropagation here I'm

0:36:06.149,0:36:14.609
gonna just bring it to the gradients and

0:36:07.679,0:36:24.509
then the right it's the same stuff we

0:36:14.609,0:36:27.630
got here right such that I don't have to

0:36:24.509,0:36:30.089
transpose it here whenever you perform

0:36:27.630,0:36:33.209
the partial derivative in PI tours you

0:36:30.089,0:36:34.799
get the same the same shape is the input

0:36:33.209,0:36:37.469
dimension so if you have a weight

0:36:34.799,0:36:39.079
whatever dimension then when you compute

0:36:37.469,0:36:41.069
the partial you still have the same

0:36:39.079,0:36:42.539
dimension they don't swap they don't

0:36:41.069,0:36:44.789
turn okay they just use this for

0:36:42.539,0:36:46.589
practicality at the correct version I

0:36:44.789,0:36:49.919
mean the the gradient should be the

0:36:46.589,0:36:52.919
transpose of that thing sorry did

0:36:49.919,0:36:54.479
Jacobian which is the transpose of the

0:36:52.919,0:36:58.709
gradient right if it's a vector but this

0:36:54.479,0:37:08.130
is a tensor so whatever we just used the

0:36:58.709,0:37:10.829
same same shape thing no so this one

0:37:08.130,0:37:13.639
should be a flipping I believe maybe I'm

0:37:10.829,0:37:17.369
wrong but I don't think all right so

0:37:13.639,0:37:19.919
this is like basic these basic PI tours

0:37:17.369,0:37:21.029
now you can do crazy stuff because we

0:37:19.919,0:37:23.609
like crazy right

0:37:21.029,0:37:25.309
I mean I do I think if you like me you

0:37:23.609,0:37:29.669
like crazy

0:37:25.309,0:37:32.009
right okay so here I just create my

0:37:29.669,0:37:34.259
vector X which is going to be a three

0:37:32.009,0:37:38.449
dimensional well a one-dimensional

0:37:34.259,0:37:43.769
tensor of three items I'm going to be

0:37:38.449,0:37:46.979
multiplying X by two then I call this

0:37:43.769,0:37:49.859
one Y then I start my counter to zero

0:37:46.979,0:37:52.859
and then until the norm of the Y is long

0:37:49.859,0:37:56.699
thousand below thousand I keep doubling

0:37:52.859,0:37:59.399
Y okay and so you can get like a dynamic

0:37:56.699,0:38:01.529
graph right the graph is base is

0:37:59.399,0:38:03.299
conditional to the actual random

0:38:01.529,0:38:04.979
initialization which you can't even tell

0:38:03.299,0:38:06.329
because I didn't even use a seed so

0:38:04.979,0:38:08.999
everyone that is running this stuff is

0:38:06.329,0:38:09.329
gonna get different numbers so these are

0:38:08.999,0:38:11.910
the

0:38:09.329,0:38:21.839
final values of the why can you tell me

0:38:11.910,0:38:23.549
how many iterations we run so the mean

0:38:21.839,0:38:26.279
of this stuff is actually lower than a

0:38:23.549,0:38:27.630
thousand yeah but then I'm asking

0:38:26.279,0:38:36.209
whether you know how many times this

0:38:27.630,0:38:41.119
loop went through no good why it's

0:38:36.209,0:38:43.440
random Rises you know it's bad question

0:38:41.119,0:38:45.539
about bad questions next time I have a

0:38:43.440,0:38:47.609
something for you okay so I'm gonna be

0:38:45.539,0:38:51.569
printing this one now I'm telling you

0:38:47.609,0:38:53.819
the grabbed are 2048 right

0:38:51.569,0:38:55.589
just check the central one for the

0:38:53.819,0:38:57.299
moment right this is the actual gradient

0:38:55.589,0:39:04.739
so can you tell me now how many times

0:38:57.299,0:39:10.950
the loop went on so someone said 11 how

0:39:04.739,0:39:14.420
many ends up for 11 okay for people just

0:39:10.950,0:39:14.420
roast their hands what about the others

0:39:14.809,0:39:17.809
21

0:39:18.289,0:39:27.299
okay any other guys 11 10

0:39:25.529,0:39:30.749
okay we have actually someone that has

0:39:27.299,0:39:33.930
the right solution and this loop went on

0:39:30.749,0:39:35.759
for 10 times why is that because you

0:39:33.930,0:39:38.549
have the first multiplication by 2 here

0:39:35.759,0:39:40.589
and then loop goes on over and over and

0:39:38.549,0:39:43.289
multiplies by 2 right so the final

0:39:40.589,0:39:45.239
number is gonna be the least number of

0:39:43.289,0:39:47.999
iterations in the loop plus the

0:39:45.239,0:39:50.779
additional like addition and

0:39:47.999,0:39:55.140
multiplication outside right yes no

0:39:50.779,0:39:56.670
you're sleeping maybe okay I told you

0:39:55.140,0:40:00.690
not to eat before class otherwise you

0:39:56.670,0:40:05.009
get groggy okay so inference this is

0:40:00.690,0:40:07.769
cool so here I'm gonna be just having

0:40:05.009,0:40:09.420
both my X & Y we are gonna just do

0:40:07.769,0:40:13.440
linear regression right linear or

0:40:09.420,0:40:17.670
whatever think the add operator is just

0:40:13.440,0:40:18.990
the scalar product okay so both the X

0:40:17.670,0:40:21.589
and W has

0:40:18.990,0:40:23.640
have the requires gradient equal to true

0:40:21.589,0:40:27.119
being this means we are going to be

0:40:23.640,0:40:29.010
keeping track of the the gradients and

0:40:27.119,0:40:31.290
the computational graph so if I execute

0:40:29.010,0:40:34.680
this one you're gonna get the partial

0:40:31.290,0:40:37.710
derivatives of the inner product with

0:40:34.680,0:40:40.380
respect to the Z with respect to the

0:40:37.710,0:40:43.920
input is gonna be the weights right so

0:40:40.380,0:40:45.869
in the range is the input right and the

0:40:43.920,0:40:47.160
ones are the weights so partial

0:40:45.869,0:40:48.570
derivative with respect to the input is

0:40:47.160,0:40:50.070
gonna be the weights partial with

0:40:48.570,0:40:54.960
respect to the weights are gonna be the

0:40:50.070,0:40:56.670
input right yes no yes okay now I just

0:40:54.960,0:40:58.589
you know usually it's this one is the

0:40:56.670,0:41:00.359
case I just have required gradients for

0:40:58.589,0:41:03.349
my parameters because I'm gonna be using

0:41:00.359,0:41:06.030
the gradients for updating later on the

0:41:03.349,0:41:09.420
the parameters of the mother is so in

0:41:06.030,0:41:12.300
this case you get none let's have in

0:41:09.420,0:41:14.460
this case instead what I usually do

0:41:12.300,0:41:17.250
wanna do inference when I do inference I

0:41:14.460,0:41:19.530
tell torch a torch stop tracking any

0:41:17.250,0:41:22.950
kind of operation so I say torch no God

0:41:19.530,0:41:25.830
please so this one regardless of whether

0:41:22.950,0:41:28.859
your input always have the required

0:41:25.830,0:41:31.970
grass true or false whatever when I say

0:41:28.859,0:41:35.060
torch no brats you do not have any

0:41:31.970,0:41:37.170
computation a graph taken care of right

0:41:35.060,0:41:41.130
therefore if I try to run back

0:41:37.170,0:41:44.130
propagation on a tensor which was

0:41:41.130,0:41:46.320
generated from like doesn't have

0:41:44.130,0:41:48.119
actually you know graph because this one

0:41:46.320,0:41:50.940
doesn't have a graph you're gonna get an

0:41:48.119,0:41:52.859
error okay so if I run this one you get

0:41:50.940,0:41:55.410
an error and you have a very angry face

0:41:52.859,0:41:58.470
here because it's an error and then it

0:41:55.410,0:42:00.720
takes your element 0 of tensor does not

0:41:58.470,0:42:05.070
require grads and does not have a god

0:42:00.720,0:42:07.650
function right so II which was the yeah

0:42:05.070,0:42:09.720
whatever they reside here actually then

0:42:07.650,0:42:11.400
you couldn't run back problems that

0:42:09.720,0:42:18.420
because there is no graph attached to

0:42:11.400,0:42:19.710
that ok questions this is so powerful

0:42:18.420,0:42:23.849
you cannot do it this time with tensor

0:42:19.710,0:42:26.790
you okay tensor flow is like whatever

0:42:23.849,0:42:29.040
yeah more stuff here actually more stuff

0:42:26.790,0:42:30.600
coming right now

0:42:29.040,0:42:33.670
[Applause]

0:42:30.600,0:42:36.340
so we go back here we have inside the

0:42:33.670,0:42:38.440
extra folder he has some nice cute

0:42:36.340,0:42:40.450
things I wanted to cover both of them

0:42:38.440,0:42:43.210
just that we go just for the second I

0:42:40.450,0:42:47.290
think sorry the second one is gonna be

0:42:43.210,0:42:49.890
the following so in this case we are

0:42:47.290,0:42:52.750
going to be generating our own specific

0:42:49.890,0:42:54.970
modules so I like let's say I'd like to

0:42:52.750,0:42:58.030
define my own function which is super

0:42:54.970,0:43:00.190
special amazing function I can decide if

0:42:58.030,0:43:02.560
I want to use it for you know training

0:43:00.190,0:43:04.450
Nets I need to get the forward pass and

0:43:02.560,0:43:06.220
also have to know what is the partial

0:43:04.450,0:43:08.110
derivative of the input respect to the

0:43:06.220,0:43:10.930
output such that I can use this module

0:43:08.110,0:43:13.660
in any kind of you know point in my

0:43:10.930,0:43:15.670
inner code such that you know by using

0:43:13.660,0:43:18.310
back prop you know chain rule you just

0:43:15.670,0:43:20.320
plug the thing no young went on several

0:43:18.310,0:43:21.640
times as long as you know partial

0:43:20.320,0:43:23.410
derivative of the output with respect to

0:43:21.640,0:43:27.010
the input you can plug these things

0:43:23.410,0:43:31.690
anywhere in your chain of operations so

0:43:27.010,0:43:33.400
in this case we define my addition which

0:43:31.690,0:43:35.620
is performing the addition of the two

0:43:33.400,0:43:38.470
inputs in this case but then when you

0:43:35.620,0:43:41.130
perform the back propagation if you have

0:43:38.470,0:43:43.420
an addition what is the back propagation

0:43:41.130,0:43:47.020
so if you have a addition of the two

0:43:43.420,0:43:48.370
things you get an output when you send

0:43:47.020,0:43:53.320
down the gradients what does it happen

0:43:48.370,0:43:55.330
with the with the gradient it gets you

0:43:53.320,0:43:57.160
know copied over both sides right and

0:43:55.330,0:43:59.080
that's why you get both of them are

0:43:57.160,0:44:01.390
copies or the same thing and they are

0:43:59.080,0:44:02.950
sent through one side of the other you

0:44:01.390,0:44:05.170
can execute this stuff you're gonna see

0:44:02.950,0:44:07.630
here you get the same gradient both ways

0:44:05.170,0:44:09.460
in this case I have a split so I come

0:44:07.630,0:44:11.200
from the same thing and then I split and

0:44:09.460,0:44:13.180
I have those two things doing something

0:44:11.200,0:44:16.960
else if I go down with the gradient what

0:44:13.180,0:44:20.080
do I do you add them right and that's

0:44:16.960,0:44:21.550
why we have here the add install you can

0:44:20.080,0:44:23.680
execute this one you're going to see

0:44:21.550,0:44:25.960
here that we had these two initial

0:44:23.680,0:44:27.910
gradients here and then when you went up

0:44:25.960,0:44:29.380
or sorry when you went down the two

0:44:27.910,0:44:30.790
things the two gradients sum together

0:44:29.380,0:44:33.820
and they are here okay

0:44:30.790,0:44:36.190
so again if you use pre-made things in a

0:44:33.820,0:44:38.470
PI torch they are correct this one you

0:44:36.190,0:44:41.080
can mess around you can put any kind of

0:44:38.470,0:44:45.130
different in

0:44:41.080,0:44:47.950
for a function and backward function I

0:44:45.130,0:44:53.980
think we ran out of time other questions

0:44:47.950,0:44:58.800
before we actually leave no all right so

0:44:53.980,0:44:58.800
I see on Monday and stay warm but


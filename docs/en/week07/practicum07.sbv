0:00:00.030,0:00:04.920
all right so foundation of deep learning me again all right

0:00:04.920,0:00:09.080
yeah so again do interrupt me is going to be the last class we have in person

0:00:09.080,0:00:12.750
we have a very bad situation especially in Italy

0:00:12.750,0:00:16.529
people are dying we don't have any more beds in the hospitals we don't have

0:00:16.529,0:00:21.449
doctors doctor doctors are working like 24/7 and they are going home and they

0:00:21.449,0:00:25.260
are effecting their own families so it's like a really bad situation right now

0:00:25.260,0:00:30.810
back home and it looks like we are two weeks away from that situation under

0:00:30.810,0:00:38.129
here so just wash your hands try not to go in very crowded places stay healthy

0:00:38.129,0:00:42.950
okay please a third class all right self or unsupervised learning

0:00:46.789,0:00:50.520
generative models capabilities so here I'm going to be giving you some eye

0:00:50.520,0:00:54.870
candies such that you can get hungry and then I can feed you with the first part

0:00:54.870,0:00:58.199
of this lesson this lesson is gonna be split in two parts half is gonna be

0:00:58.199,0:01:06.360
today hot is gonna be online through zoom I think system so who hangs up who

0:01:06.360,0:01:13.250
thinks the picture on the left it's real okay now hands up who thinks that the

0:01:18.299,0:01:29.970
picture on the right is real okay who thinks that both of them are real who

0:01:29.970,0:01:37.290
think that this one is fake who thinks that this one is fake who thinks that

0:01:37.290,0:01:44.299
both are fake okay you are right so these two images have been generated by

0:01:44.299,0:01:51.210
modeling that we trained actually this guy, Karras, train. So if you go on the

0:01:51.210,0:01:57.060
website this person does not exist you can find several examples of very

0:01:57.060,0:02:02.310
good-looking non-existent people if you keep clicking sometimes you're gonna

0:02:02.310,0:02:06.780
find some person with a hole in their faces that's kind of very easy to

0:02:06.780,0:02:13.130
recognize that is not quite likely a real person but otherwise

0:02:13.130,0:02:18.530
look pretty legit you can notice that they have very nice teeth very nice

0:02:18.530,0:02:25.730
you know cheek you know chicken feet like things or you call them you can

0:02:25.730,0:02:29.990
clearly tell here though that if you check on the background behind the

0:02:29.990,0:02:33.440
network is not producing a very accurate the ground although the faces looks very

0:02:33.440,0:02:36.920
good why is that because the network has been provided

0:02:36.920,0:02:40.130
many samples and those samples are refiguring

0:02:40.130,0:02:45.770
faces the thing that is not constant is the background right so that the

0:02:45.770,0:02:49.880
background is the variable here and therefore you can't learn any possible

0:02:49.880,0:02:55.580
background so the become the ground will look like some weird stuff because there

0:02:55.580,0:03:01.520
is much more variability then what are the possible appearances of a face how

0:03:01.520,0:03:07.370
many given a face how many in how many way you can you distort her face a human

0:03:07.370,0:03:15.230
face like if your face how many degrees of freedom does he have ah you know this

0:03:15.230,0:03:19.790
answer right we already covered this in class eight

0:03:19.790,0:03:28.490
she said eight how much thousand and fifty ? fifty yeah that's 50 correct so

0:03:28.490,0:03:32.480
you have roughly fifty a mass of something less plus you know rotation

0:03:32.480,0:03:37.820
tilting and what so not what not so you know all possible it's just a manifold

0:03:37.820,0:03:42.230
in 50 dimensions so everything nice doesn't quite is outside that manifold

0:03:42.230,0:03:46.310
although this picture you know RF several megapixel pictures so each point

0:03:46.310,0:03:50.330
here is living in this huge dimensional space although all the possible

0:03:50.330,0:03:54.740
variations are restrained to a subspace okay so that's the training manifold the

0:03:54.740,0:04:02.270
data manifold okay check out their website okay so we have here a very cute

0:04:02.270,0:04:10.910
dog oh and on the other side less cute bird sorry oh yeah if you do a linear

0:04:10.910,0:04:15.860
interpolation between the dog oh and the bird what are you expecting to see in

0:04:15.860,0:04:19.760
the middle and for this one I'm gonna be actually turning off the light not

0:04:19.760,0:04:28.129
turning it back on the next soon after so again so a blurry image what do you

0:04:28.129,0:04:32.440
mean so what do you expect to get over here

0:04:33.879,0:04:45.590
roughly yeah you can talk back so I'm gonna be doing 100% this one + 0. 90% this

0:04:45.590,0:04:50.659
one plus 10%. 80% this one + 20% this one. So what are you gonna be

0:04:50.659,0:04:56.300
seen here you should guess you're less than usual so you actually have to talk

0:04:56.300,0:04:59.500
back more than usual flying square no ok so you have a dog

0:05:03.710,0:05:07.699
you have a bird if I do like a linear interpolation of these two images in

0:05:07.699,0:05:16.789
pixel space what do you get second sorry yeah super super position right so

0:05:16.789,0:05:20.090
you're gonna get something that looks like this you can get basically overlay

0:05:20.090,0:05:24.770
of the first image with a second image alright so let's get back here and

0:05:24.770,0:05:28.849
instead let's do a interpolation in the latent space of the network so I input

0:05:28.849,0:05:33.620
both those two guys I get a 2 and representation those hidden layers then

0:05:33.620,0:05:37.490
I do a linear interpolation between the hidden layers and then I do the decoding

0:05:37.490,0:05:46.909
part what do you expect to see here and you still have to talk back to me you

0:05:46.909,0:05:53.090
have to shout the background feel a mess but then what's gonna be this the

0:05:53.090,0:06:00.440
subject right right so you're gonna be starting scene where the dog and then a

0:06:00.440,0:06:08.960
doggie bird okay and that's how it looks birdie dog in a birdie birdie birdie dog

0:06:08.960,0:06:13.819
in a doggie bird okay alright this is a network that did this

0:06:13.819,0:06:18.169
atrocious Frankie's blasphemy you should watch full make a Fullmetal

0:06:18.169,0:06:23.960
Alchemist episode where the father takes the daughter and the dog and makes

0:06:23.960,0:06:30.050
something similar it was so interesting ok if you watch cartoons Brotherhood

0:06:30.050,0:06:33.699
right a second second season okay aha

0:06:35.689,0:06:40.430
I don't know because it's a lot of green so I guess it got rid of it I don't know

0:06:40.430,0:06:46.189
and I guess it yeah I don't know good question though yeah good I so let

0:06:46.189,0:06:48.169
me show you a few more so this stuff comes from

0:06:48.169,0:06:54.500
Brock's article and that you can you have a smaller like so the first one you

0:06:54.500,0:06:58.490
have a small version of those images we start from something like looks like a

0:06:58.490,0:07:04.879
shark or a monta I think is a name then that stuff looks like a poly puss or how

0:07:04.879,0:07:11.599
do you call them again octopus right no yeah it's something like octopus and

0:07:11.599,0:07:16.039
then the octopus becomes like monkey and then it looks like a dog right so you

0:07:16.039,0:07:20.629
can see the shark becomes octopus that becomes a monkey that becomes a dog so

0:07:20.629,0:07:24.529
interesting like how you can change breeds by just having the final two

0:07:24.529,0:07:28.340
points fixed and then you walk through the latent space so you have another one

0:07:28.340,0:07:33.969
here you go from these puppy here to our bird so again you have a birdie squirrel

0:07:33.969,0:07:38.539
whatever and then you have a bird and then the other side you have a doggie or

0:07:38.539,0:07:45.440
this one you have like the dismally guy what's called a chunk skunk thank you

0:07:45.440,0:07:50.900
oh oh that's why you smell like a skunk is the same right huh okay I see so that

0:07:50.900,0:07:55.159
if you have a skunk here and then you get to a dog rabbit it actually looks a

0:07:55.159,0:08:03.319
lot of a dog after this thing here and finally you get a bird turn into a fly I

0:08:03.319,0:08:08.419
think these are pretty awesome examples so this should make you hungry such that

0:08:08.419,0:08:19.310
I can feed you with the second part of the class right so you have this image

0:08:19.310,0:08:22.430
of one side you have this image on the other side you get the embeddings and

0:08:22.430,0:08:25.699
you do some basically interpolation of the embedding you can check out the

0:08:25.699,0:08:30.439
paper for more details the point here is just to show you that the difference

0:08:30.439,0:08:35.120
between interpolation in pixel space yeah what is the difference between

0:08:35.120,0:08:38.630
interpolation in pixel space in the latent space so the latent space capture

0:08:38.630,0:08:43.090
what is the basic semantics of an image and therefore you

0:08:43.090,0:08:48.250
you go from the pixel space which kind of doesn't really play well with our

0:08:48.250,0:08:54.790
tools to kind of more well-behaved space which is our internal hidden

0:08:54.790,0:08:59.050
representation of the data of the end of the network again this is just to give

0:08:59.050,0:09:04.660
you some appetite right not I'm not gonna be I'm not form a little what next

0:09:04.660,0:09:10.720
okay you can zoom on dogs you can shift you can shift on the X on the Y you can

0:09:10.720,0:09:13.870
do change the brightness the interesting part here is that when you change the

0:09:13.870,0:09:18.610
brightness actually you change day to day to night or night today because

0:09:18.610,0:09:23.770
that's what the most normal brightness change in pictures looks like okay so

0:09:23.770,0:09:26.830
whenever you change brightness you actually change the time of the day or

0:09:26.830,0:09:31.510
you have a 2d rotation or even a 3d rotation is so interesting means that

0:09:31.510,0:09:36.190
the network and somehow have a internal representation of the 3d world

0:09:36.190,0:09:41.800
that's something yan was mentioning yesterday if you shift you know a little

0:09:41.800,0:09:47.350
bit you see this kind of parallax then the easiest way to express to basically

0:09:47.350,0:09:52.120
address this you know phenomenon is actually to imply that there is a 3D world

0:09:52.120,0:10:02.430
okay so in this case they trained the network in order to actually be able to

0:10:02.430,0:10:08.710
you know handle specific transformation actually I have to put the reference on

0:10:08.710,0:10:16.210
this paper again I'm just giving you some how to say I can this okay I'm not

0:10:16.210,0:10:21.220
giving you any more any formal thing right now in this case actually I like

0:10:21.220,0:10:29.590
this on so much you can get the animal version of your picture right so you

0:10:29.590,0:10:33.430
could try to stop by don't try with hentai right we don't want to do those

0:10:33.430,0:10:38.590
things okay all right so some of you actually know this stuff interesting

0:10:38.590,0:10:44.470
thank you right the one that don't it's okay just

0:10:44.470,0:10:50.139
forget all right okay this is other stuff which is pretty cool like you can

0:10:50.139,0:10:54.459
go from low resolution to high resolution from the left hand side to

0:10:54.459,0:10:58.449
hide right hand side and here you have a few examples but this is of course it's

0:10:58.449,0:11:01.839
black on white it's much easier to do this stuff and the same for the zebra

0:11:01.839,0:11:05.230
and this is a pre deep learning technique so they were just using some

0:11:05.230,0:11:09.879
hierarchical model nevertheless it works pretty well and then a few years later

0:11:09.879,0:11:16.209
you've actually gave Garcia getting you you know really high this is like the

0:11:16.209,0:11:20.619
hub sampling by doing like linear interpolation by linear interpolation

0:11:20.619,0:11:24.879
and instead this one is gonna be the up sampling down with the neural net and

0:11:24.879,0:11:29.679
the final one is actually the real image you can see clearly on the third row how

0:11:29.679,0:11:39.759
these Asian dude became European white that bias right correct so the network

0:11:39.759,0:11:43.899
has seen a lot of white dudes and therefore the most the easiest way to

0:11:43.899,0:11:49.929
reconstruct a kind of unknown face features it can be to plug there a white

0:11:49.929,0:11:55.299
dude face or this lady looks like she has in she's having a stroke because she

0:11:55.299,0:12:00.910
we don't have many side views of these images right these this lady became a

0:12:00.910,0:12:05.740
changed sex on the bottom side and then this guy it looks like he had an

0:12:05.740,0:12:10.869
accident because again we didn't have many glasses on the dataset therefore

0:12:10.869,0:12:15.369
the network you know so some very dark thing and then you know implied someone

0:12:15.369,0:12:22.240
just punched him very hardly okay but again this is very old stuff I mean we

0:12:22.240,0:12:26.290
are four years in the future now again these are the first results and this

0:12:26.290,0:12:32.079
allows you to leverage you know actual data in order to fill in the gaps and

0:12:32.079,0:12:36.360
the gaps are what are they actually actual detail we have very very many new

0:12:36.360,0:12:40.509
results recently but these organ are the kind of pioneering result and pioneering

0:12:40.509,0:12:47.230
examples one more here you basically block the face with a gray square and

0:12:47.230,0:12:52.820
then you ask the network to reconstruct the face such that it gives you

0:12:52.820,0:12:57.680
the best-looking closing point right so you take an image which is staying on

0:12:57.680,0:13:03.950
the training manifold you put a patch on the face this patch will make the image

0:13:03.950,0:13:08.210
go away from the training manifold now you can do for example gradient descent

0:13:08.210,0:13:13.430
in this energy space such that you can find what is the closest point like the

0:13:13.430,0:13:17.570
point with the lowest energy that is associated to that specific initial

0:13:17.570,0:13:21.110
image okay so you get an image you perturb the image you that makes it go

0:13:21.110,0:13:24.650
away from the training manifold and then you can do great in the saint's in the

0:13:24.650,0:13:29.710
energy landscape such that you can pick the whatever sample looks like the most

0:13:29.710,0:13:33.860
you know it's only the closest sample on the training manifold and this is some

0:13:33.860,0:13:38.090
stuff yan was covering yesterday about energy based model whenever you learn an

0:13:38.090,0:13:41.780
energy then you can actually use the energy to do inference to do inference

0:13:41.780,0:13:45.740
you actually have to minimize the energy right so energy minimization means

0:13:45.740,0:13:49.160
inference not training training something else

0:13:49.160,0:13:52.850
again we're going to be covering more detail energy based models in the

0:13:52.850,0:13:58.010
following classes here you have another few examples one using a variational

0:13:58.010,0:14:04.460
encoder and another using generative other cyanate this is also another

0:14:04.460,0:14:10.520
example from Reed this is crazy you can go from English description to actual

0:14:10.520,0:14:15.980
drawing of what the English description mean right so you go from a sequence I

0:14:15.980,0:14:20.600
guess to a vector which is like the concept and then from the concept you

0:14:20.600,0:14:24.850
use a decoder so it's a generative net which is going to be decoding your

0:14:24.850,0:14:33.800
specific output and that was pretty much it for the eye candy so this should make

0:14:33.800,0:14:39.400
you very hungry for the second part of the class are you hungry

0:14:39.400,0:14:48.230
yeah I didn't have dinner either either okay all right so out encoders

0:14:48.230,0:14:51.470
what are these stuff I'm supervised learning so this is our first model

0:14:51.470,0:14:55.760
we're going to be diving into in order to see how we can train a network

0:14:55.760,0:15:03.440
without targets or labels what are target's what was the difference between

0:15:03.440,0:15:09.860
target and labels you already know everything someone else

0:15:09.860,0:15:18.480
prediction means both of them are actually given both of them are

0:15:18.480,0:15:25.080
annotations thank you so labels are going to be categorical you can label

0:15:25.080,0:15:29.970
things this is a chair that's a table that doors targets are going to be you

0:15:29.970,0:15:37.230
know the target okay all right so we're going to be observing how we can train

0:15:37.230,0:15:43.590
this model without actually having targets or labels so this is the first

0:15:43.590,0:15:48.690
network which is the architecture we are going to be playing with it's very

0:15:48.690,0:15:53.370
similar to what we observe so far but the big difference is that we start from

0:15:53.370,0:15:58.200
the bottom we know already why it's pink you got to intermediate hidden layer in

0:15:58.200,0:16:05.340
green then you get top two the input so the output of the network is going to be

0:16:05.340,0:16:09.510
the prediction of the input okay so you have also a different kind of

0:16:09.510,0:16:13.590
representation you may have so that those are DEA questions the hidden

0:16:13.590,0:16:17.880
layers going to be the rotation squash rotation on my input and then the final

0:16:17.880,0:16:22.830
output is going to be the I guess squash version of the rotation of the hidden

0:16:22.830,0:16:27.450
okay where rotation means affine transformations we have some

0:16:27.450,0:16:32.480
dimensionality so we shoot towards D but then both x and x̂ live in ℝⁿ.

0:16:32.480,0:16:36.420
Therefore the second part is going to be our generative Network this one that

0:16:36.420,0:16:40.980
goes from h to x̂. Is going to be my generative net and here you have a

0:16:40.980,0:16:44.850
different diagram which is basically doing the same thing but this one is you

0:16:44.850,0:16:48.840
know some people prefer these diagrams where the actual transformations are

0:16:48.840,0:16:53.550
made in boxes okay so you have people saying this is a two layer neural net

0:16:53.550,0:17:01.680
although we know that is a how many layer neural net how many balls you see

0:17:01.680,0:17:08.580
three layers neural network fantastic that our my convention okay if we use

0:17:08.580,0:17:14.070
yawns notation then we also have okay hold on so you actually can have some

0:17:14.070,0:17:17.620
tights weights in order to be trying to reproduce a PCA

0:17:17.620,0:17:24.970
although you don't have guarantees over the order of the different weight all of

0:17:24.970,0:17:29.710
the different basis but if we use the onion notation we are going to be also

0:17:29.710,0:17:34.690
adding those kind of little project is there which are representing the

0:17:34.690,0:17:41.100
transformations okay so why are we using out encoders what's the point of

0:17:41.100,0:17:51.940
predicting my input let's say I use a identity matrix okay I provide input

0:17:51.940,0:17:56.170
which is a vector I put apply this vector I do want my

0:17:56.170,0:18:04.720
identity matrix times my vector I get the same vector this out encoder so you

0:18:04.720,0:18:18.760
get the same thing as you put in why the hell are we doing this so you don't have

0:18:18.760,0:18:22.750
the input when you predict but then I can just learn the identity matrix right

0:18:22.750,0:18:26.950
I have identity matrix I put some thinning I get something out I put

0:18:26.950,0:18:31.630
something in I put something out I get put same thing in I get the same thing

0:18:31.630,0:18:36.040
out so the most thing the most trivial thing to learn for a network would be

0:18:36.040,0:18:40.870
the identity matrix right I thing I put inside comes out and that's how we train

0:18:40.870,0:18:53.020
this stuff second sorry if d is less right okay there is a first point

0:18:53.020,0:19:00.400
so if we have a weapon if you have intermediate dimensionality D lower than

0:19:00.400,0:19:05.830
n therefore we can start seeing what this guy this stuff can be used for for

0:19:05.830,0:19:09.429
example could be used for compression so if I have an intermediate representation

0:19:09.429,0:19:13.900
which takes less space than my input representation I can use this encoder is

0:19:13.900,0:19:18.940
a compressor then I have my hinny representation my code which is you know

0:19:18.940,0:19:23.350
addressing what specific input is it takes to this space so I can use a you

0:19:23.350,0:19:28.090
know image compressor for example for example here so that was my initial idea

0:19:28.090,0:19:31.420
for out encoders but that's just one type of

0:19:31.420,0:19:36.960
and it's kind of you know not the proper way of thinking about these guys

0:19:36.960,0:19:44.760
so another encoder task is to be able to reconstruct data that lives on the

0:19:44.760,0:19:51.970
manifold okay so we have a data manifold we get some points we have data points I

0:19:51.970,0:19:57.070
use these points for training my system and I'd like my out encoder to be able

0:19:57.070,0:20:01.600
to reconstruct only things that live on the training manifold on the data

0:20:01.600,0:20:06.160
manifold okay so that's our that's actually what the task what is the task

0:20:06.160,0:20:10.480
of these out encoders only to reconstruct a small subset and I getting

0:20:10.480,0:20:17.230
tripped off by my microphone one sec I only we should okay this is too short

0:20:17.230,0:20:21.640
okay we should be only able to reconstruct we had to enforce only to be

0:20:21.640,0:20:26.140
able to reconstruct a small set of possible inputs okay now it becomes

0:20:26.140,0:20:31.300
interesting because if you can only reconstruct a small set of inputs then

0:20:31.300,0:20:36.400
you cannot reconstruct things that are away right so for example like before I

0:20:36.400,0:20:44.110
show you I have put I put a picture I have a picture and I have a gray box in

0:20:44.110,0:20:48.370
front of my face so I take my point and I take it away from my training manifold

0:20:48.370,0:20:52.780
if I try to reconstruct it and I network and only reconstruct things that are on

0:20:52.780,0:20:57.670
a manifold it will reconstruct something that is here which doesn't have that

0:20:57.670,0:21:04.030
patch on the face okay you see this or no so if you're only constrained to

0:21:04.030,0:21:08.140
reconstruct things that have been observed during training any variation

0:21:08.140,0:21:12.850
that you apply to the you know new inputs later on during when you can be

0:21:12.850,0:21:16.200
using this network is going to be read removed because the network will be

0:21:16.200,0:21:21.570
insensitive to that kind of perturbations so let's see a bit of more

0:21:21.570,0:21:25.110
details about this stuff is it clear so far

0:21:25.110,0:21:33.340
yes no okay all right so let's figure out what our D reconstruction losses we

0:21:33.340,0:21:38.590
have we can use so the first one we have the classical you know loss for the

0:21:38.590,0:21:42.640
overall data set is going to be the average between my per sample losses

0:21:42.640,0:21:47.890
okay and so there are two person Nasus the first person per loss is going

0:21:47.890,0:21:55.720
to be the binary binary cross-entropy which is going to be penalizing a lot if

0:21:55.720,0:21:59.350
you make a mistake so the output the the targets are going

0:21:59.350,0:22:04.090
to be 0 or 1 so I have a categorical distribution and then your input does

0:22:04.090,0:22:08.410
not be sorry you're out who's going to be something that also leaves between 0

0:22:08.410,0:22:12.280
and 1 so you have a sigmoid network a sigmoid a nonlinear function at the end

0:22:12.280,0:22:17.380
and then you get you try to minimize this guy here otherwise if you have real

0:22:17.380,0:22:22.900
value inputs and outputs which are for example images color images you may want

0:22:22.900,0:22:31.900
to use the MSC okay all right so as someone yeah it's the your friend

0:22:31.900,0:22:37.030
they're mentioned before we have you know pretty it's pretty obvious obvious

0:22:37.030,0:22:41.409
to think about like under complete hidden layer a hundred complete hidden

0:22:41.409,0:22:48.280
layer has a dimensionality which is smaller than the size of the input so in

0:22:48.280,0:22:54.400
this case the network not perhaps copy or use the identity matrix because you

0:22:54.400,0:22:56.919
have an intermediate representation which is smaller and then you had to

0:22:56.919,0:23:01.539
expand this one back to the original dimensionality again you can use a under

0:23:01.539,0:23:06.309
complete hidden layer how to encoder for doing compression for example ok so this

0:23:06.309,0:23:12.070
is pretty standard I would say does it make sense so far ok so we're gonna play

0:23:12.070,0:23:17.380
with this in a second on the notebook nevertheless I will say I actually like

0:23:17.380,0:23:24.880
this one more and you are gonna be telling me why you should be able to I

0:23:24.880,0:23:29.440
mean you should have all the ingredients it's gonna be what's this is the six

0:23:29.440,0:23:34.450
week I think seventh six six week seventh week I think I'm not

0:23:34.450,0:23:41.730
sure why do I want a larger intermediate representation

0:24:01.910,0:24:05.029
[Music] [Music]

0:24:31.870,0:24:37.520
right so we always said that the larger we go into intermediate presentation the

0:24:37.520,0:24:42.049
easiest is going to be the optimization right and so although the information

0:24:42.049,0:24:47.000
that is contained in the first layer and in the hidden layer is gonna be the same

0:24:47.000,0:24:51.620
I can't add information but it's much easier now for the network to play with

0:24:51.620,0:24:57.919
a representation that has you know much many more dimensions the point is that

0:24:57.919,0:25:01.820
we can simply learn now the identity matrix and we're going to be just

0:25:01.820,0:25:06.409
copying everything you know you copy the first guy in the first post post spot

0:25:06.409,0:25:09.740
the second guy you copy here the third one you copied here now you copy

0:25:09.740,0:25:13.480
everything through you haven't learned nothing have learned the identity

0:25:13.480,0:25:19.549
therefore we have to learn we have to apply some other types of constraint for

0:25:19.549,0:25:24.320
the information and so we have to learn how to introduce now we have to

0:25:24.320,0:25:29.330
introduce now a information bottleneck okay although we enlarge the

0:25:29.330,0:25:33.500
intermediate representation we have to constrain the representation we have to

0:25:33.500,0:25:40.640
constrain the possible configurations that the hidden layer can take the input

0:25:40.640,0:25:45.110
layer can take as many configurations as you want the hidden layer should be only

0:25:45.110,0:25:49.760
containing we represent the possible configuration that the training data are

0:25:49.760,0:25:53.630
the data on that manifold can have okay so the input can be everything you want

0:25:53.630,0:25:58.549
but you can be training only with data that is on the manifold therefore the

0:25:58.549,0:26:03.270
hidden layer only to be able to model to capture what

0:26:03.270,0:26:08.550
is develop ility within the training data and be insensitive to anything that

0:26:08.550,0:26:11.970
is outside okay such that we can have a selective

0:26:11.970,0:26:18.240
reconstruction of a subset of this in very large input space are you with me

0:26:18.240,0:26:28.230
is no unless we are done see now how we avoid overfitting on the training data

0:26:28.230,0:26:33.210
so a few there are a few ways to do this make this stuff on the right-hand side

0:26:33.210,0:26:37.800
work and moreover you may want to have you can have the same rationale also for

0:26:37.800,0:26:44.040
this less than side guy here so let's say I have a super awesome decoder then

0:26:44.040,0:26:48.900
my encoder should could simply put all my training data as you know first

0:26:48.900,0:26:52.830
training data is first point second training data is gonna be number two

0:26:52.830,0:26:57.510
third training data same poles gonna be number three so I can associate each of

0:26:57.510,0:27:02.760
my training data is one number one two three four five six seven whatever and

0:27:02.760,0:27:06.390
now you have the decoder has memorized all the training data points and then

0:27:06.390,0:27:11.400
you just output the training point that you want from this kind of selector

0:27:11.400,0:27:17.190
right so you potentially may only need only one ball here one only one neuron

0:27:17.190,0:27:21.120
in the hidden layer in order to have a network that does overheat as long as

0:27:21.120,0:27:23.280
the decoder in the encoder are very powerful

0:27:23.280,0:27:27.360
okay so the point is your colleague mentioned yeah how do we avoid

0:27:27.360,0:27:33.270
overfitting this stuff can over fit to this stuff will over fit unless we are

0:27:33.270,0:27:37.890
you know I'm kind of you know careful about how we design these things so

0:27:37.890,0:27:42.510
there are a few different methods there are contrastive methods in there are

0:27:42.510,0:27:46.770
there are regular ice metals and there are architectural methods we have seen

0:27:46.770,0:27:53.460
yesterday as well something we can be quarry now a few of these and we have 20

0:27:53.460,0:27:57.560
minutes left okay yeah next slide

0:28:05.650,0:28:12.520
yeah so do you know is an alt encoder how does it work I take my input the

0:28:12.520,0:28:22.210
pink one I put it away from my original point so these are my training this is

0:28:22.210,0:28:26.530
my data manifold each of these points are gonna be you know samples are gonna

0:28:26.530,0:28:32.890
be providing for the training I take my point and I displace it okay how I just

0:28:32.890,0:28:37.810
add random crap okay cool now I didn't force the network to be

0:28:37.810,0:28:42.730
reconstructing that initial point right so this is the denoising auto-encoder

0:28:42.730,0:28:46.800
you take a point from your training manifold you take it you move it away

0:28:46.800,0:28:52.780
and then you enforce the network to take it back here I take the same point I

0:28:52.780,0:28:56.200
take it away on the other direction and then I put it back here I take the same

0:28:56.200,0:29:00.400
point I put it another direction and I put it down here okay so what are we

0:29:00.400,0:29:07.030
learning right now we will be learning a vector field which has everything coming

0:29:07.030,0:29:12.550
back to this point then I start moving around my training manifold and I have

0:29:12.550,0:29:17.020
all this kind of vector field like the vector field is gonna be all pointing

0:29:17.020,0:29:21.790
towards the training sample right but if you have a training sample here and a

0:29:21.790,0:29:25.690
training sample here this guy will try to attract here these on tire tracks

0:29:25.690,0:29:30.070
there so things that are on a manifold stay there things were outside manifold

0:29:30.070,0:29:41.110
will be you know collapsing towards the manifold okay questions okay all right

0:29:41.110,0:29:48.640
so actually there is a caveat caveat or a caveat caveat let me say Kavya

0:29:48.640,0:29:52.630
thank you all right we assume that we are injecting the same noise

0:29:52.630,0:29:57.850
distribution we are going to observe in reality in this way we can learn how

0:29:57.850,0:30:03.850
vastly recover from it right so if we assume that we have access to the type

0:30:03.850,0:30:06.070
of cultivation we are going to be observing

0:30:06.070,0:30:10.240
Ronit inference then we can train the model to be insensitive deluded to those

0:30:10.240,0:30:15.090
kind of perturbations and this is a very big if okay

0:30:15.090,0:30:21.880
therefore oh nice pictures okay all right so this is my training data this

0:30:21.880,0:30:30.570
is my data pink points and I'm gonna be turning off the lights so I have here my

0:30:30.570,0:30:36.160
pink points which look white to you then I have the orange points with our which

0:30:36.160,0:30:40.690
are the displaced points against so they are originated from these points here

0:30:40.690,0:30:44.260
and then I displace them in any very dark mini directions then I train my

0:30:44.260,0:30:49.150
network to get all these orange points back to original starting point right

0:30:49.150,0:30:55.240
and so this is the output of the network I input this cloud of orange points in

0:30:55.240,0:31:00.160
the network I trained my network to output the points on the actual spiral

0:31:00.160,0:31:04.390
and therefore these are the blue points are going to be my reconstructions of

0:31:04.390,0:31:09.130
the network okay so if points were already on the manifold they didn't move

0:31:09.130,0:31:15.040
in points are far away from the manifold they moved a lot guess what I can

0:31:15.040,0:31:20.380
measure how much they moved and that's gonna be your energy how cool is this

0:31:20.380,0:31:25.480
huh okay maybe you haven't understood yet so in this case in order to be a bit

0:31:25.480,0:31:31.330
more thorough I just send every possible x,y combination on this plane inside my

0:31:31.330,0:31:35.410
network right so here you have this line because because all these bottom left

0:31:35.410,0:31:40.270
corner got squashed down here and now you can see here points are quite sparse

0:31:40.270,0:31:44.050
and then there are very very many of them densely occupying manifold

0:31:44.050,0:31:49.900
nevertheless there are a few points here right and so this is showing you with

0:31:49.900,0:31:55.570
colors what is the distance those points have traveled so points over here in the

0:31:55.570,0:32:00.390
bottom left corner have traveled one unit and they got down here I think

0:32:00.390,0:32:07.300
points over here and travel also like 0.9 something and they went down here as

0:32:07.300,0:32:15.970
you can tell points within these two branches didn't go anywhere yeah why is

0:32:15.970,0:32:20.760
that tracked by both points on this side and

0:32:20.760,0:32:26.430
both on this side you know on average during training nevertheless if you

0:32:26.430,0:32:30.180
forget about you know having stuff that is curling on its own you have

0:32:30.180,0:32:38.940
everything just drops down to here guess what I can put the points that have just

0:32:38.940,0:32:43.740
moved a little bit again inside the out encoder and I can keep doing this a few

0:32:43.740,0:32:49.680
times until these points collapse down to the manifold okay all right

0:32:49.680,0:32:55.500
or I can do something good I'm cool that is a trick right so what did I do here

0:32:55.500,0:33:00.120
this is my denoising auto-encoder where I get to the initial point where did I

0:33:00.120,0:33:03.870
where I start from my displacement from right so I got my initial point I

0:33:03.870,0:33:08.450
displaced and then they forced the network to go back to the initial point

0:33:08.450,0:33:15.570
what happened here how did I fix this how can you fix this Ridge here and this

0:33:15.570,0:33:27.150
black this dark region any guess you can send them randomly or hold on someone

0:33:27.150,0:33:32.280
their top right push it up how do I push it out oh okay pushing up would be also

0:33:32.280,0:33:34.950
very good so I also try to push up everything that

0:33:34.950,0:33:38.720
is not on the manifold didn't quite work what did I do here it's a very is a hack

0:33:42.960,0:33:48.000
okay it's not elegant it cannot be done in a high dimensional space so what I've

0:33:48.000,0:33:52.920
done here is gonna be make your point fall on the closest point on a manifold

0:33:52.920,0:33:57.000
it so I did an exhaustive search of the closest point on the manifold and then I

0:33:57.000,0:34:01.230
enforce my network to always make my points fall on the closest point

0:34:01.230,0:34:04.860
although they were may be generated from another initial point right so if this

0:34:04.860,0:34:09.900
point over here initially originated from here but it's gonna be always

0:34:09.900,0:34:14.310
falling down on this direction just a few points will not be you know they are

0:34:14.310,0:34:21.530
just in there in the middle way and they don't fall anywhere all right so

0:34:24.210,0:34:31.060
now in more dimensions everything is Farah right so it doesn't quite work we

0:34:31.060,0:34:34.330
haven't covered that not work yet okay and I show you next time I guess well

0:34:34.330,0:34:41.050
next next next time yeah so in this case I've done the denoising auto-encoder in

0:34:41.050,0:34:45.730
this case I got the displace point to fall on to the closest point on a

0:34:45.730,0:34:50.679
manifold so I did an exhaustive search it's it's simple because I have hundred

0:34:50.679,0:34:57.730
and fifty points here but it's a hack you cannot do that in reality anyhow the

0:34:57.730,0:35:02.080
point is that we kind of have developed somehow kind of understanding and this

0:35:02.080,0:35:06.280
one instead that the unruly likes but I am not yet able to make it work

0:35:06.280,0:35:13.720
so I guess I'm not that smart yet it's regularized out encoder in this case I

0:35:13.720,0:35:20.859
have a l1 regularization term cost on my hidden representation so I force my

0:35:20.859,0:35:25.630
network to come up with hidden representations which are short and they

0:35:25.630,0:35:30.369
are like short of a few dimensions right so if I have a L1 regularization of my

0:35:30.369,0:35:36.640
hidden representation I will only have a few items active at a given time the

0:35:36.640,0:35:40.630
problem is that if you set all those other elements to zero and then you have

0:35:40.630,0:35:45.070
zero gradients to send back okay and so then you may want to use target prop and

0:35:45.070,0:35:50.230
other cute fancy things and I still am working on this so I have no idea how to

0:35:50.230,0:35:54.580
make it work the point is that this is the regularization term so this is the

0:35:54.580,0:35:59.859
L1 penalty on the hidden representation and this black dark here the region

0:35:59.859,0:36:05.589
should be actually extending all around again it's very hard for the moment for

0:36:05.589,0:36:09.339
me to get this to work not saying that is impossible and just saying that I'm

0:36:09.339,0:36:14.619
not smart enough all right contract the out encoder and then we going to be seen

0:36:14.619,0:36:19.359
the not works let me turn on the lights such that or maybe not I don't know

0:36:19.359,0:36:28.710
shall I I like it so much dark okay whatever

0:36:28.940,0:36:37.800
okay back on the camera right again data manifold points training points what is

0:36:37.800,0:36:42.090
VI a contract adult encoder doing so this guy here it simply have the

0:36:42.090,0:36:46.260
reconstruction term plus that thing here what is it

0:36:46.260,0:36:54.540
gradient of my hidden representation with respect to the input norm square in

0:36:54.540,0:36:59.609
the overall loss right so my overall loss will try to minimize

0:36:59.609,0:37:05.430
the variation of my hidden layer given variations on the input okay so here you

0:37:05.430,0:37:09.630
want to have a representation for the input which is not changing that much is

0:37:09.630,0:37:16.980
I wiggled my input and so this one basically makes makes you insensitive to

0:37:16.980,0:37:23.880
what makes you sorry insensitive to reconstruction of analyzing sensitivity

0:37:23.880,0:37:28.290
to the reconstruction directions so you actually will be able to reconstruct

0:37:28.290,0:37:35.400
things over the manifold but it will make you otherwise insensitive to any

0:37:35.400,0:37:39.830
other possible direction and so this one we don't have an assumption over the

0:37:39.830,0:37:43.589
perturbation I'm applying adjust insensitive to everything but then I

0:37:43.589,0:37:49.980
still have many points here so you will have to minimize the reconstruction as i

0:37:49.980,0:37:59.099
provide different samples okay and you penalize incentivize as well and that's

0:37:59.099,0:38:07.050
just penalized finally okay ten minutes left

0:38:07.050,0:38:13.440
finally what does this out in color tone as you can see I can use matplotlib very

0:38:13.440,0:38:21.210
well here we have this training manifold which is my single dimensional you know

0:38:21.210,0:38:27.750
thing going in three dimensions and here I have all those data points okay cool

0:38:27.750,0:38:38.730
so the x lives on this set of data and it lives in ℝⁿ. What an out encoder has

0:38:38.730,0:38:44.500
to do is going to be basically getting that curly line stretched down

0:38:44.500,0:38:48.850
in one direction right and therefore you have there your Z in this case is called

0:38:48.850,0:38:53.170
latent space and so you get the first one there and then the second one over

0:38:53.170,0:39:02.590
there the point is that how do I know how can I go from these back to here I

0:39:02.590,0:39:06.730
know if I'm in this first location I can go back to this location I know if I'm

0:39:06.730,0:39:11.800
in this location I can go back there I'm not entirely sure what's happening here

0:39:11.800,0:39:18.670
there is no I only have training samples right so I only have the correspondence

0:39:18.670,0:39:24.580
between points in the input space and points on the latent space I don't have

0:39:24.580,0:39:29.140
any correspondence between regions of the input space and regions of the

0:39:29.140,0:39:37.320
latent space okay so as as of right now you only know how to connect input to

0:39:37.320,0:39:41.770
regions here in the latent space and how to get back then we have learned that

0:39:41.770,0:39:47.530
the denoising auto-encoder takes the input shakes it but you enforced to go

0:39:47.530,0:39:50.170
back here same point and then you go back to the

0:39:50.170,0:39:54.490
other location to define a location right so you take this one you shake it

0:39:54.490,0:39:58.300
it's gonna be always going here and then you'll get back to the correct location

0:39:58.300,0:40:03.700
or the Dino is in the contractive you're going to be the input and every you try

0:40:03.700,0:40:08.890
to penalize any possible wiggling of this one when you wiggle this okay this

0:40:08.890,0:40:15.990
is contractive out encoder nevertheless how can I start from here move around

0:40:15.990,0:40:21.400
and get something that actually looks like a decent output meaning if I

0:40:21.400,0:40:26.770
translate this one if I have a dog here and a bird here the embedding speedy

0:40:26.770,0:40:33.070
latent space if I move on this line how can I assure that the things on this

0:40:33.070,0:40:38.890
line here will actually look like meaningful transformations in here we

0:40:38.890,0:40:42.850
don't know that right now we only know that this image is connected to this

0:40:42.850,0:40:47.080
point this image is connected to this point we don't have any knowledge about

0:40:47.080,0:40:54.220
what kind of behavior how well we have the space is whenever I move

0:40:54.220,0:40:59.740
in this space convert to down here right so we don't know how this decoder that

0:40:59.740,0:41:05.410
goes from the latent space to the input space is behaving when we are not

0:41:05.410,0:41:10.150
exactly in the points right so right now we have points mapping all day next time

0:41:10.150,0:41:15.910
we're gonna watching we're going to be learning how to map regions of the input

0:41:15.910,0:41:21.700
space with regions of the hidden space okay right now we have a point point all

0:41:21.700,0:41:28.950
right so not books in the last seven minutes thank you for sticking with me

0:41:28.950,0:41:46.930
yeah I'm moving too much. cd Work Github pDL, conda activate pDL, jn

0:41:46.930,0:41:55.650
Okay so I'm gonna be using this out

0:41:59.410,0:42:05.170
encoder just the number ten okay over there it's invisible I know but it's

0:42:05.170,0:42:14.580
gonna be number ten all right so I'm gonna be just executing stuff through

0:42:15.900,0:42:24.339
okay all right so what are we doing here let me see we import some random crap

0:42:24.339,0:42:29.290
can you see right yeah you can complain if you don't see things right I can't

0:42:29.290,0:42:33.190
check too many things so we import some stuff we have an image conversion

0:42:33.190,0:42:42.130
routine which is simply adding adding one and then multiplying by zero half

0:42:42.130,0:42:47.109
because I have otherwise my data when I get my data I try to get it to zero mean

0:42:47.109,0:42:52.000
and I have also in a range that is between minus 0.5 to plus 5 so it is

0:42:52.000,0:42:56.830
centered then here to get it back by someone so instead of being 0 meaning go

0:42:56.830,0:43:02.440
0.5 mean and then I actually have so it goes from my it starts from minus 1 to

0:43:02.440,0:43:07.640
plus 1 then here I just someone so it goes from 0 to 2 and then I get it 0

0:43:07.640,0:43:13.720
- one this is a some display in routines and here okay I show you that I subtract

0:43:13.720,0:43:19.970
0.5 and I divide by 0.5 my data those are the em nice digits from me on

0:43:19.970,0:43:26.420
website here we set the device if you want to run on CP on GPU and in this

0:43:26.420,0:43:31.040
case we have images that are the digit we saw that when we were training the

0:43:31.040,0:43:36.320
convolutional net which are 28 by 28 pixels and in this case I'm going to be

0:43:36.320,0:43:41.510
creating a note encoder which has a 30 dimensional intermediate layer right

0:43:41.510,0:43:48.980
hidden layer so we go from 784 to 30 and time and then back to 784 so here is

0:43:48.980,0:43:53.500
gonna be my out encoder model just a linear layer affine transformation like

0:43:53.500,0:43:58.610
228 square 2d hyperbolic tangent and then I have the decoder which is my

0:43:58.610,0:44:03.280
generative model which goes from the hidden space the latent space which is D

0:44:03.280,0:44:08.360
228 square and then I have again hyperbolic tangent such that I limit my

0:44:08.360,0:44:15.470
output range to minus one to plus one and my forward simply is gonna be send

0:44:15.470,0:44:19.850
things through the encoder and decoder I created my model and then I create my

0:44:19.850,0:44:26.090
criterion which is gonna be the MSC loss learning rate and atom optimizer and so

0:44:26.090,0:44:29.930
this is gonna be the training part so you're gonna be having whatever 20

0:44:29.930,0:44:35.330
evokes the first part is going to be sending the images through the model

0:44:35.330,0:44:40.270
this number one right number two is going to be computation of the loss

0:44:40.270,0:44:48.830
which is line number 15 then third point is going to be clearing the gradient

0:44:48.830,0:44:53.600
otherwise we accumulate so that's number 17 then we do back propagation

0:44:53.600,0:44:57.230
computation of the partial derivative or the final loss with respect to the

0:44:57.230,0:45:02.420
weights that is number 18 and finally we do a step backwards in the direction of

0:45:02.420,0:45:07.970
the easy depth direction of the gradient you step backward I'm talking a lot

0:45:07.970,0:45:13.790
because the computer is just training ok all right so you can see here we went

0:45:13.790,0:45:23.150
through 20 books and actually let me okay let me show you how they look so

0:45:23.150,0:45:28.430
these are the reconstruction of my network okay these are the output of the

0:45:28.430,0:45:32.480
network given that we compress them - that's 30 dimensional intermediate

0:45:32.480,0:45:35.930
representation I'm gonna show you the currents in a sec let me change this one

0:45:35.930,0:45:39.980
to a denoising auto-encoder so here I create a drop out module which is

0:45:39.980,0:45:45.500
randomly turning off neurons I create my nose mask noise mask and

0:45:45.500,0:45:50.090
then I create my dead images which are multiplying those images to this binary

0:45:50.090,0:45:56.510
mask and then I send to the network those bad images these I traded images

0:45:56.510,0:46:01.010
and I train this stuff again we also like to get to 500 dimensions right so

0:46:01.010,0:46:06.560
it is owner complete hidden layer and then we train again okay this is correct

0:46:06.560,0:46:14.780
and this is training okay all right so just recap of what's the difference

0:46:14.780,0:46:19.040
between the previous training at the current training alright so we were

0:46:19.040,0:46:24.590
saying that before we were just using a under complete out encoder so we were

0:46:24.590,0:46:31.220
going from 784 dimensional is input to a 30 dimensional hidden layer but now we

0:46:31.220,0:46:36.680
are going to be using a over complete still I use 500 here which is less than

0:46:36.680,0:46:46.160
a semi 784 so one proper question would be why are 500 dimensions like why it

0:46:46.160,0:46:51.230
does a elting color with 500 Meg dimensional hidden layer is consider or

0:46:51.230,0:46:56.090
can be considered over complete think about the number of pixels that are

0:46:56.090,0:47:01.130
black for example on average in these images alright so we actually already

0:47:01.130,0:47:06.500
run this part so we were down to the training how does the training changes

0:47:06.500,0:47:12.500
right now I have a dropout mask here which is allowing me to introduce some

0:47:12.500,0:47:17.660
kind of perturbation on the original images then I have my noise which is

0:47:17.660,0:47:22.619
simply applying a drop of mask on a vector of

0:47:22.619,0:47:27.479
is this going to be useful for later visualizations then I create my images

0:47:27.479,0:47:32.009
bed the perturbed images which are simply the multiplication of my image

0:47:32.009,0:47:38.429
times this noise so if we didn't have any neuron dropped the noise would be

0:47:38.429,0:47:42.799
just those ones and that you get 1 times image so you got the same image

0:47:42.799,0:47:47.640
otherwise when the neurons are dropping set to 0 the image now is going to be

0:47:47.640,0:47:52.410
multiplied by 0 for no specific values of the pixels so this image bed our

0:47:52.410,0:47:59.569
images with black dot then I input inside my model this image pad ok and

0:47:59.569,0:48:06.739
then the criterion it's between like is the distance between the output and the

0:48:06.739,0:48:12.449
original image man so before we were like in here we are inputting these

0:48:12.449,0:48:16.650
perturbed images inside the model so those are points that are outside the

0:48:16.650,0:48:21.390
training manifold but then I enforce them to be the original point right so

0:48:21.390,0:48:25.140
you get the original point you perturb it so you put it away and then you

0:48:25.140,0:48:28.919
enforce the network to actually I'll put this on so it's going to be trying to

0:48:28.919,0:48:35.159
contrast any kind of perturbation that happened to this original point ok all

0:48:35.159,0:48:40.499
right the rest is the same rank 0 guard backwardness step so this is also train

0:48:40.499,0:48:47.130
and we can check how this reconstruction look and if I can remember from the

0:48:47.130,0:48:52.679
previous iteration they look much more actually clean because I guess we are

0:48:52.679,0:48:58.799
using a much larger hidden layer right but before we couldn't use such a large

0:48:58.799,0:49:01.999
hidden layer because you would have been overfitting right if you try to

0:49:01.999,0:49:05.669
reconstruct things that are always in the same point we just can copy them

0:49:05.669,0:49:09.569
over in this case you can't copyright because the input is not this point but

0:49:09.569,0:49:13.799
the input is actually the displace point right so you learn a vector field that

0:49:13.799,0:49:20.039
is bringing you back to the original position on the training manifold ok so

0:49:20.039,0:49:24.209
let's go down and let's visualize the actually let's have a look to the

0:49:24.209,0:49:28.890
previous filters I didn't show you so these are the filters of the outer

0:49:28.890,0:49:34.949
corner with a under complete hidden layer ok and so you can see here there

0:49:34.949,0:49:37.850
are some of patterns in this central area of

0:49:37.850,0:49:43.280
these filters so these are the filters which are simply my rows of my W metrics

0:49:43.280,0:49:48.640
that have been reshaped in a basically no image such that I can visualize

0:49:48.640,0:49:52.400
formula so in this case in this notebook we are not using any convolutional

0:49:52.400,0:49:54.680
network we are just using those images that have

0:49:54.680,0:49:59.210
been enrolled in protectors and they are compared there and you know multiply it

0:49:59.210,0:50:05.420
like scalar product against the DD vectors right of the of my matrix so

0:50:05.420,0:50:09.200
these are the rows of my matrix that have been reshaped such that you can

0:50:09.200,0:50:13.430
make sense of what they represent so here for example it looks like there is

0:50:13.430,0:50:18.470
like a loop upper loop detector or not a detector because there are purple right

0:50:18.470,0:50:24.320
so these are would be negative the output here you have like a C rule here

0:50:24.320,0:50:29.300
looks like some eight over three and then you have this kind of right this

0:50:29.300,0:50:34.130
this this kernel over here and that has basically learned nothing or well that's

0:50:34.130,0:50:39.320
the only one that didn't learn much moreover you can notice that all those

0:50:39.320,0:50:43.490
points outside the region where the number happened or any kind of

0:50:43.490,0:50:47.150
interesting thing happened all these points are multiplied by a constant

0:50:47.150,0:50:52.150
right because it's outside a digit and so things don't change there and

0:50:52.150,0:50:58.820
therefore these noisy kernels over there you know the on average they will

0:50:58.820,0:51:03.109
produce a score of zero right and therefore you're gonna have that the

0:51:03.109,0:51:08.300
network didn't care about giving any specific value to these outstanding

0:51:08.300,0:51:13.850
without out there other points outer pictures because I can on average they

0:51:13.850,0:51:19.300
will not contribute to the final score what happened now when we are inputting

0:51:19.300,0:51:27.440
the data which have a variable amount of like pixel set to zero now these points

0:51:27.440,0:51:32.930
will in matter right because it's not longer continuous the value of the image

0:51:32.930,0:51:39.590
and so if I show you the new kernels um how cool is this right this is

0:51:39.590,0:51:42.410
completely different can you remember so here you can still

0:51:42.410,0:51:47.450
have some pattern right but then in the majority of these kernels regardless

0:51:47.450,0:51:51.150
like if you don't consider the one that did not did not learn anything so this

0:51:51.150,0:51:55.470
colonel here are didn't learn much but all the other kernels that I have have

0:51:55.470,0:51:59.700
learned some kind of specific edge filter or shape specific features

0:51:59.700,0:52:07.110
you know shape filter all the outside pixels have now been set to some zero

0:52:07.110,0:52:11.940
value I think of some somebody that is uniform right because again the input

0:52:11.940,0:52:17.250
images now are no longer constant in the areas outside digit and therefore the

0:52:17.250,0:52:21.420
value of the pixels well the value of the of the value of the kernel thing

0:52:21.420,0:52:26.540
though specifically specific regions now do matter this is a big big difference

0:52:26.540,0:52:34.050
and again this this this maps here this courier didn't learn anything okay so

0:52:34.050,0:52:38.610
let's now compare our denoising auto-encoder we've failed

0:52:38.610,0:52:44.369
state-of-the-art algorithms for denoising images so here we're going to

0:52:44.369,0:52:49.140
be importing some functions from the opencv library there is the Navier-Stokes

0:52:49.140,0:52:55.650
and then the Telea algorithm. So it's imported and let's see how this

0:52:55.650,0:53:01.530
stuff look so here the first image is the noise image the one we generated

0:53:01.530,0:53:05.630
before these are the maps of own ones where we have dropped out some specific

0:53:05.630,0:53:11.700
values to set to 0 right so yellow is one and purple is 0 in this case then

0:53:11.700,0:53:16.920
the second part is going to be the bed images so these are the bed images

0:53:16.920,0:53:22.859
meaning purple is minus 1 yellow is plus 1 and then this green is the zero value

0:53:22.859,0:53:28.460
so all those black points like purple points in the first row are here

0:53:28.460,0:53:32.940
represented in in green so those are the values that are being set to 0 there are

0:53:32.940,0:53:40.650
mask value then we have the original images and the reconstructions from our

0:53:40.650,0:53:46.230
you know is it not encoder which broke reasonably okay if you think that half

0:53:46.230,0:53:51.390
of the pixels were actually nice right so these are like half of the pixels are

0:53:51.390,0:53:54.660
provided to the network and then the network actually reconstructed

0:53:54.660,0:54:00.119
both net and what look like the original image more or less right cool Quan

0:54:00.119,0:54:06.510
so let's now have a look to what the set of the art algorithms output are

0:54:06.510,0:54:13.130
so we can start with the Telea and then their strokes and so this is Telea and

0:54:13.130,0:54:19.730
this is Navier-Stokes. Right so as you can tell here the quality of our modern is

0:54:19.730,0:54:25.640
clearly a superior in terms of you know qualitative qualitative output

0:54:25.640,0:54:30.360
nevertheless pay attention that this modern works just for this kind of

0:54:30.360,0:54:33.660
specific perturbation that we have introduced and then we haven't learned

0:54:33.660,0:54:41.070
how to counteract okay so again he note encoder that we have trained in a in a

0:54:41.070,0:54:45.270
minute performs much better than state-of-the-art computer vision

0:54:45.270,0:54:51.660
algorithms again when the data is available okay and so I think that's it

0:54:51.660,0:54:55.500
for today thank you for listening subscribe to my channel the non de

0:54:55.500,0:55:03.500
notification bed if you'd like to have information about latest videos and follow me on Twitter peace

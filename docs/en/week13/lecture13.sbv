0:00:00.080,0:00:04.319
so welcome everyone and to this lecture

0:00:02.720,0:00:08.080
on graph conventional

0:00:04.319,0:00:10.080
networks um

0:00:08.080,0:00:11.120
okay so this is the outline of the

0:00:10.080,0:00:13.920
lecture

0:00:11.120,0:00:14.880
so first i will go quickly um on the

0:00:13.920,0:00:17.840
traditional

0:00:14.880,0:00:18.880
coordinates the architecture and then i

0:00:17.840,0:00:22.480
will introduce

0:00:18.880,0:00:26.160
graphs and i will also

0:00:22.480,0:00:28.880
remind definitions of convolutions to

0:00:26.160,0:00:29.920
extend it to two graphs then i will

0:00:28.880,0:00:33.040
represent

0:00:29.920,0:00:34.800
two classes of graph coordinates

0:00:33.040,0:00:36.719
the first one is what i call a spectral

0:00:34.800,0:00:39.040
graph of let's and the second one is the

0:00:36.719,0:00:40.399
spatial graph problems

0:00:39.040,0:00:42.399
and we'll talk a little bit about the

0:00:40.399,0:00:45.840
benchmarking graph neural networks

0:00:42.399,0:00:45.840
and finally i will conclude

0:00:47.360,0:00:51.120
okay so let's start with the traditional

0:00:49.200,0:00:53.520
covenant

0:00:51.120,0:00:56.000
so we all know coordinates are a

0:00:53.520,0:00:59.440
breakthrough in computer vision

0:00:56.000,0:01:00.559
so when uh for the imagenet competition

0:00:59.440,0:01:03.039
you know for the

0:01:00.559,0:01:04.320
image classification task um when

0:01:03.039,0:01:06.400
combinate was used

0:01:04.320,0:01:07.439
uh they decreased by almost a factor two

0:01:06.400,0:01:10.640
the error

0:01:07.439,0:01:11.760
of classification it was in 2012 and it

0:01:10.640,0:01:15.520
was basically the end

0:01:11.760,0:01:17.759
of uh and crafting features and we shift

0:01:15.520,0:01:18.640
the paradigm to and crafting learning

0:01:17.759,0:01:20.640
systems

0:01:18.640,0:01:22.320
and now for this very specific task we

0:01:20.640,0:01:25.680
all know that we

0:01:22.320,0:01:29.360
we go to superhuman performance

0:01:25.680,0:01:29.840
coordinates are also a breakthrough in

0:01:29.360,0:01:31.520
speech

0:01:29.840,0:01:33.360
and natural language processing so a

0:01:31.520,0:01:34.960
facebook when you want to translate you

0:01:33.360,0:01:39.520
are also using

0:01:34.960,0:01:41.840
coordinates so contents are powerful

0:01:39.520,0:01:42.640
architectures to actually solve high

0:01:41.840,0:01:44.240
dimensional

0:01:42.640,0:01:46.000
learning problems so we all know about

0:01:44.240,0:01:49.280
the curse of dimensionality

0:01:46.000,0:01:52.560
so if you have an image let's say 1 000

0:01:49.280,0:01:54.799
by 1 000 pixels so it's you have 1

0:01:52.560,0:01:56.799
million variables an image can be seen

0:01:54.799,0:01:57.920
as a point in a space of one million

0:01:56.799,0:02:00.320
dimensions

0:01:57.920,0:02:01.840
and for each dimension if you sample by

0:02:00.320,0:02:04.880
using um

0:02:01.840,0:02:06.399
by using 10 samples then you have a 10

0:02:04.880,0:02:08.560
to the power 1 million

0:02:06.399,0:02:09.599
possible images so these spaces are

0:02:08.560,0:02:11.520
really huge

0:02:09.599,0:02:13.680
and of course this is the question how

0:02:11.520,0:02:14.640
do you find the needle of information in

0:02:13.680,0:02:17.360
this big

0:02:14.640,0:02:18.879
haystack so covenants are really

0:02:17.360,0:02:21.040
powerful to extract

0:02:18.879,0:02:22.800
basically this this information of the

0:02:21.040,0:02:25.920
best possible representation

0:02:22.800,0:02:28.080
of your of your image data to uh to

0:02:25.920,0:02:30.080
solve problems

0:02:28.080,0:02:32.400
of course we don't know yet everything

0:02:30.080,0:02:32.400
about

0:02:32.959,0:02:37.840
yeah we don't know yet everything about

0:02:35.040,0:02:40.720
convnets so it's a kind of a miracle how

0:02:37.840,0:02:43.440
and how powerful how good they are and

0:02:40.720,0:02:45.519
it's also quite exciting because

0:02:43.440,0:02:48.840
this opened many research areas to

0:02:45.519,0:02:51.440
understand better and to develop new

0:02:48.840,0:02:54.080
architectures

0:02:51.440,0:02:55.120
okay so when you use convnets you are

0:02:54.080,0:02:57.040
doing an assumption

0:02:55.120,0:02:58.879
and the main assumption that you are

0:02:57.040,0:03:02.640
using is that your data

0:02:58.879,0:03:04.800
so images videos speech is compositional

0:03:02.640,0:03:05.840
it means that it is form of patterns

0:03:04.800,0:03:08.159
that are

0:03:05.840,0:03:10.319
local so you know this is the

0:03:08.159,0:03:12.480
contribution of urban envision so

0:03:10.319,0:03:14.159
if you are at this layer for this neuron

0:03:12.480,0:03:16.080
this neuron is going to be connected to

0:03:14.159,0:03:18.400
a few neurons in the previous layer

0:03:16.080,0:03:20.159
and not all neurons okay so this is the

0:03:18.400,0:03:23.599
local reception field

0:03:20.159,0:03:24.959
um assumption then you have also the

0:03:23.599,0:03:27.040
property of stationary

0:03:24.959,0:03:28.080
stationarity so basically you have some

0:03:27.040,0:03:31.200
patterns

0:03:28.080,0:03:34.239
um that are similar and that are

0:03:31.200,0:03:34.640
shared across your image domain okay so

0:03:34.239,0:03:37.680
like

0:03:34.640,0:03:39.200
the yellow uh the yellow patches and the

0:03:37.680,0:03:40.879
blue patches so they are they are all

0:03:39.200,0:03:43.599
similar to each other

0:03:40.879,0:03:46.000
the last property is the article so you

0:03:43.599,0:03:48.720
make the assumption that your

0:03:46.000,0:03:49.200
data is hierarchical in the sense that

0:03:48.720,0:03:51.200
your

0:03:49.200,0:03:52.319
low level features are going to be

0:03:51.200,0:03:56.080
combined together

0:03:52.319,0:03:57.760
to form a medium level features and then

0:03:56.080,0:03:58.480
this medium feature are going to be

0:03:57.760,0:04:01.519
again

0:03:58.480,0:04:02.159
combined to each other to form higher

0:04:01.519,0:04:05.280
and higher

0:04:02.159,0:04:08.720
abstract features

0:04:05.280,0:04:11.040
so um so any convent work the same way

0:04:08.720,0:04:11.760
so the first part of the architecture is

0:04:11.040,0:04:14.400
to extract

0:04:11.760,0:04:16.000
these conventional features and then the

0:04:14.400,0:04:17.440
second part will be to solve your

0:04:16.000,0:04:19.359
specific task

0:04:17.440,0:04:21.120
you know like classification

0:04:19.359,0:04:22.479
recommendation

0:04:21.120,0:04:24.320
and so on and this is what we call you

0:04:22.479,0:04:24.639
know end-to-end systems and the first

0:04:24.320,0:04:26.400
part

0:04:24.639,0:04:29.840
is to learn the features the second part

0:04:26.400,0:04:29.840
is to solve your task

0:04:31.120,0:04:37.440
okay okay let's see

0:04:34.320,0:04:40.880
more precisely what is the data domain

0:04:37.440,0:04:43.680
so if you have images volumes

0:04:40.880,0:04:45.520
or videos basically so for example you

0:04:43.680,0:04:46.320
can see this image and if you zoom in

0:04:45.520,0:04:49.280
this image

0:04:46.320,0:04:50.320
what you have is a 2d grid okay you have

0:04:49.280,0:04:54.000
a 2d grid

0:04:50.320,0:04:55.199
this is the structure of of the domain

0:04:54.000,0:04:57.360
of this image

0:04:55.199,0:04:58.720
and on the top of the of this grid you

0:04:57.360,0:05:00.960
have some features

0:04:58.720,0:05:02.000
so for example in the case of a color

0:05:00.960,0:05:04.800
image you will have

0:05:02.000,0:05:05.600
three features which are red green and

0:05:04.800,0:05:08.840
blue

0:05:05.600,0:05:12.560
okay now if i'm looking at

0:05:08.840,0:05:13.759
um natural language processing so like

0:05:12.560,0:05:16.479
sentences

0:05:13.759,0:05:18.320
you will have a sequence of words and

0:05:16.479,0:05:19.520
basically you can see that you know as a

0:05:18.320,0:05:21.840
1d grid

0:05:19.520,0:05:24.320
and on the top of this grid for each

0:05:21.840,0:05:26.639
node of the grid you will have a word

0:05:24.320,0:05:27.520
okay so a word can be represented by

0:05:26.639,0:05:30.800
just

0:05:27.520,0:05:31.520
an integer for example the same also for

0:05:30.800,0:05:34.320
speech

0:05:31.520,0:05:35.680
so what you see here is um the variation

0:05:34.320,0:05:37.600
of the air pressure

0:05:35.680,0:05:40.080
and it's the same you know it's like you

0:05:37.600,0:05:42.960
have the support is a one degree

0:05:40.080,0:05:44.080
and each for each node of the grid you

0:05:42.960,0:05:46.800
will have

0:05:44.080,0:05:48.560
the the air pressure value okay which is

0:05:46.800,0:05:52.080
which is a real number

0:05:48.560,0:05:54.479
so uh i think it's clear we all

0:05:52.080,0:05:55.520
we all use all the time grids and grits

0:05:54.479,0:05:57.520
as you know as

0:05:55.520,0:05:58.720
very strong regular special structure

0:05:57.520,0:06:01.759
and for this for this

0:05:58.720,0:06:05.600
um for this structure this is good

0:06:01.759,0:06:06.960
because we mathematically we can define

0:06:05.600,0:06:09.120
the confident operations like

0:06:06.960,0:06:12.080
convolution and pulling and also in

0:06:09.120,0:06:14.800
practice it's very fast to do it

0:06:12.080,0:06:16.800
so everything everything is good now

0:06:14.800,0:06:19.840
let's look at you know

0:06:16.800,0:06:20.560
new new data so for example social

0:06:19.840,0:06:22.800
networks

0:06:20.560,0:06:24.000
okay so you want you want to do your

0:06:22.800,0:06:24.800
task for example would be to do

0:06:24.000,0:06:28.000
advertisement

0:06:24.800,0:06:29.280
or to also make recommendation so for a

0:06:28.000,0:06:30.800
social network

0:06:29.280,0:06:32.240
i'm going to it's going to be clear but

0:06:30.800,0:06:34.319
i'm going to show you that if you take

0:06:32.240,0:06:36.880
two notes so for example

0:06:34.319,0:06:38.720
you know you have this user let's say

0:06:36.880,0:06:41.120
this user i and user j

0:06:38.720,0:06:42.160
and all the others you see that this is

0:06:41.120,0:06:44.319
not a grid

0:06:42.160,0:06:46.160
okay so the connection the pairwise

0:06:44.319,0:06:47.840
connection between all users

0:06:46.160,0:06:50.720
they do not form a grid they have a very

0:06:47.840,0:06:53.840
special pattern of connections

0:06:50.720,0:06:56.080
and this is basically a graph okay so

0:06:53.840,0:06:58.240
uh how do you define your graph you're

0:06:56.080,0:06:58.720
gonna see the connection between users

0:06:58.240,0:07:01.840
so

0:06:58.720,0:07:03.520
if i user i user j are friends you're

0:07:01.840,0:07:05.039
gonna have you know connection and then

0:07:03.520,0:07:06.880
for this you are going to use what we

0:07:05.039,0:07:10.160
call an adjacency matrix

0:07:06.880,0:07:13.039
which is just going to record all the

0:07:10.160,0:07:14.400
connection or non-connection um between

0:07:13.039,0:07:17.360
notes

0:07:14.400,0:07:18.000
in your in your social networks okay and

0:07:17.360,0:07:20.400
on the top

0:07:18.000,0:07:21.520
of your network uh for each user you

0:07:20.400,0:07:23.280
will have features

0:07:21.520,0:07:26.560
so for example you have you know

0:07:23.280,0:07:28.800
messages you have images you have videos

0:07:26.560,0:07:31.840
so they form you know some feature in a

0:07:28.800,0:07:31.840
d dimensional space

0:07:32.080,0:07:37.199
in in neuroscience

0:07:35.440,0:07:39.039
in brain analysis for example we are

0:07:37.199,0:07:42.160
really interesting to understand

0:07:39.039,0:07:44.960
uh you know the fundamental relationship

0:07:42.160,0:07:47.199
between structure and function of the

0:07:44.960,0:07:48.720
brain so they are really um

0:07:47.199,0:07:50.160
connected to each other and it's very

0:07:48.720,0:07:51.919
fundamental to understand that we also

0:07:50.160,0:07:54.240
want for example to predict

0:07:51.919,0:07:55.680
uh neurodegenerative disease different

0:07:54.240,0:07:58.240
stages of this disease

0:07:55.680,0:07:59.599
so that this is very important um for

0:07:58.240,0:08:00.240
this we need to understand the brain and

0:07:59.599,0:08:02.879
the brain

0:08:00.240,0:08:03.599
if you look at the brain the brain is uh

0:08:02.879,0:08:06.879
composed

0:08:03.599,0:08:08.720
of what we call region of interest okay

0:08:06.879,0:08:10.400
and this region of interest if you take

0:08:08.720,0:08:12.160
one region of interest

0:08:10.400,0:08:13.759
this region is not connected to all

0:08:12.160,0:08:15.360
other regions in the brain actually they

0:08:13.759,0:08:16.400
are only connected to a few other

0:08:15.360,0:08:18.639
regions

0:08:16.400,0:08:20.000
so it's it's and again you can see

0:08:18.639,0:08:23.680
nothing to do with the grid

0:08:20.000,0:08:25.520
okay so this special connection

0:08:23.680,0:08:27.520
uh between different region of the

0:08:25.520,0:08:29.120
brains they can be measured by the

0:08:27.520,0:08:31.199
structural mri

0:08:29.120,0:08:32.800
signal and then you also have an

0:08:31.199,0:08:35.839
adjacency matrix

0:08:32.800,0:08:37.599
between region i and region j and and

0:08:35.839,0:08:39.279
here you have a strength of connection

0:08:37.599,0:08:41.200
which depends how many

0:08:39.279,0:08:43.360
connections how many fibers do you have

0:08:41.200,0:08:47.040
to connect region iron region j

0:08:43.360,0:08:48.800
okay and then on the top of this graph

0:08:47.040,0:08:50.959
so if you look at the region i

0:08:48.800,0:08:53.120
then you will have activations you know

0:08:50.959,0:08:54.880
functional

0:08:53.120,0:08:57.120
activation which is basically a time

0:08:54.880,0:08:58.080
series that you can see here and also we

0:08:57.120,0:08:59.760
can record this

0:08:58.080,0:09:02.640
activation of the brain with a

0:08:59.760,0:09:02.640
functional mri

0:09:02.959,0:09:07.279
okay the last example i want to show you

0:09:05.680,0:09:09.440
is in quantum chemistry

0:09:07.279,0:09:10.880
so for example the task would be to

0:09:09.440,0:09:14.240
design

0:09:10.880,0:09:16.480
new molecules for drugs and materials

0:09:14.240,0:09:18.480
so so you see again the connection

0:09:16.480,0:09:19.040
between atoms has nothing to do with the

0:09:18.480,0:09:22.000
grid

0:09:19.040,0:09:23.519
okay it really depends um how you're

0:09:22.000,0:09:24.959
going to connect

0:09:23.519,0:09:27.279
your atoms and then you will have you

0:09:24.959,0:09:29.680
know molecules

0:09:27.279,0:09:31.279
so uh so the connection between atoms

0:09:29.680,0:09:33.839
they are called bond

0:09:31.279,0:09:36.000
um and you have you know different kind

0:09:33.839,0:09:38.320
of bonds they can be a

0:09:36.000,0:09:40.480
single bond double bond aromatic bond

0:09:38.320,0:09:43.120
and you have and you have also different

0:09:40.480,0:09:44.240
features like energy and many other

0:09:43.120,0:09:47.279
features that you can use

0:09:44.240,0:09:48.720
from from chemistry um

0:09:47.279,0:09:50.480
for for the node of the graph so they

0:09:48.720,0:09:52.480
are atoms and again you you

0:09:50.480,0:09:53.680
you may have different features like the

0:09:52.480,0:09:56.240
type of atom

0:09:53.680,0:09:56.720
if it is you know hydrogen if it is

0:09:56.240,0:09:58.480
hazard

0:09:56.720,0:10:00.480
all this all these types you have also

0:09:58.480,0:10:01.680
the 3d coordinates you have the charge

0:10:00.480,0:10:04.720
and so on you may have

0:10:01.680,0:10:08.000
multiple features

0:10:04.720,0:10:11.200
okay so um and it's not uh

0:10:08.000,0:10:11.760
the list actually goes on to give you

0:10:11.200,0:10:15.519
example

0:10:11.760,0:10:17.360
of graph graph domains so you also have

0:10:15.519,0:10:20.560
you know computer graphics

0:10:17.360,0:10:21.600
with 3d meshes you also want maybe to

0:10:20.560,0:10:25.120
analyze

0:10:21.600,0:10:28.320
transportation network and the density

0:10:25.120,0:10:30.720
of of cars or maybe i don't know

0:10:28.320,0:10:32.079
trains you have also you know gene

0:10:30.720,0:10:34.959
regulatory network

0:10:32.079,0:10:36.880
you have knowledge graphs um world

0:10:34.959,0:10:39.519
relationships

0:10:36.880,0:10:41.200
you know users products where you want

0:10:39.519,0:10:42.800
to do recommendations you have also seen

0:10:41.200,0:10:44.640
understanding you want to

0:10:42.800,0:10:46.640
give more common sense to your computer

0:10:44.640,0:10:48.079
vision machine so you want to understand

0:10:46.640,0:10:48.880
the relationship between between your

0:10:48.079,0:10:51.519
objects

0:10:48.880,0:10:52.959
you also have you know for example if

0:10:51.519,0:10:55.839
you want to detect

0:10:52.959,0:10:58.000
high energy physics particles so you

0:10:55.839,0:11:00.800
have characters and the captures are not

0:10:58.000,0:11:01.120
you know structure as a regular grid so

0:11:00.800,0:11:03.519
for

0:11:01.120,0:11:04.560
all this you see that there is a

0:11:03.519,0:11:06.959
denominator

0:11:04.560,0:11:07.839
uh which is basically you can represent

0:11:06.959,0:11:11.680
all these

0:11:07.839,0:11:15.120
um problems as graphs

0:11:11.680,0:11:17.760
okay and

0:11:15.120,0:11:19.760
and here is the command setting i would

0:11:17.760,0:11:23.040
say the mathematical command setting

0:11:19.760,0:11:26.079
for all these uh problems um

0:11:23.040,0:11:29.360
so the graphs uh let's let's call it

0:11:26.079,0:11:32.000
uh g okay they are defined by three

0:11:29.360,0:11:33.440
entities so the first entity is going to

0:11:32.000,0:11:35.760
be the set of vertices

0:11:33.440,0:11:37.839
so usually you are going to index the

0:11:35.760,0:11:40.800
set of vertices from one to n

0:11:37.839,0:11:41.519
n is is the number of nodes in your in

0:11:40.800,0:11:44.160
your graph

0:11:41.519,0:11:45.120
okay so for example this will be the

0:11:44.160,0:11:48.720
index one

0:11:45.120,0:11:50.639
two three and so on then you will have

0:11:48.720,0:11:51.440
you know the set of edges basically they

0:11:50.639,0:11:55.120
are the

0:11:51.440,0:11:56.720
connections between um between the notes

0:11:55.120,0:11:58.959
and finally you will have the adjacency

0:11:56.720,0:12:02.000
matrix a which will give you the

0:11:58.959,0:12:05.600
strength of the connection

0:12:02.000,0:12:09.440
of your of your edge okay um

0:12:05.600,0:12:12.240
okay then you have graph features so

0:12:09.440,0:12:13.120
for example for each node not i or not j

0:12:12.240,0:12:16.000
you will have

0:12:13.120,0:12:17.600
some um some node features so it's

0:12:16.000,0:12:20.800
basically a vector

0:12:17.600,0:12:22.800
of dimensionality dv okay

0:12:20.800,0:12:25.120
the same also it's possible that you can

0:12:22.800,0:12:25.120
get

0:12:25.680,0:12:29.760
you can get h features and it's going to

0:12:28.639,0:12:33.519
be a vector

0:12:29.760,0:12:35.120
of dimensionality d e so for example for

0:12:33.519,0:12:36.800
molecules

0:12:35.120,0:12:38.720
the node feature maybe you know the atom

0:12:36.800,0:12:41.040
type and the edge feature may be the

0:12:38.720,0:12:43.040
bond type to give you an example

0:12:41.040,0:12:44.160
and finally you can have also some graph

0:12:43.040,0:12:45.920
feature okay

0:12:44.160,0:12:47.440
for all for the whole graph you can have

0:12:45.920,0:12:50.839
some feature so again

0:12:47.440,0:12:52.320
it's um it's a vector of dimensionality

0:12:50.839,0:12:54.720
dj

0:12:52.320,0:12:55.920
and and in the case of of uh of

0:12:54.720,0:12:59.040
chemistry that that

0:12:55.920,0:13:01.600
that may be the molecule energy

0:12:59.040,0:13:06.399
okay so this is um i would say the

0:13:01.600,0:13:08.480
general definition of graphs

0:13:06.399,0:13:10.959
okay so now what i'm going to do is that

0:13:08.480,0:13:12.560
i'm going to talk about convolution

0:13:10.959,0:13:16.000
and the question how do we extend

0:13:12.560,0:13:16.000
convolution to graphs

0:13:17.440,0:13:22.720
okay so first let me remind you um

0:13:20.720,0:13:24.800
the classical way to use convolutional

0:13:22.720,0:13:28.320
layer uh for grids when we use

0:13:24.800,0:13:31.760
confinets for computer vision so

0:13:28.320,0:13:35.680
let's say um i have this image

0:13:31.760,0:13:39.040
and or maybe this is some uh you know

0:13:35.680,0:13:41.600
hidden uh feature at layer l okay

0:13:39.040,0:13:42.399
and i'm going to do the convolution with

0:13:41.600,0:13:45.519
uh

0:13:42.399,0:13:46.839
some pattern or kernel um

0:13:45.519,0:13:48.880
that of course i will learn by back

0:13:46.839,0:13:49.680
propagation and then i will get some

0:13:48.880,0:13:53.279
activation

0:13:49.680,0:13:54.480
okay so this is the the features at the

0:13:53.279,0:13:56.880
next layer

0:13:54.480,0:13:58.959
so to give you maybe some dimensionality

0:13:56.880,0:14:00.160
so for example n1 and 2 is going to be

0:13:58.959,0:14:02.959
the number of pixels

0:14:00.160,0:14:04.399
in the x and y direction and d is the

0:14:02.959,0:14:07.279
dimensionality

0:14:04.399,0:14:08.959
of uh of each pixel so if this is a

0:14:07.279,0:14:09.360
color image the dimensionality is going

0:14:08.959,0:14:12.079
to be

0:14:09.360,0:14:12.480
three for the three colors and if this

0:14:12.079,0:14:14.880
is like

0:14:12.480,0:14:17.440
intermediate uh hidden feature maybe you

0:14:14.880,0:14:19.760
have 100 you know dimensions

0:14:17.440,0:14:22.240
for the kernel usually you take small

0:14:19.760,0:14:24.720
kernels because you want to know the

0:14:22.240,0:14:26.560
local reception field so that might be

0:14:24.720,0:14:29.519
you know three by three pixels

0:14:26.560,0:14:30.720
kernel or five by one five by five and

0:14:29.519,0:14:33.120
of course you have d

0:14:30.720,0:14:34.800
because you need to uh to respect the

0:14:33.120,0:14:38.079
dimensionality of your input

0:14:34.800,0:14:38.079
uh after input features

0:14:39.040,0:14:43.199
okay so maybe for this one so you see

0:14:41.120,0:14:44.000
that uh so you are going to convert this

0:14:43.199,0:14:45.920
image with

0:14:44.000,0:14:48.079
this feature which is oriented in this

0:14:45.920,0:14:51.600
direction so you will

0:14:48.079,0:14:54.240
basically identify uh you know

0:14:51.600,0:14:56.160
lines in in in this direction of the

0:14:54.240,0:14:58.079
image

0:14:56.160,0:14:59.839
so that was just an example and we use

0:14:58.079,0:15:01.680
padding right right now right so we have

0:14:59.839,0:15:04.560
the same dimensionality of the

0:15:01.680,0:15:06.160
uh yes yes absolutely against padding so

0:15:04.560,0:15:06.959
you basically you don't reduce the size

0:15:06.160,0:15:10.560
of your image

0:15:06.959,0:15:13.120
right yeah okay

0:15:10.560,0:15:15.040
so so how do we mathematically define

0:15:13.120,0:15:18.399
convolution

0:15:15.040,0:15:18.720
so the first definition is to do is to

0:15:18.399,0:15:22.160
see

0:15:18.720,0:15:25.680
convolution as a template matching okay

0:15:22.160,0:15:28.079
so so template matching so here is the

0:15:25.680,0:15:30.560
definition the mathematical definition

0:15:28.079,0:15:31.360
of a convolution so what you're going to

0:15:30.560,0:15:33.600
do is that

0:15:31.360,0:15:35.040
you are going to take your template you

0:15:33.600,0:15:38.839
are going to take your image

0:15:35.040,0:15:41.920
and then you are going to sum over

0:15:38.839,0:15:46.079
um the index in the whole

0:15:41.920,0:15:48.880
image domain omega okay of wj

0:15:46.079,0:15:49.360
and this is going to be a product

0:15:48.880,0:15:53.120
between

0:15:49.360,0:15:55.040
vector w j and vector h i minus g

0:15:53.120,0:15:56.320
okay so this is the pure definition of

0:15:55.040,0:15:58.880
convolution

0:15:56.320,0:16:00.480
and what we do usually in a in computer

0:15:58.880,0:16:02.880
vision is that we don't take minus

0:16:00.480,0:16:04.560
we take plus okay and we call that

0:16:02.880,0:16:06.320
because because when when we do that we

0:16:04.560,0:16:08.320
have the definition of correlation

0:16:06.320,0:16:09.519
and this is this is you know because

0:16:08.320,0:16:11.120
it's more uh

0:16:09.519,0:16:13.279
like it's it's exactly like template

0:16:11.120,0:16:16.639
matching okay so it doesn't change

0:16:13.279,0:16:19.040
anything if you if you do i minus j or i

0:16:16.639,0:16:20.639
plus j in the learning sense because the

0:16:19.040,0:16:23.600
only thing that you do is that you flip

0:16:20.639,0:16:24.079
up and down and left and right your uh

0:16:23.600,0:16:27.120
your

0:16:24.079,0:16:28.720
your your kernel and when you learn you

0:16:27.120,0:16:30.399
it doesn't change anything basically

0:16:28.720,0:16:32.000
okay but this is the definition of

0:16:30.399,0:16:33.120
correlation so it's it's really a

0:16:32.000,0:16:34.720
template matching

0:16:33.120,0:16:36.480
and then i'm going to take for the

0:16:34.720,0:16:38.480
notation i j okay

0:16:36.480,0:16:40.320
so basically and yeah and something very

0:16:38.480,0:16:43.279
important that you have here is that

0:16:40.320,0:16:45.199
you when when we do um convolutional

0:16:43.279,0:16:47.600
layers we are using

0:16:45.199,0:16:50.720
kernel with compact support you know

0:16:47.600,0:16:53.199
like a 3x3 it's very small support

0:16:50.720,0:16:54.320
when we do that we don't do the sum over

0:16:53.199,0:16:56.880
the whole domain the

0:16:54.320,0:16:57.920
image domain we just do the sum over the

0:16:56.880,0:17:01.440
neighborhood

0:16:57.920,0:17:02.880
of the node i okay and this is very

0:17:01.440,0:17:03.759
important it's very important because

0:17:02.880,0:17:06.319
suddenly

0:17:03.759,0:17:07.120
the sum is not over the whole pixel it's

0:17:06.319,0:17:10.160
just you know

0:17:07.120,0:17:12.400
um in the neighborhood and then the

0:17:10.160,0:17:13.760
complexity of doing convolution is

0:17:12.400,0:17:16.079
actually um

0:17:13.760,0:17:17.520
to the order of the number of nodes so

0:17:16.079,0:17:20.640
the number of pixels

0:17:17.520,0:17:23.360
in your in your image so so the

0:17:20.640,0:17:24.319
the complete is quite easy to to compute

0:17:23.360,0:17:25.520
so what you're going to do is that

0:17:24.319,0:17:27.199
you're going to take your

0:17:25.520,0:17:29.600
uh your pattern you're going to slice

0:17:27.199,0:17:32.640
your pattern so it's going to be n

0:17:29.600,0:17:34.559
slicing because n number of locations

0:17:32.640,0:17:36.640
and then you're gonna do you know a

0:17:34.559,0:17:38.799
scalar product of three by three

0:17:36.640,0:17:40.000
elements and and you're gonna do you

0:17:38.799,0:17:43.200
know um

0:17:40.000,0:17:45.520
um the vector a

0:17:43.200,0:17:47.760
product of vectors of dimensionality so

0:17:45.520,0:17:48.720
you see the complexity of doing this

0:17:47.760,0:17:50.960
operation is just

0:17:48.720,0:17:51.840
n times three times three times d so the

0:17:50.960,0:17:53.440
completed is n

0:17:51.840,0:17:55.120
and and again everything can be done in

0:17:53.440,0:17:56.720
parallel if you have a gpu the

0:17:55.120,0:17:59.039
computation that you are doing

0:17:56.720,0:18:00.240
in this in this location is independent

0:17:59.039,0:18:01.440
to the competition that you're doing in

0:18:00.240,0:18:04.880
this location so

0:18:01.440,0:18:08.400
everything is is linear complexity

0:18:04.880,0:18:10.400
okay so doing that uh okay

0:18:08.400,0:18:12.240
so so at the end of the day if you want

0:18:10.400,0:18:14.400
to do convolution with template matching

0:18:12.240,0:18:17.440
you're just going to compute this

0:18:14.400,0:18:21.039
um scalar products between your template

0:18:17.440,0:18:25.120
and between uh your your image

0:18:21.039,0:18:27.200
um i would say your image patch okay

0:18:25.120,0:18:28.880
um okay so something that is very

0:18:27.200,0:18:32.320
important to see

0:18:28.880,0:18:34.559
in the case of the graph being

0:18:32.320,0:18:36.240
grid so this is for standard convolution

0:18:34.559,0:18:38.240
in computer vision

0:18:36.240,0:18:40.080
if you look at if you are looking at you

0:18:38.240,0:18:42.880
know your template which is here

0:18:40.080,0:18:43.440
okay so you see that i'm going to give

0:18:42.880,0:18:48.720
some

0:18:43.440,0:18:51.280
node ordering j1 g2 j3 and so on to g9

0:18:48.720,0:18:52.160
and this node ordering is actually very

0:18:51.280,0:18:56.720
important

0:18:52.160,0:18:58.559
okay because for for all time i mean

0:18:56.720,0:19:00.400
it's not i mean this this notes so for

0:18:58.559,0:19:03.600
example the node g3

0:19:00.400,0:19:04.720
will always be positioned at the same

0:19:03.600,0:19:06.160
location so it's

0:19:04.720,0:19:08.559
always going to be at the top right

0:19:06.160,0:19:11.440
corner of the pattern

0:19:08.559,0:19:13.280
okay so that's that's very important why

0:19:11.440,0:19:15.200
it's very important so let me go to the

0:19:13.280,0:19:17.760
next slide

0:19:15.200,0:19:19.360
so why it's very important is so when i

0:19:17.760,0:19:19.840
will do the convolution so the pattern

0:19:19.360,0:19:22.640
matching

0:19:19.840,0:19:24.080
again i will take my my pattern and i

0:19:22.640,0:19:26.480
will slice the pattern

0:19:24.080,0:19:27.520
over my image domain okay so that would

0:19:26.480,0:19:30.400
be maybe here

0:19:27.520,0:19:32.720
and i put it here and and also this is

0:19:30.400,0:19:33.600
position i position i prime that i put

0:19:32.720,0:19:35.760
here

0:19:33.600,0:19:37.120
so when i'm going to do the template

0:19:35.760,0:19:40.320
matching between

0:19:37.120,0:19:41.039
the kernel and the image what i will do

0:19:40.320,0:19:44.559
is that for

0:19:41.039,0:19:46.799
this index so the index g3

0:19:44.559,0:19:47.679
it will always match you know the

0:19:46.799,0:19:51.440
information

0:19:47.679,0:19:54.559
in the image at this index here

0:19:51.440,0:19:56.880
okay so this is very important so

0:19:54.559,0:19:58.480
you when you have a grid the node

0:19:56.880,0:20:00.720
ordering the node positioning

0:19:58.480,0:20:01.600
is always the same whatever the position

0:20:00.720,0:20:03.200
in your image

0:20:01.600,0:20:04.799
so when you do the template matching

0:20:03.200,0:20:06.880
between index g3

0:20:04.799,0:20:09.200
and this index here in the image you

0:20:06.880,0:20:11.919
always compare the same information

0:20:09.200,0:20:12.400
you always compare the feature uh at the

0:20:11.919,0:20:15.120
top

0:20:12.400,0:20:17.280
right corner uh of your pattern and the

0:20:15.120,0:20:20.480
type of corner of the image patch

0:20:17.280,0:20:23.200
okay so this um this

0:20:20.480,0:20:24.720
uh you see this matching scores they are

0:20:23.200,0:20:27.280
for the same information

0:20:24.720,0:20:30.880
okay so that's very important now let's

0:20:27.280,0:20:30.880
look at what happened for graphs

0:20:32.559,0:20:36.960
okay so the question is can we extend

0:20:34.799,0:20:38.080
this definition of template matching for

0:20:36.960,0:20:40.080
graphs

0:20:38.080,0:20:41.440
and there are two main issues so the

0:20:40.080,0:20:44.480
first issue

0:20:41.440,0:20:48.559
is basically on a graph you don't have

0:20:44.480,0:20:52.080
any uh ordering of your notes

0:20:48.559,0:20:54.960
okay so on the graph you you have

0:20:52.080,0:20:55.919
no given position uh for your notes so

0:20:54.960,0:20:59.360
let's say for example

0:20:55.919,0:21:02.320
i have this uh graph template

0:20:59.360,0:21:04.640
okay so there are like four notes with

0:21:02.320,0:21:07.760
this connection and i have this

0:21:04.640,0:21:08.480
uh vertex here the thing is for this

0:21:07.760,0:21:10.640
vertex

0:21:08.480,0:21:11.760
i know nothing about the position the

0:21:10.640,0:21:14.240
only thing that i know

0:21:11.760,0:21:15.520
is the index okay so maybe this is the

0:21:14.240,0:21:17.760
index number three

0:21:15.520,0:21:18.960
for this one and then when i when if i

0:21:17.760,0:21:20.799
want to use the template machine

0:21:18.960,0:21:23.840
definition what i'm going to do is that

0:21:20.799,0:21:27.039
i need to match you know this

0:21:23.840,0:21:29.039
uh index with other index uh

0:21:27.039,0:21:30.559
in the graph domain so this is my graph

0:21:29.039,0:21:32.320
and let's say this is for

0:21:30.559,0:21:33.760
the node i and they are the neighbors of

0:21:32.320,0:21:35.919
the node i so

0:21:33.760,0:21:37.600
for this neighbor this is the index the

0:21:35.919,0:21:40.720
same index j3

0:21:37.600,0:21:42.720
but here i mean how can i match

0:21:40.720,0:21:44.000
you know this information with this

0:21:42.720,0:21:46.400
information when

0:21:44.000,0:21:47.919
i do not know if they match to it they

0:21:46.400,0:21:50.000
mature to each other

0:21:47.919,0:21:51.440
because on the graph you don't have any

0:21:50.000,0:21:54.640
ordering of your notes

0:21:51.440,0:21:56.480
you don't know if it's not it's for the

0:21:54.640,0:21:58.559
top right corner

0:21:56.480,0:22:01.280
of any information you don't know that

0:21:58.559,0:22:03.280
so on the graph you have no notion of

0:22:01.280,0:22:04.799
where is the up where is the down where

0:22:03.280,0:22:07.440
is the right where is the left

0:22:04.799,0:22:08.080
okay so when you do this matching

0:22:07.440,0:22:11.120
between

0:22:08.080,0:22:13.440
uh this feature vector and this picture

0:22:11.120,0:22:16.720
vector actually this matching usually

0:22:13.440,0:22:18.320
generally has no meaning okay you

0:22:16.720,0:22:20.159
you you don't know what you compare to

0:22:18.320,0:22:22.480
each other okay

0:22:20.159,0:22:23.440
and again the index is completely

0:22:22.480,0:22:25.280
arbitrary

0:22:23.440,0:22:26.480
okay so you can have the value 3 here

0:22:25.280,0:22:29.039
but it can be here

0:22:26.480,0:22:30.080
the value number 2 or value number 12.

0:22:29.039,0:22:33.120
you you don't have

0:22:30.080,0:22:34.880
this is this is not you know any good

0:22:33.120,0:22:37.360
information

0:22:34.880,0:22:39.280
so basically because you don't have any

0:22:37.360,0:22:41.679
ordering of your notes on graphs

0:22:39.280,0:22:42.559
you cannot choose the definition of

0:22:41.679,0:22:44.559
template matching

0:22:42.559,0:22:47.360
you cannot use that directly so we need

0:22:44.559,0:22:47.360
to do something else

0:22:49.840,0:22:53.360
okay the second issue with template

0:22:52.320,0:22:56.880
matching for graphs

0:22:53.360,0:22:58.480
is what happens if um the number of

0:22:56.880,0:23:00.720
nodes in your template

0:22:58.480,0:23:02.080
does not match the number of nodes you

0:23:00.720,0:23:04.000
know uh

0:23:02.080,0:23:05.840
in your in your graph so for example

0:23:04.000,0:23:08.400
here i have four nodes

0:23:05.840,0:23:10.400
here i have four nodes fine maybe i can

0:23:08.400,0:23:13.120
find a way to compare

0:23:10.400,0:23:14.000
the two in the two sets of of nodes but

0:23:13.120,0:23:16.559
here i have

0:23:14.000,0:23:18.559
uh i have seven nodes so how i'm going

0:23:16.559,0:23:23.840
to compare seven nodes to four nodes

0:23:18.559,0:23:23.840
so that's also you know an open issue

0:23:24.559,0:23:28.480
okay so the third mathematical

0:23:26.640,0:23:30.240
definition was to use template matching

0:23:28.480,0:23:32.159
to define convolution

0:23:30.240,0:23:34.640
now the second definition is to use the

0:23:32.159,0:23:37.679
convolution the convolution theorem

0:23:34.640,0:23:39.360
so the convolution theorem from um from

0:23:37.679,0:23:41.200
fourier is basically the fourier

0:23:39.360,0:23:42.240
transform of the convolution of two

0:23:41.200,0:23:44.240
functions

0:23:42.240,0:23:45.919
is the pointwise product of the fourier

0:23:44.240,0:23:48.960
transform this is what you see here

0:23:45.919,0:23:50.159
okay so the fourier transform of the

0:23:48.960,0:23:53.039
convolution of

0:23:50.159,0:23:54.640
a function w and function h is the

0:23:53.039,0:23:56.159
fourier transform of f

0:23:54.640,0:23:58.159
and point twice multiplication the

0:23:56.159,0:23:59.679
fourier transform of h then if you do

0:23:58.159,0:24:02.559
the inverse for your transform

0:23:59.679,0:24:03.360
you go back to your to your convolution

0:24:02.559,0:24:06.080
so nice

0:24:03.360,0:24:07.679
okay we have a very um nice formula to

0:24:06.080,0:24:10.159
do the convolution of w and

0:24:07.679,0:24:10.720
h and but the thing is in the general

0:24:10.159,0:24:12.880
case

0:24:10.720,0:24:15.600
doing the fourier transform is n square

0:24:12.880,0:24:18.799
complexity we come back to that

0:24:15.600,0:24:22.159
however if you if your domain uh like

0:24:18.799,0:24:23.520
uh like the image grid has some very

0:24:22.159,0:24:25.279
particular structure

0:24:23.520,0:24:27.520
then you can reduce the complexity to n

0:24:25.279,0:24:30.799
log n uh by using you know

0:24:27.520,0:24:31.520
uh fast fourier transform okay so the

0:24:30.799,0:24:34.640
question is

0:24:31.520,0:24:38.159
can we extend this definition of

0:24:34.640,0:24:40.799
uh of convolution theorem to graphs

0:24:38.159,0:24:41.840
so the question is how do we redefine a

0:24:40.799,0:24:45.200
fourier transform

0:24:41.840,0:24:47.039
for four graphs okay and

0:24:45.200,0:24:48.559
and the thing is how to make it fast

0:24:47.039,0:24:51.520
okay so remember that

0:24:48.559,0:24:52.240
in the case of uh of template matching

0:24:51.520,0:24:54.880
uh

0:24:52.240,0:24:56.720
we have linear complexity so how do we

0:24:54.880,0:24:58.000
have a fast spectral convolution in

0:24:56.720,0:25:00.159
linear time

0:24:58.000,0:25:01.120
for compact kernels so that's that's the

0:25:00.159,0:25:03.200
two open question

0:25:01.120,0:25:04.559
okay so basically we are going to use

0:25:03.200,0:25:08.400
these two definitions

0:25:04.559,0:25:11.520
of convolution to uh design two classes

0:25:08.400,0:25:12.880
of graph neural network so the

0:25:11.520,0:25:15.520
this will be the template machine will

0:25:12.880,0:25:17.200
be for the spatial graph cornets

0:25:15.520,0:25:19.600
and the conversion rotary i'm going to

0:25:17.200,0:25:21.520
use that for the spectral graph net and

0:25:19.600,0:25:22.960
this is the next

0:25:21.520,0:25:25.440
the next part that i'm going to talk

0:25:22.960,0:25:25.440
about now

0:25:27.279,0:25:36.799
okay so let's talk about

0:25:31.520,0:25:36.799
how we do spectral convolution okay

0:25:39.520,0:25:42.799
so i there is a book that i like very

0:25:42.400,0:25:45.360
much

0:25:42.799,0:25:45.919
uh which is the book of uh fan chung

0:25:45.360,0:25:48.320
which is

0:25:45.919,0:25:49.600
a spectrograph theory so there is

0:25:48.320,0:25:51.919
everything in nice like

0:25:49.600,0:25:53.440
harmonic analysis graphery combinatorial

0:25:51.919,0:25:56.400
problems and optimization

0:25:53.440,0:25:57.520
so i really recommend you know people to

0:25:56.400,0:26:00.159
read the books

0:25:57.520,0:26:01.039
they want to know more and a lot more

0:26:00.159,0:26:03.600
about about these

0:26:01.039,0:26:04.720
these questions so how do we perform

0:26:03.600,0:26:07.039
spectral convolution

0:26:04.720,0:26:08.320
so we are going to use four steps so the

0:26:07.039,0:26:11.120
first step will be to do

0:26:08.320,0:26:12.000
would be to define graph application

0:26:11.120,0:26:14.880
second step

0:26:12.000,0:26:15.520
will be to define fourier functions then

0:26:14.880,0:26:17.919
we will do

0:26:15.520,0:26:18.880
fourier transform and eventually

0:26:17.919,0:26:21.919
convolution

0:26:18.880,0:26:24.880
theorem okay

0:26:21.919,0:26:25.919
so the what is the graph location so the

0:26:24.880,0:26:27.840
graph calculation

0:26:25.919,0:26:29.600
this is the core operator in spectral

0:26:27.840,0:26:32.159
graph theory okay

0:26:29.600,0:26:33.760
so remember how we define a graph we

0:26:32.159,0:26:35.520
have a set of vertices

0:26:33.760,0:26:36.880
a set of edges and then we have the

0:26:35.520,0:26:39.440
adjacency matrix

0:26:36.880,0:26:40.799
so if the graph has n vertices the

0:26:39.440,0:26:44.000
adjacency matrix

0:26:40.799,0:26:45.919
is a n by n matrix

0:26:44.000,0:26:47.279
so we are going simply to define the

0:26:45.919,0:26:49.919
laplacian which is

0:26:47.279,0:26:50.960
also going to be an n by n matrix to be

0:26:49.919,0:26:53.760
the identity

0:26:50.960,0:26:56.080
minus the adjacency matrix and we are

0:26:53.760,0:27:00.159
going to normalize

0:26:56.080,0:27:02.720
the adjacency matrix by using

0:27:00.159,0:27:05.520
the degree of each node so d is

0:27:02.720,0:27:07.039
basically a diagonal matrix

0:27:05.520,0:27:09.360
and the diagonal each element of the

0:27:07.039,0:27:09.919
diagonal is basically the degree of the

0:27:09.360,0:27:12.080
node

0:27:09.919,0:27:13.600
okay so we are doing and this is called

0:27:12.080,0:27:15.760
the normalized location

0:27:13.600,0:27:17.679
okay so this is i would say this is by

0:27:15.760,0:27:21.679
default the definition of

0:27:17.679,0:27:24.480
uh laplacian that we use for for graphs

0:27:21.679,0:27:25.520
so we can interpret uh this uh this

0:27:24.480,0:27:28.799
operator

0:27:25.520,0:27:31.360
so the laplacian is this combination so

0:27:28.799,0:27:32.000
the a was that matrix with basically all

0:27:31.360,0:27:33.600
zeros

0:27:32.000,0:27:35.200
and the one was representing the

0:27:33.600,0:27:38.320
connection between

0:27:35.200,0:27:40.799
edges right um yes so

0:27:38.320,0:27:42.799
uh for facebook for example i would say

0:27:40.799,0:27:45.919
that this is exactly the definition so

0:27:42.799,0:27:48.720
if not a user i is

0:27:45.919,0:27:49.360
a friend with a user j then you will

0:27:48.720,0:27:52.080
have

0:27:49.360,0:27:52.960
adjacency matrix value will be i j equal

0:27:52.080,0:27:56.080
to one

0:27:52.960,0:27:58.640
and if two users are not friends

0:27:56.080,0:28:00.240
then you will get the value zero but

0:27:58.640,0:28:02.720
sometimes you have a real value

0:28:00.240,0:28:05.279
for a for example for the for the brain

0:28:02.720,0:28:08.880
connectivity graph

0:28:05.279,0:28:10.399
the value of aij is the degree of

0:28:08.880,0:28:11.520
connection between the two regions so

0:28:10.399,0:28:15.360
basically what we

0:28:11.520,0:28:17.679
say the number of fibers that connect

0:28:15.360,0:28:19.760
region i and region j so that can be

0:28:17.679,0:28:20.320
binary that can be also a continuous

0:28:19.760,0:28:23.200
value

0:28:20.320,0:28:24.240
and also this is symmetric if it's non

0:28:23.200,0:28:27.600
oriented graph

0:28:24.240,0:28:30.880
otherwise yes so yeah for

0:28:27.600,0:28:32.080
usually it is symmetric uh and you want

0:28:30.880,0:28:33.600
you want the symmetry for for

0:28:32.080,0:28:36.320
mathematical reasons

0:28:33.600,0:28:38.080
um but you may have some not so here

0:28:36.320,0:28:39.760
this is the normalized elevation but if

0:28:38.080,0:28:42.640
you have the random walk

0:28:39.760,0:28:43.039
laplacian then this is non-symmetric

0:28:42.640,0:28:44.960
okay

0:28:43.039,0:28:46.480
so it's um it's it's different

0:28:44.960,0:28:48.000
definition of the application so

0:28:46.480,0:28:49.919
in the case of laplacian it's very

0:28:48.000,0:28:51.440
interesting so in the continuous setting

0:28:49.919,0:28:52.799
you have only one definition for the

0:28:51.440,0:28:54.240
application this is called the laplace

0:28:52.799,0:28:55.919
beltrami operator

0:28:54.240,0:28:57.600
in the discrete setting you have

0:28:55.919,0:28:59.279
multiple definitions you can do your own

0:28:57.600,0:29:00.640
definition of the application depending

0:28:59.279,0:29:03.200
on on the assumptions

0:29:00.640,0:29:03.760
that that you are going to use i

0:29:03.200,0:29:07.760
understand

0:29:03.760,0:29:09.120
thank you okay so we can interpret the

0:29:07.760,0:29:10.399
application so the application is

0:29:09.120,0:29:11.520
nothing else than a measure of

0:29:10.399,0:29:14.880
smoothness

0:29:11.520,0:29:16.880
of a function on a on a graph

0:29:14.880,0:29:18.080
so this is nothing else then you you see

0:29:16.880,0:29:19.520
you see so i'm doing the elaboration

0:29:18.080,0:29:22.399
that i reply to a function

0:29:19.520,0:29:23.120
h okay on the graph and i'm looking at

0:29:22.399,0:29:26.000
what happened

0:29:23.120,0:29:26.720
at the vertex i and if i expand this

0:29:26.000,0:29:30.399
definition

0:29:26.720,0:29:33.840
i will have the value of h i minus

0:29:30.399,0:29:36.080
the mean value of the neighborhood okay

0:29:33.840,0:29:38.159
so basically if your signal is smooth

0:29:36.080,0:29:41.039
you know if it doesn't vary much

0:29:38.159,0:29:41.360
then this difference will be very small

0:29:41.039,0:29:43.600
but

0:29:41.360,0:29:44.480
if your signal you know there is a lot

0:29:43.600,0:29:46.240
it oscillates

0:29:44.480,0:29:48.000
a lot then the difference will be very

0:29:46.240,0:29:50.240
high so the laplacian is nothing else

0:29:48.000,0:29:55.840
and a measure of smoothness of function

0:29:50.240,0:29:55.840
on a on a on a graph

0:29:57.200,0:30:03.600
okay all right so

0:30:01.279,0:30:04.880
now let's define fourier functions so

0:30:03.600,0:30:07.840
let's let's take the

0:30:04.880,0:30:09.279
laplacian matrix and let's do a little

0:30:07.840,0:30:11.360
bit of linear algebra

0:30:09.279,0:30:12.559
let's do eigen decomposition of the

0:30:11.360,0:30:14.640
graph operation

0:30:12.559,0:30:15.760
so when you do eigen decomposition you

0:30:14.640,0:30:18.159
will have the

0:30:15.760,0:30:19.279
you will you are going to factorize your

0:30:18.159,0:30:22.080
laplacian matrix

0:30:19.279,0:30:23.039
into three matrices so you have a phi

0:30:22.080,0:30:26.480
transpose

0:30:23.039,0:30:30.000
lambda and five so this

0:30:26.480,0:30:32.240
matrix five of the size n by n actually

0:30:30.000,0:30:33.200
uh have the what are the laplacian

0:30:32.240,0:30:35.520
eigenvectors

0:30:33.200,0:30:36.480
okay for each column and the laplacian

0:30:35.520,0:30:38.640
eigenvectors

0:30:36.480,0:30:40.960
they are called the fourier functions

0:30:38.640,0:30:43.039
okay the famous fourier functions

0:30:40.960,0:30:44.000
and of course this is an orthonormal

0:30:43.039,0:30:46.640
basis

0:30:44.000,0:30:49.039
um so when you do the the product

0:30:46.640,0:30:50.480
between two bases you will get one

0:30:49.039,0:30:52.799
if they are the same and then you get

0:30:50.480,0:30:55.840
zero if they are orthogonal

0:30:52.799,0:30:59.120
if they are different this is also an

0:30:55.840,0:31:02.880
invertible matrix this

0:30:59.120,0:31:05.279
guy so this matrix this is the

0:31:02.880,0:31:06.159
diagonal matrix of the laplacian

0:31:05.279,0:31:09.279
eigenvalues

0:31:06.159,0:31:12.000
so lambda 1 to number n

0:31:09.279,0:31:12.559
and and we know that for the normalized

0:31:12.000,0:31:14.799
application

0:31:12.559,0:31:16.080
that these values are bonded between

0:31:14.799,0:31:17.760
zero and between

0:31:16.080,0:31:19.840
two so this is the maximum value that

0:31:17.760,0:31:21.919
you can get this guy

0:31:19.840,0:31:23.200
the laplacian eigenvalues they are known

0:31:21.919,0:31:25.679
as the spectrum

0:31:23.200,0:31:27.200
of the graph okay so if you take a graph

0:31:25.679,0:31:30.080
here you have 27

0:31:27.200,0:31:32.080
nodes if i compute the laplacian

0:31:30.080,0:31:34.799
eigenvalues and if i put them

0:31:32.080,0:31:36.320
i have a signature of the graph which is

0:31:34.799,0:31:38.320
called the spectrum of the graph

0:31:36.320,0:31:39.840
okay that which would be different for

0:31:38.320,0:31:42.559
each each graph

0:31:39.840,0:31:43.519
okay and here you have okay this is what

0:31:42.559,0:31:45.760
i say so this is

0:31:43.519,0:31:47.360
doing um again decomposition so if you

0:31:45.760,0:31:49.519
take your

0:31:47.360,0:31:50.880
laplacian matrix and you apply to a

0:31:49.519,0:31:54.559
vector

0:31:50.880,0:31:57.200
phi of k then you will get the

0:31:54.559,0:31:58.320
eigenvalue lambda k times the same

0:31:57.200,0:32:00.399
vector

0:31:58.320,0:32:02.399
phi of k okay so this is the definition

0:32:00.399,0:32:05.679
of the um

0:32:02.399,0:32:08.080
eigen decomposition okay so you see that

0:32:05.679,0:32:08.720
fourier functions they are nothing else

0:32:08.080,0:32:15.840
than

0:32:08.720,0:32:15.840
the laplacian eigenvectors

0:32:16.799,0:32:22.480
okay let me illustrate uh these uh

0:32:20.240,0:32:24.880
fourier functions so we actually we

0:32:22.480,0:32:26.720
already know uh for your functions

0:32:24.880,0:32:28.480
uh if you if you take the greater for

0:32:26.720,0:32:30.720
example you take here a one degree

0:32:28.480,0:32:32.480
and you compute the fourier functions so

0:32:30.720,0:32:35.840
you so you will get a phi

0:32:32.480,0:32:38.960
zero okay then you will get phi one

0:32:35.840,0:32:41.919
which is this one which is smooth phi

0:32:38.960,0:32:42.320
two which is less a little less mousse

0:32:41.919,0:32:44.960
and

0:32:42.320,0:32:46.880
phi 3 and so on and so on so this is

0:32:44.960,0:32:49.360
well known this is the cosine function

0:32:46.880,0:32:50.559
and the semi-zoids and we use that you

0:32:49.360,0:32:52.480
know for

0:32:50.559,0:32:54.320
for image compression so if we take an

0:32:52.480,0:32:56.640
image and we project the image

0:32:54.320,0:32:58.480
on the fourier functions then the image

0:32:56.640,0:32:59.279
is going to be the the transformation is

0:32:58.480,0:33:00.799
going to be sparse

0:32:59.279,0:33:02.799
so you only keep you know the highest

0:33:00.799,0:33:04.320
coefficient and you can do compression

0:33:02.799,0:33:05.200
so this is something that we've used for

0:33:04.320,0:33:08.080
a very

0:33:05.200,0:33:10.559
long time for for graph for the graph

0:33:08.080,0:33:12.880
domain this is this is quite interesting

0:33:10.559,0:33:14.399
so you see that this is a graph and i'm

0:33:12.880,0:33:17.360
computing here the

0:33:14.399,0:33:18.799
the first uh four uh you know fourier

0:33:17.360,0:33:21.919
function of the graphs

0:33:18.799,0:33:23.600
so you see for phi one uh you still have

0:33:21.919,0:33:24.159
oscillations you know between positive

0:33:23.600,0:33:25.919
and negative

0:33:24.159,0:33:28.880
value the same with positive and

0:33:25.919,0:33:32.080
negative value and and here as well

0:33:28.880,0:33:32.880
um what is interesting is that this

0:33:32.080,0:33:35.840
oscillation

0:33:32.880,0:33:36.640
uh depends on the topology of the graph

0:33:35.840,0:33:38.559
okay

0:33:36.640,0:33:40.480
so so it's related to the to the

0:33:38.559,0:33:43.279
geometry of the graph like communities

0:33:40.480,0:33:44.080
like hubs and so on and we know that uh

0:33:43.279,0:33:46.960
so for example

0:33:44.080,0:33:47.840
if you want to capture k communities on

0:33:46.960,0:33:51.200
graph

0:33:47.840,0:33:54.480
uh a very good algorithm is to apply

0:33:51.200,0:33:56.799
k-means on the first key

0:33:54.480,0:33:58.240
fourier functions if you do that you

0:33:56.799,0:33:58.960
have something that we call spectrograph

0:33:58.240,0:34:01.840
theory

0:33:58.960,0:34:02.480
and it's it's a it's a huge literature

0:34:01.840,0:34:04.000
uh

0:34:02.480,0:34:06.080
and and if you want to know more about

0:34:04.000,0:34:07.039
this there is this very nice tutorial by

0:34:06.080,0:34:09.679
van looks pro

0:34:07.039,0:34:11.359
about about spectrograph clustering and

0:34:09.679,0:34:13.760
using all these notions of fourier

0:34:11.359,0:34:13.760
functions

0:34:14.800,0:34:21.679
okay okay now let me introduce you

0:34:17.919,0:34:23.040
a fourier transform okay so for this i'm

0:34:21.679,0:34:25.200
going to

0:34:23.040,0:34:26.560
do the fourier series for your series if

0:34:25.200,0:34:29.440
is nothing else

0:34:26.560,0:34:30.320
then you take a function h defined on

0:34:29.440,0:34:32.000
your graph

0:34:30.320,0:34:34.480
and then you are going to decompose this

0:34:32.000,0:34:37.679
function using the fourier function

0:34:34.480,0:34:38.399
okay so i take my function h i project

0:34:37.679,0:34:41.760
my function

0:34:38.399,0:34:44.960
h on each fourier function

0:34:41.760,0:34:47.040
phi of k and i will get you know

0:34:44.960,0:34:48.639
this coefficient of this fourier series

0:34:47.040,0:34:51.679
it's going to be a scalar

0:34:48.639,0:34:52.399
multiply by my function phi of k okay of

0:34:51.679,0:34:55.440
the time and

0:34:52.399,0:34:59.040
n by one of the size and

0:34:55.440,0:35:01.040
n by one okay so and doing that you know

0:34:59.040,0:35:02.160
just projecting my function on the

0:35:01.040,0:35:04.560
fourier functions

0:35:02.160,0:35:06.000
give me the fourier transform okay so

0:35:04.560,0:35:07.760
the fourier transform

0:35:06.000,0:35:09.680
is is just you know the coefficient of

0:35:07.760,0:35:13.119
the fourier series nothing else

0:35:09.680,0:35:15.119
okay then h you know is basically

0:35:13.119,0:35:16.480
a linear combination of the fourier

0:35:15.119,0:35:19.440
transform times

0:35:16.480,0:35:20.000
the the fourier functions okay i can

0:35:19.440,0:35:22.880
rewrite

0:35:20.000,0:35:24.160
everything in matrix vector

0:35:22.880,0:35:27.440
representation

0:35:24.160,0:35:30.720
and these guys so doing the um phi

0:35:27.440,0:35:32.560
times the the fourier transform this is

0:35:30.720,0:35:35.680
actually the inverse for your transform

0:35:32.560,0:35:39.280
okay so let me summarize this

0:35:35.680,0:35:40.880
if i do if i project h on the fourier

0:35:39.280,0:35:42.880
functions i will have the fourier

0:35:40.880,0:35:46.240
transform okay so i'm taking

0:35:42.880,0:35:48.079
the matrix of the fourier functions and

0:35:46.240,0:35:50.880
multiply by h so this is n by

0:35:48.079,0:35:51.359
n by n this is n by one so this is n by

0:35:50.880,0:35:54.800
one

0:35:51.359,0:35:58.160
okay uh and now if i do

0:35:54.800,0:35:58.800
inverse fourier transform of the fourier

0:35:58.160,0:36:02.560
transform

0:35:58.800,0:36:05.280
okay so i would have phi of um

0:36:02.560,0:36:05.920
fourier transform of h and this guy is

0:36:05.280,0:36:09.440
here

0:36:05.920,0:36:13.040
okay so i just put phi transpose h

0:36:09.440,0:36:15.920
and we know that um the the basis

0:36:13.040,0:36:17.280
is orthonormal so this guy is actually

0:36:15.920,0:36:20.000
identity function

0:36:17.280,0:36:22.079
that am sorry identity matrix okay so

0:36:20.000,0:36:25.440
this is energy matrix so i come back to

0:36:22.079,0:36:29.040
h so so the inverse for your transform

0:36:25.440,0:36:31.440
is is of of the fourier transform is h

0:36:29.040,0:36:32.480
obviously okay so one thing that you can

0:36:31.440,0:36:35.119
observe is that

0:36:32.480,0:36:36.160
the fourier transform and the inverse

0:36:35.119,0:36:38.800
fourier transform

0:36:36.160,0:36:39.599
can be done in one line of code okay you

0:36:38.800,0:36:41.839
just take

0:36:39.599,0:36:43.520
your vector h you multiply by this

0:36:41.839,0:36:45.040
matrix and that's it

0:36:43.520,0:36:47.680
and the same also to do the inverse for

0:36:45.040,0:36:50.720
your transform you take your your signal

0:36:47.680,0:36:52.800
and you multiply by this matrix so it's

0:36:50.720,0:36:55.200
basically just linear operations

0:36:52.800,0:36:56.240
uh just multiplying a matrix by a vector

0:36:55.200,0:36:58.000
and this is how you do fourier

0:36:56.240,0:37:00.400
transforming inversely transform on

0:36:58.000,0:37:00.400
graphs

0:37:03.520,0:37:07.200
okay now let's let's do the convolution

0:37:06.079,0:37:09.200
theorem

0:37:07.200,0:37:11.280
so again the convolution theorem the

0:37:09.200,0:37:14.240
fourier transform

0:37:11.280,0:37:16.320
the fourier transform of your um the

0:37:14.240,0:37:18.640
free transform of the convolution

0:37:16.320,0:37:20.079
is is going to be the pointwise product

0:37:18.640,0:37:23.599
of the fourier transform of

0:37:20.079,0:37:27.119
each signal okay so let's say i have um

0:37:23.599,0:37:30.000
w convolution h uh so i'm going

0:37:27.119,0:37:30.400
first to do the fourier transform of w

0:37:30.000,0:37:32.320
then

0:37:30.400,0:37:33.520
this is going to be a vector of the size

0:37:32.320,0:37:35.920
n by one

0:37:33.520,0:37:37.839
then i'm going to multiply point twice

0:37:35.920,0:37:38.480
by another vector which is the fourier

0:37:37.839,0:37:40.720
transform of

0:37:38.480,0:37:42.320
h okay so how do we get the free

0:37:40.720,0:37:45.440
transform just by doing

0:37:42.320,0:37:46.800
phi transpose w and phi transpose h and

0:37:45.440,0:37:47.839
then i'm going to do the inverse fourier

0:37:46.800,0:37:49.839
transform to come

0:37:47.839,0:37:52.320
to go back to the spatial domain so i

0:37:49.839,0:37:55.599
just multiplied by the matrix five

0:37:52.320,0:37:55.920
okay and by n so this is what i write

0:37:55.599,0:37:59.119
here

0:37:55.920,0:38:01.440
okay i have phi i have um

0:37:59.119,0:38:02.160
w hat which is a fourier transform and i

0:38:01.440,0:38:03.839
have this

0:38:02.160,0:38:06.400
this i'm going to change it i'm going to

0:38:03.839,0:38:08.400
change it to this line what is this line

0:38:06.400,0:38:11.359
um shouldn't there be a phi transpose

0:38:08.400,0:38:11.359
before w hat

0:38:11.839,0:38:17.680
sorry shouldn't there be a 5's transpose

0:38:15.359,0:38:20.480
before w hat

0:38:17.680,0:38:21.520
no the inverse fourier transform is fine

0:38:20.480,0:38:23.760
okay

0:38:21.520,0:38:25.119
so you do phi and you multiply by the

0:38:23.760,0:38:26.480
fourier transform which is a phi

0:38:25.119,0:38:29.839
transpose w

0:38:26.480,0:38:31.520
which i call hat w

0:38:29.839,0:38:33.280
so i'm going i'm going to use that a lot

0:38:31.520,0:38:34.560
uh i will come back to this

0:38:33.280,0:38:36.320
and then here you have the fourier

0:38:34.560,0:38:38.720
transform of h which is just phi

0:38:36.320,0:38:42.000
transpose h which is here

0:38:38.720,0:38:44.640
okay so this guy um

0:38:42.000,0:38:46.160
okay this guy is actually what we call

0:38:44.640,0:38:49.280
the spectral function

0:38:46.160,0:38:50.640
okay the spectral filter so this guy is

0:38:49.280,0:38:53.359
a vector of n by one

0:38:50.640,0:38:54.320
okay and i'm writing i'm writing here uh

0:38:53.359,0:38:56.640
this vector here

0:38:54.320,0:38:57.760
so you see this is a vector of uh n

0:38:56.640,0:38:59.920
elements

0:38:57.760,0:39:01.040
and this is actually the spectral

0:38:59.920,0:39:04.240
function

0:39:01.040,0:39:08.079
which is um evaluated

0:39:04.240,0:39:09.920
at the at the uh at the eigenvalue

0:39:08.079,0:39:10.800
lambda 1 which is here so this is this

0:39:09.920,0:39:14.160
point here

0:39:10.800,0:39:16.160
then you have a w uh hat

0:39:14.160,0:39:18.960
number two which is this this value here

0:39:16.160,0:39:20.800
and so on and so on okay

0:39:18.960,0:39:22.400
and then i'm going to rewrite this you

0:39:20.800,0:39:24.560
know i'm going to put this

0:39:22.400,0:39:25.520
uh in a diagonal okay so i will do

0:39:24.560,0:39:28.480
diagonal of

0:39:25.520,0:39:29.119
this vector so this will create a matrix

0:39:28.480,0:39:32.160
of the size

0:39:29.119,0:39:33.280
n by n okay and i'm putting this guy

0:39:32.160,0:39:36.000
back here

0:39:33.280,0:39:38.160
so i'm going to change the the point

0:39:36.000,0:39:39.119
twice multiplication of this vector n by

0:39:38.160,0:39:41.359
one and this vector

0:39:39.119,0:39:42.480
by one by the matrix vector

0:39:41.359,0:39:44.640
multiplication

0:39:42.480,0:39:46.640
and it's going to be the same right this

0:39:44.640,0:39:47.119
is a diagonal matrix which contains this

0:39:46.640,0:39:50.480
guy

0:39:47.119,0:39:52.000
multiply multiply by this by this vector

0:39:50.480,0:39:53.839
so this it's exactly the same these two

0:39:52.000,0:39:55.440
lines but what i want to do that

0:39:53.839,0:39:57.040
because i want to get rid of the

0:39:55.440,0:39:58.160
parenthesis okay so i don't have the

0:39:57.040,0:40:00.480
parenthesis anymore

0:39:58.160,0:40:02.240
and i have just you know matrix matrix

0:40:00.480,0:40:06.560
multiplication

0:40:02.240,0:40:08.960
okay so this is this is what i get

0:40:06.560,0:40:10.720
um then i'm going to do something is

0:40:08.960,0:40:11.520
that we know that when you apply a

0:40:10.720,0:40:14.160
function

0:40:11.520,0:40:15.359
on the eigenvalues okay if you have some

0:40:14.160,0:40:17.119
orthogonal basis

0:40:15.359,0:40:18.720
then you can put it inside you can put

0:40:17.119,0:40:22.000
it inside and this is what i do here

0:40:18.720,0:40:24.480
i put phi and phi transpose inside

0:40:22.000,0:40:26.079
and this guy is precisely the definition

0:40:24.480,0:40:28.880
of the application

0:40:26.079,0:40:30.240
okay the operation uh when i do the

0:40:28.880,0:40:33.599
again the composition

0:40:30.240,0:40:37.119
is phi launder file transpose

0:40:33.599,0:40:39.440
okay um then so

0:40:37.119,0:40:40.400
what i have is basically the spectral

0:40:39.440,0:40:42.960
function

0:40:40.400,0:40:44.079
that i applied to the application uh

0:40:42.960,0:40:47.680
operator

0:40:44.079,0:40:50.400
and this is n by n uh matrix

0:40:47.680,0:40:51.440
and applied to the vector n by one so at

0:40:50.400,0:40:55.200
the end i would get

0:40:51.440,0:40:57.359
an n by one vector okay so you see that

0:40:55.200,0:40:58.720
if you want to do so it's important now

0:40:57.359,0:41:02.000
so if you want to do

0:40:58.720,0:41:02.960
a convolution of two functions on graph

0:41:02.000,0:41:04.960
w and h

0:41:02.960,0:41:06.079
what you're going to do is that you're

0:41:04.960,0:41:09.680
going to take the

0:41:06.079,0:41:10.480
spectral function of w you will apply it

0:41:09.680,0:41:12.079
to the

0:41:10.480,0:41:13.839
uh to the laplacian and then you

0:41:12.079,0:41:16.960
multiply by h

0:41:13.839,0:41:20.400
okay this is the definition of uh

0:41:16.960,0:41:21.040
of spectral convolution okay and and the

0:41:20.400,0:41:23.440
thing is

0:41:21.040,0:41:25.440
this is very expensive uh in practice to

0:41:23.440,0:41:28.480
do it why it is expensive

0:41:25.440,0:41:31.680
it's because the matrix phi is a

0:41:28.480,0:41:35.599
full matrix okay it contains uh

0:41:31.680,0:41:37.440
the n uh the n um fourier functions

0:41:35.599,0:41:39.440
and they are not zero okay so it's a

0:41:37.440,0:41:40.880
dense matrix and you are going to pay

0:41:39.440,0:41:42.400
the price of n square

0:41:40.880,0:41:44.160
and you don't have any fft because the

0:41:42.400,0:41:46.880
thing you don't have any fft

0:41:44.160,0:41:47.200
for uh for general graph okay so this is

0:41:46.880,0:41:49.280
a

0:41:47.200,0:41:50.880
this is this is a lot and why it is a

0:41:49.280,0:41:53.200
lot because n remember

0:41:50.880,0:41:54.160
and this is the number of nodes in your

0:41:53.200,0:41:57.359
domain

0:41:54.160,0:41:58.800
so if you have um if you have a big

0:41:57.359,0:42:02.240
graph for example if you have

0:41:58.800,0:42:05.040
uh the web the web has you know billions

0:42:02.240,0:42:05.839
of nodes n is equal to the billions so

0:42:05.040,0:42:08.319
you need to do b

0:42:05.839,0:42:09.599
and square which is going to be a huge

0:42:08.319,0:42:12.560
computation to do so

0:42:09.599,0:42:12.560
you cannot really do it

0:42:12.640,0:42:16.000
can i summarize so h is going to be a

0:42:15.040,0:42:19.440
function defined

0:42:16.000,0:42:21.200
over every vertex in your graph right

0:42:19.440,0:42:23.599
uh and w instead is going to be like a

0:42:21.200,0:42:26.800
kernel as well or is he

0:42:23.599,0:42:30.319
but w is going to be a function

0:42:26.800,0:42:30.960
like this so w is a spectral function w

0:42:30.319,0:42:33.760
hat

0:42:30.960,0:42:34.240
is the spectral function so you are

0:42:33.760,0:42:36.400
working

0:42:34.240,0:42:37.920
in the frequency space in the frequency

0:42:36.400,0:42:40.160
space you are working with this

0:42:37.920,0:42:41.440
what this is this is a spectral function

0:42:40.160,0:42:43.280
so for example

0:42:41.440,0:42:45.200
if you if you know image processing a

0:42:43.280,0:42:46.880
little bit so for example if you want to

0:42:45.200,0:42:49.440
do image denoising

0:42:46.880,0:42:50.640
if you want to do image denoising what

0:42:49.440,0:42:53.359
you what you know is that

0:42:50.640,0:42:54.960
you know that the noise is usually in

0:42:53.359,0:42:57.359
the high frequency part

0:42:54.960,0:42:58.000
of your image of your signal so what you

0:42:57.359,0:43:00.400
can do is

0:42:58.000,0:43:02.400
that you can design a spectral filter

0:43:00.400,0:43:04.640
which is going to be zero

0:43:02.400,0:43:06.079
for the high frequency and you are going

0:43:04.640,0:43:07.040
to preserve you know the low frequency

0:43:06.079,0:43:09.599
to preserve

0:43:07.040,0:43:11.040
your your geometry so this is just you

0:43:09.599,0:43:14.160
know doing filtering

0:43:11.040,0:43:15.520
of the frequencies you know contain in

0:43:14.160,0:43:17.440
your signal

0:43:15.520,0:43:19.040
okay okay but the wd without the hat

0:43:17.440,0:43:20.160
would be still a small guy right would

0:43:19.040,0:43:23.440
be a small

0:43:20.160,0:43:25.440
uh filter exactly so w without hat

0:43:23.440,0:43:27.280
is the special filter yeah the small one

0:43:25.440,0:43:30.000
right which is exactly so

0:43:27.280,0:43:30.319
exactly so if you have the grid w will

0:43:30.000,0:43:33.760
be

0:43:30.319,0:43:34.720
you know a three by three uh a three by

0:43:33.760,0:43:38.240
three you know the

0:43:34.720,0:43:41.359
patch for example now

0:43:38.240,0:43:44.720
i see okay okay okay

0:43:41.359,0:43:46.960
thanks yeah sure um so

0:43:44.720,0:43:49.440
in the context of graph uh so it's it's

0:43:46.960,0:43:50.880
a small property um

0:43:49.440,0:43:52.720
to know is that you don't have any

0:43:50.880,0:43:54.640
shifting values um

0:43:52.720,0:43:56.400
so if you have a grid and if you are

0:43:54.640,0:43:59.680
using the convolutional

0:43:56.400,0:44:01.040
um theorem uh to to move around you know

0:43:59.680,0:44:03.440
your function for example the function

0:44:01.040,0:44:04.880
is aggression here on the grid you are

0:44:03.440,0:44:05.839
not going to change the shape of your

0:44:04.880,0:44:08.000
function

0:44:05.839,0:44:10.240
but on a graph because you have you know

0:44:08.000,0:44:12.720
uh irregular structure

0:44:10.240,0:44:14.240
if you move around your gaussian then

0:44:12.720,0:44:16.160
you will have different shapes

0:44:14.240,0:44:17.520
okay so this is something that that you

0:44:16.160,0:44:20.160
lose when you go

0:44:17.520,0:44:22.319
to graphs but in practice actually it

0:44:20.160,0:44:23.440
has absolutely no effect so it's not

0:44:22.319,0:44:25.119
really important it's just

0:44:23.440,0:44:27.920
a mathematical property that you lose

0:44:25.119,0:44:30.480
when you go to graphs

0:44:27.920,0:44:31.920
okay there is another question there is

0:44:30.480,0:44:34.400
another question i got here

0:44:31.920,0:44:36.079
so can you ask can you remind us what is

0:44:34.400,0:44:37.599
actually the overall goal here

0:44:36.079,0:44:39.520
what is the goal of defining these

0:44:37.599,0:44:41.760
convolutions or the spectral

0:44:39.520,0:44:44.720
correspondence over these graphs i think

0:44:41.760,0:44:45.760
uh maybe it's not yeah if we can remind

0:44:44.720,0:44:48.720
everyone it's going to be

0:44:45.760,0:44:49.680
yeah so so what i'm trying the the goal

0:44:48.720,0:44:52.880
of the lecture

0:44:49.680,0:44:55.920
is to define a convolutional

0:44:52.880,0:44:59.359
graph convolutional nets okay

0:44:55.920,0:45:02.480
so i need to redefine a convolution

0:44:59.359,0:45:04.400
in the case of graphs and there are two

0:45:02.480,0:45:06.319
ways to define convolutions

0:45:04.400,0:45:08.160
uh you can do convolution with template

0:45:06.319,0:45:11.520
matching or you can do convolution

0:45:08.160,0:45:14.400
with a graph spectral theory

0:45:11.520,0:45:17.119
so what i'm doing here um i'm i'm

0:45:14.400,0:45:19.119
redefining convolution in the case of

0:45:17.119,0:45:20.640
spectral theory and then i'm going to

0:45:19.119,0:45:24.240
use this definition

0:45:20.640,0:45:26.400
of convolution to define

0:45:24.240,0:45:27.920
graph convolutional nets so my goal is

0:45:26.400,0:45:28.640
just to define convolution in the case

0:45:27.920,0:45:32.160
of graphs

0:45:28.640,0:45:36.720
so i can i can i can design a graph

0:45:32.160,0:45:36.720
convolutional nodes okay sounds great

0:45:37.200,0:45:42.800
okay so let's go to um now okay so now

0:45:40.400,0:45:44.240
the first part was okay i defined a

0:45:42.800,0:45:44.640
spectral convolution now i'm going to

0:45:44.240,0:45:49.440
use

0:45:44.640,0:45:49.440
spectral convolution to define gcn

0:45:49.520,0:45:55.920
okay okay so the first model

0:45:53.280,0:45:56.560
what i call vanilla spectral gcn was

0:45:55.920,0:46:00.079
introduced

0:45:56.560,0:46:00.640
actually by januka and his collaborators

0:46:00.079,0:46:03.760
so

0:46:00.640,0:46:06.960
john brenner zahamba and

0:46:03.760,0:46:07.760
archer slam in 2014 i think it was for

0:46:06.960,0:46:12.160
the first uh

0:46:07.760,0:46:14.960
iqr conference um and what they did

0:46:12.160,0:46:15.520
they did you know the the simple uh idea

0:46:14.960,0:46:18.160
to do

0:46:15.520,0:46:20.319
okay let's let's let's uh you know

0:46:18.160,0:46:21.200
define a graph uh spectral convolutional

0:46:20.319,0:46:24.319
layer

0:46:21.200,0:46:26.720
so we know what is you know a standard

0:46:24.319,0:46:28.240
convolutional layer so you you this is

0:46:26.720,0:46:30.640
the activation at the next layer

0:46:28.240,0:46:33.359
a plus one this is your non-linear

0:46:30.640,0:46:35.520
activation so this is for example

0:46:33.359,0:46:36.800
um and then i'm going to do the spatial

0:46:35.520,0:46:39.920
filter so

0:46:36.800,0:46:40.560
the template wl convolution by hm okay

0:46:39.920,0:46:43.119
so this is

0:46:40.560,0:46:43.920
in the special domain uh the graph

0:46:43.119,0:46:45.440
domain

0:46:43.920,0:46:47.359
and then i'm going to do that and

0:46:45.440,0:46:48.880
remember that what i just defined

0:46:47.359,0:46:51.520
so doing this convolution in the

0:46:48.880,0:46:55.040
spectral domain it's just doing that

0:46:51.520,0:46:57.119
okay so this is the spectral filter

0:46:55.040,0:46:58.560
apply to the application and then you

0:46:57.119,0:47:02.319
multiply by hl

0:46:58.560,0:47:04.400
okay so this guy is is a

0:47:02.319,0:47:05.359
i can decompose this guy i will get the

0:47:04.400,0:47:08.000
fourier

0:47:05.359,0:47:09.280
matrix times the spectral function that

0:47:08.000,0:47:12.880
i apply to the

0:47:09.280,0:47:16.160
eigenvalues uh phi transpose hm

0:47:12.880,0:47:17.520
okay and and and this is my uh this is

0:47:16.160,0:47:20.480
my spectral filter

0:47:17.520,0:47:22.160
okay so i do not work directly here okay

0:47:20.480,0:47:25.119
i work directly here

0:47:22.160,0:47:25.520
and and here the thing that i'm going to

0:47:25.119,0:47:28.319
learn

0:47:25.520,0:47:29.200
i'm going to actually to learn this

0:47:28.319,0:47:32.319
function

0:47:29.200,0:47:34.720
w hat number one so i'm going to learn

0:47:32.319,0:47:36.640
um the spectral filter and i'm going to

0:47:34.720,0:47:40.079
learn it by back propagation

0:47:36.640,0:47:43.839
okay so i don't need to um

0:47:40.079,0:47:45.040
you know handcraft the the the spectral

0:47:43.839,0:47:47.040
filter i don't need to do that

0:47:45.040,0:47:48.319
this will be learned by by propagation

0:47:47.040,0:47:51.440
so that was really

0:47:48.319,0:47:52.720
a great idea to do it and this was the

0:47:51.440,0:47:54.800
first spectral technique

0:47:52.720,0:47:56.319
but but it has some limitations so the

0:47:54.800,0:47:58.480
first limitation is that you don't have

0:47:56.319,0:47:59.440
any guarantee of special localization of

0:47:58.480,0:48:02.400
filters

0:47:59.440,0:48:04.640
uh so remember that uh what we want we

0:48:02.400,0:48:07.040
want to have the local reception field

0:48:04.640,0:48:08.559
because it's it's a very good property

0:48:07.040,0:48:11.839
to be able to extract

0:48:08.559,0:48:13.520
uh you know multi-scale um multiscale

0:48:11.839,0:48:16.000
feature which is scale patterns

0:48:13.520,0:48:17.280
from from your signal so you don't have

0:48:16.000,0:48:19.119
you don't have this guarantee

0:48:17.280,0:48:21.040
the second thing is that how many

0:48:19.119,0:48:24.720
parameters do you need to learn

0:48:21.040,0:48:27.119
so you need to learn n parameters

0:48:24.720,0:48:28.000
okay you need to learn this w hat number

0:48:27.119,0:48:29.920
one to

0:48:28.000,0:48:31.200
do when you have number n so it's n

0:48:29.920,0:48:34.400
parameters so

0:48:31.200,0:48:37.440
if again if the graph is is large

0:48:34.400,0:48:38.319
like uh like like the the web you know

0:48:37.440,0:48:41.359
or facebook

0:48:38.319,0:48:43.280
then this is gonna be billions uh of

0:48:41.359,0:48:44.800
parameters to learn

0:48:43.280,0:48:46.720
and this is for each layer so it's gonna

0:48:44.800,0:48:48.079
be really huge and again the learning

0:48:46.720,0:48:51.280
complexity is going to be n square

0:48:48.079,0:48:54.960
because your phi is a dense matrix so

0:48:51.280,0:48:58.400
so we need to improve this okay so

0:48:54.960,0:48:59.920
so uh so jan and his collaborator so

0:48:58.400,0:49:02.319
they improve

0:48:59.920,0:49:03.520
the improved true properties so the

0:49:02.319,0:49:06.880
first property was

0:49:03.520,0:49:10.319
okay how do we get localized

0:49:06.880,0:49:13.520
spatial filters okay so for this

0:49:10.319,0:49:14.839
what what they propose is to um okay to

0:49:13.520,0:49:17.920
get

0:49:14.839,0:49:19.520
um localized special filter so you want

0:49:17.920,0:49:23.760
something which is localized

0:49:19.520,0:49:26.720
uh what you need to do is to uh compute

0:49:23.760,0:49:27.920
smooth spectral filters something very

0:49:26.720,0:49:30.240
smooth like this

0:49:27.920,0:49:32.000
okay so why do you why do you want

0:49:30.240,0:49:34.160
smooth spectral filter

0:49:32.000,0:49:35.359
it's because if you are smooth in the

0:49:34.160,0:49:37.680
frequency space

0:49:35.359,0:49:38.800
then you are going to be localized in

0:49:37.680,0:49:40.559
the space domain

0:49:38.800,0:49:42.880
okay so this is in physics you know the

0:49:40.559,0:49:44.319
eisenberg's entity principle

0:49:42.880,0:49:46.160
and you can see that you know with the

0:49:44.319,0:49:48.000
personal identity

0:49:46.160,0:49:50.079
if let's let's say that k is equal to

0:49:48.000,0:49:51.280
one if k is equal to one you have the

0:49:50.079,0:49:53.680
first derivative of

0:49:51.280,0:49:54.640
uh of the spectral function so if you

0:49:53.680,0:49:57.119
want this to be

0:49:54.640,0:49:58.800
small okay so you're going to have a

0:49:57.119,0:50:01.359
smooth function

0:49:58.800,0:50:03.200
and for k equal to one you see here is

0:50:01.359,0:50:06.240
this is going to be the variance

0:50:03.200,0:50:06.640
of your spatial filter so if this is

0:50:06.240,0:50:08.800
small

0:50:06.640,0:50:11.599
if the variance is small it means that

0:50:08.800,0:50:13.359
you're gonna have a small

0:50:11.599,0:50:15.440
you're going to have a special filter

0:50:13.359,0:50:19.280
with a small compact

0:50:15.440,0:50:21.040
support okay so if you are smooth

0:50:19.280,0:50:22.640
in the frequency space you're going to

0:50:21.040,0:50:24.720
be localized

0:50:22.640,0:50:26.000
in the spatial space okay so you need

0:50:24.720,0:50:28.480
smoothness how do you get

0:50:26.000,0:50:29.760
smoothness for spectral feature so you

0:50:28.480,0:50:31.920
can also think about the

0:50:29.760,0:50:33.040
uh the transform of the delta of the

0:50:31.920,0:50:35.119
dirac right so we

0:50:33.040,0:50:36.880
if we have a delta in the iraq in the in

0:50:35.119,0:50:37.599
the time domain then in the frequency

0:50:36.880,0:50:39.760
we're going to have

0:50:37.599,0:50:40.720
basically a flat a completely flat

0:50:39.760,0:50:43.119
transform right

0:50:40.720,0:50:44.079
so there's another maybe a way to see uh

0:50:43.119,0:50:46.800
if someone doesn't

0:50:44.079,0:50:47.359
quite know the parseval identity yeah

0:50:46.800,0:50:51.200
exactly

0:50:47.359,0:50:52.880
right um and so so how do you get a

0:50:51.200,0:50:55.839
smooth spectral filter

0:50:52.880,0:50:56.880
so the idea is okay we can simply

0:50:55.839,0:50:59.839
decompose

0:50:56.880,0:51:01.280
you know the spectral filter to be a

0:50:59.839,0:51:04.319
linear combination

0:51:01.280,0:51:07.200
of smooth kernels okay so

0:51:04.319,0:51:08.480
the smooth kernel was chosen to be

0:51:07.200,0:51:10.079
splines

0:51:08.480,0:51:12.319
because splines are nice they are you

0:51:10.079,0:51:14.400
know with compact support

0:51:12.319,0:51:16.559
and they are smooth and basically the

0:51:14.400,0:51:19.520
idea is okay now let's learn

0:51:16.559,0:51:20.880
a vector of k coefficient uh and this is

0:51:19.520,0:51:22.400
the case muscular node

0:51:20.880,0:51:25.280
okay and you learn this coefficient by

0:51:22.400,0:51:27.200
by propagation but suddenly you know

0:51:25.280,0:51:29.839
everything is nice because you have

0:51:27.200,0:51:31.359
locality localization in space

0:51:29.839,0:51:33.119
and the number of parameters that you're

0:51:31.359,0:51:35.680
going to learn is going to be

0:51:33.119,0:51:36.880
key parameters so here for example let's

0:51:35.680,0:51:39.920
say it's nine

0:51:36.880,0:51:40.319
okay remember that before uh in the case

0:51:39.920,0:51:42.400
of

0:51:40.319,0:51:44.240
um of convolution so you have a three by

0:51:42.400,0:51:45.599
three which is which is nine parameters

0:51:44.240,0:51:47.599
so that can be the same you can have

0:51:45.599,0:51:48.800
nine parameters to learn you're gonna

0:51:47.599,0:51:52.480
you're gonna learn

0:51:48.800,0:51:54.960
a combination of nine spline functions

0:51:52.480,0:51:57.599
and and that's it so you have a constant

0:51:54.960,0:52:00.000
number of parameters to learn earlier

0:51:57.599,0:52:00.640
so this is nice but we still have you

0:52:00.000,0:52:04.240
know

0:52:00.640,0:52:05.040
the the phi matrix so we the learning

0:52:04.240,0:52:08.800
complexity

0:52:05.040,0:52:08.800
is still quadratic okay

0:52:12.000,0:52:15.520
okay so so the question is um how do we

0:52:14.960,0:52:18.720
learn

0:52:15.520,0:52:22.400
in linear time okay so how do we learn

0:52:18.720,0:52:25.440
uh with respect to the to the graph size

0:52:22.400,0:52:26.559
n so the problem of the quadratic

0:52:25.440,0:52:28.640
complexity

0:52:26.559,0:52:30.319
comes from directly from the use of the

0:52:28.640,0:52:33.599
laplacian again vectors

0:52:30.319,0:52:34.800
okay so you see that uh the thing that

0:52:33.599,0:52:38.079
is that is annoying

0:52:34.800,0:52:39.440
uh in this spectral convolution is not

0:52:38.079,0:52:42.319
this diagonal matrix

0:52:39.440,0:52:44.160
it's not this vector it's this guy okay

0:52:42.319,0:52:45.920
this is this is the phi

0:52:44.160,0:52:47.280
matrix because it's a full matrix right

0:52:45.920,0:52:49.440
it's a dense matrix

0:52:47.280,0:52:51.040
and and then and then it's n square

0:52:49.440,0:52:52.319
number of elements so this is the price

0:52:51.040,0:52:55.359
that we need to pay

0:52:52.319,0:52:57.599
so we know that if we want to avoid

0:52:55.359,0:53:00.880
the quadratic complexity we need to

0:52:57.599,0:53:04.480
avoid the eigen decomposition okay

0:53:00.880,0:53:06.480
um and and okay so we can avoid

0:53:04.480,0:53:08.160
organic position but simply directly

0:53:06.480,0:53:10.000
learn function of the application

0:53:08.160,0:53:11.440
okay so this is what what we proposed in

0:53:10.000,0:53:15.119
2007

0:53:11.440,0:53:18.480
2016. so the spectral

0:53:15.119,0:53:20.480
function is just going to be you know a

0:53:18.480,0:53:23.280
monomial

0:53:20.480,0:53:24.640
function of the application that's it so

0:53:23.280,0:53:27.040
we just have a sum

0:53:24.640,0:53:28.160
of some parameters that we learned by by

0:53:27.040,0:53:31.599
propagation

0:53:28.160,0:53:35.200
w k and laplacian to the power of k

0:53:31.599,0:53:37.839
okay so so when we do that

0:53:35.200,0:53:39.119
uh first there is something uh which is

0:53:37.839,0:53:41.680
which is good is that

0:53:39.119,0:53:43.280
uh we're gonna have filters that are

0:53:41.680,0:53:46.160
exactly localized

0:53:43.280,0:53:48.160
in a k-hop support okay so if we have

0:53:46.160,0:53:51.200
the application to the power k

0:53:48.160,0:53:51.839
the spectral um i mean the spatial

0:53:51.200,0:53:54.079
filters

0:53:51.839,0:53:54.960
will be exactly localized in the support

0:53:54.079,0:53:57.280
of k-hop

0:53:54.960,0:53:58.000
so what is the what is the one hope

0:53:57.280,0:54:00.160
neighbor

0:53:58.000,0:54:01.119
neighborhood so let's say for example

0:54:00.160,0:54:03.200
you have this

0:54:01.119,0:54:05.280
graph and here i'm going to put a heat

0:54:03.200,0:54:08.800
source so the value is going to be 1

0:54:05.280,0:54:10.720
at this node and 0 for all other nodes

0:54:08.800,0:54:12.559
if i apply the application to this heat

0:54:10.720,0:54:14.480
source then the signal

0:54:12.559,0:54:16.000
the support of the signal is going to be

0:54:14.480,0:54:19.680
increased by one hop

0:54:16.000,0:54:22.079
so every uh basically every node

0:54:19.680,0:54:23.520
that can be reached by one jump okay

0:54:22.079,0:54:26.240
that you do that

0:54:23.520,0:54:26.800
and and if you do two jumps from this

0:54:26.240,0:54:30.400
you will

0:54:26.800,0:54:33.040
you will reach the the second hop

0:54:30.400,0:54:34.000
uh neighborhood which is your range uh

0:54:33.040,0:54:37.119
the orange

0:54:34.000,0:54:38.799
nodes here okay so if you apply the

0:54:37.119,0:54:40.640
application uh two times

0:54:38.799,0:54:42.160
this is gonna be the support okay if you

0:54:40.640,0:54:44.079
apply the application k times

0:54:42.160,0:54:45.520
then you will have the support of k-hops

0:54:44.079,0:54:48.640
so you you exactly

0:54:45.520,0:54:49.200
uh control the the size of your spatial

0:54:48.640,0:54:53.839
filters

0:54:49.200,0:54:53.839
okay so that that was the first point

0:54:54.640,0:54:58.079
the second point let me show you that

0:54:56.160,0:55:01.200
you get uh

0:54:58.079,0:55:04.400
learning complexity okay so so

0:55:01.200,0:55:05.760
again you have your convolution wh

0:55:04.400,0:55:08.079
you have your spectral convolution

0:55:05.760,0:55:12.079
definition i'm using here

0:55:08.079,0:55:14.240
as a spectral convolution um monomials

0:55:12.079,0:55:15.680
of the of the application and then i'm

0:55:14.240,0:55:18.799
going to replace this guy

0:55:15.680,0:55:19.280
so the laplacian power of k times the

0:55:18.799,0:55:22.319
vector

0:55:19.280,0:55:25.680
h by the vector x

0:55:22.319,0:55:27.200
k okay and x k is actually given by your

0:55:25.680,0:55:29.599
recursive equation

0:55:27.200,0:55:31.200
okay so recursive is always good right

0:55:29.599,0:55:32.000
so it's given by this recursive equation

0:55:31.200,0:55:35.520
which is

0:55:32.000,0:55:38.720
the application times the vector

0:55:35.520,0:55:42.240
xk minus one and the x k equal to zero

0:55:38.720,0:55:46.240
is simply the original uh function h

0:55:42.240,0:55:47.280
okay so so when i do that you see that

0:55:46.240,0:55:50.319
this sequence

0:55:47.280,0:55:51.839
x of k is generated by multiplying a

0:55:50.319,0:55:54.160
matrix

0:55:51.839,0:55:55.119
so the operation and the vector x k

0:55:54.160,0:55:57.440
minus 1.

0:55:55.119,0:55:59.119
so the complexity of doing that is the

0:55:57.440,0:56:01.520
number of edges

0:55:59.119,0:56:03.359
okay and you do it that you know k times

0:56:01.520,0:56:06.720
so number of edges

0:56:03.359,0:56:07.440
times k and the thing is uh for real

0:56:06.720,0:56:10.400
graph

0:56:07.440,0:56:11.599
real world graphs um basically they are

0:56:10.400,0:56:13.760
all sparse

0:56:11.599,0:56:16.000
okay because sparsity is structure so

0:56:13.760,0:56:19.280
remember for example for

0:56:16.000,0:56:22.640
um for for the web the web has a

0:56:19.280,0:56:23.520
billions of of web pages but for each

0:56:22.640,0:56:25.920
webpage

0:56:23.520,0:56:26.880
it is in average connected to 50 other

0:56:25.920,0:56:30.000
webpage so

0:56:26.880,0:56:32.000
comparing 50 to 1 billion is nothing

0:56:30.000,0:56:34.799
so usually and the same also for the

0:56:32.000,0:56:37.200
brain the brain it's very highly sparse

0:56:34.799,0:56:38.160
the same also for uh transport networks

0:56:37.200,0:56:40.880
so everything

0:56:38.160,0:56:42.839
every natural graph is usually sparse

0:56:40.880,0:56:45.680
because sparsity is structure

0:56:42.839,0:56:48.240
okay so so the number of edges

0:56:45.680,0:56:49.200
is you know some value times n so at the

0:56:48.240,0:56:52.400
end of the day

0:56:49.200,0:56:55.520
uh you have linear complexity uh because

0:56:52.400,0:56:58.799
for for sparse real world graphs okay

0:56:55.520,0:56:59.839
um okay so and you see here is that i'm

0:56:58.799,0:57:02.799
using the laplacian

0:56:59.839,0:57:04.160
and i never do any eigen decomposition

0:57:02.799,0:57:07.839
of the elaboration

0:57:04.160,0:57:08.160
okay um and there is so there is a bit

0:57:07.839,0:57:12.559
of

0:57:08.160,0:57:15.040
um of confusion that sometimes i see

0:57:12.559,0:57:16.319
is that so i call this spectral uh you

0:57:15.040,0:57:19.040
know gcn

0:57:16.319,0:57:20.079
but this is this might be misguided

0:57:19.040,0:57:23.040
because i don't do

0:57:20.079,0:57:24.640
any spectral uh operations like you know

0:57:23.040,0:57:26.640
i don't use any either again the

0:57:24.640,0:57:28.400
composition with the application

0:57:26.640,0:57:29.839
we i don't have any eigenvectors again

0:57:28.400,0:57:31.599
values so

0:57:29.839,0:57:33.440
so at the end of the day even if i use

0:57:31.599,0:57:36.720
you know the spectral theory

0:57:33.440,0:57:38.559
to define this gcn um

0:57:36.720,0:57:40.480
at the end of the day the computation

0:57:38.559,0:57:42.799
are all done in the special domain using

0:57:40.480,0:57:44.960
the operation

0:57:42.799,0:57:46.000
okay i don't use any i don't choose the

0:57:44.960,0:57:48.000
spectral domain

0:57:46.000,0:57:49.760
for the computation i use i do

0:57:48.000,0:57:53.119
everything in the special domain

0:57:49.760,0:57:55.599
so even we call that spectral gcn

0:57:53.119,0:57:56.319
we don't choose you know in practice and

0:57:55.599,0:57:59.280
the spectral

0:57:56.319,0:58:00.400
uh decomposition so just just one one

0:57:59.280,0:58:02.799
one command

0:58:00.400,0:58:04.079
okay and the last like the last comment

0:58:02.799,0:58:06.160
i want to do is that so graph

0:58:04.079,0:58:08.640
conditional layers again this is just

0:58:06.160,0:58:09.200
linear operations so you just multiply a

0:58:08.640,0:58:11.760
vector

0:58:09.200,0:58:13.520
a matrix by a vector so you're just

0:58:11.760,0:58:14.240
doing in operation so this is gpu

0:58:13.520,0:58:17.760
friendly

0:58:14.240,0:58:20.880
the issue um is that here you are doing

0:58:17.760,0:58:21.599
sparse linear algebra and the existing

0:58:20.880,0:58:23.760
gpu

0:58:21.599,0:58:24.880
are not optimized for that so this is i

0:58:23.760,0:58:26.400
think one of the

0:58:24.880,0:58:28.720
limitations today for graph neural

0:58:26.400,0:58:30.319
networks we need to have specialized

0:58:28.720,0:58:31.520
eyewear for graph neural networks we

0:58:30.319,0:58:35.200
need to have hardware that

0:58:31.520,0:58:37.200
adapt uh to the to the sparsity

0:58:35.200,0:58:38.559
uh of of these operations and we don't

0:58:37.200,0:58:41.760
have this today so

0:58:38.559,0:58:43.680
uh if we want this to to to get far

0:58:41.760,0:58:45.599
very far with graphene network we need

0:58:43.680,0:58:46.400
to have this this uh this specialized

0:58:45.599,0:58:48.799
hardware

0:58:46.400,0:58:50.160
what about tpus do you know whether tpus

0:58:48.799,0:58:51.440
can handle that's the same that's the

0:58:50.160,0:58:54.559
same they are optimized

0:58:51.440,0:58:55.440
for uh full uh you know linear

0:58:54.559,0:58:57.839
operations

0:58:55.440,0:58:59.119
like full matrices uh they're

0:58:57.839,0:59:01.119
specialized for that

0:58:59.119,0:59:02.160
but if you if you want to do sparse

0:59:01.119,0:59:04.799
linear algebra

0:59:02.160,0:59:05.359
you need specialized hardware to do that

0:59:04.799,0:59:08.160
gotcha

0:59:05.359,0:59:08.160
thanks yeah

0:59:08.640,0:59:13.839
okay so how do we implement um

0:59:12.000,0:59:16.720
how do we implement this and so for

0:59:13.839,0:59:19.280
example we have a signal

0:59:16.720,0:59:21.599
we have a function defined on the on the

0:59:19.280,0:59:23.359
graph so n is the number of vertices

0:59:21.599,0:59:24.640
of your graph and d is the

0:59:23.359,0:59:28.400
dimensionality

0:59:24.640,0:59:29.119
uh of uh of the feature right so for

0:59:28.400,0:59:32.160
each node

0:59:29.119,0:59:32.960
you have a feature a vector of d

0:59:32.160,0:59:36.480
dimension

0:59:32.960,0:59:37.920
okay so how we do that so we have x k

0:59:36.480,0:59:39.520
and what we do is that we are just going

0:59:37.920,0:59:42.640
to uh do a

0:59:39.520,0:59:42.960
shape stuff to do just linear operations

0:59:42.640,0:59:46.960
so

0:59:42.960,0:59:50.720
x k are going to be arranged in a matrix

0:59:46.960,0:59:51.280
you know um x bar which is of the size

0:59:50.720,0:59:54.400
of k

0:59:51.280,0:59:54.960
times nd okay so we just reshape you

0:59:54.400,0:59:58.000
know this

0:59:54.960,1:00:00.319
xk to be 1 times nd and

0:59:58.000,1:00:01.440
and we have k times nd and then we

1:00:00.319,1:00:04.160
multiply this

1:00:01.440,1:00:06.000
by the vector that we will learn by back

1:00:04.160,1:00:06.319
propagation which is of the size k by

1:00:06.000,1:00:08.559
one

1:00:06.319,1:00:10.720
okay we do that the operation will give

1:00:08.559,1:00:11.760
you one times nd you reshape and you get

1:00:10.720,1:00:14.000
n times d

1:00:11.760,1:00:15.119
so this is how how i implement it you

1:00:14.000,1:00:17.040
know

1:00:15.119,1:00:18.480
with pi torch or tensorflow that would

1:00:17.040,1:00:21.040
be the same

1:00:18.480,1:00:22.960
and this is how you do this spectral

1:00:21.040,1:00:25.520
convolution

1:00:22.960,1:00:27.520
so again the properties is that filters

1:00:25.520,1:00:29.680
are exactly localized

1:00:27.520,1:00:31.839
you have a constant number of parameters

1:00:29.680,1:00:33.760
to learn so this is a key

1:00:31.839,1:00:35.040
you know this is this is this k a

1:00:33.760,1:00:36.319
parameters that you need to learn by

1:00:35.040,1:00:38.720
back propagation

1:00:36.319,1:00:40.480
you have a learning complexity a linear

1:00:38.720,1:00:43.200
learning complexity

1:00:40.480,1:00:43.920
but the thing uh which uh which is not

1:00:43.200,1:00:47.040
good is that

1:00:43.920,1:00:49.599
um here i'm using monomial um basis

1:00:47.040,1:00:51.440
okay so i'm using uh laplacian to the

1:00:49.599,1:00:53.200
power zero laplacian to the power one

1:00:51.440,1:00:54.559
power two power three and so on okay

1:00:53.200,1:00:56.720
this is what i use here

1:00:54.559,1:00:57.920
and the thing is monomial bases are

1:00:56.720,1:01:00.880
unstable for

1:00:57.920,1:01:01.599
uh for for optimization because this

1:01:00.880,1:01:05.119
basis

1:01:01.599,1:01:07.520
you know is not too orthogonal

1:01:05.119,1:01:08.640
so if you change one coefficient then

1:01:07.520,1:01:09.920
you are going to

1:01:08.640,1:01:12.640
change the approximation of your

1:01:09.920,1:01:15.839
function so you need orthogonality

1:01:12.640,1:01:19.119
if you want to learn

1:01:15.839,1:01:21.760
with stability okay so

1:01:19.119,1:01:23.200
then you can use your favorite you know

1:01:21.760,1:01:25.520
orthonormal

1:01:23.200,1:01:26.240
basis but your favorite orthonormal

1:01:25.520,1:01:29.680
basis

1:01:26.240,1:01:31.359
must have a recursive equation okay so

1:01:29.680,1:01:33.119
this is the only thing that that that

1:01:31.359,1:01:35.440
you that you need you need

1:01:33.119,1:01:36.640
your autonomous basis to to have a

1:01:35.440,1:01:37.440
relationship equation because this is

1:01:36.640,1:01:40.559
the key

1:01:37.440,1:01:41.520
to have the linear complexity so we use

1:01:40.559,1:01:44.000
a chebychev

1:01:41.520,1:01:45.839
polynomials so this is uh something very

1:01:44.000,1:01:47.760
well known in signal processing

1:01:45.839,1:01:49.359
um so we are going to approximate you

1:01:47.760,1:01:52.799
know the spectral convolution

1:01:49.359,1:01:53.599
uh with the cheby uh jb chef chebyshev

1:01:52.799,1:01:56.240
function

1:01:53.599,1:01:57.440
the chef functions apply to h again can

1:01:56.240,1:02:00.240
be represented by

1:01:57.440,1:02:01.200
xk and xk is given by this recursive

1:02:00.240,1:02:03.119
equation

1:02:01.200,1:02:04.319
okay so it's a little more complex than

1:02:03.119,1:02:06.880
before but

1:02:04.319,1:02:08.079
in practice this is just doing again

1:02:06.880,1:02:11.119
multiplication of

1:02:08.079,1:02:11.760
uh your elaboration times the vec one

1:02:11.119,1:02:13.680
vector

1:02:11.760,1:02:15.200
okay at the end of the day the the

1:02:13.680,1:02:16.400
complexity is still linear you don't

1:02:15.200,1:02:20.079
change anything

1:02:16.400,1:02:23.760
um and this time you have stability

1:02:20.079,1:02:25.680
during your during the learning process

1:02:23.760,1:02:28.559
okay so what we did we did the sanity

1:02:25.680,1:02:31.359
check with mnist

1:02:28.559,1:02:31.920
um so and you see that so this is the

1:02:31.359,1:02:35.520
number

1:02:31.920,1:02:36.960
of vertices so so for mnist the the

1:02:35.520,1:02:39.920
graph is the standard grid

1:02:36.960,1:02:40.799
okay we use the k-nearest neighbor grid

1:02:39.920,1:02:42.960
to do that

1:02:40.799,1:02:45.119
and you see that um you have linear

1:02:42.960,1:02:48.319
complexity okay this is the

1:02:45.119,1:02:48.799
number of vertices and and and you have

1:02:48.319,1:02:50.960
this

1:02:48.799,1:02:52.480
number of of uh you have the linear

1:02:50.960,1:02:54.799
complexity so this is good

1:02:52.480,1:02:56.000
for the accuracy you get to see 99

1:02:54.799,1:02:59.599
percent of accuracy

1:02:56.000,1:03:02.880
compared to the standard n5

1:02:59.599,1:03:04.960
okay so chad net uh social

1:03:02.880,1:03:06.319
is basically combat for arbitrary graph

1:03:04.960,1:03:09.520
and we have the same linear

1:03:06.319,1:03:12.400
learning complexity of course um the

1:03:09.520,1:03:13.039
complexity constant is much larger than

1:03:12.400,1:03:16.160
than the

1:03:13.039,1:03:18.559
standard uh than the standard net

1:03:16.160,1:03:19.760
so it's something like 20 or 30. so it's

1:03:18.559,1:03:22.079
much much smaller

1:03:19.760,1:03:23.920
to learn on this but you get you know

1:03:22.079,1:03:26.799
covenant for any arbitrary graph

1:03:23.920,1:03:28.000
so that's that's uh that's what you mean

1:03:26.799,1:03:31.119
another limitation is

1:03:28.000,1:03:33.680
um it's an isotropic model

1:03:31.119,1:03:36.000
so so let me talk a little bit about i

1:03:33.680,1:03:37.520
uh isotropy versus anisotropy

1:03:36.000,1:03:40.160
so if you look at you know the standard

1:03:37.520,1:03:42.799
complex then you are going to produce

1:03:40.160,1:03:44.160
anisotropic filters like this one okay

1:03:42.799,1:03:44.799
so you see that this filter is an

1:03:44.160,1:03:48.000
isotropic

1:03:44.799,1:03:49.440
it goes in this direction okay and we

1:03:48.000,1:03:52.160
can get anisotropic

1:03:49.440,1:03:53.119
filters with standard coordinates

1:03:52.160,1:03:56.799
because we are using

1:03:53.119,1:03:59.839
a grid and on a grid we have you know

1:03:56.799,1:04:00.880
a directional we have directions we know

1:03:59.839,1:04:02.640
where is the up

1:04:00.880,1:04:04.400
where is down where is left where is

1:04:02.640,1:04:08.160
right right remember that

1:04:04.400,1:04:10.480
we know the ordering of uh of the nodes

1:04:08.160,1:04:11.760
on on the grid we know that but this is

1:04:10.480,1:04:14.160
different for graphs

1:04:11.760,1:04:15.280
we don't have any notion of direction we

1:04:14.160,1:04:17.359
we don't know where is the

1:04:15.280,1:04:18.319
where is up where is down where is left

1:04:17.359,1:04:20.640
or is right

1:04:18.319,1:04:21.920
so the thing the only thing that we can

1:04:20.640,1:04:24.559
do at this point

1:04:21.920,1:04:25.520
is that we can only compute isotropic

1:04:24.559,1:04:27.839
filters

1:04:25.520,1:04:28.799
isotropic filters means that the value

1:04:27.839,1:04:32.240
of the filter

1:04:28.799,1:04:33.280
will be the same you know for in in all

1:04:32.240,1:04:36.720
directions

1:04:33.280,1:04:39.680
uh for for for four cycles

1:04:36.720,1:04:40.000
okay for for cycles of the same radius

1:04:39.680,1:04:41.920
okay

1:04:40.000,1:04:43.039
so this is uh this is what we can get we

1:04:41.920,1:04:46.400
can only get

1:04:43.039,1:04:48.160
isotropic filters when we use a chip net

1:04:46.400,1:04:50.319
because we have no notion of direction

1:04:48.160,1:04:51.839
on arbitrary graphs

1:04:50.319,1:04:55.119
and i will come back to that i will come

1:04:51.839,1:04:57.760
back to the isotropy versus anisotropy

1:04:55.119,1:04:57.760
a bit later

1:04:59.839,1:05:05.599
okay so what we what we did also is to

1:05:03.440,1:05:08.640
uh very quickly i don't i don't have the

1:05:05.599,1:05:10.160
time now oh wow the time is uh so i need

1:05:08.640,1:05:13.119
to speed up a little bit so

1:05:10.160,1:05:14.720
we did we did expand also this um

1:05:13.119,1:05:17.039
spectral convolution from

1:05:14.720,1:05:18.640
one graph to multiple graphs so you can

1:05:17.039,1:05:20.880
do that you know it's like

1:05:18.640,1:05:22.799
extending from 1d signal processing to

1:05:20.880,1:05:25.039
2d image processing

1:05:22.799,1:05:26.480
so extension is mathematically

1:05:25.039,1:05:29.039
straightforward

1:05:26.480,1:05:30.720
to do and we did that you know for for

1:05:29.039,1:05:32.400
example for recommender systems

1:05:30.720,1:05:34.000
because we have users of movies and

1:05:32.400,1:05:36.720
users of graphs

1:05:34.000,1:05:38.480
so with that we also as i said before is

1:05:36.720,1:05:41.359
that you can use your favorite

1:05:38.480,1:05:42.240
uh you know orthogonal uh polynomial

1:05:41.359,1:05:45.760
basis

1:05:42.240,1:05:48.480
uh so we use uh kylie nets because uh

1:05:45.760,1:05:50.240
chebychev are unstable to localize

1:05:48.480,1:05:52.319
frequency bands of interest which are

1:05:50.240,1:05:55.599
basically the graph communities

1:05:52.319,1:05:58.640
we use that something a

1:05:55.599,1:06:00.480
more powerful you know more powerful

1:05:58.640,1:06:03.839
spectral functions

1:06:00.480,1:06:05.359
okay which is kind of neat okay so now

1:06:03.839,1:06:07.680
let me go to the

1:06:05.359,1:06:08.799
to the to this class of uh graph

1:06:07.680,1:06:11.520
continents that i call

1:06:08.799,1:06:14.000
special graphs and then for this class

1:06:11.520,1:06:16.640
i'm going back to the template matching

1:06:14.000,1:06:18.720
uh you know definition of convolution so

1:06:16.640,1:06:23.119
how we do clamping matching for graphs

1:06:18.720,1:06:25.280
so remember that um the main issue

1:06:23.119,1:06:27.440
um the main issue when you want to do

1:06:25.280,1:06:29.760
template matching for graph is that

1:06:27.440,1:06:30.640
you you don't have any node ordering or

1:06:29.760,1:06:33.760
positioning

1:06:30.640,1:06:35.839
for your template okay uh

1:06:33.760,1:06:37.599
we don't have any positioning so

1:06:35.839,1:06:38.400
basically with the only thing that we

1:06:37.599,1:06:41.599
have we have the

1:06:38.400,1:06:42.960
the index of the notes and that's it but

1:06:41.599,1:06:45.280
the index is not enough

1:06:42.960,1:06:46.480
to match you know information between

1:06:45.280,1:06:49.599
between nodes

1:06:46.480,1:06:52.640
um so how can we design

1:06:49.599,1:06:54.160
template matching to be invariant to

1:06:52.640,1:06:56.960
node reparametrization

1:06:54.160,1:06:57.599
okay so you have a graph this index of

1:06:56.960,1:07:01.039
the node

1:06:57.599,1:07:02.960
is maybe let's say six but

1:07:01.039,1:07:04.559
it's completely arbitrary i can have an

1:07:02.960,1:07:07.760
index with the number

1:07:04.559,1:07:08.079
122 for example so i want to be able to

1:07:07.760,1:07:11.119
do

1:07:08.079,1:07:11.680
template matching independently of the

1:07:11.119,1:07:14.880
index

1:07:11.680,1:07:15.680
of this node okay so how i do that so

1:07:14.880,1:07:18.880
the simplest

1:07:15.680,1:07:22.880
thing you can do is actually to

1:07:18.880,1:07:26.079
have only one uh you know template

1:07:22.880,1:07:30.640
vector to do the matching this is

1:07:26.079,1:07:32.240
so you don't have you know w j1 wg2 wj3

1:07:30.640,1:07:32.720
you don't have this you just have one

1:07:32.240,1:07:35.440
vector

1:07:32.720,1:07:36.240
w and you are doing the matching of this

1:07:35.440,1:07:39.359
vector

1:07:36.240,1:07:42.640
with all other you know feature

1:07:39.359,1:07:43.440
on your on your graph okay this is the

1:07:42.640,1:07:46.240
simplest

1:07:43.440,1:07:47.799
template feature matching you can do

1:07:46.240,1:07:49.280
which is environment by node

1:07:47.799,1:07:50.799
reparameterization

1:07:49.280,1:07:53.200
and actually this property is going to

1:07:50.799,1:07:54.799
be used for most graph neural networks

1:07:53.200,1:07:56.640
today

1:07:54.799,1:07:58.000
okay so here is the mathematical

1:07:56.640,1:08:01.039
definition

1:07:58.000,1:08:03.520
um so i'm just going to do the

1:08:01.039,1:08:04.079
the product between uh the template

1:08:03.520,1:08:07.680
vector

1:08:04.079,1:08:11.359
w at layer l so this is the d by one

1:08:07.680,1:08:13.119
and and i have the vector at node j

1:08:11.359,1:08:15.760
which is also the dimensionality d by

1:08:13.119,1:08:17.199
one okay i will get to scalar so here

1:08:15.760,1:08:18.480
this is only for one feature

1:08:17.199,1:08:20.319
of course you will have to get more

1:08:18.480,1:08:23.440
features so instead of having

1:08:20.319,1:08:24.640
a vector d by one you're going to use a

1:08:23.440,1:08:27.679
matrix

1:08:24.640,1:08:31.279
d by d so this way you can get you know

1:08:27.679,1:08:33.600
d features for each node i okay

1:08:31.279,1:08:35.520
and then i so this is the the

1:08:33.600,1:08:38.000
representation at

1:08:35.520,1:08:38.880
a node i i can put everything in vector

1:08:38.000,1:08:43.040
representation

1:08:38.880,1:08:46.239
okay this is my um this is my activation

1:08:43.040,1:08:49.120
at layer a plus one it is defined on

1:08:46.239,1:08:50.000
on the graph of m vertices and it has d

1:08:49.120,1:08:53.120
dimensions

1:08:50.000,1:08:56.239
okay and and this can be rewritten as

1:08:53.120,1:08:59.359
uh the adjacency matrix a so this is

1:08:56.239,1:09:00.000
n by n matrix this is my activation at

1:08:59.359,1:09:03.279
the layer

1:09:00.000,1:09:07.279
l so this is the n by d uh you know

1:09:03.279,1:09:08.400
matrix and this is the the template

1:09:07.279,1:09:10.480
that i'm going to learn by back

1:09:08.400,1:09:12.640
propagation of the size d by d

1:09:10.480,1:09:15.679
okay so you do you do this product you

1:09:12.640,1:09:15.679
get n by d

1:09:17.520,1:09:22.880
okay so based on this template matching

1:09:20.880,1:09:26.640
of graph now i'm going to define

1:09:22.880,1:09:30.400
uh two classes of uh spatial

1:09:26.640,1:09:33.440
gcn which are the isotropy gcn

1:09:30.400,1:09:35.120
and the anisotropy gcn so let's start

1:09:33.440,1:09:37.839
with the isotropic gcn

1:09:35.120,1:09:39.279
so this is act actually as a as quite

1:09:37.839,1:09:42.640
some history

1:09:39.279,1:09:44.640
okay so uh the simplest formulation of

1:09:42.640,1:09:47.920
special gcn was introduced by

1:09:44.640,1:09:49.759
uh scarcely and his co-author so he was

1:09:47.920,1:09:51.759
in 2009 before the deep learning

1:09:49.759,1:09:53.520
revolution

1:09:51.759,1:09:55.120
and then more recently by uh thomas

1:09:53.520,1:09:58.320
keith maxwelling

1:09:55.120,1:10:02.480
um and also saiyan suk bata

1:09:58.320,1:10:06.159
and archer slam and rob fergus in 2016.

1:10:02.480,1:10:08.080
so this is actually um this graph neural

1:10:06.159,1:10:11.120
networks of what they call the vennina

1:10:08.080,1:10:12.800
graph combination nets okay this is

1:10:11.120,1:10:14.480
exactly the same definition that i have

1:10:12.800,1:10:15.199
before just here i put you know the gb

1:10:14.480,1:10:17.040
matrix

1:10:15.199,1:10:18.800
in such a way that i have the mean value

1:10:17.040,1:10:20.560
okay i just do the mean value

1:10:18.800,1:10:22.400
over the neighborhood okay but this is

1:10:20.560,1:10:25.840
exactly the

1:10:22.400,1:10:29.040
the equation that i used before okay

1:10:25.840,1:10:32.560
um and you see that so

1:10:29.040,1:10:34.640
this equation is um it can handle

1:10:32.560,1:10:36.800
uh absence of no ordering so this is

1:10:34.640,1:10:40.080
completely invariant to another or

1:10:36.800,1:10:43.199
parametrization so again if this index

1:10:40.080,1:10:44.000
is maybe six and i change to be 122 is

1:10:43.199,1:10:47.760
not going to change

1:10:44.000,1:10:49.280
anything in the computation of the of h

1:10:47.760,1:10:51.760
at the next layer it's not going to

1:10:49.280,1:10:54.560
change anything you can also deal with

1:10:51.760,1:10:56.080
a neighborhood of different sizes okay

1:10:54.560,1:10:58.159
you don't care if you have

1:10:56.080,1:10:59.520
a neighborhood of four nodes or

1:10:58.159,1:11:00.800
neighborhood of 10 nodes it's not going

1:10:59.520,1:11:01.679
to change or one or not it's going to

1:11:00.800,1:11:03.840
change anything

1:11:01.679,1:11:05.679
you have the local reception field by

1:11:03.840,1:11:07.600
design with graph neural network you

1:11:05.679,1:11:09.920
just need to look at the neighbors

1:11:07.600,1:11:11.679
and and that's it so it's given to you

1:11:09.920,1:11:12.880
you have weight sharing

1:11:11.679,1:11:14.960
okay you have weight sharing it means

1:11:12.880,1:11:16.719
that for for

1:11:14.960,1:11:19.199
all features you are going to use the

1:11:16.719,1:11:19.840
same w whatever the position on the

1:11:19.199,1:11:22.960
graph

1:11:19.840,1:11:25.199
okay so this is a convolution property

1:11:22.960,1:11:26.159
um this formulation is also independent

1:11:25.199,1:11:28.800
of the graph size

1:11:26.159,1:11:30.560
because all operations are done locally

1:11:28.800,1:11:31.600
okay you just use you know local

1:11:30.560,1:11:33.440
information

1:11:31.600,1:11:35.679
for the next from the next layer so you

1:11:33.440,1:11:36.320
can have a graph of 10 knots or you can

1:11:35.679,1:11:39.120
have a graph

1:11:36.320,1:11:40.800
a graph of 10 billion nodes it doesn't

1:11:39.120,1:11:42.159
care so you can do also everything in

1:11:40.800,1:11:44.159
parallel

1:11:42.159,1:11:46.159
and but this is limited to isotropic

1:11:44.159,1:11:48.880
capability so the w

1:11:46.159,1:11:50.480
is the same for all neighbors so it's

1:11:48.880,1:11:52.320
again it's an isotropic

1:11:50.480,1:11:54.480
model it's going to give the same value

1:11:52.320,1:11:54.960
for all neighbors okay but at the end of

1:11:54.480,1:11:57.360
the day

1:11:54.960,1:11:58.719
this model can be represented by this

1:11:57.360,1:12:01.840
figure

1:11:58.719,1:12:04.159
so um this uh

1:12:01.840,1:12:05.280
so the activation at the next layer is

1:12:04.159,1:12:08.000
basically a function

1:12:05.280,1:12:08.400
of the activation at the current layer

1:12:08.000,1:12:12.080
at

1:12:08.400,1:12:15.199
uh index at at the node i

1:12:12.080,1:12:16.719
and the neighborhood of the node i okay

1:12:15.199,1:12:18.320
and the only thing that we're going to

1:12:16.719,1:12:19.920
do basically uh

1:12:18.320,1:12:21.520
is to change the function the

1:12:19.920,1:12:23.120
instantiation of

1:12:21.520,1:12:25.360
of the function and then you will get

1:12:23.120,1:12:28.159
all family of graph

1:12:25.360,1:12:29.679
neural network by just you know deciding

1:12:28.159,1:12:32.080
a different function here but

1:12:29.679,1:12:32.800
everything is based on this equation so

1:12:32.080,1:12:36.000
again

1:12:32.800,1:12:37.760
you have your your your core

1:12:36.000,1:12:39.040
node and then you have your neighborhood

1:12:37.760,1:12:42.560
to decide what will be

1:12:39.040,1:12:42.560
the activation at the next layer

1:12:43.840,1:12:47.280
okay so i'm running out of time so i'm

1:12:45.760,1:12:47.920
not going to take too much time on this

1:12:47.280,1:12:50.320
but

1:12:47.920,1:12:51.440
what you can show is that this previous

1:12:50.320,1:12:53.679
vernier gcn

1:12:51.440,1:12:55.360
i just showed you is actually a

1:12:53.679,1:12:57.760
simplification of chebnet

1:12:55.360,1:13:00.080
so if you truncate the expansion of

1:12:57.760,1:13:03.440
chain by using the first two

1:13:00.080,1:13:06.159
chef function that at the end you end up

1:13:03.440,1:13:08.960
with the same equation so so this is

1:13:06.159,1:13:12.159
this is the relationship

1:13:08.960,1:13:15.199
uh okay so one interesting

1:13:12.159,1:13:18.239
uh gcn is graph sage

1:13:15.199,1:13:21.440
that was introduced by william in turn

1:13:18.239,1:13:23.360
lee and yuri leskovek so let's say for

1:13:21.440,1:13:25.199
let's let's go back to the vanilla gcn

1:13:23.360,1:13:26.480
so and let's suppose that the adjacency

1:13:25.199,1:13:29.520
matrix

1:13:26.480,1:13:31.679
has a value one for for for the edges

1:13:29.520,1:13:34.080
okay so i have this equation

1:13:31.679,1:13:34.960
so the thing is for this equation i'm

1:13:34.080,1:13:38.480
going to

1:13:34.960,1:13:40.880
treat you know the central vertex i

1:13:38.480,1:13:42.239
and the neighborhood with the same

1:13:40.880,1:13:44.480
template weight

1:13:42.239,1:13:46.800
okay but i can differentiate that you

1:13:44.480,1:13:49.600
know i can have a template

1:13:46.800,1:13:50.400
for the central node w1 and i can have a

1:13:49.600,1:13:52.800
template

1:13:50.400,1:13:53.520
for the one hope neighborhood okay by

1:13:52.800,1:13:55.920
doing that

1:13:53.520,1:13:57.760
you improve already a lot you know your

1:13:55.920,1:14:00.080
the performance of your official graph

1:13:57.760,1:14:02.640
neural networks

1:14:00.080,1:14:03.280
so you go from here to here so you have

1:14:02.640,1:14:06.320
again

1:14:03.280,1:14:07.760
um some templates for the central node

1:14:06.320,1:14:08.480
and the template for for the

1:14:07.760,1:14:11.600
neighborhood

1:14:08.480,1:14:13.760
okay but this is still an isotropic uh

1:14:11.600,1:14:15.440
isotopic gcn okay because you are

1:14:13.760,1:14:16.400
treating all the neighbors with the same

1:14:15.440,1:14:18.159
weight

1:14:16.400,1:14:20.080
uh here this is the mean but you can

1:14:18.159,1:14:21.120
change you can take the sum you can also

1:14:20.080,1:14:22.400
take the max

1:14:21.120,1:14:25.520
you can take also something more

1:14:22.400,1:14:28.560
elaborated like lstm

1:14:25.520,1:14:29.440
okay now more recently people um try to

1:14:28.560,1:14:32.800
improve

1:14:29.440,1:14:36.080
the uh the theoretical understanding

1:14:32.800,1:14:39.360
of uh of gcn so there was um the

1:14:36.080,1:14:44.960
graph isomorphous i i think isomorphism

1:14:39.360,1:14:44.960
networks are introduced by yuri leskovec

1:14:45.440,1:14:49.600
in 2018 so the idea is can we design an

1:14:48.080,1:14:50.880
architecture that can differentiate

1:14:49.600,1:14:54.400
graphs that are not

1:14:50.880,1:14:56.960
isomorphic so you know isomorphic

1:14:54.400,1:14:58.400
is basically a measure of equivalence

1:14:56.960,1:15:00.480
between between graphs so these two

1:14:58.400,1:15:02.000
graphs are isomorphic to each other

1:15:00.480,1:15:04.400
and of course you want to treat them the

1:15:02.000,1:15:06.320
same way but if you are not isomorphic

1:15:04.400,1:15:08.080
you want especially to treat them in a

1:15:06.320,1:15:10.400
different way okay

1:15:08.080,1:15:11.280
so so there was um a graph neural

1:15:10.400,1:15:13.120
network uh

1:15:11.280,1:15:15.360
based on this one on this on this

1:15:13.120,1:15:16.239
definition but this is still an

1:15:15.360,1:15:19.840
isotropic

1:15:16.239,1:15:21.840
gcn okay

1:15:19.840,1:15:23.520
so so now i'm going to talk about

1:15:21.840,1:15:27.280
anisotropic gcn so again

1:15:23.520,1:15:28.800
um again so i go back to what i said

1:15:27.280,1:15:29.360
before is that standard cornet can

1:15:28.800,1:15:32.480
produce

1:15:29.360,1:15:34.800
uh isotropic filters because there is a

1:15:32.480,1:15:36.400
notion of directions on grids okay so

1:15:34.800,1:15:38.880
you have this um

1:15:36.400,1:15:42.159
isotropic filter in this direction uh

1:15:38.880,1:15:43.600
gcn like a chemnet kylie net veneer gcn

1:15:42.159,1:15:45.920
graph sage and gene

1:15:43.600,1:15:47.280
they compute isotropic filters so you

1:15:45.920,1:15:49.520
have this kind of

1:15:47.280,1:15:50.840
filters that you learn during the

1:15:49.520,1:15:53.360
process but they are they are

1:15:50.840,1:15:54.159
isotropic but we know that isotropy is

1:15:53.360,1:15:56.560
very powerful

1:15:54.159,1:15:58.640
right so um how do we get back

1:15:56.560,1:16:00.320
anesthotropine graphene our networks

1:15:58.640,1:16:01.840
so you can you can get an isotropic

1:16:00.320,1:16:05.040
naturally for example

1:16:01.840,1:16:06.800
if you have edge features um

1:16:05.040,1:16:08.640
for like if you're taking chemistry

1:16:06.800,1:16:10.719
molecules you know that

1:16:08.640,1:16:12.800
the bond features can be different they

1:16:10.719,1:16:13.360
can be you know single double aromatic

1:16:12.800,1:16:15.840
bonds

1:16:13.360,1:16:16.719
so naturally you would get an isotropic

1:16:15.840,1:16:20.800
you know

1:16:16.719,1:16:23.520
gcm and again if we want to

1:16:20.800,1:16:25.520
design um a mechanism for isotropy we

1:16:23.520,1:16:27.760
want this mechanism to be independent

1:16:25.520,1:16:28.880
with respect to the node parametrization

1:16:27.760,1:16:31.920
so to do that

1:16:28.880,1:16:34.320
we can use for example h degrees and

1:16:31.920,1:16:35.280
so that was proposed by monet edge gate

1:16:34.320,1:16:38.159
that we propose

1:16:35.280,1:16:38.480
um in getty gcn or attention mechanism

1:16:38.159,1:16:41.120
uh

1:16:38.480,1:16:42.480
in get and the idea is what what i put

1:16:41.120,1:16:45.440
here as an illustration

1:16:42.480,1:16:46.480
okay so um here you're going to treat

1:16:45.440,1:16:49.280
your neighbors

1:16:46.480,1:16:50.560
in the same way okay so with the same

1:16:49.280,1:16:52.480
template

1:16:50.560,1:16:54.000
but you want you want to treat your

1:16:52.480,1:16:55.760
neighbors in a different way right

1:16:54.000,1:16:57.920
if this is j1 you want a different

1:16:55.760,1:16:59.840
weight than if it was for j2

1:16:57.920,1:17:01.040
what do you want that is for example if

1:16:59.840,1:17:02.560
you want to analyze

1:17:01.040,1:17:04.080
graphs you know that you have

1:17:02.560,1:17:07.040
communities of people

1:17:04.080,1:17:08.560
um which are different for example i

1:17:07.040,1:17:10.320
don't know if it is politics you have

1:17:08.560,1:17:12.640
republicans and democrats

1:17:10.320,1:17:14.640
so you don't want to know to have the

1:17:12.640,1:17:15.280
the same analysis for for the same group

1:17:14.640,1:17:17.120
of people

1:17:15.280,1:17:20.159
so you want anisotropy for graph that

1:17:17.120,1:17:20.159
that's quite important

1:17:20.560,1:17:25.040
okay so the first model uh who deal with

1:17:23.600,1:17:26.880
endoscopy was monette so he was

1:17:25.040,1:17:29.280
introduced by federico monty

1:17:26.880,1:17:30.400
michael bronstein and their co-author

1:17:29.280,1:17:33.040
and the idea was to do

1:17:30.400,1:17:34.400
was to use gmm so gaussian mixture model

1:17:33.040,1:17:35.360
and to learn the parameters of the

1:17:34.400,1:17:38.000
gaussian mixture so

1:17:35.360,1:17:39.679
here they have k washington model and

1:17:38.000,1:17:40.239
they learn the parameters by using the

1:17:39.679,1:17:43.520
degree

1:17:40.239,1:17:46.880
of uh of the graph

1:17:43.520,1:17:47.760
um then there is also gat so uh it was

1:17:46.880,1:17:50.560
developed by

1:17:47.760,1:17:51.679
peter velikovic and uh joshua banjo and

1:17:50.560,1:17:53.440
their co-author

1:17:51.679,1:17:54.719
was basically to use the attention

1:17:53.440,1:17:58.320
mechanism developed

1:17:54.719,1:17:59.760
by jimmy badeno yeshua banjo

1:17:58.320,1:18:01.520
to introduce anisotropy in the

1:17:59.760,1:18:04.080
neighborhood allegation function

1:18:01.520,1:18:04.560
okay and so this is this is what you see

1:18:04.080,1:18:06.239
here

1:18:04.560,1:18:07.760
so you have um you're going to

1:18:06.239,1:18:10.960
concatenate so it's a multi-head

1:18:07.760,1:18:12.320
um wiki and architecture and here you

1:18:10.960,1:18:14.560
have uh

1:18:12.320,1:18:15.440
this weight which are basically the soft

1:18:14.560,1:18:17.679
max

1:18:15.440,1:18:19.199
um on the neighborhood okay you do the

1:18:17.679,1:18:22.480
soft max on the neighborhood so

1:18:19.199,1:18:23.760
some info some um some notes will be

1:18:22.480,1:18:27.040
more important than the others

1:18:23.760,1:18:30.719
you know by given by soft softmax

1:18:27.040,1:18:34.000
what we use um with um thomas laurent um

1:18:30.719,1:18:37.120
and me in 2017 we we use a simple uh

1:18:34.000,1:18:37.440
age uh getting mechanism which is which

1:18:37.120,1:18:39.520
is

1:18:37.440,1:18:41.040
which is you know what a sort of soft

1:18:39.520,1:18:42.159
attention process

1:18:41.040,1:18:45.679
compared to the sparse attention

1:18:42.159,1:18:48.000
mechanism of your show avenger

1:18:45.679,1:18:49.520
and and here what we did also we use

1:18:48.000,1:18:51.679
edge feature explicitly

1:18:49.520,1:18:53.040
and this actually recently we discovered

1:18:51.679,1:18:54.719
that this is very important for edge

1:18:53.040,1:18:55.920
prediction task if you have explicit

1:18:54.719,1:18:59.199
explanation task

1:18:55.920,1:19:02.400
this is important to keep it okay

1:18:59.199,1:19:05.520
so so this is the model that we used

1:19:02.400,1:19:07.920
uh okay um

1:19:05.520,1:19:09.440
then okay so if i take transformer and

1:19:07.920,1:19:11.360
if i if i

1:19:09.440,1:19:12.640
write down the equation of the graph

1:19:11.360,1:19:15.679
version of transformer this is

1:19:12.640,1:19:16.560
that i would get okay so you recognize

1:19:15.679,1:19:18.800
here the value

1:19:16.560,1:19:20.960
here you have the query here you have

1:19:18.800,1:19:22.320
the key and here you have the softmax

1:19:20.960,1:19:24.080
but the softmax is done in the

1:19:22.320,1:19:26.320
neighborhood in one hot neighborhood

1:19:24.080,1:19:28.719
okay that would be this and here i'm

1:19:26.320,1:19:32.080
going to make um

1:19:28.719,1:19:34.840
um a connection with a transformer of

1:19:32.080,1:19:36.000
uh vesmani and and their and his

1:19:34.840,1:19:39.040
collaborators

1:19:36.000,1:19:39.920
so what is a transformer so a standard

1:19:39.040,1:19:42.480
transformer

1:19:39.920,1:19:43.840
is actually a special case of uh graph

1:19:42.480,1:19:46.320
conventional nets

1:19:43.840,1:19:47.040
when the graph is fully connected okay

1:19:46.320,1:19:49.120
so

1:19:47.040,1:19:51.520
this is a fully connected graph so you

1:19:49.120,1:19:53.760
take any node

1:19:51.520,1:19:55.840
i and this node is going to be connected

1:19:53.760,1:19:58.960
to all other nodes in your graph

1:19:55.840,1:20:00.560
and included itself okay so

1:19:58.960,1:20:02.000
if you look at this equation the

1:20:00.560,1:20:05.199
equation i just wrote before

1:20:02.000,1:20:07.600
you know if the the neighborhood

1:20:05.199,1:20:10.080
uh is this time not the one hop

1:20:07.600,1:20:12.000
neighborhood but the whole graph

1:20:10.080,1:20:13.440
then you will get you know uh the

1:20:12.000,1:20:16.320
standard equation that

1:20:13.440,1:20:17.360
if if you do nlp and and transformer you

1:20:16.320,1:20:20.480
will recognize

1:20:17.360,1:20:22.560
directly okay we saw this last week so

1:20:20.480,1:20:24.320
just exactly so that's that's a nice

1:20:22.560,1:20:26.400
connect and that's a

1:20:24.320,1:20:28.159
transition so so you see you have the

1:20:26.400,1:20:29.040
concatenation so this is multi-head you

1:20:28.159,1:20:30.960
have the softmax

1:20:29.040,1:20:32.159
the query the key and the value and then

1:20:30.960,1:20:34.960
you have the weight

1:20:32.159,1:20:36.719
uh for the meeting head so and the only

1:20:34.960,1:20:38.000
thing that i do here mathematically just

1:20:36.719,1:20:41.760
having the neighborhood

1:20:38.000,1:20:43.040
uh that that use you know all connection

1:20:41.760,1:20:45.440
and when i do that so there is the

1:20:43.040,1:20:48.719
question so what does it mean to do

1:20:45.440,1:20:51.120
you know graph convolutional nets

1:20:48.719,1:20:51.760
for fully connected graphs and i think

1:20:51.120,1:20:53.679
in this case

1:20:51.760,1:20:55.840
it becomes less useful to talk about

1:20:53.679,1:20:58.480
graphs because when you when you have

1:20:55.840,1:21:00.000
each data connected to all other data

1:20:58.480,1:21:01.679
then you don't have any more you know

1:21:00.000,1:21:02.960
specific graph structure

1:21:01.679,1:21:04.320
because the graph which is what is

1:21:02.960,1:21:05.679
really interesting is graph is the

1:21:04.320,1:21:08.320
sparsity structure

1:21:05.679,1:21:09.120
right like the brain connectivity like

1:21:08.320,1:21:10.960
uh you know

1:21:09.120,1:21:12.239
the social networks what is interesting

1:21:10.960,1:21:12.880
is not everything to be connected to

1:21:12.239,1:21:15.120
each other

1:21:12.880,1:21:17.040
it's only to have a sparse connection

1:21:15.120,1:21:18.320
between between between the nodes

1:21:17.040,1:21:20.080
so i think in this case it would be

1:21:18.320,1:21:22.239
better to talk about sets

1:21:20.080,1:21:23.920
than to talk about graphs and and we

1:21:22.239,1:21:24.560
know that we know that transformers are

1:21:23.920,1:21:28.000
set

1:21:24.560,1:21:29.679
neural networks so so in some sense

1:21:28.000,1:21:31.280
instead of you know looking at you know

1:21:29.679,1:21:33.600
a fully connected graph

1:21:31.280,1:21:34.960
with with uh with feature the thing that

1:21:33.600,1:21:35.520
we should look at is more a set of

1:21:34.960,1:21:37.280
features

1:21:35.520,1:21:39.040
and transformers are really good you

1:21:37.280,1:21:43.440
know to process uh sets

1:21:39.040,1:21:46.560
of uh of of feature vectors

1:21:43.440,1:21:50.159
um okay so so there is a lab um

1:21:46.560,1:21:53.120
that i i uh i put here

1:21:50.159,1:21:54.159
so um the lab is based on um so this is

1:21:53.120,1:21:56.880
the gate gcn so

1:21:54.159,1:21:58.560
the model i'm i i propose and this is

1:21:56.880,1:22:00.560
with ggl so this is the deep graph

1:21:58.560,1:22:03.920
library so it was developed by nyu

1:22:00.560,1:22:05.679
shanghai by professor zanzen and uh

1:22:03.920,1:22:07.840
and and here this is the link to the lab

1:22:05.679,1:22:09.440
so if you click on this link you will go

1:22:07.840,1:22:12.320
directly

1:22:09.440,1:22:14.000
to the lab uh and this is this using

1:22:12.320,1:22:15.520
google google collab so you will just

1:22:14.000,1:22:16.800
need a you know a gmail account to

1:22:15.520,1:22:17.440
access to this and you will be able to

1:22:16.800,1:22:20.400
run it

1:22:17.440,1:22:22.159
on on the google cloud and what i put

1:22:20.400,1:22:26.320
here i put really you know

1:22:22.159,1:22:28.000
the the only the the most interesting uh

1:22:26.320,1:22:32.080
functions that you need to develop

1:22:28.000,1:22:33.760
uh a gcm so uh so so maybe tomorrow uh

1:22:32.080,1:22:35.679
yeah tomorrow tomorrow we're gonna be

1:22:33.760,1:22:38.800
going over everything okay perfect

1:22:35.679,1:22:41.120
okay okay so and and here

1:22:38.800,1:22:42.159
uh i again you know i put some comments

1:22:41.120,1:22:45.760
on the code

1:22:42.159,1:22:47.920
yeah and also yeah also understand dgl

1:22:45.760,1:22:49.280
uh how how dga works so probably you do

1:22:47.920,1:22:52.320
that tomorrow yes yes

1:22:49.280,1:22:53.920
nice okay so let me

1:22:52.320,1:22:56.239
now i'm going to the answer let me talk

1:22:53.920,1:22:59.440
a little bit about um benchmarking

1:22:56.239,1:23:02.560
graph neural networks so recently we

1:22:59.440,1:23:04.800
um we have this paper of benchmarking

1:23:02.560,1:23:05.840
networks so why we did this matchmark

1:23:04.800,1:23:07.440
because

1:23:05.840,1:23:08.880
if you look at you know most published

1:23:07.440,1:23:12.159
um

1:23:08.880,1:23:14.320
gcn papers um most of the work actually

1:23:12.159,1:23:16.560
use small data set like a quora

1:23:14.320,1:23:17.840
rtu data set and only one task the

1:23:16.560,1:23:19.360
classification

1:23:17.840,1:23:21.600
and when i you know when i started doing

1:23:19.360,1:23:22.080
some experiment on that i just realized

1:23:21.600,1:23:24.320
that

1:23:22.080,1:23:26.159
if you use gcn or if you don't use an

1:23:24.320,1:23:26.880
egcn you will get statistically the same

1:23:26.159,1:23:28.719
performance

1:23:26.880,1:23:30.880
because the standard deviation is very

1:23:28.719,1:23:34.000
high for the small data sets

1:23:30.880,1:23:37.280
so so the thing is um we

1:23:34.000,1:23:39.360
we cannot identify you know good uh gcm

1:23:37.280,1:23:40.400
uh we need we need we need something

1:23:39.360,1:23:42.320
else

1:23:40.400,1:23:43.679
and also recently so there has been you

1:23:42.320,1:23:46.719
know a new

1:23:43.679,1:23:48.480
theoretical development for for gcm

1:23:46.719,1:23:50.320
and the question is you know how good

1:23:48.480,1:23:52.239
they are in practice

1:23:50.320,1:23:54.800
it's important to to have some good

1:23:52.239,1:23:56.960
mathematical justification but

1:23:54.800,1:23:59.040
you know we need to be able to prove

1:23:56.960,1:24:01.840
that this is something that is useful

1:23:59.040,1:24:03.600
and and i think also benchmark i think

1:24:01.840,1:24:06.800
very essential to make progress

1:24:03.600,1:24:08.639
uh in many fields um like you know of

1:24:06.800,1:24:11.679
course deep learning with imagenet

1:24:08.639,1:24:12.560
by faith lee um but the thing is what i

1:24:11.679,1:24:14.239
upsell is actually

1:24:12.560,1:24:16.320
the people are quite really time to give

1:24:14.239,1:24:18.800
credit to benchmarks

1:24:16.320,1:24:19.440
um anyway so we introduced this open

1:24:18.800,1:24:22.159
benchmark

1:24:19.440,1:24:23.679
infrastructure so it's it's on github

1:24:22.159,1:24:26.239
and it's based on pi torch

1:24:23.679,1:24:27.840
and dgl and we introduced you know six

1:24:26.239,1:24:29.760
new medium scale data sets

1:24:27.840,1:24:31.760
for the four fundamental graph problems

1:24:29.760,1:24:34.000
like you know graph classification

1:24:31.760,1:24:35.600
graph regression not classification and

1:24:34.000,1:24:37.199
edge classification which i think

1:24:35.600,1:24:39.600
if you cover these four fundamental

1:24:37.199,1:24:41.440
problems you you already

1:24:39.600,1:24:43.040
know quite a lot about the performance

1:24:41.440,1:24:44.480
of your of your gcn

1:24:43.040,1:24:46.400
can you spend a few words more about

1:24:44.480,1:24:48.080
these four fundamental graph problems i

1:24:46.400,1:24:51.600
think we haven't mentioned them

1:24:48.080,1:24:54.239
so far i think yeah um exactly

1:24:51.600,1:24:56.400
but but well so what i mentioned is

1:24:54.239,1:24:57.360
basically the first part of any you know

1:24:56.400,1:25:00.320
convolutional nets

1:24:57.360,1:25:01.679
is how do you extract a powerful feature

1:25:00.320,1:25:04.639
the rest is quite easy

1:25:01.679,1:25:04.960
no if you want to do uh regression you

1:25:04.639,1:25:06.800
just

1:25:04.960,1:25:08.639
use an mlp if you want to do

1:25:06.800,1:25:09.760
classification you should use mlp with

1:25:08.639,1:25:12.159
cross entropy

1:25:09.760,1:25:13.600
and the thing i think is is i mean i can

1:25:12.159,1:25:15.840
take more time to do that but

1:25:13.600,1:25:17.120
what i present is i think is more uh you

1:25:15.840,1:25:19.199
know uh

1:25:17.120,1:25:20.639
is more interesting than doing just

1:25:19.199,1:25:23.040
these these guys

1:25:20.639,1:25:24.080
but would you give me another hour we

1:25:23.040,1:25:26.320
could do that

1:25:24.080,1:25:27.920
i was making the point that i i think i

1:25:26.320,1:25:29.840
understand now how we can build a

1:25:27.920,1:25:31.840
basically a representation of a graph

1:25:29.840,1:25:33.600
but then so you would have like this uh

1:25:31.840,1:25:35.679
basically uh feature

1:25:33.600,1:25:37.600
per node but then how would you go from

1:25:35.679,1:25:39.600
this feature per node to the final

1:25:37.600,1:25:41.199
task so maybe we can mention this such

1:25:39.600,1:25:44.639
that we can give someone sure

1:25:41.199,1:25:46.960
so so what you do basically um

1:25:44.639,1:25:48.320
so for example if uh so you have feature

1:25:46.960,1:25:49.679
exactly so you extract convolutional

1:25:48.320,1:25:51.120
features per node

1:25:49.679,1:25:53.040
and and then suddenly if you want to do

1:25:51.120,1:25:54.560
for example graph classification

1:25:53.040,1:25:56.320
so what you will do you will do some

1:25:54.560,1:25:58.800
kind of aggregation

1:25:56.320,1:25:59.840
you know function on this feature node

1:25:58.800,1:26:02.480
so for example

1:25:59.840,1:26:04.320
the most common one is to do the average

1:26:02.480,1:26:06.239
so you do the average of all

1:26:04.320,1:26:08.560
feature nodes and then on the top of

1:26:06.239,1:26:09.440
that you use an mlp and then you will do

1:26:08.560,1:26:11.920
classification

1:26:09.440,1:26:14.080
uh of of your graph and this would be

1:26:11.920,1:26:15.760
for always the same kind of structure of

1:26:14.080,1:26:17.600
the graph or you have different

1:26:15.760,1:26:19.440
structures like different numbers of

1:26:17.600,1:26:22.080
nodes so is it like

1:26:19.440,1:26:23.760
would you use if you use the mean it's

1:26:22.080,1:26:24.239
completely independent of the number of

1:26:23.760,1:26:26.000
nodes

1:26:24.239,1:26:27.520
right right right the same so if you do

1:26:26.000,1:26:29.040
the sum if you do the max

1:26:27.520,1:26:31.570
so you have many operators which are

1:26:29.040,1:26:34.560
independent of the number of nodes

1:26:31.570,1:26:37.520
[Music]

1:26:34.560,1:26:39.040
and yeah so so we have this and um and

1:26:37.520,1:26:40.719
this medium says uh

1:26:39.040,1:26:42.719
this medium size are actually enough you

1:26:40.719,1:26:46.080
know statistically separate

1:26:42.719,1:26:47.840
the performance of graph neural networks

1:26:46.080,1:26:49.760
so we make easy you know for new users

1:26:47.840,1:26:53.440
to add a new

1:26:49.760,1:26:54.080
new graph graph models and also new data

1:26:53.440,1:26:55.679
sets

1:26:54.080,1:26:57.360
and this is this is the link to the to

1:26:55.679,1:26:59.760
the repo so

1:26:57.360,1:27:01.520
let me now explain the graph neural

1:26:59.760,1:27:03.120
network pipeline

1:27:01.520,1:27:05.040
so a standard graph neural network

1:27:03.120,1:27:06.960
pipeline is composed of three layers

1:27:05.040,1:27:08.800
so the first layer is going to be an

1:27:06.960,1:27:09.679
input layer and it's going to make an

1:27:08.800,1:27:12.880
embedding

1:27:09.679,1:27:14.080
of the input node and edge features then

1:27:12.880,1:27:17.199
you will have

1:27:14.080,1:27:18.000
a series of kraftner network layers and

1:27:17.199,1:27:19.520
finally you have

1:27:18.000,1:27:21.040
a task layer so there will be a

1:27:19.520,1:27:24.800
prediction layer for

1:27:21.040,1:27:27.360
graph node and edge task

1:27:24.800,1:27:30.800
let me now describe in details you know

1:27:27.360,1:27:30.800
each of these three layers

1:27:31.199,1:27:35.199
so for the input layer um so again we

1:27:33.600,1:27:36.800
will have you know the input node and

1:27:35.199,1:27:40.080
edge features so this comes

1:27:36.800,1:27:42.400
from the application that can be you

1:27:40.080,1:27:44.159
know a node feature for example for

1:27:42.400,1:27:45.920
um you know for recommend for

1:27:44.159,1:27:47.199
recommender system for products so it

1:27:45.920,1:27:48.320
will give you you know some feature of

1:27:47.199,1:27:50.000
your product

1:27:48.320,1:27:51.679
so what you will do is that you will

1:27:50.000,1:27:53.360
take this um

1:27:51.679,1:27:55.520
this raw feature and you will make you

1:27:53.360,1:27:56.800
know an embedding a linear medium

1:27:55.520,1:27:59.440
and you will get a vector of d

1:27:56.800,1:28:01.120
dimensions we can do the same

1:27:59.440,1:28:03.199
if we have some edge feature we can do

1:28:01.120,1:28:05.040
an embedding of the input

1:28:03.199,1:28:06.239
edge feature and we'll get a vector of

1:28:05.040,1:28:09.040
the dimension

1:28:06.239,1:28:09.920
so basically the output of the embedding

1:28:09.040,1:28:13.520
layer will be

1:28:09.920,1:28:16.960
um for h it's going to be a matrix

1:28:13.520,1:28:18.639
of endnotes and d dimensions for the

1:28:16.960,1:28:21.520
features

1:28:18.639,1:28:22.880
for the edge is going to be a matrix of

1:28:21.520,1:28:27.280
e the number of edges

1:28:22.880,1:28:30.639
times the the number of features

1:28:27.280,1:28:30.639
and then so we will give that

1:28:33.760,1:28:39.600
and we will give you know this output

1:28:37.360,1:28:40.719
of the embedding layer it's going to be

1:28:39.600,1:28:43.760
the input of the

1:28:40.719,1:28:47.520
graph neural network layers

1:28:43.760,1:28:47.520
okay which is which is here

1:28:47.679,1:28:51.600
then what we will do is that we will

1:28:49.280,1:28:54.639
apply our favorite

1:28:51.600,1:28:57.920
graph normal layer

1:28:54.639,1:29:00.320
a number of l times okay so we have

1:28:57.920,1:29:01.600
um the the node and the edge

1:29:00.320,1:29:04.639
representation

1:29:01.600,1:29:06.560
at layer l it will go through uh the

1:29:04.639,1:29:07.840
gnn layer and it will give you node

1:29:06.560,1:29:10.000
representation of h

1:29:07.840,1:29:11.120
and e at the next layer and we will do

1:29:10.000,1:29:13.520
that you know

1:29:11.120,1:29:14.800
l number of times this will give us you

1:29:13.520,1:29:17.199
know

1:29:14.800,1:29:17.920
uh the output of the graph network

1:29:17.199,1:29:19.760
layers

1:29:17.920,1:29:21.280
and again it's going to be a matrix of

1:29:19.760,1:29:23.920
endnotes and d dimensions

1:29:21.280,1:29:26.239
for the nodes and for the edges is going

1:29:23.920,1:29:29.120
to be a matrix of

1:29:26.239,1:29:30.400
e which is the number of edges times the

1:29:29.120,1:29:32.560
dimensionality

1:29:30.400,1:29:35.040
okay so this is the output of our graph

1:29:32.560,1:29:38.000
and all network layers

1:29:35.040,1:29:38.639
and then finally for the last layer so

1:29:38.000,1:29:41.760
this is

1:29:38.639,1:29:43.920
you know task based uh layer so

1:29:41.760,1:29:44.800
if we if we are doing some prediction at

1:29:43.920,1:29:46.719
the graph

1:29:44.800,1:29:48.320
levels what happened is that we are

1:29:46.719,1:29:50.320
going to take you know

1:29:48.320,1:29:53.040
the the output of the kraftner network

1:29:50.320,1:29:56.159
layers and we're going to make a mean

1:29:53.040,1:29:58.960
with respect to all nodes of the graph

1:29:56.159,1:30:01.199
okay so this will give us a

1:29:58.960,1:30:04.880
representation of the graph

1:30:01.199,1:30:07.760
of the dimension then we will give that

1:30:04.880,1:30:09.040
through to an mlp multi-layer perceptron

1:30:07.760,1:30:11.440
and it will give us you know

1:30:09.040,1:30:13.360
a score which can be a scatter if we are

1:30:11.440,1:30:16.000
doing some graph degradation

1:30:13.360,1:30:17.040
like you know chemical property

1:30:16.000,1:30:20.239
estimation

1:30:17.040,1:30:22.159
or it can be a classification

1:30:20.239,1:30:23.520
if we are trying to classify you know

1:30:22.159,1:30:26.639
molecules to

1:30:23.520,1:30:26.639
some classes

1:30:27.199,1:30:31.520
we can also have you know a nod level

1:30:29.360,1:30:32.880
prediction so what we will do is that we

1:30:31.520,1:30:35.280
will take

1:30:32.880,1:30:36.880
the node representation at the output of

1:30:35.280,1:30:39.520
the graph neural network

1:30:36.880,1:30:40.639
and we will give that to an mlp and we

1:30:39.520,1:30:43.120
will get a score

1:30:40.639,1:30:44.159
for the node i which can be a scalar for

1:30:43.120,1:30:46.960
regression

1:30:44.159,1:30:48.320
or can be a k dimensional vector for

1:30:46.960,1:30:50.560
classification

1:30:48.320,1:30:52.480
we can also do you know edge level

1:30:50.560,1:30:55.920
prediction

1:30:52.480,1:30:56.800
so we have a link between node i and

1:30:55.920,1:31:00.080
node j

1:30:56.800,1:31:02.639
uh it's going to be concatenation of the

1:31:00.080,1:31:05.360
you know the graph neural network uh

1:31:02.639,1:31:07.040
representation for node i and not j

1:31:05.360,1:31:08.480
we give that to an mlp and again we'll

1:31:07.040,1:31:10.400
have a score for the link

1:31:08.480,1:31:13.199
between node i and j and it can be

1:31:10.400,1:31:15.440
regression or classification

1:31:13.199,1:31:16.400
okay so quickly because i'm running out

1:31:15.440,1:31:17.760
of time so

1:31:16.400,1:31:19.840
the task you have the graph

1:31:17.760,1:31:20.560
classification task the graph regression

1:31:19.840,1:31:23.120
path sorry

1:31:20.560,1:31:23.600
so this is for molecules so here we want

1:31:23.120,1:31:25.840
to

1:31:23.600,1:31:27.679
predict you know the molecular

1:31:25.840,1:31:30.239
solubility

1:31:27.679,1:31:31.040
and here you have the table so this is

1:31:30.239,1:31:34.320
like you know

1:31:31.040,1:31:35.840
agnostic gcn so we don't use any graph

1:31:34.320,1:31:38.239
structure

1:31:35.840,1:31:40.320
the lower the better and and here this

1:31:38.239,1:31:43.280
is isotropic

1:31:40.320,1:31:44.000
gcn and this is anisotropic gcn so

1:31:43.280,1:31:47.520
usually you will see

1:31:44.000,1:31:50.560
that for most experiments um

1:31:47.520,1:31:52.080
anisotropic gcn do better job than than

1:31:50.560,1:31:54.080
isotropic

1:31:52.080,1:31:56.320
gcn because you use some of course on

1:31:54.080,1:31:58.239
directional property

1:31:56.320,1:32:00.000
so this is for graph regression this is

1:31:58.239,1:32:00.800
for graph classification so you have

1:32:00.000,1:32:05.040
super nodes

1:32:00.800,1:32:08.239
of um of images and you want to classify

1:32:05.040,1:32:12.400
the the image to belong to one of

1:32:08.239,1:32:15.920
the classes uh you also have

1:32:12.400,1:32:18.080
h classification so this is here the

1:32:15.920,1:32:21.280
combinatorial optimization problem of

1:32:18.080,1:32:23.040
tsp so the traveling salesman problem

1:32:21.280,1:32:24.800
so you have a graph and then you want to

1:32:23.040,1:32:25.520
know if this edge belongs to the

1:32:24.800,1:32:26.880
solution

1:32:25.520,1:32:29.120
so if it belongs to a solution this is

1:32:26.880,1:32:30.639
the class one and if it doesn't belong

1:32:29.120,1:32:33.040
this is class zero

1:32:30.639,1:32:34.880
and we see that here you need explicit

1:32:33.040,1:32:36.560
edge feature so you see that the only

1:32:34.880,1:32:38.560
model that does a good job compared to

1:32:36.560,1:32:41.920
the naive

1:32:38.560,1:32:45.520
stick is is by using explicit um

1:32:41.920,1:32:46.000
edge feature okay so here i'm using i'm

1:32:45.520,1:32:48.080
using

1:32:46.000,1:32:49.679
this combinatorial example to make a

1:32:48.080,1:32:50.320
workshop announcement so this is also

1:32:49.679,1:32:52.690
what we

1:32:50.320,1:32:56.330
organizing next year with jan

1:32:52.690,1:32:56.330
[Music]

1:33:00.159,1:33:03.760
organizing a workshop on combining deep

1:33:02.080,1:33:05.040
learning and combinatory optimization

1:33:03.760,1:33:07.040
which i think is very interesting

1:33:05.040,1:33:10.639
directional research

1:33:07.040,1:33:12.880
okay conclusion um so

1:33:10.639,1:33:13.760
we generalize the covenant to data on

1:33:12.880,1:33:16.560
graphs

1:33:13.760,1:33:17.920
for this we we needed to redesign a

1:33:16.560,1:33:19.520
convolution opera

1:33:17.920,1:33:21.760
operator on graphs so we do that for

1:33:19.520,1:33:25.280
template matching

1:33:21.760,1:33:27.040
which lead to the class of special gcn

1:33:25.280,1:33:29.040
we also did that with a spectral

1:33:27.040,1:33:32.719
convolution which leads to the class

1:33:29.040,1:33:34.719
of spectral convolution per spectral gcm

1:33:32.719,1:33:36.719
we have linear complexity from

1:33:34.719,1:33:38.639
real-world graphs

1:33:36.719,1:33:40.800
we have gpu implementation but yet it's

1:33:38.639,1:33:43.520
not optimized for the gpu that we

1:33:40.800,1:33:45.280
we have today we have universal learning

1:33:43.520,1:33:47.040
capacity so this is uh the recent

1:33:45.280,1:33:49.520
theoretical works

1:33:47.040,1:33:51.520
and we can do that for multiple graphs

1:33:49.520,1:33:52.320
and also for graphs that that that can

1:33:51.520,1:33:55.280
change you know

1:33:52.320,1:33:56.080
dynamically application so i'm happy

1:33:55.280,1:33:58.639
that now that

1:33:56.080,1:34:00.719
i don't need to justify anymore the why

1:33:58.639,1:34:03.040
we are doing graph conventional nets to

1:34:00.719,1:34:04.800
to anybody so it's it's getting more and

1:34:03.040,1:34:06.880
more application

1:34:04.800,1:34:07.840
uh we see that at the nice uh at the

1:34:06.880,1:34:10.960
last um

1:34:07.840,1:34:14.880
actually this week um

1:34:10.960,1:34:18.080
iclear uh conference so um

1:34:14.880,1:34:18.880
the the the keyword that gets you know

1:34:18.080,1:34:21.600
the most uh

1:34:18.880,1:34:22.159
improvement was graph neural networks

1:34:21.600,1:34:24.000
and and

1:34:22.159,1:34:25.679
you can have you know you have now you

1:34:24.000,1:34:26.960
know a workshop and tutorials on

1:34:25.679,1:34:29.440
graphical networks at

1:34:26.960,1:34:30.560
many of the top uh deep learning and ai

1:34:29.440,1:34:33.040
conferences

1:34:30.560,1:34:34.719
and this is um the first probably

1:34:33.040,1:34:36.719
tutorials on graph

1:34:34.719,1:34:39.440
deep learning that we we organized at

1:34:36.719,1:34:41.600
norips in 2017 and cdpr

1:34:39.440,1:34:44.560
and also if you want some materials to

1:34:41.600,1:34:47.920
uh to look more so we have this

1:34:44.560,1:34:50.400
ipam workshop um

1:34:47.920,1:34:51.440
organized in 2018 and also a follow-up

1:34:50.400,1:34:53.440
in 2019

1:34:51.440,1:34:56.320
and for this we have the video talks so

1:34:53.440,1:34:58.800
if you want to know more about this

1:34:56.320,1:35:00.400
uh so yeah thank michael writer so you

1:34:58.800,1:35:04.400
saw banjo michael bronstein

1:35:00.400,1:35:07.679
felicia monty chattana joshi

1:35:04.400,1:35:10.000
uh g willie yo leo

1:35:07.679,1:35:10.800
thomas lawrence archer slam only michael

1:35:10.000,1:35:16.080
duffy are playing

1:35:10.800,1:35:16.080
earlier so thank you

1:35:17.199,1:35:21.920
thank you it was really impressive and i

1:35:19.440,1:35:23.760
think everyone here was stunned by your

1:35:21.920,1:35:25.040
the quality of the slides and your

1:35:23.760,1:35:26.719
explanations we

1:35:25.040,1:35:29.280
really really enjoyed like i'm getting

1:35:26.719,1:35:31.440
so many private messages here i'm

1:35:29.280,1:35:32.320
it's like like everyone's pretty very

1:35:31.440,1:35:35.679
excited

1:35:32.320,1:35:36.000
um i i have actually a few questions if

1:35:35.679,1:35:39.040
you

1:35:36.000,1:35:40.840
have some time left yes um

1:35:39.040,1:35:42.400
we haven't talked about generative

1:35:40.840,1:35:44.719
models um

1:35:42.400,1:35:46.000
do you have any any words about like how

1:35:44.719,1:35:48.639
we can for example

1:35:46.000,1:35:49.360
generate i don't know like new proteins

1:35:48.639,1:35:51.679
for

1:35:49.360,1:35:52.960
uh i don't know uh figuring out whether

1:35:51.679,1:35:54.800
we can

1:35:52.960,1:35:56.159
find a cure for this kovid right now

1:35:54.800,1:35:58.320
just you know

1:35:56.159,1:36:00.719
actually like how do you say current

1:35:58.320,1:36:03.440
question for the current world

1:36:00.719,1:36:05.520
yeah absolutely so so yeah the community

1:36:03.440,1:36:07.920
is also working on the graph

1:36:05.520,1:36:08.960
generative models so you have two

1:36:07.920,1:36:10.560
directions

1:36:08.960,1:36:12.719
the first direction you can do it in a

1:36:10.560,1:36:14.400
recursive way so

1:36:12.719,1:36:16.719
um what you're going to do is that you

1:36:14.400,1:36:18.960
are create you know your molecule atom

1:36:16.719,1:36:20.080
after atom okay so you create you you

1:36:18.960,1:36:21.600
start with an atom

1:36:20.080,1:36:24.239
then you have a candidate for the next

1:36:21.600,1:36:25.199
atom and also the next bound between the

1:36:24.239,1:36:26.639
two atoms

1:36:25.199,1:36:29.520
and you can do that you know it's a kind

1:36:26.639,1:36:32.159
of lstm style

1:36:29.520,1:36:32.560
and the second direction is to do it you

1:36:32.159,1:36:35.520
know

1:36:32.560,1:36:37.679
in uh in one shot so you need um a

1:36:35.520,1:36:40.800
network that can

1:36:37.679,1:36:43.520
that can predict

1:36:40.800,1:36:44.080
uh what is the length or the size of

1:36:43.520,1:36:47.280
your

1:36:44.080,1:36:49.280
uh of your molecule and then uh

1:36:47.280,1:36:50.639
and then what are the connection so you

1:36:49.280,1:36:52.159
have this two direction you can do it in

1:36:50.639,1:36:52.719
a recursive way or you can do it in one

1:36:52.159,1:36:55.520
shot

1:36:52.719,1:36:56.560
so they are they are different um so the

1:36:55.520,1:36:58.159
community

1:36:56.560,1:36:59.600
is more interesting in the recursive way

1:36:58.159,1:37:02.320
today um

1:36:59.600,1:37:03.600
and i i have a paper on the one shot and

1:37:02.320,1:37:04.719
basically they are they are performing

1:37:03.600,1:37:07.360
the same i mean

1:37:04.719,1:37:08.560
it's uh i don't i don't see any any

1:37:07.360,1:37:10.480
difference

1:37:08.560,1:37:11.840
but you can do it yeah so the only thing

1:37:10.480,1:37:13.679
is how do you treat

1:37:11.840,1:37:15.040
yeah how the question is your molecule

1:37:13.679,1:37:18.239
can have different uh

1:37:15.040,1:37:21.400
size uh and this is the key

1:37:18.239,1:37:23.040
um this is the key uh

1:37:21.400,1:37:25.280
[Music]

1:37:23.040,1:37:26.800
i would say the challenge here so how do

1:37:25.280,1:37:30.880
you deal with different sizes

1:37:26.800,1:37:30.880
but we have different options to do that

1:37:31.119,1:37:34.400
which is very interesting related to the

1:37:32.639,1:37:35.600
chemistry of that is that so graph what

1:37:34.400,1:37:36.320
i want to make is that graph neural

1:37:35.600,1:37:39.520
network

1:37:36.320,1:37:43.040
in some sense are too much flexible

1:37:39.520,1:37:45.360
okay so um what you need so

1:37:43.040,1:37:46.239
when you go from from the standard of

1:37:45.360,1:37:48.639
net

1:37:46.239,1:37:50.000
um so this this as you know the grid is

1:37:48.639,1:37:51.920
very structured

1:37:50.000,1:37:53.199
okay so you can get a lot of information

1:37:51.920,1:37:54.800
for the structure

1:37:53.199,1:37:56.239
of the grid but you don't have this in

1:37:54.800,1:37:59.440
graph again you lose

1:37:56.239,1:38:00.320
you know um the node ordering and

1:37:59.440,1:38:02.960
everything

1:38:00.320,1:38:04.560
so we need to find a way to you know to

1:38:02.960,1:38:06.000
have more and more structure inside the

1:38:04.560,1:38:07.520
graph neural networks

1:38:06.000,1:38:09.040
one way to do that is uh the

1:38:07.520,1:38:10.800
architecture so the architecture for

1:38:09.040,1:38:13.520
example you would like to combine

1:38:10.800,1:38:14.719
uh for example if you do uh chemistry

1:38:13.520,1:38:15.600
you would like to combine schroedinger

1:38:14.719,1:38:18.639
equation

1:38:15.600,1:38:21.119
you know like hamilton energy so people

1:38:18.639,1:38:22.719
are doing that yes to constrain better

1:38:21.119,1:38:23.119
you know your graph neural network so

1:38:22.719,1:38:25.840
again

1:38:23.119,1:38:27.360
graphene network are in some sense um

1:38:25.840,1:38:28.239
you know too much flexible you need to

1:38:27.360,1:38:31.600
find a way

1:38:28.239,1:38:32.560
to to add more um universal you know

1:38:31.600,1:38:34.080
constraints

1:38:32.560,1:38:35.679
yeah actually about the universal

1:38:34.080,1:38:37.360
constraints i got here a question uh

1:38:35.679,1:38:39.440
what do you mean by universal learning

1:38:37.360,1:38:41.199
capacity

1:38:39.440,1:38:43.119
yeah so this is the recent works on

1:38:41.199,1:38:45.040
rafale network so people are trained to

1:38:43.119,1:38:46.639
um

1:38:45.040,1:38:48.239
in some sense you're trying to classify

1:38:46.639,1:38:50.080
your neural networks right there are

1:38:48.239,1:38:52.080
many publication neural networks

1:38:50.080,1:38:54.239
so how do you classify them so you need

1:38:52.080,1:38:56.239
to find mathematical properties like you

1:38:54.239,1:38:57.040
know isotropic properties anisotropic

1:38:56.239,1:38:59.360
properties

1:38:57.040,1:39:01.360
and more recently there are you know

1:38:59.360,1:39:04.480
theoretical work on you know

1:39:01.360,1:39:06.480
isomorphism and you know um

1:39:04.480,1:39:07.679
exposibility of craft neural network

1:39:06.480,1:39:11.040
depending on some class

1:39:07.679,1:39:13.440
of of theoretical

1:39:11.040,1:39:15.520
graphs graphs are you know starting by

1:39:13.440,1:39:16.719
earlier like two 200 years ago so we

1:39:15.520,1:39:18.880
know a lot about graphs

1:39:16.719,1:39:20.400
and we want to classify graphs according

1:39:18.880,1:39:22.719
to some mathematical property

1:39:20.400,1:39:24.360
so this is what i was i was trying to

1:39:22.719,1:39:27.360
mention that

1:39:24.360,1:39:28.400
um uh you can you can design graph

1:39:27.360,1:39:30.960
neural networks

1:39:28.400,1:39:31.840
for some special mathematical properties

1:39:30.960,1:39:34.960
let's see

1:39:31.840,1:39:35.920
thank you um guys feel free to ask

1:39:34.960,1:39:38.080
questions you can

1:39:35.920,1:39:40.320
also write to me if you're too shy i

1:39:38.080,1:39:42.560
mean i'm not shy i can just read

1:39:40.320,1:39:44.639
uh i have a question and thank you so

1:39:42.560,1:39:46.719
much for this great lecture uh and

1:39:44.639,1:39:48.400
you mentioned that you created like a

1:39:46.719,1:39:50.639
benchmark data set

1:39:48.400,1:39:52.320
uh type of like uh so people can

1:39:50.639,1:39:53.040
benchmark their different graph neural

1:39:52.320,1:39:55.520
networks

1:39:53.040,1:39:56.960
um but i i feel like a lot of those

1:39:55.520,1:39:59.440
networks also learn

1:39:56.960,1:40:01.199
some like representation in the graph

1:39:59.440,1:40:03.119
and a lot of downstream tasks could be

1:40:01.199,1:40:05.600
like an unsupervised setting

1:40:03.119,1:40:06.400
where i think in the benchmarking data

1:40:05.600,1:40:09.600
sets you're

1:40:06.400,1:40:11.440
all just using accuracy uh more or less

1:40:09.600,1:40:12.560
so it's like you have labels ground

1:40:11.440,1:40:15.199
truth labels so

1:40:12.560,1:40:17.760
it's more in the supervised setting so

1:40:15.199,1:40:20.639
do you have any thoughts on how we could

1:40:17.760,1:40:22.719
benchmark like the graph network's

1:40:20.639,1:40:25.080
performance in an unsupervised

1:40:22.719,1:40:26.960
setting or i don't know like some

1:40:25.080,1:40:29.119
semi-supervised setting

1:40:26.960,1:40:30.960
um or like by measuring their

1:40:29.119,1:40:33.760
performance in some

1:40:30.960,1:40:35.360
common downstream tasks or application

1:40:33.760,1:40:36.800
uh i would like to hear your thoughts on

1:40:35.360,1:40:38.480
that thank you

1:40:36.800,1:40:40.320
so i think this is one of the most

1:40:38.480,1:40:43.440
favorite uh topics of jan

1:40:40.320,1:40:43.440
and the self-supervised

1:40:44.320,1:40:48.000
yeah that's right uh yeah as you can

1:40:46.400,1:40:49.440
tell i brainwashed

1:40:48.000,1:40:53.600
the students in the class really well

1:40:49.440,1:40:56.880
yes yes so that's why i'm asking

1:40:53.600,1:41:00.560
no of course i mean um of course

1:40:56.880,1:41:04.400
one important question um is um

1:41:00.560,1:41:06.719
you you want to to learn efficiently

1:41:04.400,1:41:08.480
right you don't want to have too much

1:41:06.719,1:41:11.520
labels to be able to

1:41:08.480,1:41:13.280
to predict well so so self-supervised

1:41:11.520,1:41:16.880
learning um

1:41:13.280,1:41:18.960
um is is one way to do that right you

1:41:16.880,1:41:20.400
want to um and you can do that also with

1:41:18.960,1:41:21.679
graph right you can

1:41:20.400,1:41:23.679
you can hide some part of the

1:41:21.679,1:41:24.320
information of your graph and then you

1:41:23.679,1:41:26.639
can predict

1:41:24.320,1:41:28.159
this hidden information to get you know

1:41:26.639,1:41:31.679
a presentation

1:41:28.159,1:41:32.080
so um i guess now it's hard for me to

1:41:31.679,1:41:35.199
follow

1:41:32.080,1:41:35.760
uh the recent all the recent gcn work

1:41:35.199,1:41:37.040
but

1:41:35.760,1:41:38.800
i guess if you google it there will

1:41:37.040,1:41:40.000
probably be already one or two papers on

1:41:38.800,1:41:43.679
this on this idea

1:41:40.000,1:41:45.920
i mean there is nothing special with gcn

1:41:43.679,1:41:47.280
so you can apply the same ideas like

1:41:45.920,1:41:50.880
self-supervised learning

1:41:47.280,1:41:51.440
to gcn so we don't we don't put that in

1:41:50.880,1:41:53.600
the

1:41:51.440,1:41:54.719
in the benchmark yet it's a good idea so

1:41:53.600,1:41:58.400
that's something maybe we

1:41:54.719,1:42:01.199
we could do so actually arguably

1:41:58.400,1:42:02.560
uh all of self-supervised learning

1:42:01.199,1:42:03.199
actually exploits some sort of graph

1:42:02.560,1:42:06.480
structure

1:42:03.199,1:42:07.840
right so when you do

1:42:06.480,1:42:09.360
self-supervised running in text for

1:42:07.840,1:42:10.719
example you take you know you take a

1:42:09.360,1:42:14.000
sequence of words

1:42:10.719,1:42:15.520
and you learn uh you know to predict uh

1:42:14.000,1:42:17.679
a word in the middle of missing words

1:42:15.520,1:42:19.520
whatever they are uh there is a graph

1:42:17.679,1:42:23.360
structure and that graph structure

1:42:19.520,1:42:25.199
is uh how many times a word appears

1:42:23.360,1:42:26.639
uh you know some distance away from

1:42:25.199,1:42:28.400
another word so make

1:42:26.639,1:42:30.159
um you know imagine you have all the

1:42:28.400,1:42:32.800
words and then you

1:42:30.159,1:42:34.560
uh you say you know within this context

1:42:32.800,1:42:36.080
you know make a graph between words

1:42:34.560,1:42:37.920
so this would be a very simplified

1:42:36.080,1:42:38.800
version of it but make a graph that

1:42:37.920,1:42:41.840
indicates

1:42:38.800,1:42:42.639
uh how many times this word appears at

1:42:41.840,1:42:45.199
distance

1:42:42.639,1:42:45.840
three from that uh from that other word

1:42:45.199,1:42:47.119
right

1:42:45.840,1:42:49.440
then you have another graph for distance

1:42:47.119,1:42:51.679
one and another one for distance two etc

1:42:49.440,1:42:53.280
right so that constitutes a graph and

1:42:51.679,1:42:54.960
it's a graph that sort of indicates you

1:42:53.280,1:42:57.600
know in white context two words

1:42:54.960,1:42:58.239
seven simultaneously appear um you can

1:42:57.600,1:43:01.280
think of

1:42:58.239,1:43:02.880
uh uh you know a text as you know

1:43:01.280,1:43:05.360
basically a linear graph and

1:43:02.880,1:43:05.920
you know the the neighbors that you take

1:43:05.360,1:43:07.440
uh

1:43:05.920,1:43:08.960
in a you know when you train a

1:43:07.440,1:43:10.239
transformer basically you know sticking

1:43:08.960,1:43:13.679
a neighborhood in this graph

1:43:10.239,1:43:15.840
right so um when you do metric learning

1:43:13.679,1:43:16.880
uh the type of stuff that is trying

1:43:15.840,1:43:19.280
talked about

1:43:16.880,1:43:21.040
uh you know using contrastive training

1:43:19.280,1:43:22.480
where you have two samples that you know

1:43:21.040,1:43:23.360
are similar and two samples you know are

1:43:22.480,1:43:24.800
dissimilar

1:43:23.360,1:43:27.040
this basically is a graph it's a

1:43:24.800,1:43:28.719
similarity graph that you're using

1:43:27.040,1:43:30.480
you're saying you're telling the system

1:43:28.719,1:43:32.320
here are two samples that are linked

1:43:30.480,1:43:33.760
uh because i know they are similar and

1:43:32.320,1:43:35.840
here are two samples that i know are not

1:43:33.760,1:43:37.440
linked because i know they're dissimilar

1:43:35.840,1:43:39.440
and i'm trying to find a graph embedding

1:43:37.440,1:43:40.880
essentially you can think of

1:43:39.440,1:43:43.119
those neural nets are learning a graph

1:43:40.880,1:43:45.840
embedding for nodes

1:43:43.119,1:43:47.760
so that uh nodes that are linked in the

1:43:45.840,1:43:49.600
graph have similar vectors and

1:43:47.760,1:43:51.119
nodes that are not are dissimilar

1:43:49.600,1:43:52.800
vectors so

1:43:51.119,1:43:54.320
there is a very very strong connection

1:43:52.800,1:43:57.600
between self-supervised running

1:43:54.320,1:44:00.719
and uh uh you know kind of the graph

1:43:57.600,1:44:02.639
view of uh of a training set

1:44:00.719,1:44:04.080
i don't think it's been exploited or

1:44:02.639,1:44:05.360
kind of realized yet

1:44:04.080,1:44:06.800
by a lot of people so there might be

1:44:05.360,1:44:07.760
really interesting stuff to do there i

1:44:06.800,1:44:09.119
don't know what you think about this

1:44:07.760,1:44:10.639
have you but

1:44:09.119,1:44:13.119
yeah exactly this is completely related

1:44:10.639,1:44:13.920
to the you know on the on the graph you

1:44:13.119,1:44:16.480
don't have any

1:44:13.920,1:44:18.320
uh non-positioning and what you are

1:44:16.480,1:44:19.440
saying and is exactly that so how do we

1:44:18.320,1:44:21.440
get you know

1:44:19.440,1:44:23.760
positioning between notes that are

1:44:21.440,1:44:25.600
relevant to your particular application

1:44:23.760,1:44:27.119
and and and you want to do it in a

1:44:25.600,1:44:28.639
self-supervised way

1:44:27.119,1:44:30.320
because because then you will learn you

1:44:28.639,1:44:32.639
know all possible you know

1:44:30.320,1:44:35.440
configurations and you don't need to

1:44:32.639,1:44:36.480
to have labels to do that so yeah this

1:44:35.440,1:44:38.880
is the point see

1:44:36.480,1:44:39.920
you will get if you if you know how to

1:44:38.880,1:44:43.440
compare you know

1:44:39.920,1:44:45.840
um notes so basically how do you extract

1:44:43.440,1:44:46.960
positional encoding then you you will do

1:44:45.840,1:44:48.560
a great job you know

1:44:46.960,1:44:50.639
that's that's that's that's one of the

1:44:48.560,1:44:52.239
most important uh question in graph

1:44:50.639,1:44:55.119
neural networks and also for

1:44:52.239,1:44:55.600
for nlp and many other applications

1:44:55.119,1:44:58.560
great

1:44:55.600,1:45:00.320
thank you a question just arrived here

1:44:58.560,1:45:02.480
uh so could you possibly highlight the

1:45:00.320,1:45:04.639
most important parts of graph

1:45:02.480,1:45:05.520
with attention i think we maybe have

1:45:04.639,1:45:09.040
gone a little fast

1:45:05.520,1:45:12.239
there and someone got a little bit lost

1:45:09.040,1:45:14.719
yeah so uh graph attention network so

1:45:12.239,1:45:16.080
the first technique was developed by

1:45:14.719,1:45:18.239
joshua banjo peter

1:45:16.080,1:45:20.080
will actually wix and uh so it's

1:45:18.239,1:45:21.280
probably you know the first work you you

1:45:20.080,1:45:24.239
you would like to see

1:45:21.280,1:45:25.760
um but you can also do like uh you can

1:45:24.239,1:45:27.440
take you know the transformer

1:45:25.760,1:45:28.800
the standard transformer and then you

1:45:27.440,1:45:31.280
can make it you know a graph

1:45:28.800,1:45:31.920
version it's it's quite straightforward

1:45:31.280,1:45:34.000
to do it

1:45:31.920,1:45:35.199
just by multiplying the just by

1:45:34.000,1:45:37.840
multiplying with the

1:45:35.199,1:45:38.560
urgency agency matrix right yeah exactly

1:45:37.840,1:45:40.080
so

1:45:38.560,1:45:42.960
you can already do it you know with spy

1:45:40.080,1:45:44.960
touch transformer so there is a mask

1:45:42.960,1:45:46.960
exactly with a minus infinity yeah

1:45:44.960,1:45:49.040
exactly so if you put minus infinite

1:45:46.960,1:45:50.800
with soft max you will get zero exactly

1:45:49.040,1:45:51.199
so i think i'm gonna show these tomorrow

1:45:50.800,1:45:52.960
so

1:45:51.199,1:45:54.639
they are yeah exactly so you can already

1:45:52.960,1:45:55.360
do graph you know transformer very

1:45:54.639,1:45:58.080
easily with

1:45:55.360,1:45:59.840
with spy touch but the thing is it's

1:45:58.080,1:46:01.920
it's going to be a full matrix

1:45:59.840,1:46:03.280
yeah so so it's going to take you it's

1:46:01.920,1:46:03.679
going to it's going to use a lot of your

1:46:03.280,1:46:06.239
gm

1:46:03.679,1:46:07.119
memory because there are many values

1:46:06.239,1:46:09.679
that you don't need

1:46:07.119,1:46:10.800
so if you if you want to to scale to

1:46:09.679,1:46:12.239
larger graphs

1:46:10.800,1:46:14.159
then you need something that exploits

1:46:12.239,1:46:16.560
the sparsity like dgn

1:46:14.159,1:46:18.239
or python geometric for example yeah so

1:46:16.560,1:46:20.400
last week we coded by

1:46:18.239,1:46:22.080
by from scratch so we we actually see

1:46:20.400,1:46:23.600
all the operations inside

1:46:22.080,1:46:25.360
and then maybe we can just add one

1:46:23.600,1:46:26.239
additional matrix there just to make

1:46:25.360,1:46:28.480
like this

1:46:26.239,1:46:30.639
masked part such that we can retrieve

1:46:28.480,1:46:31.760
the graph convolutional net from the

1:46:30.639,1:46:34.000
code that we already

1:46:31.760,1:46:34.800
have written so that would be i think a

1:46:34.000,1:46:36.880
connection

1:46:34.800,1:46:38.320
for tomorrow and hold on there are more

1:46:36.880,1:46:40.880
questions coming

1:46:38.320,1:46:41.600
is there any application where using

1:46:40.880,1:46:46.239
chebnet

1:46:41.600,1:46:51.119
might be better than spatial gcn

1:46:46.239,1:46:51.119
um so

1:46:52.960,1:46:56.000
i would say they are part of the you

1:46:54.719,1:46:59.199
know isotropic

1:46:56.000,1:47:01.679
um this is the class i uh yeah this is

1:46:59.199,1:47:04.159
what i call you know isotropic uh gcns

1:47:01.679,1:47:07.119
so um

1:47:04.159,1:47:08.239
uh for me i mean of course it will

1:47:07.119,1:47:10.080
depend on your data

1:47:08.239,1:47:11.520
and you will depend on your task you

1:47:10.080,1:47:12.639
know if you have some tasks where your

1:47:11.520,1:47:14.400
data is uh

1:47:12.639,1:47:15.920
you know isotropic this kind of

1:47:14.400,1:47:16.800
information then chemnet will do a very

1:47:15.920,1:47:19.119
good job

1:47:16.800,1:47:20.960
uh for sure now if you have information

1:47:19.119,1:47:22.000
where the topic is important for example

1:47:20.960,1:47:23.360
for social networks

1:47:22.000,1:47:25.119
you don't want you know to treat people

1:47:23.360,1:47:26.239
the same way the same

1:47:25.119,1:47:28.880
you don't want to treat the neighbors

1:47:26.239,1:47:30.719
the same way then is not going to do a

1:47:28.880,1:47:31.440
good job so it really depends on on your

1:47:30.719,1:47:34.159
task

1:47:31.440,1:47:35.600
where isotropy is is very important if

1:47:34.159,1:47:36.159
isotropic is very important then you

1:47:35.600,1:47:39.520
should use

1:47:36.159,1:47:41.440
net because chemnet you know is using um

1:47:39.520,1:47:43.199
all bit of information about your graph

1:47:41.440,1:47:44.880
in an isotopic way

1:47:43.199,1:47:47.520
and if you are using you know gcn the

1:47:44.880,1:47:49.920
veneer gcn you are just using you know

1:47:47.520,1:47:50.639
the first two terms of approximation of

1:47:49.920,1:47:53.119
gemini

1:47:50.639,1:47:55.040
yes it is there we can learn the edges

1:47:53.119,1:47:56.239
right we can learn the representation

1:47:55.040,1:47:58.880
for the edges such that they

1:47:56.239,1:48:01.040
discriminate between neighbors right

1:47:58.880,1:48:02.400
no no no this is um no this is that this

1:48:01.040,1:48:04.960
one is isotropic

1:48:02.400,1:48:06.320
oh okay okay isotropic what i mean by

1:48:04.960,1:48:10.159
isotropic is that

1:48:06.320,1:48:12.400
if um if you have a pure isotropic

1:48:10.159,1:48:13.920
you know uh graph problems then you

1:48:12.400,1:48:15.920
should use a chemical

1:48:13.920,1:48:17.840
otherwise but otherwise yeah it's better

1:48:15.920,1:48:20.159
to use enzotropic

1:48:17.840,1:48:20.159
of course

1:48:21.360,1:48:25.520
more questions guys i'm really um hey i

1:48:23.920,1:48:27.600
have a question thanks for the talk

1:48:25.520,1:48:28.880
uh i was wondering a lot of these

1:48:27.600,1:48:32.560
methods require

1:48:28.880,1:48:35.119
a existing adjacency matrix and for

1:48:32.560,1:48:36.719
uh some problems for example like you

1:48:35.119,1:48:38.000
don't you know that there is a graph

1:48:36.719,1:48:39.360
structure but you don't know the

1:48:38.000,1:48:41.199
underlying connections

1:48:39.360,1:48:42.639
do you know of any work that addresses

1:48:41.199,1:48:45.840
this problem

1:48:42.639,1:48:46.960
yeah absolutely so um so far most works

1:48:45.840,1:48:49.199
uh

1:48:46.960,1:48:50.960
focus on having already you know the

1:48:49.199,1:48:54.159
graph structure

1:48:50.960,1:48:55.520
and and and of course uh you

1:48:54.159,1:48:57.440
like sometimes sometimes you just have

1:48:55.520,1:48:58.480
data like for example you know you just

1:48:57.440,1:49:01.360
have a set of

1:48:58.480,1:49:03.199
uh set of features and and you want to

1:49:01.360,1:49:06.719
learn you know some graph structure

1:49:03.199,1:49:09.360
it's very hard very very hard

1:49:06.719,1:49:11.119
so um there are some works you know

1:49:09.360,1:49:12.800
doing that

1:49:11.119,1:49:14.480
so they are trying to learn some some

1:49:12.800,1:49:15.520
graph structure at the same time they're

1:49:14.480,1:49:18.239
trying to learn

1:49:15.520,1:49:19.520
you know another presentation uh so

1:49:18.239,1:49:20.560
that's that's promising that that's

1:49:19.520,1:49:22.159
that's interesting

1:49:20.560,1:49:23.679
and this is also somewhat that i'm

1:49:22.159,1:49:25.599
trying to do now but i can tell you

1:49:23.679,1:49:28.719
it's very hard to do and especially

1:49:25.599,1:49:30.960
because um if you

1:49:28.719,1:49:32.960
if you if you let you know the adjacency

1:49:30.960,1:49:36.239
matrix to be a variable

1:49:32.960,1:49:36.719
then you are n square okay you have n

1:49:36.239,1:49:40.000
square

1:49:36.719,1:49:42.960
unknown parameters to learn so um so

1:49:40.000,1:49:43.520
it's uh it's not easy um but yeah this

1:49:42.960,1:49:46.239
is a

1:49:43.520,1:49:47.440
so i would say that these techniques um

1:49:46.239,1:49:50.159
there are many natural

1:49:47.440,1:49:51.840
that are coming with graphs okay you

1:49:50.159,1:49:53.760
don't need to build any graphs

1:49:51.840,1:49:55.280
and this is already giving you you know

1:49:53.760,1:49:58.080
a lot of

1:49:55.280,1:49:58.480
of good tools now if you can give me

1:49:58.080,1:50:00.320
maybe

1:49:58.480,1:50:01.520
what you have in mind what kind of

1:50:00.320,1:50:02.960
application you have in mind what you

1:50:01.520,1:50:04.320
want to use you know when you want to

1:50:02.960,1:50:06.960
learn graphs

1:50:04.320,1:50:08.639
uh at the same time maybe we can talk

1:50:06.960,1:50:09.040
about it so i can tell you xavier of

1:50:08.639,1:50:11.920
course

1:50:09.040,1:50:12.320
zimming uh you know will will correct me

1:50:11.920,1:50:14.480
but

1:50:12.320,1:50:16.480
zooming is actually uh working on uh

1:50:14.480,1:50:17.119
predicting uh protein function predict

1:50:16.480,1:50:18.480
uh

1:50:17.119,1:50:21.040
you know protein function prediction

1:50:18.480,1:50:22.000
basically and and so the underlying

1:50:21.040,1:50:24.880
graph would be the

1:50:22.000,1:50:26.480
for example a contact map or the the

1:50:24.880,1:50:27.679
kind of proximity graph of different

1:50:26.480,1:50:29.599
sites on a protein

1:50:27.679,1:50:31.199
and you don't have that i mean in most

1:50:29.599,1:50:32.320
cases you don't that's that's kind of

1:50:31.199,1:50:34.000
one of the things you

1:50:32.320,1:50:35.520
you have to predict so you could view

1:50:34.000,1:50:38.800
this as some sort of related

1:50:35.520,1:50:40.480
you know graph variable let's see maybe

1:50:38.800,1:50:43.040
you had some other idea

1:50:40.480,1:50:43.840
in mind yeah i i think actually so the

1:50:43.040,1:50:46.400
more specific

1:50:43.840,1:50:47.679
problem is that uh some of these graphs

1:50:46.400,1:50:50.480
you know the edges

1:50:47.679,1:50:51.040
and you uh you know some of the edges

1:50:50.480,1:50:53.119
but you

1:50:51.040,1:50:55.119
don't know the other ones for example in

1:50:53.119,1:50:57.119
protein function prediction

1:50:55.119,1:50:59.760
you can imagine like two proteins that

1:50:57.119,1:51:00.960
have um similar functions as having an

1:50:59.760,1:51:03.040
edge between them

1:51:00.960,1:51:04.400
um but they might not have the same

1:51:03.040,1:51:07.040
function so you don't know sort of the

1:51:04.400,1:51:10.560
edge weights and you kind of have like a

1:51:07.040,1:51:12.239
human labels that are inaccurate so

1:51:10.560,1:51:13.599
uh you know that they're connected in

1:51:12.239,1:51:14.719
some way but you don't know the edge

1:51:13.599,1:51:16.239
weights and you know that

1:51:14.719,1:51:18.800
there are other proteins that should be

1:51:16.239,1:51:20.320
connected but you don't have labels for

1:51:18.800,1:51:22.239
so i guess this is more of a graph

1:51:20.320,1:51:25.360
completion problem yeah and but

1:51:22.239,1:51:27.040
and this one is easy this one if you

1:51:25.360,1:51:27.840
have it's like the semi you know the

1:51:27.040,1:51:29.760
semi

1:51:27.840,1:51:31.119
graph clustering problem so if you

1:51:29.760,1:51:33.599
already have some label

1:51:31.119,1:51:35.199
just a few labels and you have some

1:51:33.599,1:51:37.119
structure around this

1:51:35.199,1:51:39.040
that's something you can you can live

1:51:37.119,1:51:40.320
with uh if you are absolutely no

1:51:39.040,1:51:41.760
structure on the edge

1:51:40.320,1:51:43.440
and then you need to learn the graph

1:51:41.760,1:51:46.639
that is very hard

1:51:43.440,1:51:50.320
i see okay thank you

1:51:46.639,1:51:53.280
hey i have a question about uh

1:51:50.320,1:51:54.320
uh splits of the data when you're

1:51:53.280,1:51:57.280
actually training

1:51:54.320,1:51:58.000
a graph neural network um because it's

1:51:57.280,1:52:00.000
uh

1:51:58.000,1:52:02.000
like can you talk about some of the

1:52:00.000,1:52:03.760
things that you would want to consider

1:52:02.000,1:52:05.440
when actually splitting the data into

1:52:03.760,1:52:08.960
say training and validation

1:52:05.440,1:52:10.880
um like you might want to have uh

1:52:08.960,1:52:12.639
all of the nodes in the training data

1:52:10.880,1:52:13.840
for it to actually be exposed to

1:52:12.639,1:52:17.440
everything that's

1:52:13.840,1:52:20.239
um in in the in in the graph data

1:52:17.440,1:52:22.320
um and you might have a case where

1:52:20.239,1:52:24.320
different types of edges are

1:52:22.320,1:52:26.000
imbalanced in the data set um can you

1:52:24.320,1:52:27.840
talk about when that would be important

1:52:26.000,1:52:30.960
what are some of the considerations in

1:52:27.840,1:52:32.320
splitting the data training

1:52:30.960,1:52:34.159
sorry i'm not sure i understand the

1:52:32.320,1:52:37.360
question so you are talking about uh

1:52:34.159,1:52:39.520
unbalanced training sets uh

1:52:37.360,1:52:40.960
yes and also like so if you have like a

1:52:39.520,1:52:44.080
huge relational

1:52:40.960,1:52:45.920
uh data set right um

1:52:44.080,1:52:47.360
you can talk about some of the

1:52:45.920,1:52:49.040
considerations for uh

1:52:47.360,1:52:52.800
for splitting the data when you're

1:52:49.040,1:52:56.480
trying to train a graph network

1:52:52.800,1:52:58.880
so for a condition on the data sets um

1:52:56.480,1:53:00.480
so you may have you know millions of

1:52:58.880,1:53:02.159
small graphs

1:53:00.480,1:53:03.599
and it is fine i mean because this graph

1:53:02.159,1:53:06.320
neural network they're independent

1:53:03.599,1:53:08.320
of the size of your graph so uh so this

1:53:06.320,1:53:10.480
is this is not issue to learn some

1:53:08.320,1:53:12.320
good graphene representation there is no

1:53:10.480,1:53:15.520
issue with that now

1:53:12.320,1:53:18.800
if you have unbalanced data set um

1:53:15.520,1:53:20.960
i don't know um so that's maybe you can

1:53:18.800,1:53:21.280
maybe apply some standard techniques to

1:53:20.960,1:53:23.360
uh

1:53:21.280,1:53:25.199
to do that so uh you can also you know

1:53:23.360,1:53:27.280
for cross entropy for example you can

1:53:25.199,1:53:29.760
you can weight your cross-entropy

1:53:27.280,1:53:32.800
depending on the size of each class

1:53:29.760,1:53:33.360
so that may be something you can do yeah

1:53:32.800,1:53:35.280
but i

1:53:33.360,1:53:36.960
never you know thought too much about

1:53:35.280,1:53:40.800
this

1:53:36.960,1:53:42.719
okay thank you

1:53:40.800,1:53:44.960
any more questions i'm still getting

1:53:42.719,1:53:48.719
things written here but

1:53:44.960,1:53:48.719
you can voice yourself if you are

1:53:51.199,1:53:54.800
i have a question actually uh first of

1:53:53.199,1:53:57.040
all thanks a lot for the lecture at this

1:53:54.800,1:53:59.679
time especially for you

1:53:57.040,1:54:01.679
so how do you deal with cases where the

1:53:59.679,1:54:03.599
nodes do not have the same dimension

1:54:01.679,1:54:05.280
like if i want to run a small

1:54:03.599,1:54:07.360
simple vanilla graph convolutional

1:54:05.280,1:54:09.840
network but my nodes

1:54:07.360,1:54:10.400
are something like even for facebook uh

1:54:09.840,1:54:12.320
people

1:54:10.400,1:54:13.840
and then pages and i want different

1:54:12.320,1:54:14.800
dimensions so how do you think about

1:54:13.840,1:54:16.800
graph

1:54:14.800,1:54:19.119
like very a very simple graph neural

1:54:16.800,1:54:21.360
network in that

1:54:19.119,1:54:23.119
uh nothing it has nothing to do with

1:54:21.360,1:54:25.520
graph neural networks if you have

1:54:23.119,1:54:26.880
different dimensions for your vector so

1:54:25.520,1:54:28.239
probably you need to

1:54:26.880,1:54:29.760
to to put everything on the same

1:54:28.239,1:54:31.360
dimension and then you need to use some

1:54:29.760,1:54:32.800
indicator function

1:54:31.360,1:54:33.840
like one when you have the information

1:54:32.800,1:54:35.440
and then when you don't have any

1:54:33.840,1:54:38.080
information and this will be used

1:54:35.440,1:54:39.760
during the computation of the loss and

1:54:38.080,1:54:41.440
then when you back propagate you

1:54:39.760,1:54:43.679
if you if you don't have any feature

1:54:41.440,1:54:45.199
information you will not use it

1:54:43.679,1:54:48.320
but i i don't think he has anything to

1:54:45.199,1:54:48.320
do with graph neural network

1:54:48.880,1:54:51.840
okay thank you

1:54:59.599,1:55:05.840
hold on you're writing i'm reading so

1:55:02.840,1:55:05.840
much

1:55:13.280,1:55:16.480
so maybe i don't understand the question

1:55:15.040,1:55:20.000
but i will read it

1:55:16.480,1:55:22.080
out loud anyway uh is there any gcn

1:55:20.000,1:55:25.199
which can work on multiple

1:55:22.080,1:55:28.080
urgency matrices together for example

1:55:25.199,1:55:30.000
a bidirectional graph i don't know what

1:55:28.080,1:55:32.080
this mean

1:55:30.000,1:55:33.840
so if the question is about hyper graph

1:55:32.080,1:55:36.480
so you know you may have more than one

1:55:33.840,1:55:37.280
age uh connecting your nodes yes there

1:55:36.480,1:55:39.280
are some work

1:55:37.280,1:55:41.040
about this it's an extension to natural

1:55:39.280,1:55:43.280
extension mathematically

1:55:41.040,1:55:45.840
so you can yeah you can do that there is

1:55:43.280,1:55:46.639
a there is no limitation to go to hyper

1:55:45.840,1:55:49.040
graph

1:55:46.639,1:55:49.840
it's fine and there are now some data

1:55:49.040,1:55:53.040
sets

1:55:49.840,1:55:58.080
for this for this task so

1:55:53.040,1:56:00.159
if there is a an application

1:55:58.080,1:56:01.520
so students would be interesting to do

1:56:00.159,1:56:04.880
yeah it's there is already

1:56:01.520,1:56:05.679
data set and papers okay uh another

1:56:04.880,1:56:07.440
question would be

1:56:05.679,1:56:09.599
does it make sense to have notes that

1:56:07.440,1:56:12.000
are features of a person

1:56:09.599,1:56:14.480
and do graph classification or have

1:56:12.000,1:56:17.520
notes as person and do

1:56:14.480,1:56:17.520
node classification

1:56:17.599,1:56:22.400
i don't know i don't know um so

1:56:21.119,1:56:25.280
often you know people ask me the

1:56:22.400,1:56:27.040
question can i do a graph given this

1:56:25.280,1:56:29.599
data

1:56:27.040,1:56:31.040
so it's really task dependent i think

1:56:29.599,1:56:32.800
it's ready you know

1:56:31.040,1:56:33.840
when when it's going to be useful or not

1:56:32.800,1:56:35.119
when you get you know some good

1:56:33.840,1:56:36.560
relationships

1:56:35.119,1:56:38.560
um because what is the graph you know

1:56:36.560,1:56:41.520
it's just a collection of pairwise

1:56:38.560,1:56:42.320
you know uh or pairwise connections so

1:56:41.520,1:56:45.040
that's it

1:56:42.320,1:56:45.679
so the question is when when it is

1:56:45.040,1:56:47.840
relevant

1:56:45.679,1:56:49.360
to solve your task sometimes it is

1:56:47.840,1:56:51.920
relevant sometimes it's not

1:56:49.360,1:56:53.199
so it really depends on the on the yeah

1:56:51.920,1:56:53.760
it's obvious but it really depends on

1:56:53.199,1:56:56.000
the data

1:56:53.760,1:56:57.360
and and the task you want to solve so

1:56:56.000,1:57:01.040
yeah yeah the

1:56:57.360,1:57:01.040
student is satisfied with your answer

1:57:02.400,1:57:05.920
i think we run out of questions unless

1:57:04.719,1:57:10.159
there are more

1:57:05.920,1:57:13.360
coming my way uh

1:57:10.159,1:57:13.679
no it's it starts getting bright outside

1:57:13.360,1:57:15.760
there

1:57:13.679,1:57:17.920
exactly exactly it was notice here the

1:57:15.760,1:57:21.040
sun is rising

1:57:17.920,1:57:24.800
uh that's nice okay i think

1:57:21.040,1:57:25.679
that was it uh thank you so so much it

1:57:24.800,1:57:28.080
was like

1:57:25.679,1:57:29.360
i mean really those were so pretty uh

1:57:28.080,1:57:32.800
these slides were so pretty

1:57:29.360,1:57:36.080
i i i had to learn so much from

1:57:32.800,1:57:39.040
the way it teach this world um

1:57:36.080,1:57:40.800
yeah thank you again uh for waking up so

1:57:39.040,1:57:42.400
early

1:57:40.800,1:57:45.040
i mean i think this is a fascinating

1:57:42.400,1:57:47.520
topic you know as you know i've been uh

1:57:45.040,1:57:49.280
involved in this at the beginning and uh

1:57:47.520,1:57:50.960
i think it opens a completely new door

1:57:49.280,1:57:52.480
to applications of machine learning and

1:57:50.960,1:57:53.520
neural nets

1:57:52.480,1:57:55.599
you know it's a new world it's

1:57:53.520,1:57:56.639
completely different world um i know

1:57:55.599,1:57:58.800
your

1:57:56.639,1:58:00.719
phd advisor had been working on you know

1:57:58.800,1:58:02.320
graph signal processing for a long time

1:58:00.719,1:58:04.080
so this was kind of a natural

1:58:02.320,1:58:06.239
transition for for him and for you i

1:58:04.080,1:58:08.800
guess but um

1:58:06.239,1:58:11.360
uh i you know i i think we haven't seen

1:58:08.800,1:58:12.800
the the end of this we'll we'll uh

1:58:11.360,1:58:14.239
we're going to be surprised by what's

1:58:12.800,1:58:16.719
going to come out of this i mean there's

1:58:14.239,1:58:19.599
really already sort of fascinating work

1:58:16.719,1:58:21.360
in that area in high energy physics in

1:58:19.599,1:58:24.480
computational chemistry

1:58:21.360,1:58:28.480
in uh you know social uh

1:58:24.480,1:58:30.159
network uh applications and uh

1:58:28.480,1:58:32.000
you you kind of cited all the big names

1:58:30.159,1:58:33.520
in the you know if you are interested in

1:58:32.000,1:58:35.520
this topic if you're listening to this

1:58:33.520,1:58:36.960
yuri leskovitch is is one of the big

1:58:35.520,1:58:38.080
names you know in addition to xavier

1:58:36.960,1:58:40.560
obviously but

1:58:38.080,1:58:41.840
um and uh jean broner whom you know

1:58:40.560,1:58:43.040
because he's a professor here and he

1:58:41.840,1:58:46.239
talks about it in in this

1:58:43.040,1:58:48.239
in this course uh mikhail bronstein uh

1:58:46.239,1:58:50.000
is also a big contributor he's made some

1:58:48.239,1:58:51.840
really interesting uh

1:58:50.000,1:58:53.679
uh contributions to the to the topic

1:58:51.840,1:58:54.560
also on sort of different methods than

1:58:53.679,1:58:57.360
the one

1:58:54.560,1:58:58.159
uh that you talked about today uh on

1:58:57.360,1:59:01.440
like you know

1:58:58.159,1:59:04.400
uh like you know using graph

1:59:01.440,1:59:05.520
neural nets for like 3d meshes and uh

1:59:04.400,1:59:08.639
for computer

1:59:05.520,1:59:09.599
graphics and things like that so um i

1:59:08.639,1:59:11.520
agree i think

1:59:09.599,1:59:12.960
i think this is also a field you know

1:59:11.520,1:59:14.800
where

1:59:12.960,1:59:16.320
there is a back and forth between you

1:59:14.800,1:59:18.719
know mathematics

1:59:16.320,1:59:20.239
and also applications so if you look at

1:59:18.719,1:59:23.119
for example this protein stuff

1:59:20.239,1:59:24.560
it's very very hard but at the same

1:59:23.119,1:59:25.679
times we can learn a lot you know from

1:59:24.560,1:59:28.159
the mathematical side

1:59:25.679,1:59:30.080
we can we can and it's very exciting

1:59:28.159,1:59:31.119
right because you want both if you want

1:59:30.080,1:59:34.400
to be able to make

1:59:31.119,1:59:37.520
a scientific discovery you need to be

1:59:34.400,1:59:39.760
you know um uh driven by some

1:59:37.520,1:59:41.040
real world very hard problem and then at

1:59:39.760,1:59:42.840
the same time you have these new tools

1:59:41.040,1:59:45.760
you know coming up with a

1:59:42.840,1:59:47.360
graphic neural networks and

1:59:45.760,1:59:48.960
and it's a way also for us to better

1:59:47.360,1:59:49.840
understand you know why neural networks

1:59:48.960,1:59:52.560
work so well

1:59:49.840,1:59:53.679
and and this is you know a direction

1:59:52.560,1:59:55.440
where

1:59:53.679,1:59:57.280
it looks like you know each day they are

1:59:55.440,1:59:58.239
like a new problem in this in this

1:59:57.280,2:00:00.800
direction

1:59:58.239,2:00:02.639
so the pie is big and for everyone for

2:00:00.800,2:00:05.599
for the young students to come and

2:00:02.639,2:00:08.159
to uh to enjoy you know this this area

2:00:05.599,2:00:08.159
of research

2:00:08.639,2:00:12.000
great well thank you again and uh enjoy

2:00:11.119,2:00:16.480
your day

2:00:12.000,2:00:20.920
yeah thanks again thank you so much guys

2:00:16.480,2:00:23.920
all right bye-bye bye-bye guys see you

2:00:20.920,2:00:23.920
tomorrow


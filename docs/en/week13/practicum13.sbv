0:00:00.320,0:00:03.360
um yeah so i was saying that i i prepared this

0:00:03.360,0:00:06.560
lecture from scratch and uh it's been challenging

0:00:10.240,0:00:13.679
it's the second week i'm not doing anything but

0:00:13.679,0:00:16.720
preparing and reading and studying and getting crazy but

0:00:16.720,0:00:21.600
i think it's a improvement i'm i'm growing

0:00:21.760,0:00:29.199
okay um ah what do you do research on

0:00:29.599,0:00:32.960
how to teach better uh this week i'll be just reading

0:00:35.520,0:00:38.640
everything i could about this graph neural net graph

0:00:38.640,0:00:44.000
convolutional networks uh i don't know i read maybe a few

0:00:44.000,0:00:49.760
tens of publications uh i'm a bit drunk to be honest

0:00:50.480,0:00:54.480
okay so so so so what are we talking today about

0:00:54.480,0:00:58.399
graph convolutional networks exploiting domain sparsity right

0:00:58.399,0:01:04.239
well like yesterday we saw that also Xavier mentioned the three property

0:01:04.239,0:01:10.320
of natural signals that are locality stationarity and

0:01:10.320,0:01:14.080
he called it hierarchical well i called compositionality he used the term

0:01:14.080,0:01:17.439
compositionality for meaning the whole the all three things

0:01:17.439,0:01:21.040
but uh again i guess it's just uh it's just

0:01:21.040,0:01:25.200
jurgen we mean the same thing right um

0:01:25.200,0:01:29.280
and so what are these graph convolutional net networks are again

0:01:29.280,0:01:34.240
another type of uh architecture or the another way of exploiting what is the

0:01:34.240,0:01:38.240
structure of your data right and so let's uh

0:01:38.240,0:01:41.920
actually get it let's get there from just last

0:01:41.920,0:01:45.200
week lesson so last week let's get let's have

0:01:45.200,0:01:48.159
a quick recap right we talk about

0:01:48.159,0:01:51.920
self-attention in self-attention we had this set of

0:01:51.920,0:01:58.159
axes right so x we can go x1, x2, and so on until xt

0:01:58.159,0:02:02.320
and they you can you know stack these x's one after each other you get that

0:02:02.320,0:02:07.439
capital x right matrix uh each small x x

0:02:07.439,0:02:11.280
is the size of ℝⁿ and then my hidden layer

0:02:11.280,0:02:15.760
for the whatever x i take in consideration

0:02:15.760,0:02:22.959
is going to be this linear linear combination of these vectors in

0:02:22.959,0:02:27.040
the set okay and we know it's actually from i think

0:02:27.040,0:02:31.120
lab number four that a linear combination of vectors can

0:02:31.120,0:02:35.840
be written as a matrix vector multiplication and so we

0:02:35.840,0:02:39.920
have here that the h it can be is equal to this capital

0:02:39.920,0:02:44.000
x or times a right so a contains the

0:02:44.000,0:02:48.640
coefficient that are scaling these vectors okay

0:02:48.640,0:02:52.800
then we had like we were saying that all these coefficients are positive

0:02:52.800,0:02:56.879
uh they have to sum to one and then if only one

0:02:56.879,0:03:00.480
is actually one then we have the heart attention okay and this big

0:03:00.480,0:03:04.879
x is just this collection of x's okay but then again it's a set

0:03:04.879,0:03:10.080
right a set means it's not a sequence there is no order okay

0:03:10.080,0:03:14.640
um so so far you should be familiar right you should be very actually

0:03:14.640,0:03:16.879
comfortable with this kind of notation right

0:03:16.879,0:03:21.360
this linear combination of columns it's just a matrix

0:03:21.360,0:03:24.640
multiplication okay so then i was reading

0:03:24.640,0:03:27.840
the literature about this graph convolutional network and i read and i

0:03:27.840,0:03:30.480
read them like oh it's actually the same thing

0:03:34.640,0:03:40.640
what the heck so so so so actually let's get there from this perspective right

0:03:40.640,0:03:44.480
it is my perspective again might be not the best but

0:03:44.480,0:03:51.920
you know you have me so you deal with me so let's start with this gcn

0:03:51.920,0:03:55.680
uh graphic ethnographic graph convolutional networks

0:03:55.680,0:03:58.959
so my a which is this vector uh on the left

0:03:58.959,0:04:02.879
here in the in that in the attention that is containing all the coefficients

0:04:02.879,0:04:06.720
that are basically weighting these columns

0:04:06.720,0:04:10.239
in this case is going to be i'm going to call this my

0:04:10.239,0:04:14.879
agency vector okay and what the heck is this agency vector

0:04:14.879,0:04:18.959
so we have to start introducing a little bit of notation

0:04:18.959,0:04:23.280
in this case i introduce here my first vertex uh the red one

0:04:23.280,0:04:27.120
which is uh also has you know representing

0:04:27.120,0:04:33.680
representing my x uh you know my even x input and it's gonna have you know my

0:04:33.680,0:04:37.520
h uh hidden layer like we were seeing before

0:04:37.520,0:04:41.520
on the on the attention part we had like this generic

0:04:41.520,0:04:47.440
x and generic h so i'm gonna keep using this kind of generic notation

0:04:47.440,0:04:51.440
so i have my generic vertex v where i can have my

0:04:51.440,0:04:56.479
generic x and my generic h okay and then of course you're going to have

0:04:56.479,0:04:59.440
all the other vertices right which i'm going to be calling them

0:04:59.440,0:05:04.160
vj on which you can find the signal which is going to be the

0:05:04.160,0:05:09.280
xj and the hj okay they hit the representation and the

0:05:09.280,0:05:14.320
input value for this specific vertex or node

0:05:14.320,0:05:18.080
and then what well you have many no you have just the whole

0:05:18.080,0:05:23.840
collection of data points but then there is a difference now we

0:05:23.840,0:05:27.120
have that these uh nodes these vertices are

0:05:27.120,0:05:32.560
actually connected and so we draw a set of arrows

0:05:32.560,0:05:37.759
and so right now basically we're going to have that my capital a my my sorry my

0:05:37.759,0:05:42.160
vector a is going to be having components αj

0:05:42.160,0:05:45.919
which are equal to 1 whenever there is an incoming

0:05:45.919,0:05:52.800
arrow from vector vj to myself okay so

0:05:52.800,0:05:56.960
if you think about how we were doing this before for the attention

0:05:56.960,0:06:02.000
we were computing a as is of dark max or just the arc max if it's a hard

0:06:02.000,0:06:06.160
version of the attention between uh like of my

0:06:06.160,0:06:09.919
uh scalar product the scalar product of all those

0:06:09.919,0:06:14.639
keys or all those rows times my query right so you had all the keys times the

0:06:14.639,0:06:17.520
query and then you had this scores now we're

0:06:17.520,0:06:22.720
performing soft arg max or arc max and then you have basically

0:06:22.720,0:06:28.080
these values that are telling you who you should look at

0:06:28.080,0:06:32.240
in this case here in the graphical graph convolutional network

0:06:32.240,0:06:35.440
we have this structure that is given to you

0:06:35.440,0:06:41.120
already okay and so again this agency vector can be thought again as

0:06:41.120,0:06:45.520
this vector with ones corresponding to these

0:06:45.520,0:06:49.440
vertices that are having arrows pointing towards

0:06:49.440,0:06:54.720
myself the red guy okay so if you understand this it's finished the

0:06:54.720,0:06:58.560
lesson is concluded right because exactly everything else

0:06:58.560,0:07:02.720
will follow uh automatically right so

0:07:02.720,0:07:08.880
d is going to be my one norm which is what the number of ones i have right so

0:07:08.880,0:07:11.919
if in my case here d is going to be two

0:07:11.919,0:07:14.960
right what is the size of a in this case in

0:07:14.960,0:07:18.080
this case can you tell me if you're following are you following

0:07:21.840,0:07:29.360
answer my question are you hearing me oh no no yeah and the number of nodes

0:07:29.360,0:07:31.680
right so the number of nodes in this case is six

0:07:31.680,0:07:35.759
right and we call this from in this alpha in the surface tension we were

0:07:35.759,0:07:39.759
calling this lowercase t right and so capita

0:07:39.759,0:07:42.960
lower lowercase a a vector is going to be of course

0:07:42.960,0:07:48.160
of size t because you have to multiply t vectors right so you're going to have t

0:07:48.160,0:07:51.680
nodes of t vectors and therefore you need t coefficients

0:07:51.680,0:07:57.360
right so of course i think a has size t

0:07:57.360,0:08:00.800
and d which is the number of ones basically

0:08:00.800,0:08:05.919
is going to be the um is going to be basically the degree

0:08:05.919,0:08:08.879
right i think this can be also written as the norm

0:08:08.879,0:08:13.840
zero i think yeah this is just normal zero right

0:08:14.080,0:08:20.639
cool cool all right um what next so in self attention we had that my

0:08:20.639,0:08:25.759
hidden layer was this matrix multiplication

0:08:25.759,0:08:30.080
of my x times a right so this means the columns of the x

0:08:30.080,0:08:34.880
are scaled by the factor factors inside a

0:08:34.880,0:08:37.919
okay first issue so if you have multiple ones this h is

0:08:41.519,0:08:45.279
going to be larger for uh vertices that have many incoming

0:08:45.279,0:08:48.640
connections right and if he has like let's say just one

0:08:48.640,0:08:52.800
incoming connection is going to be you know small right so this stuff is

0:08:52.800,0:08:56.560
proportional to the number of uh incoming connections so how can we

0:08:56.560,0:09:00.800
fix that oh hold on messages incoming messages

0:09:00.800,0:09:04.399
yeah of course you divide by the number of items right

0:09:04.399,0:09:09.600
and so we multiply that with by d to the minus one cool cool cool

0:09:09.600,0:09:13.200
um so what next or maybe we want to rotate

0:09:13.200,0:09:17.040
things so let's put there a rotating metrics

0:09:17.040,0:09:22.959
um and then we haven't we haven't considered ourselves right so

0:09:22.959,0:09:26.399
this is basically considering all the incoming edges

0:09:26.399,0:09:30.959
but we don't consider ourselves we might want to consider ourselves right

0:09:30.959,0:09:35.279
as there would be a self connection so we can add this another

0:09:35.279,0:09:40.160
another guy right in this ul rotated version of the x

0:09:40.160,0:09:44.320
cool uh then just to make the whole thing

0:09:44.320,0:09:48.320
uh looking like a neural network what do we add

0:09:48.320,0:09:52.560
yeah you know linear function of course right

0:09:52.560,0:09:58.160
ReLU sigmoid and tanh whatever okay uh we said that we have like several of

0:09:58.160,0:10:01.839
these vertices right we don't have just one vertex right we don't just have one

0:10:01.839,0:10:06.000
x we have many of these guys right so we had a set of

0:10:06.000,0:10:12.640
vertices or set of inputs right for i that goes from one to t and this leads

0:10:12.640,0:10:17.040
therefore to this matrix notation right so you just stack multiple edges

0:10:17.040,0:10:22.480
you get a matrix by stacking multiple x's you know you rotate multiple x's you

0:10:22.480,0:10:27.440
just get the stack and then you sum this to the attention

0:10:27.440,0:10:32.320
where the attention like the agency vector now it's going to be agency

0:10:32.320,0:10:35.440
matrix it's going to be all these columns right

0:10:35.440,0:10:38.640
uh where they tell you where where are the

0:10:38.640,0:10:41.440
incoming connections right those incoming arrows

0:10:41.440,0:10:46.000
and that d is going to be the uh the inverse of the diagonal

0:10:46.000,0:10:50.560
where you have all the degrees on the diagonal okay

0:10:50.560,0:10:57.120
finish that was it right graph convolutional networks it looks

0:10:57.120,0:11:01.360
like attention to me but okay so what do we do today for the

0:11:01.360,0:11:05.519
lab okay are there questions so far i mean are you with me are you have you

0:11:05.519,0:11:08.079
been following right there is nothing here that we haven't

0:11:08.079,0:11:11.680
seen last time basically well non-linearities

0:11:11.680,0:11:15.680
self-connection where do features come in

0:11:15.680,0:11:21.040
isn't x a feature okay x is a feature yes

0:11:21.040,0:11:24.720
x is a feature and the feature here are so there is

0:11:24.720,0:11:28.800
like there is a graph that is telling you which

0:11:28.800,0:11:32.079
vertices are connected and each vertex has

0:11:32.079,0:11:35.519
a x which is the input and then it's going to have a

0:11:35.519,0:11:40.560
hidden value right are the previous hidden vectors used to

0:11:40.560,0:11:45.040
compute the new one uh are they they are not right here

0:11:45.040,0:11:48.560
you can have multiple layers right and so the second layer the

0:11:48.560,0:11:53.920
h layer next layer is going to be using the hidden layers like the hidden values

0:11:53.920,0:11:56.480
of the previous layer right it's going to be just a

0:11:56.480,0:12:02.000
normal way like you stack multiple of these blocks right

0:12:02.959,0:12:09.120
uh the u the u is just the uh a term that allows me to consider also

0:12:09.120,0:12:13.760
my own value x okay so right now a is going to

0:12:13.760,0:12:19.040
basically give me the average of these columns that are

0:12:19.040,0:12:22.880
incoming and then u allows me to you know perform

0:12:22.880,0:12:26.240
a rotation of my own self vector so whenever you have

0:12:26.240,0:12:29.040
like a graph in this case there are two options

0:12:29.040,0:12:32.240
or it's you and you're like the v the red v

0:12:32.240,0:12:35.600
or is the other which is the vj okay and so

0:12:35.600,0:12:38.639
here you have two terms one is taking care of the v

0:12:38.639,0:12:42.959
red v the other one is taking care of the vj

0:12:42.959,0:12:46.880
final question the agency matrix does not

0:12:46.880,0:12:51.680
have self connections the urgency matrix has zeros on the diagonal

0:12:51.680,0:12:55.360
if you want to consider the agency metrics with the

0:12:55.360,0:12:59.120
ones on the diagonal you can have like identity plus d a

0:12:59.120,0:13:04.959
right okay so next slide which is going to be the

0:13:04.959,0:13:08.800
thing that we are implementing today okay otherwise did i miss any

0:13:08.800,0:13:14.480
question um so alph is in this a so the diagonal

0:13:14.480,0:13:18.160
is all zeros yeah yeah so agency vectors

0:13:18.160,0:13:24.560
the agency vector here has a one only if the vj which is my neighbor is

0:13:24.560,0:13:27.760
actually connected to me okay and since there is

0:13:27.760,0:13:30.800
no a arrow from myself that goes back into

0:13:30.800,0:13:35.600
myself there is no one in correspondence to

0:13:35.600,0:13:42.480
my own position so a agency matrix has you know in the diagonal all zeros

0:13:42.480,0:13:46.160
and then has ones corresponding to the incoming connections

0:13:46.160,0:13:50.160
if you have a non-directed graph then you have a symmetric

0:13:50.160,0:13:53.760
matrix because you don't you have you know the same one

0:13:53.760,0:13:57.440
for both directions it's going to basically having it's like having a

0:13:57.440,0:14:04.639
arrow on both direction of the edge okay okay got it thank you sure

0:14:04.639,0:14:11.279
how is x represented x is a vector which uh refers to a node

0:14:11.279,0:14:16.320
so how do you represent a node using a vector

0:14:16.320,0:14:22.880
how you represent the node uh so x is a vector right of dimension n

0:14:22.880,0:14:28.320
and this is your set of vectors right this is your set of inputs

0:14:28.320,0:14:31.920
you have 1 to t this is from self attention

0:14:31.920,0:14:35.040
vector self analysis self attention set right

0:14:35.040,0:14:41.440
so this is a set and from this other slide basically you have that only

0:14:41.440,0:14:46.320
some of these axes are connected to other axes

0:14:46.320,0:14:52.160
so you have a set of axes and then you're gonna have basically a

0:14:52.160,0:14:56.839
connectivity you know specified between these

0:14:56.839,0:15:02.639
vertices so x and h and h next layer and x h next layer and

0:15:02.639,0:15:07.519
so on are basically values in a set

0:15:07.519,0:15:11.440
but then the point is that these elements in this set are connected

0:15:11.440,0:15:17.279
through these arrows okay and so that that's simply it like there

0:15:17.279,0:15:21.040
is no magic here like we were telling you suppose you

0:15:21.040,0:15:24.639
have a graph whose no vote dices are labeled one two three

0:15:24.639,0:15:30.720
four five so how do you convert from label 1 to x1

0:15:30.720,0:15:34.800
but each of these is gonna be just a number right whatever

0:15:34.800,0:15:38.639
and you're gonna just play with this so you just have to think about

0:15:38.639,0:15:43.120
you know this can be thought as a sequence of again words for a sentence

0:15:43.120,0:15:46.320
or can be thought as the pixels in an image right

0:15:46.320,0:15:50.880
it can be just one linear image you know or you can have you know whatever to a

0:15:50.880,0:15:54.720
normal image so these are just the values the one

0:15:54.720,0:15:58.880
that we called uh in the domain in ℝᶜ

0:15:58.880,0:16:01.920
right whenever we are mapping the mapping the

0:16:01.920,0:16:05.839
domain capital omega to these image values

0:16:05.839,0:16:12.079
right so this is simply a set of values and in this case here we just

0:16:12.079,0:16:16.000
specify a specific domain which has connections

0:16:16.000,0:16:20.800
between vertices simple as that

0:16:20.800,0:16:24.000
anyhow so we're gonna be checking the code right now so that you can

0:16:24.000,0:16:27.519
understand everything that is going on okay don't don't don't get too scared

0:16:27.519,0:16:33.759
but i don't think there is any more uh craziness going on uh the only craziness

0:16:33.759,0:16:37.199
part is gonna be the type of graph convolutional network

0:16:37.199,0:16:39.120
we are going to be implementing right now

0:16:39.120,0:16:44.160
and so we're going to be starting we're going to be implementing something cool

0:16:44.160,0:16:47.839
because otherwise otherwise would be boring which is the residual

0:16:47.839,0:16:53.920
gated graph convolutional network with a mouthful and of course it's from

0:16:53.920,0:16:57.600
bresson and laurent you can see from the reference below

0:16:57.600,0:17:00.959
so here again we can think about you know our

0:17:00.959,0:17:05.039
own vertex v the red guy which again has this

0:17:05.039,0:17:08.799
input feature x and the hidden representation

0:17:08.799,0:17:12.319
representation h and then you have the vj now

0:17:12.319,0:17:15.760
again with the all representing all the other

0:17:15.760,0:17:19.199
and then you have all these these guys right

0:17:19.199,0:17:22.799
uh in this specific case actually we are gonna be

0:17:22.799,0:17:26.480
uh naming as well the edges so in this case

0:17:26.480,0:17:34.240
my edge has also a feature on it okay so in this like graph

0:17:34.240,0:17:37.760
in this residual gated graph convolutional network

0:17:37.760,0:17:41.200
uh edges also have a representation on them

0:17:41.200,0:17:44.960
and so this is called ej okay and so you have

0:17:44.960,0:17:49.039
all these vertices that were they were white before now they have like

0:17:49.039,0:17:53.280
a color we're going to have an edge representation for the input layer

0:17:53.280,0:17:57.039
x and for the hidden layer right so we're going to have

0:17:57.039,0:18:03.360
ex and then eh so what are the update equations for

0:18:03.360,0:18:08.720
this receivable gated graph convolutional network so

0:18:08.720,0:18:11.760
since it's a residual we're going to start with our serial connection we have

0:18:11.760,0:18:14.559
an input x the pink one and then we have

0:18:14.559,0:18:17.600
class something right uh something that is

0:18:17.600,0:18:22.320
always positive so actually this could diverge and an easy fix for

0:18:22.320,0:18:24.880
this first equation would be actually having

0:18:24.880,0:18:30.799
a additional weight multiplying the parenthesis right anyhow let's go

0:18:30.799,0:18:35.919
for this version so we have x plus something

0:18:35.919,0:18:40.080
for which we take the positive part and inside we're going to have

0:18:40.080,0:18:43.840
my rotation of the input which is exactly the same

0:18:43.840,0:18:50.320
as you were seeing before right so here we have that

0:18:50.320,0:18:58.480
h equal rotation of the input x right so the same here

0:18:58.480,0:19:03.679
we have that h equal okay there is the received one and then the rotation of

0:19:03.679,0:19:07.760
myself and then we have plus

0:19:07.760,0:19:15.280
a rotation of the xj the incoming j right this rotation it's also scaled

0:19:15.280,0:19:20.000
by eta and eta is going to be our gate so

0:19:20.000,0:19:23.760
now you know why it's called residual gated graph convolutional network

0:19:23.760,0:19:28.799
because we have a gate eta which is based on the representation

0:19:28.799,0:19:33.039
living on the incoming edge ej which is modulating the

0:19:33.039,0:19:38.240
amplitude of the rotated incoming vertex xj

0:19:38.240,0:19:41.679
right and finally we're going to be summing for

0:19:41.679,0:19:47.200
all the edges that are coming towards my own vertex right so for all the edges

0:19:47.200,0:19:51.600
that are incoming i'm going to be rotating the vertex

0:19:51.600,0:19:56.960
representation of the incoming vertex and i'm gonna be then scaling

0:19:56.960,0:20:00.880
modulating the amplitude of this incoming rotated vertex

0:20:00.880,0:20:04.080
with this gate right again this gate it's

0:20:04.080,0:20:09.440
a function of the ej so what is ej let's figure out the equation so we have

0:20:09.440,0:20:14.159
ej is going to be a rotation of my initial

0:20:14.159,0:20:18.240
edge representation that is populated with the input data so

0:20:18.240,0:20:25.440
ex is going to be my input data that is living on the edge and so i rotate that

0:20:25.440,0:20:32.000
i sum the rotated representation of my incoming feature x j

0:20:32.000,0:20:35.840
and then i sum as well a rotation of my own

0:20:35.840,0:20:41.200
feature ex right x is my own feature i rotate it with the matrix E

0:20:41.200,0:20:47.760
sweet so this is my ej representation and then eta is going to be the

0:20:47.760,0:20:51.600
following so it's a sort of similar

0:20:51.600,0:20:58.960
like a variant of our soft dark mags where um we have that the numerator at

0:20:58.960,0:21:04.480
the numerator we have the sigmoid of my ej which is the sum of

0:21:04.480,0:21:08.960
these three components at the on the bottom which is divided by the

0:21:08.960,0:21:13.039
summation of all the sigmoids

0:21:13.039,0:21:19.840
of the incoming edges right so we have a given edge we compute the

0:21:19.840,0:21:23.360
usually if you have the soft arc max you're going to have the exponential

0:21:23.360,0:21:27.840
of the specific value divided by the sum of the exponentials

0:21:27.840,0:21:31.120
in this case this gate is given to you by

0:21:31.120,0:21:37.520
the sigmoid of the given edge divided by the sum of the all incoming edges

0:21:37.520,0:21:42.240
right all incoming yeah connections finally we have that the next layer so

0:21:46.240,0:21:50.480
for the hidden layer the next layer uh edge representation we're gonna have

0:21:50.480,0:21:52.559
a residual connection so it's gonna be my

0:21:52.559,0:21:59.440
initial value ex plus the positive part of this ej again this

0:21:59.440,0:22:03.280
may blow up because you're going to be summing always positive

0:22:03.280,0:22:09.600
terms therefore i would suggest additional weight multiplying these

0:22:09.600,0:22:12.960
positive parts such that you know you can have even negative

0:22:12.960,0:22:19.520
values cool cool so that that's pretty much it right so if

0:22:19.520,0:22:23.760
we compare to what we were seeing before before we had that my hidden

0:22:26.799,0:22:31.520
representation was some non-linear function in this

0:22:31.520,0:22:36.240
case we chose the uh the ReLU the positive part

0:22:36.240,0:22:43.280
right so in this case we have f is going to be the positive part here

0:22:44.240,0:22:52.240
of my rotated representation of myself plus this term over here which is

0:22:52.240,0:22:56.880
so this Xad⁻¹ it means take the average

0:22:56.880,0:23:00.240
of the incoming axis right because a was equal

0:23:00.240,0:23:05.440
one for the vertexes that are incoming towards my own vertex

0:23:05.440,0:23:10.000
and then i divide by the d which is the degree which is the number of incoming

0:23:10.000,0:23:14.799
edges right and so i basically sum all these

0:23:14.799,0:23:19.600
incoming values and then i divided by the number of the incoming values so i

0:23:19.600,0:23:22.960
compute the mean and then i rotate the mean right the

0:23:22.960,0:23:26.960
similarly here we're going to do exactly the same thing we have the

0:23:26.960,0:23:31.120
rotation of all the incoming edges right so these

0:23:31.120,0:23:36.159
are all incoming edges and then i sum them but in this case my

0:23:36.159,0:23:40.960
eta is not just a constant that is equal to one over the number of incoming

0:23:40.960,0:23:45.279
connections but is going to be a number

0:23:45.279,0:23:49.919
from zero to one which is weighting my incoming

0:23:49.919,0:23:52.960
vertex representation based on what is the

0:23:52.960,0:23:57.360
representation living on the edge so there are many

0:24:00.640,0:24:04.320
colors and numbers and symbols but i don't think it's that different from

0:24:04.320,0:24:08.080
what we have seen before the main differences are this gate which

0:24:08.080,0:24:12.080
is no longer a like a constant factor now it's going

0:24:12.080,0:24:16.000
to be function of the representation and then we have this received by

0:24:16.000,0:24:20.880
connection again i would say that here is missing a

0:24:20.880,0:24:25.919
additional parameter right here and here i would suggest to

0:24:25.919,0:24:29.679
have an additional an additional matrix multiplying here

0:24:29.679,0:24:34.880
and here such that we can allow for you know positive and negative values

0:24:34.880,0:24:39.600
otherwise this representation may blow up now how

0:24:39.600,0:24:45.520
do we compute the representation for the second hidden layer so we can call x

0:24:45.520,0:24:52.799
hl so it's going to be my layer l representation and therefore

0:24:52.799,0:24:58.559
the xj becomes hlj and so all we have to do now is going to

0:24:58.559,0:25:04.159
be basically saying that my age at layer l plus 1 is going to be

0:25:04.159,0:25:10.960
this current h right but i prefer to use h and x's in order to remove this

0:25:10.960,0:25:13.520
additional index that may create cows right maybe

0:25:17.039,0:25:21.120
chaotic all right um so i had a question about

0:25:21.120,0:25:25.600
if um in terms of like maybe a potential

0:25:25.600,0:25:29.840
example um i'm not clear what it means to have

0:25:29.840,0:25:37.120
this sort of um gated like recurrent type of

0:25:37.120,0:25:44.559
model in the context of graphs like um so what is an example of

0:25:44.559,0:25:49.200
yeah yeah yeah sure so this gating part here uh the point is that

0:25:49.200,0:25:54.080
all these different uh vertices here they don't have ordering right i don't

0:25:54.080,0:25:58.320
know which one is v1, v2, i mean i know the order but this guy

0:25:58.320,0:26:02.640
here this red vertex doesn't know how many neurons sorry how

0:26:02.640,0:26:07.520
many vertices are connected to its own and then it doesn't know how to you know

0:26:07.520,0:26:12.720
think about them in different ways unless there is some information

0:26:12.720,0:26:18.880
coming from this edge and so this edge allows me to basically change you know

0:26:18.880,0:26:24.000
modulate this incoming incoming message

0:26:24.000,0:26:30.400
so this guy here is transiting this x transits down this uh

0:26:30.400,0:26:36.559
this line but then it gets modulated by the representation of by this gate

0:26:36.559,0:26:40.480
which is again based on the representation that lives on that edge

0:26:40.480,0:26:46.159
so the edge has a representation and this eta it gives me a multiplier

0:26:46.159,0:26:50.559
basically a factor that i can use a scalar to multiply

0:26:50.559,0:26:54.880
each component of this vector here and so it allows me to

0:26:54.880,0:27:00.640
tune you know what kind of part of the vector i might be interested in

0:27:00.640,0:27:03.520
okay so this is going to be any way trained with backprop so the network

0:27:03.520,0:27:07.919
will figure out what the heck is interesting what is not

0:27:07.919,0:27:11.120
uh but yeah the rationale here is going to

0:27:11.120,0:27:13.919
be basically given that all the vertices look the

0:27:13.919,0:27:17.520
same to me in this case because you know if you if

0:27:17.520,0:27:22.080
you remove this part here you just get the summation of all these

0:27:22.080,0:27:25.279
h's right and this is going to be oh just let's average

0:27:25.279,0:27:30.640
everything well like the thing i told you here right so this is exactly

0:27:30.640,0:27:34.480
what i'm telling you here in this case here you just have a average

0:27:34.480,0:27:38.080
all the vertices will average all the representation

0:27:38.080,0:27:42.640
on the incoming vertices right and so this is like hey

0:27:42.640,0:27:45.919
let's blur out everything can't say let's let's

0:27:45.919,0:27:50.240
let's throw away all the information in this case instead it's going to be hey

0:27:50.240,0:27:53.520
we are not going to be just averaging out all these incoming

0:27:53.520,0:27:56.720
values but we're going to be weighting them we're going to be

0:27:56.720,0:27:59.919
modulating them based on what we think it might be

0:27:59.919,0:28:03.679
relevant or might not so that would be and is the

0:28:03.679,0:28:10.000
um is that superscript l and l plus one for the h's like does that mean that

0:28:10.000,0:28:14.559
this is a graph structure over time layer layer layer do

0:28:14.559,0:28:19.200
you have several layers right in this network so h so

0:28:19.200,0:28:22.960
hl with l equals zero is going to be my x

0:28:22.960,0:28:26.000
it's it's talking about layers and not like

0:28:26.000,0:28:30.080
time yeah there are several layers right so you have multiple layers

0:28:30.080,0:28:34.640
and all of these layers always leave these layers are still sets right

0:28:34.640,0:28:38.799
so as you have like a set of inputs you have a set of inputs then you have a

0:28:40.960,0:28:44.960
set so these are my set of inputs right then you are going to have a set of

0:28:44.960,0:28:49.279
hidden layers a set of second like hidden layer of the second layer and so

0:28:49.279,0:28:52.960
like a second hidden layers at the second layer and so

0:28:52.960,0:28:55.279
on right and so but in here we just have sets the

0:28:58.480,0:29:00.799
only difference is going to be that in this case

0:29:00.799,0:29:04.640
there are sets but then there is also connections between these elements in

0:29:04.640,0:29:06.960
the set okay and that's the only difference we

0:29:06.960,0:29:10.080
have so the only difference between attention and this stuff here

0:29:10.080,0:29:17.039
is that these guys here are given to you by this agency metrics instead of

0:29:17.039,0:29:22.080
being computed with uh attention like attending and computing the soft

0:29:22.080,0:29:26.000
arguments and so on so this this is my perspective from last

0:29:26.000,0:29:29.039
week lesson so the only step that is the difference

0:29:29.039,0:29:34.240
from last week is going to be that this connection are given to you finish

0:29:34.240,0:29:38.240
same everything else is going to be basically the same

0:29:38.240,0:29:41.919
all right so time to go to the notebook because it's going to be actually taking

0:29:41.919,0:29:46.720
forever unless there are imminent questions all

0:29:46.720,0:29:49.679
right so this was taken it was uh heavily

0:29:49.679,0:29:53.679
inspired by the notebook from xavier but i changed

0:29:53.679,0:29:57.360
everything so yeah i didn't like what he wrote

0:29:57.360,0:30:02.399
as in now it's in our format or in my format so everything is going

0:30:02.399,0:30:07.840
to be familiar uh at least for you when i first read the thing was like

0:30:07.840,0:30:13.760
what's going on here uh okay okay so import uh crap

0:30:13.760,0:30:17.840
uh the only difference here we have these uh import os

0:30:17.840,0:30:22.080
which allow me to set an environment environment variable so this dgl

0:30:22.080,0:30:26.240
backhand set to pi torch allows me to tell dgl to

0:30:26.240,0:30:29.679
use pytorch what is dgl so actually you have to install

0:30:29.679,0:30:33.760
ppinstall dgl it's actually in the environment description

0:30:33.760,0:30:42.080
um dji is going to be my library to use convolutional nets on graphs

0:30:42.080,0:30:46.960
very easily so we import this stuff and also we import this network x it allows

0:30:46.960,0:30:51.679
me to to print a very pretty uh very pretty

0:30:51.679,0:30:55.440
charts okay i set some default oh you can see

0:30:55.440,0:30:59.600
now we are using PyTorch so first of all we're gonna be showing

0:30:59.600,0:31:02.559
seeing these so this is mini graph classification data set

0:31:02.559,0:31:09.039
it's gonna it's called mini gcd this mini gc data set i specify the

0:31:09.039,0:31:12.640
number of graphs the minimum number of values vectors on

0:31:12.640,0:31:18.000
the maximum number of vectors no vectors uh vertices right and so

0:31:18.000,0:31:21.919
uh here i just call this with the different names they have

0:31:21.919,0:31:25.760
and then i show you here these different guys okay

0:31:25.760,0:31:28.960
so here you have the first type is going to be the circle type

0:31:28.960,0:31:33.279
so the circle graph where you have each of these are connected to the other one

0:31:33.279,0:31:37.360
and see again there is double r alright then we have the star graph which is

0:31:39.519,0:31:42.720
basically everyone connected to the first body

0:31:42.720,0:31:49.279
then we have the wheel graph okay so you can understand what it means

0:31:49.279,0:31:53.600
uh then we have the lollipop lollipop lollipop

0:31:53.600,0:31:58.480
no this is it okay never mind uh anyhow so it's a cluster of points connected by

0:31:58.480,0:32:02.159
a string it looks like a kite to me but okay

0:32:02.159,0:32:05.120
there is the hypercube which is super cute

0:32:05.120,0:32:10.720
which is this crazy guy here um and then there is this classic green

0:32:10.720,0:32:15.840
right so this can be thought as like an image or whatever right uh

0:32:15.840,0:32:19.519
there is a click which is you know fully connected graph

0:32:19.519,0:32:23.519
and then we have this circular ladder and graph so it's a ladder which is

0:32:23.519,0:32:28.000
closing itself right and so what is going to be our task our task

0:32:28.000,0:32:34.320
is going to be given a graph structure try to classify as being one or the

0:32:34.320,0:32:37.039
other right so each of these graphs are going to be

0:32:37.039,0:32:41.360
basically defined by is this agency matrix and given these

0:32:41.360,0:32:45.279
agency metrics we are going to be basically trying to figure out whether

0:32:45.279,0:32:49.600
one graph is uh one type or the other the point is

0:32:49.600,0:32:53.039
that this agency matrix is going to be of variable size right

0:32:53.039,0:32:57.039
because as you have seen this here before right where

0:32:57.039,0:33:00.640
is it uh you can give a minimum and a maximum

0:33:00.640,0:33:05.760
number of nodes and so you can't really do a straightforward

0:33:05.760,0:33:12.320
classification right okay cool cool um

0:33:12.320,0:33:15.279
i didn't say google okay my google is protesting here all

0:33:17.519,0:33:21.200
right so let's add some signal to the domain right so those are the domain

0:33:21.200,0:33:24.399
this is where the the information stay right so if you

0:33:24.399,0:33:28.880
have this guy here where is it uh if you had this one no this is the

0:33:28.880,0:33:32.480
domain and then on top of this you're gonna have the values the colors if you

0:33:32.480,0:33:34.640
have a color image right so these are the

0:33:34.640,0:33:37.679
domains then we're gonna put some signal on top

0:33:37.679,0:33:41.440
so in this case let's actually read together we can assign

0:33:41.440,0:33:45.600
features to nodes and edges of dgl graphs

0:33:45.600,0:33:49.200
the features are represented as dictionary of of names

0:33:49.200,0:33:56.159
strings and tensors call fields and data and e n data and e data

0:33:56.159,0:34:02.559
are syntax sugar to access the features data of all nodes and edges so in this

0:34:02.559,0:34:04.880
case here i'm going to be just telling that

0:34:04.880,0:34:11.839
each of my node information so my x's are going to be in the degree which is

0:34:11.839,0:34:17.359
the basically incoming number of vertices i have okay

0:34:17.359,0:34:21.599
so each node each x has its value on the number

0:34:21.599,0:34:28.399
of the connected guys each edge instead has the just a num

0:34:28.399,0:34:33.359
one so each edge has a number one the other one has the number of connected

0:34:33.359,0:34:38.560
guys cool uh so here i just generate my

0:34:38.560,0:34:42.720
training set and testing set and then i just plot

0:34:42.720,0:34:46.879
these uh to just show you on that these guys have a feature

0:34:46.879,0:34:50.159
both of them are called called feet right and there is a feet

0:34:50.159,0:34:53.359
not like the foot the feet whatever is pronounced the same

0:34:53.359,0:34:56.879
a feet for the node n and a fit for the e

0:34:56.879,0:35:02.800
edge right and so here we go with the equations for the gated

0:35:02.800,0:35:07.680
graph convolutional networks and again they look terrible because

0:35:07.680,0:35:11.280
it's a notebook so we're going to be using this one right that are a little

0:35:11.280,0:35:16.480
bit prettier all right so before actually reading

0:35:16.480,0:35:23.040
uh these instructions let's read how the the the main let's read the

0:35:23.040,0:35:28.880
initialization part of this module okay so here we can see that we have a few

0:35:28.880,0:35:34.400
matrices we have a b c d and e right so we need those

0:35:34.400,0:35:38.079
matrices and therefore whenever i start my module

0:35:38.079,0:35:40.800
which is going to be just a neural network module from

0:35:40.800,0:35:44.000
PyTorch right we're going to be initializing

0:35:44.000,0:35:50.320
uh four uh different matrices so a b c and d e are n dot linear so

0:35:50.320,0:35:54.880
actually in this case there is also the bias there is not just rotation so these

0:35:54.880,0:35:59.119
are a fine transformations right modules um

0:35:59.119,0:36:02.720
moreover we have a batch normalization for the

0:36:02.720,0:36:06.800
hidden representation and basic normalization for the

0:36:06.800,0:36:13.680
for the edges right uh whenever we do the forward pass we send

0:36:13.680,0:36:19.200
basically uh the g the graph x capital x is going to be the

0:36:19.200,0:36:24.320
collection of all these vertices right so like we have seen in

0:36:24.320,0:36:28.160
the attention module in the attention lesson we had that my

0:36:28.160,0:36:31.680
small x uh we can have like a the set of all

0:36:31.680,0:36:35.119
small x's as represented by the big x right

0:36:35.119,0:36:39.359
uh there is no like it's not a syn sequence it's just a way of

0:36:39.359,0:36:45.359
representing a set right so in this case graphs are made of sets of

0:36:45.359,0:36:49.040
vertices but where i can specify the uh

0:36:49.040,0:36:52.320
relationship now between which vertex is connected to

0:36:52.320,0:36:55.920
which so i have X and then E_X

0:36:55.920,0:36:59.760
right which is the all those edges so we can have like a

0:36:59.760,0:37:03.280
set of uh edges and then we can consider the

0:37:03.280,0:37:06.320
matrix where i have all these columns right

0:37:06.320,0:37:11.119
and so here i'm going to be populating my graph with this representation

0:37:11.119,0:37:18.320
on the g n data i'm going to be defining the variable h which i just

0:37:18.320,0:37:21.520
give all my initial representation and then i'm going to have

0:37:21.520,0:37:27.200
AX, BX, DX and EX which are going to be the matrix

0:37:27.200,0:37:29.599
multiplying all these columns right so you're going

0:37:29.599,0:37:36.400
to get the rotation of all the columns which are simply obtained by passing the

0:37:36.400,0:37:39.760
capital x not the collection of all the x's to my

0:37:39.760,0:37:46.720
matrix A, B, D and E okay whereas C ? C was multiplying

0:37:46.720,0:37:50.800
it was rotating just the edge representation right so we have that

0:37:50.800,0:37:54.400
c is multiplying the edge and then we have

0:37:54.400,0:37:59.040
this uh function here that is the the new function right we don't know about

0:37:59.040,0:38:03.359
this stuff so let's figure out what it is so maybe

0:38:03.359,0:38:06.720
now we have to read what's going on here so in dgl

0:38:06.720,0:38:13.760
the message function are expressed as edge udf user define functions

0:38:13.760,0:38:20.640
edge udf user defined functions take in a single argument edges it has

0:38:20.640,0:38:27.200
three members source destination and data for accessing source node

0:38:27.200,0:38:30.560
features destination node features and edge

0:38:30.560,0:38:34.960
features right so whenever we have this uh edge here

0:38:34.960,0:38:38.880
we're gonna have air presentation living on the edge

0:38:38.880,0:38:42.640
then there is a representation living on the

0:38:42.640,0:38:47.920
source vertex right the incoming vertex i used to call and then we have

0:38:47.920,0:38:51.760
ourselves which is the destination vertex right so we have

0:38:51.760,0:38:55.599
source vertex our edge connecting the source to

0:38:55.599,0:38:58.960
destination and then we have our own destination

0:38:58.960,0:39:01.599
vertex right so you have a representation

0:39:01.599,0:39:05.920
living on the source vertex a representation living on the

0:39:05.920,0:39:08.320
edge and then a representation living on the

0:39:08.320,0:39:13.040
destination and those are x axis if they are

0:39:13.040,0:39:17.200
associated to the first uh layer of my network right are

0:39:17.200,0:39:20.800
gonna be called h if they are associated to the second and

0:39:20.800,0:39:25.440
so on layers of my network right uh so h is my first hidden layer which

0:39:25.440,0:39:32.560
is the second layer of a network right all right so back here uh again so this

0:39:32.560,0:39:36.560
uh edge you the user defined function have a source

0:39:36.560,0:39:41.440
that vj destination the just v and the data leaving on the

0:39:41.440,0:39:48.240
on the edge right all right cool then the reduce function are node

0:39:48.240,0:39:52.240
udfs right user defined functions not the udfs

0:39:52.240,0:39:57.200
user defined functions have a single argument nodes before you had hedge

0:39:57.200,0:40:02.720
edges right so the node acts like on a given node right

0:40:02.720,0:40:09.760
so which has two members data and mailbox so data contains the node

0:40:09.760,0:40:15.040
features and mailbox contains all the incoming messages features

0:40:15.040,0:40:18.960
stacked along the second dimension okay finally

0:40:18.960,0:40:23.200
we have that update all which was

0:40:23.200,0:40:27.680
the function we just seen here the new function

0:40:27.680,0:40:32.240
update all has two parameters message function and reduce

0:40:32.240,0:40:35.839
function send messages through all the edges

0:40:35.839,0:40:41.760
and update all nodes optionally apply a function to update the node features

0:40:41.760,0:40:45.920
after receive this is convenient combination for performing

0:40:45.920,0:40:52.480
send from all the edges the message and then receive for all the nodes

0:40:52.480,0:40:55.760
the radios reduce function right so this is like a

0:40:55.760,0:40:59.359
condensed version and so let's figure out what are

0:40:59.359,0:41:03.200
my message function and reduce function right

0:41:03.200,0:41:06.480
so message function we are going to be first

0:41:06.480,0:41:12.160
extracting the Bx_j so the the edge is going to be

0:41:12.160,0:41:16.400
connecting my vj to my v and so i'm extracting

0:41:16.400,0:41:20.960
here the representation that lives on vj right so my

0:41:20.960,0:41:27.760
Bx_j is going to be the bx associated to my vertex j

0:41:27.760,0:41:35.839
right so this guy here Bx_j cool then i have that my edge

0:41:35.839,0:41:40.240
e_j is going to be the summation of the rotated

0:41:40.240,0:41:47.520
edge of this edge right the rotated source right

0:41:47.520,0:41:56.319
and then the destination um vertex right so here you have

0:41:56.319,0:41:59.680
so you have the edge representation like c

0:41:59.680,0:42:06.160
rotation c c rotation of e x right now we have the d of the source so

0:42:06.160,0:42:11.599
Dx_j and then finally E_x for the destination so which is E_x

0:42:11.599,0:42:16.560
right cool then i actually store

0:42:16.560,0:42:21.040
this e_j in this E so that we're going to be ending up with all the

0:42:21.040,0:42:24.720
representation for later usage because later we're going to

0:42:24.720,0:42:30.079
be using this e_j over here on the bottom right okay so now we have

0:42:30.079,0:42:35.599
computed the message and therefore so after the message

0:42:35.599,0:42:40.319
um after the message is computed we're gonna be calling the reduce function

0:42:40.319,0:42:44.560
and the reduce function finished to compute the update

0:42:44.560,0:42:51.680
formulas right so we have the ax is going to be the ax for my own data

0:42:51.680,0:42:54.720
right so this ax capital x is going to be all the

0:42:54.720,0:42:58.400
vertices and lowercase x is going to be this one right so

0:42:58.400,0:43:02.800
lowercase x then i checked my mailbox right

0:43:02.800,0:43:09.520
so the message function sent a message through the uh through the edge and

0:43:09.520,0:43:13.440
now at the receiving end we get a message right so

0:43:13.440,0:43:16.800
we check the mailbox and we receive this Bx_j.

0:43:16.800,0:43:23.920
Right uh so here we got Bx_j then i also listen and we have the

0:43:23.920,0:43:28.240
representation e_j right so that's coming too then i

0:43:28.240,0:43:32.079
compute the sigmoid here i have the sigmoid for the

0:43:32.079,0:43:38.640
incoming edge right so the sigmoid of the incoming edge and then

0:43:38.640,0:43:42.319
all we have to do now is going to be having that my

0:43:42.319,0:43:45.920
h this is going to be my rotated x right so ax

0:43:45.920,0:43:50.800
is the rotated myself the vertex representation of my own

0:43:50.800,0:43:58.240
and then i have to sum right over all the incoming edges of my gate

0:43:58.240,0:44:01.359
which is multiplying my incoming representation

0:44:01.359,0:44:04.960
incoming rotated representation and then we divide

0:44:04.960,0:44:11.520
by all those sigmas right all those sigmoids sigmoids which is

0:44:11.520,0:44:16.160
this one right so we multiply sigma to this bx

0:44:16.160,0:44:20.160
and then we divide by all all of them right by the sum of a lot of them and

0:44:20.160,0:44:24.079
then we sum uh all these guys right so we have the

0:44:24.079,0:44:28.240
summation of this scaled

0:44:28.240,0:44:31.599
Bj which is then also normalized by the sum

0:44:31.599,0:44:35.359
of all the sigmas and that's it so we have now

0:44:35.359,0:44:38.880
the lower a h which is gonna be uh written

0:44:38.880,0:44:46.640
in the big uh container of x's h's and so that's how we uh

0:44:46.640,0:44:49.760
write down these three equations right four questions

0:44:49.760,0:44:52.800
well three right we haven't seen this last one

0:44:52.800,0:44:58.720
so what else so now we can retrieve h right because we have just updated all

0:44:58.720,0:45:02.560
the representation which has been computed here and

0:45:02.560,0:45:06.800
returned there uh then we can also get new the new

0:45:06.800,0:45:11.280
edges right because we we wrote the edge information here right

0:45:11.280,0:45:14.079
so here we were writing the new edge information

0:45:14.079,0:45:21.280
and here we've been writing the new x h information so we retrieve the new h

0:45:21.280,0:45:25.040
and the new e we divide by the square root of the

0:45:25.040,0:45:29.040
size such that things don't change with the

0:45:29.040,0:45:33.680
size of the um of the of the hidden representation this is

0:45:33.680,0:45:39.280
just you know technicality but it allows you to have like uh a consistent

0:45:39.280,0:45:43.119
uh scaling factor like we have seen last week during the

0:45:43.119,0:45:47.200
uh set to set not the attention we were dividing by the square root of the

0:45:47.200,0:45:51.839
dimensions such that the soft arc max was behaving uh similarly

0:45:51.839,0:45:55.680
right regardless of the um of the dimension

0:45:55.680,0:46:01.680
right so we don't change the temperature then we apply a batch normalization such

0:46:01.680,0:46:05.440
that we get nice gradients and doesn't overfit

0:46:05.440,0:46:07.839
and you know all the nice things that batch normal

0:46:07.839,0:46:13.280
gives us finally we apply the nonlinearity right the

0:46:13.280,0:46:20.880
h the plus so we compute this non-linearity for this one and this one

0:46:20.880,0:46:27.200
and then we can write that my new h so the representation for the first

0:46:27.200,0:46:29.920
hidden layer so my second layer is going to be my

0:46:29.920,0:46:36.000
input x plus h right so we have the input x plus this

0:46:36.000,0:46:39.599
guy here this little positive part

0:46:39.599,0:46:43.119
and the same we're going to have the e representation is gonna be my initial

0:46:43.119,0:46:47.920
representation plus this new e finish and we return

0:46:47.920,0:46:53.760
h and e uh i have a multiplier perceptron and then here i have this

0:46:53.760,0:47:00.160
stack of layers so here i just call my gated cn gcn

0:47:00.160,0:47:04.560
and so you can see all these matrices but again we don't care

0:47:04.560,0:47:10.800
um some stuff for collecting an accuracy computation okay let's test the forward

0:47:10.800,0:47:16.000
pass so how do we test the forward pass here i just define my my data and then i

0:47:16.000,0:47:20.240
have my batch of x's is going to be the data that leaves on

0:47:20.240,0:47:24.640
the vertices okay so my x is going to be the data that lists on

0:47:24.640,0:47:28.079
the vertices and my e is going to be the data that lives on

0:47:28.079,0:47:32.800
the on the on the edges these are all ones and these are just

0:47:32.800,0:47:35.040
the degree [Music]

0:47:35.040,0:47:38.559
so i i'll show you a few a few of these values

0:47:38.559,0:47:44.720
yes again that your ease will be quote unquote all once

0:47:44.720,0:47:49.200
just for the first uh first layer or the first step first

0:47:49.200,0:47:54.400
uh sorry what's the word that stack and the rest of them you will

0:47:54.400,0:47:56.720
pass the outputs of the previous yeah yeah yeah yeah yeah

0:47:56.720,0:48:00.000
absolutely yes so these are the input values right so my

0:48:00.000,0:48:04.160
my graph which is the domain has i i put something some signal

0:48:04.160,0:48:07.440
on at the beginning which is kind of arbitrary right now

0:48:07.440,0:48:10.960
uh for the nodes i put how many input connections i have

0:48:10.960,0:48:15.359
and for the edges i just put one and then you have several layers of this

0:48:15.359,0:48:20.720
uh graph convolutional net uh like here so you have this gated graph

0:48:20.720,0:48:25.359
convolutional net has a few a few layers right so if you

0:48:25.359,0:48:29.839
have l which is the number of layers you're gonna have as many uh graph

0:48:29.839,0:48:33.280
convolutional network layers that are the one i showed you before

0:48:33.280,0:48:39.200
as the numbers you know as as l right and so you you stuck several of these

0:48:39.200,0:48:41.440
layers and at the beginning you have this

0:48:41.440,0:48:45.680
degree and all ones and then as you have multiple stacks you start

0:48:45.680,0:48:50.800
you know having some representation that evolves right makes

0:48:50.800,0:48:53.280
sense right yes no is would that kind of be like

0:48:57.839,0:49:01.040
the value of e would kind of be like the weight

0:49:01.040,0:49:07.440
of the edge sort of is that right the value on e is the

0:49:07.440,0:49:12.240
uh representation right so we e has like a

0:49:12.240,0:49:17.280
he e here uh okay right now it's just one right but later on in the

0:49:17.280,0:49:20.960
in the following up layers this is gonna have a vector

0:49:20.960,0:49:24.800
and this vector basically allow you to tune this gate

0:49:24.800,0:49:30.960
for this incoming message let's finish the notebook okay otherwise

0:49:30.960,0:49:35.839
we don't finish the notebook and then i can answer every question you have all

0:49:35.839,0:49:40.880
right so i show you here um like this dgl graph

0:49:40.880,0:49:44.720
which had these features these features are going to be my input

0:49:44.720,0:49:52.480
i have in this case 133 nodes and 739 edges how how many are what is the

0:49:52.480,0:50:00.880
maximum number of edges i can have you're following

0:50:01.599,0:50:08.800
133 square right uh divided by two yeah

0:50:08.800,0:50:15.839
on the order of 133 square right uh cool all right so let's execute

0:50:15.839,0:50:20.640
this one so we see at the beginning the network

0:50:20.640,0:50:27.680
doesn't uh doesn't cannot really classify this uh cannot classify

0:50:27.680,0:50:29.839
correctly and this is just a stupid thing

0:50:29.839,0:50:32.720
so let's actually train now let's actually figure out how to train this

0:50:32.720,0:50:36.079
network so i have my j my my objective function

0:50:36.079,0:50:42.400
which is going to be the cross entropy of the you know the the scores

0:50:42.400,0:50:46.640
the bad scores and the batch labels right so these are what my network

0:50:46.640,0:50:50.240
uh tells me right this is batch scores the logits

0:50:50.240,0:50:53.359
and then i have the the labels those are the the original

0:50:53.359,0:50:57.440
the original labels for the for the graphs

0:50:57.440,0:51:00.480
and then this is actually it ran fine so we it

0:51:00.480,0:51:03.599
everything was working we have the forward pass

0:51:03.599,0:51:08.079
you know loss computation zero graph backward optimizer step

0:51:08.079,0:51:11.920
and so we define here a training function which is exactly the same as we

0:51:11.920,0:51:17.280
have seen all the time let me run this line as well

0:51:17.280,0:51:20.400
so training function we exactly know everything

0:51:20.400,0:51:24.640
right so x's are the data the features on the

0:51:24.640,0:51:28.880
nodes on the vertices e is going to be the features on the edges

0:51:28.880,0:51:32.960
the batch scores the logits are basically the output of my model

0:51:32.960,0:51:37.359
the j the objective function is going to be the cross entropy between the logics

0:51:37.359,0:51:41.760
and the targets then you optimize the you you clear up the gradients

0:51:41.760,0:51:46.400
you compute backwards and then you step right that's those these are the five

0:51:46.400,0:51:51.119
steps one two three four five steps

0:51:51.119,0:51:57.200
finish evaluation the same without the updating of the parameters

0:51:57.200,0:52:00.640
so here we just have the training data set and the

0:52:00.640,0:52:06.400
testing data set and we can check what's the progress here so far

0:52:06.400,0:52:11.359
so here i just show you the uh training and the testing accuracy

0:52:11.359,0:52:19.839
oh sorry let's put 40 epochs maybe and so let's see whether it works or not

0:52:26.480,0:52:33.839
it's getting better and yep accuracy is starts to

0:52:43.680,0:52:51.040
the accuracy starts to grow test still low okay it's getting up as well

0:52:51.040,0:52:55.680
yeah there we go convergence yes yes so again all we want to think about

0:52:59.040,0:53:02.559
right uh if you think about from the perspective of the

0:53:02.559,0:53:06.960
the attention we have a set of values right and in attention we didn't have

0:53:06.960,0:53:09.440
any kind of connection between these values it's

0:53:09.440,0:53:13.760
just a set everything looks at everyone right so the attention

0:53:13.760,0:53:16.079
you have to check everything that is going on

0:53:16.079,0:53:20.720
because you have no idea which one should be uh looking at what

0:53:20.720,0:53:24.160
in this case we okay that's the main point right

0:53:24.160,0:53:27.200
what did xavier said yesterday the main point

0:53:27.200,0:53:32.319
is the sparsity in the agency metrics right because the sparsity

0:53:32.319,0:53:35.599
gives you structure and structure is the number one

0:53:35.599,0:53:39.440
that are telling you who's connected with whom so if everyone is connected

0:53:39.440,0:53:43.200
with everyone you get everything one right everywhere

0:53:43.200,0:53:46.000
uh see it's converging here so if you have

0:53:46.000,0:53:49.359
everyone looking at everyone your agency metrics is gonna be just

0:53:49.359,0:53:54.160
a matrix with all ones um if you have you know just a few uh

0:53:56.640,0:54:00.960
vectors that are connected to each other then you get you know some uh some

0:54:00.960,0:54:05.119
sporadic ones right so you're gonna get a uh you

0:54:05.119,0:54:08.559
know a sparse matrix okay this stuff was going to 100

0:54:08.559,0:54:12.240
accuracy before i guess i should set a seed such that i

0:54:12.240,0:54:17.680
can show you better better trials um

0:54:17.680,0:54:21.680
that was pretty much everything from it so really there is no much

0:54:21.680,0:54:25.200
big deal i think at least from this first

0:54:25.200,0:54:29.680
perspective and what i learned in this past week about these networks

0:54:29.680,0:54:33.359
um are there questions all right i can take questions right now i mean i didn't

0:54:33.359,0:54:35.760
want to to take forever to finish the class

0:54:35.760,0:54:39.839
otherwise if people have to leave they can't it's

0:54:39.839,0:54:45.119
also i am nine minutes over so i'm also not on time but you know

0:54:45.119,0:54:51.440
better than worse yes so what exactly did we

0:54:51.440,0:54:56.880
predict here yeah no we had the classes the seven classes of graphs in the

0:54:56.880,0:55:03.599
beginning um so so what are the classes

0:55:04.400,0:55:07.599
these are the classes and then i'm going to be generating

0:55:07.599,0:55:15.200
down here i'm generating my uh where is the training data set

0:55:15.680,0:55:24.240
i can't see how long train train train [Music]

0:55:24.240,0:55:32.079
here so training data set it creates um this data set of 350 graphs

0:55:32.079,0:55:38.319
that have anything between 10 to 20 vertices each and then

0:55:38.319,0:55:44.160
they can be anything uh of those eight classes we have seen

0:55:44.160,0:55:48.400
before okay it can be uh anything like let me

0:55:48.400,0:55:55.920
zoom here so it can be class seven six five four three two and one okay

0:55:55.920,0:55:59.520
and zero so you can have any of these one and now

0:55:59.520,0:56:04.319
now you're asking this this stuff has a variable number of vertices right

0:56:04.319,0:56:07.359
and now you're asking your question in your uh

0:56:07.359,0:56:11.359
your network which net which type of graph did i give you

0:56:11.359,0:56:17.760
right and so our convolutional graph convolutional network tells you

0:56:17.760,0:56:22.240
which type of graph you're looking at right so it's doing

0:56:22.240,0:56:27.119
basically a classification of your urgency metrics right which is

0:56:27.119,0:56:32.720
specifying the connectivity of these vertices

0:56:32.880,0:56:37.119
so about the so the train set is a set of

0:56:37.119,0:56:43.599
small graphs right and the batch size there is 50 so that's

0:56:43.599,0:56:46.240
50 graphs of varying sizes varying number

0:56:46.240,0:56:50.319
of nodes from 10 to 20 yes it's not necessary that each batch

0:56:50.319,0:56:53.440
should have graphs with the same number of nodes uh

0:56:53.440,0:56:57.359
that's what's done inside that's what was it was done in terms of

0:56:57.359,0:57:02.319
uh from dgl for for giving you you know uh speed in

0:57:02.319,0:57:08.640
in training right so but that's that's that's that's done uh behind the

0:57:08.640,0:57:13.119
behind your back right uh it's the same as you when you train for language model

0:57:13.119,0:57:16.240
right so you want to batch all the sentences with similar

0:57:16.240,0:57:20.559
lengths such that you don't waste computation so it's the similar way in a

0:57:20.559,0:57:22.720
similar way you can do here as well right

0:57:22.720,0:57:26.319
but what did the what was the dimensions of the output

0:57:26.319,0:57:33.200
look like um here so my

0:57:33.200,0:57:39.359
yeah so my here i have an mrp that goes from hidden hidden dimension

0:57:39.359,0:57:43.760
of the whatever thing to my output dimension so what is the

0:57:43.760,0:57:47.920
output dimension let's go figure out out the output

0:57:47.920,0:57:52.160
dimension number eight right so eight are the possible classes

0:57:52.160,0:57:56.400
therefore i will give you eight you know eight vector

0:57:56.400,0:58:00.799
a logic of dimension eight uh finally whenever you have this

0:58:00.799,0:58:03.920
logic of dimension eight so it's just a classifier right

0:58:03.920,0:58:08.720
you plug these inside the cross entropy uh

0:58:08.720,0:58:15.680
here loss of my logics against my labels and this loss is defined

0:58:15.680,0:58:20.079
down here is my cross entropy which expects logics

0:58:20.079,0:58:24.400
and then computes you know the final score

0:58:24.400,0:58:31.119
and then you just run back propagation so did every node

0:58:31.119,0:58:37.760
have a like a logit um and the label each node

0:58:37.760,0:58:42.240
has the same label which corresponds to the class of the overall graph or is

0:58:42.240,0:58:47.119
that not how that works each graph has a

0:58:47.119,0:58:50.480
vector of logics right so you want to classify

0:58:50.480,0:58:54.640
graphs different graphs so you provide these graphs

0:58:54.640,0:58:58.720
to the network and these graphs have arbitrary structure

0:58:58.720,0:59:02.799
right they don't have a finite number of vertices

0:59:02.799,0:59:05.920
so you have you know let's say you have like

0:59:05.920,0:59:09.599
10 graphs uh and each of them are like you have

0:59:09.599,0:59:16.240
a graph of size 5 size 10 size 15 size 20. so you have sets

0:59:16.240,0:59:20.880
with different number of vertices and a specific connection between these

0:59:20.880,0:59:25.359
vertices so given these variable length

0:59:25.359,0:59:30.079
sets you have to specif and you specify you you tell the connection between

0:59:30.079,0:59:34.160
these vertices you ask your network to tell you give me

0:59:34.160,0:59:38.000
a logit vector where you're going to be

0:59:38.000,0:59:42.160
showing me basically what graph this belongs to

0:59:42.160,0:59:47.280
right which family it belongs to so 8 eight are the possibilities you

0:59:47.280,0:59:53.040
have you know circle star blah blah blah each input that input graph will be

0:59:53.040,0:59:56.160
mapped towards one specific of this guy right

0:59:56.160,1:00:00.559
so you just do a classification of the type of graph

1:00:00.559,1:00:04.000
but the point is that this graph have a variable number of

1:00:04.000,1:00:09.599
vertices okay so you have to basically uh uh

1:00:09.599,1:00:13.200
query somehow the the the structure right the

1:00:13.200,1:00:17.040
graph convolutional net has to intrinsically

1:00:17.040,1:00:23.200
extract what is the typology of connectivity you

1:00:23.200,1:00:30.400
provide okay cool thank you we are using this example

1:00:30.400,1:00:32.799
that was shown was classification of graphs

1:00:32.799,1:00:37.040
yeah but also they can be a use care mostly there would be a use case in real

1:00:37.040,1:00:41.040
world where we have one single graph where each node represents a

1:00:41.040,1:00:44.240
particular entity and we need to classify those entities

1:00:44.240,1:00:47.680
so then how do we do that like take some part of a graph

1:00:47.680,1:00:52.640
and drain the model over that and then redress so so so okay this one

1:00:52.640,1:00:55.680
here you get uh you have like different graphs and

1:00:55.680,1:00:58.720
then at the end you're gonna be getting them all together right so

1:00:58.720,1:01:04.319
whenever you have the uh gated cnn here you at the end

1:01:04.319,1:01:10.559
you you have like forward you get this uh gc this part here right and you have

1:01:10.559,1:01:15.040
mean nodes right so dgl y you get the mean of

1:01:15.040,1:01:22.000
all the nodes and then you apply the mlp on this mean representation uh this is

1:01:22.000,1:01:24.319
for this classification of the whole graph

1:01:24.319,1:01:27.760
right uh but then if you'd like to do the

1:01:27.760,1:01:31.920
other stuff right which is gonna be you you don't have this function right

1:01:31.920,1:01:37.040
you just don't have this line and you're gonna be applying like a a

1:01:37.040,1:01:42.640
vector like a logic vector like out of each vector right so each

1:01:42.640,1:01:47.440
vertice each vertice you're gonna have a logic

1:01:47.440,1:01:50.720
a logic for like a vector logic for each vertex

1:01:50.720,1:01:56.559
a vertex vertex right it's called yes yeah okay cool so you

1:01:56.559,1:01:58.640
have a logic per vertex and then you do the

1:01:58.640,1:02:05.200
uh training on each of these guys right and so that would be the um for example

1:02:05.200,1:02:09.039
if there is a if you go to dgl

1:02:09.039,1:02:14.640
d dgl.ai this class took really a lot of effort

1:02:14.640,1:02:20.000
guys tutorial uh this one

1:02:20.000,1:02:24.480
tutorial now what is it there is a come on

1:02:29.200,1:02:31.839
okay here yeah cool so in this case they are doing

1:02:35.920,1:02:41.680
um they are doing classification of the nodes okay this is

1:02:41.680,1:02:46.319
the second type of thing so in this case you have a

1:02:46.319,1:02:50.400
karate karate club i'm not sure if you're familiar

1:02:50.400,1:02:54.079
and there is like the instructor number zero and then there is the

1:02:54.079,1:03:00.799
uh the manager number three 33. and then these edges represent uh

1:03:00.799,1:03:07.440
what are the um basically the uh interaction outside the club right in

1:03:07.440,1:03:10.960
in real life right so number four you know interacts a lot with the

1:03:10.960,1:03:15.119
instructor outside the club and number 26 interacts a lot with

1:03:15.119,1:03:19.200
number with the manager outside the club so you only

1:03:19.200,1:03:24.240
have two labels you have instructor and manager and now you'd like to get a

1:03:24.240,1:03:27.839
label for all these other nodes all these are the

1:03:27.839,1:03:30.799
vertices so whenever you do the training part

1:03:30.799,1:03:35.280
this is called semi supervised learning because you only have a few labels right

1:03:35.280,1:03:38.240
so whenever you train this stuff you're going to have

1:03:38.240,1:03:42.799
you know okay you have your comp net not your graph content

1:03:42.799,1:03:46.720
which is outputting a number of classes for each

1:03:46.720,1:03:50.720
vertex right so each vertex has the the full logic

1:03:50.720,1:03:53.920
but then during training you have here that

1:03:53.920,1:03:57.359
the the training is going to be the uh you get the logic

1:03:57.359,1:04:00.559
these are logics for every vertex but then when you

1:04:00.559,1:04:04.319
compute the final loss which is the negative log likelihood

1:04:04.319,1:04:11.039
you only select the labels that you have which is label for

1:04:11.039,1:04:14.960
the 33 and the what are the two guys right where so you

1:04:14.960,1:04:19.039
have 0 and 33. these are the only two uh labels

1:04:19.039,1:04:22.480
you have and so there is a vector which is called

1:04:22.480,1:04:27.200
0 into 33 33 somewhere yeah here this is my label

1:04:27.200,1:04:32.720
nodes they are just these two guys and so in my uh training laws here

1:04:32.720,1:04:38.720
you only select the two nodes that are that have a label you enforce those

1:04:38.720,1:04:42.799
variables to be this one right and then you train classical stuff right

1:04:42.799,1:04:47.359
of zero grad backprop optimize step and these allow

1:04:47.359,1:04:51.119
you to basically propagate through out the

1:04:51.119,1:04:54.240
whole network structure the whole graph

1:04:54.240,1:04:58.880
structure what is this information that has to come out from the logics of two

1:04:58.880,1:05:02.079
specific vertices right so you have several layers stack of

1:05:02.079,1:05:07.200
convolutional graph convolutional nets and then you

1:05:07.200,1:05:11.200
enforce you know those two vertices to output

1:05:11.200,1:05:16.079
that specific label uh and then you backprop and then

1:05:16.079,1:05:19.680
basically all this information propagates through network

1:05:19.680,1:05:23.039
which propagates a kind of representation

1:05:23.039,1:05:27.280
across this domain and it shows you

1:05:27.280,1:05:32.079
if you do a plotting uh this is at the beginning so this is the representation

1:05:32.079,1:05:38.240
or of the vertices um without training and then after you train after a few

1:05:38.240,1:05:42.319
epochs you can see how this representation gets attracted

1:05:42.319,1:05:45.520
right so 0 and 33 gets pulled away such that

1:05:45.520,1:05:49.680
they are linearly classified

1:05:49.680,1:05:54.720
uh when they are you know easy told apart and then these are basically

1:05:54.720,1:05:58.880
dragging uh vertices close to them based on

1:05:58.880,1:06:02.960
basically on the number of connections they have sort of right

1:06:02.960,1:06:06.400
and that's how you do uh classification on

1:06:06.400,1:06:10.240
on vertices rather than classification of

1:06:10.240,1:06:13.680
graphs right maybe yeah yesterday we didn't quite mention

1:06:13.680,1:06:18.160
how you apply uh these things but again like xavier like myself

1:06:18.160,1:06:23.920
are not maybe too interested in the um application part but perhaps more in the

1:06:23.920,1:06:29.039
in the algorithmic part did i answer your question oh that makes

1:06:29.039,1:06:34.000
sense yeah okay awesome at least i make sense sometimes

1:06:34.000,1:06:36.720
more questions no you're done you're fed up

1:06:41.039,1:06:46.640
everybody else left huh there are 18 people

1:06:46.640,1:06:50.079
yeah dinner i'm hungry my roommate just ate my dinner like

1:06:50.079,1:06:58.839
the hell these two weeks were crazy i really worked a lot all right

1:06:58.839,1:07:02.160
peace bye-bye all right so this was like i think

1:07:05.359,1:07:11.920
one of my most intensive lectures so far um i prepared like in

1:07:11.920,1:07:16.000
under one week and i think i may have covered

1:07:16.000,1:07:22.319
a lot of materials so it's totally reasonable to be like somehow

1:07:22.319,1:07:26.559
overwhelmed at the moment but then how can you actually

1:07:26.559,1:07:30.559
squeeze out everything out of this video right so there are a few steps

1:07:30.559,1:07:34.000
i would highly recommend you to follow starting with

1:07:34.000,1:07:37.039
you know comprehensions issues right again

1:07:37.039,1:07:40.319
i might have been confusing i have also re-recorded

1:07:40.319,1:07:43.359
a few new chunks because i messed up in class

1:07:43.359,1:07:46.720
so if you have any question i have not yet addressed

1:07:46.720,1:07:50.559
just type it down in the comment section below this video

1:07:50.559,1:07:54.480
moreover if you'd like to follow up with me on the latest news about

1:07:54.480,1:07:58.160
teaching and machine learning and very cute and pretty things

1:07:58.160,1:08:01.599
just follow me on twitter i will you know

1:08:01.599,1:08:08.720
talk about the latest news over there moreover if you'd like to be always up

1:08:08.720,1:08:11.599
to date with my latest content i would recommend you

1:08:11.599,1:08:16.319
to subscribe to my channel and click on the notification bell such

1:08:16.319,1:08:21.359
that you don't miss any new video if you like this video

1:08:21.359,1:08:24.960
you know don't forget to press the like button

1:08:24.960,1:08:31.440
i really appreciate that searching so each of these videos have a

1:08:31.440,1:08:38.560
english transcription we have a japanese spanish italian turkish mandarin korean

1:08:38.560,1:08:41.920
translations as well available for you if english is not your

1:08:41.920,1:08:45.759
primary language and again if you'd like to help out in the translation

1:08:45.759,1:08:51.199
process please feel free to contact me by email or on twitter moreover

1:08:51.199,1:08:54.400
you should really really really take the time to go over

1:08:54.400,1:09:00.319
the notebook we have covered today uh and check every line of code right so

1:09:00.319,1:09:04.319
because there are many things i may have you know not have spent

1:09:04.319,1:09:08.560
enough time today just for sake of you know keeping this

1:09:08.560,1:09:12.880
video under a within a specific time amount

1:09:12.880,1:09:16.400
but then you should really go through every line and deeply understand what's

1:09:16.400,1:09:20.640
going on and if you find a typo or error a bug like many of you

1:09:20.640,1:09:25.040
have already found do please report it on github and if you

1:09:25.040,1:09:27.839
feel inclined just you can also send a pull

1:09:27.839,1:09:34.319
request by fixing this error okay that's great because we can you know all

1:09:34.319,1:09:38.239
benefit from your contribution and you also get some value

1:09:38.239,1:09:42.159
out of you know getting your hands dirty right with the code

1:09:42.159,1:09:45.679
and finally you're going to be helping me and the whole machine

1:09:45.679,1:09:50.480
deep learning community that are using this material

1:09:50.480,1:09:54.480
and that was pretty much it thanks again for sticking with me

1:09:54.480,1:09:58.719
and as i've been told to say like share and subscribe

1:09:58.719,1:10:03.360
see you next time bye bye

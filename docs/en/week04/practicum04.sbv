0:00:00.030,0:00:04.730
so since last time, okay, welcome back,
thank you for being here.

0:00:04.730,0:00:09.059
Last time Yann was using the tablet,
right? and how can you use the tablet and I

0:00:09.059,0:00:13.040
don't use the tablet, right? So I should
be as cool as Yann at least I think.

0:00:13.040,0:00:18.900
One more thing to begin with, there is a
spreadsheet where you can decide whether

0:00:18.900,0:00:22.890
you'd like to join the Slack channel
where we collaborate over making

0:00:22.890,0:00:28.529
some drawings for the website, fixing
some mathematical notations, having some

0:00:28.529,0:00:32.640
kind of, you know, fixing the error in
English in the English grammar or

0:00:32.640,0:00:37.290
whatever, so if you if you're interested
in helping in improving the content of

0:00:37.290,0:00:42.450
this class, feel free to fill out the
spreadsheet, okay? We are already a few of

0:00:42.450,0:00:49.789
us on the Slack channel, so I mean, if you
want to join, you're welcome. So

0:00:49.789,0:00:53.399
instead of writing on the whiteboard
because it's impossible to see I think

0:00:53.399,0:01:00.270
from the upper side we're gonna be going
to experiment with a new toy here. All

0:01:00.270,0:01:03.930
right.
First time, so you know, I'm a little bit

0:01:03.930,0:01:11.250
tense. Last time I screw up with a
notebook, so okay, alright. So we're gonna

0:01:11.250,0:01:15.659
be starting with a small review
about linear algebra. I hope I'm not

0:01:15.659,0:01:20.670
offending anyone, I'm aware that you
already taken linear algebra and you're

0:01:20.670,0:01:25.920
very strong in it but nevertheless I'd
like to provide you my intuition, my

0:01:25.920,0:01:31.320
perspective, okay? it's just one
slide, not too much, so maybe you'd like

0:01:31.320,0:01:36.290
to take out some paper and pen or you
can just you know, whatever, follow along.

0:01:36.290,0:01:49.850
So this one is going to be 
linear algebra review.

0:01:51.170,0:02:06.450
Okay. Am I waiting a little bit? Let me
wait for a sec. Ready?? Yes? No? shake your

0:02:06.450,0:02:12.150
head. Okay fantastic, alright so we were
talking last time we had a neural

0:02:12.150,0:02:15.270
network with the input on the bottom
then we had an affine

0:02:15.270,0:02:18.510
transformation, then we have a hidden
layer right. So I'm gonna just write the

0:02:18.510,0:02:23.280
first equation. We're gonna have that my
hidden layer, and since I'm writing

0:02:23.280,0:02:26.970
with a pen, can you see anything? yeah?
so since I'm writing with a pen I'm

0:02:26.970,0:02:30.360
going to be putting a underscore
underneath the variable in order to

0:02:30.360,0:02:35.120
indicate it's a vector. Okay? that's how I
write vectors. So my H is gonna be a

0:02:35.120,0:02:42.660
nonlinear function f applied to my z and
z is going to be my linear input,

0:02:42.660,0:02:46.230
the output of the
affine transformation, so in this case

0:02:46.230,0:02:54.989
I'm gonna be writing here z is going to
be equal to my matrix A times x. We

0:02:54.989,0:03:00.630
can imagine there is no bias in this
case, it's generic enough because we can

0:03:00.630,0:03:06.030
include it bias inside the matrix and
have the first item of the x equal to 

0:03:06.030,0:03:18.360
1. So if this x here belongs to R n and this
z here belongs to R m, first question:

0:03:18.360,0:03:24.989
what is the size of this matrix? Okay,
fantastic, so this matrix here is

0:03:24.989,0:03:30.360
gonna be our m times n, you have as many
rows as the dimension where you shoot to

0:03:30.360,0:03:34.200
and you have as many columns as the
dimension where you're shooting from, ok?

0:03:34.200,0:03:39.930
All right so let's expand this one, 
so this matrix here is gonna be equal to

0:03:39.930,0:03:49.549
what do you have a_{1,1} a_{1,2} so on until
the last one, which is gonna be? shout.

0:03:49.549,0:03:56.269
Thank you, 1...?
yeah 1 n then you have second one's

0:03:56.269,0:04:04.249
gonna have a_{2,1} a_{2,2} so on until the
last one which is a{2,n}, right? and then you

0:04:04.249,0:04:14.840
keep going down until the last one which
is going to be? what are the indexes? m 1,

0:04:14.840,0:04:24.740
right? Okay so you have a_{m,1}, a_{m,2} and
so on until a_{m,n}, okay thank you. 

0:04:24.740,0:04:37.220
And then we have here our x, right?
so you have x_1, x_2 and so on until x_n,

0:04:37.220,0:04:39.490
right?

0:04:41.680,0:04:47.479
You're more responsive than last year,
good, thank you. Alright, so we can also

0:04:47.479,0:04:52.099
rewrite this one in different ways. So
the first way I'm gonna write this one

0:04:52.099,0:05:00.020
is gonna be the following, so I'm gonna
have here these a 1 then I'm gonna have

0:05:00.020,0:05:12.409
here my a 2 and then I have the last one
which is gonna be my a n, ok? and then

0:05:12.409,0:05:15.800
here I'm gonna be multiplying this by a
column vector, right? so my column vector

0:05:15.800,0:05:23.409
I'm gonna be write it like this.
Alright, so what is the output of this operation?

0:05:23.409,0:05:28.820
so these are metrics, you have a vector,
the outcome is going to be a? vector. So

0:05:28.820,0:05:35.780
what is gonna be the first item of my
vector? I don't use dots because I'm not

0:05:35.780,0:05:40.219
a physicist, actually I am but we are
doing linear algebra, so what should I

0:05:40.219,0:05:42.400
write?

0:05:43.810,0:05:47.509
Alright, so that's already transpose
because those are a row vector so I just

0:05:47.509,0:05:54.979
write a I'm gonna just mean just right
right so I have a 1 x ok so there is no

0:05:54.979,0:06:02.780
transposition here, no dots around and so
on. Second element is gonna be? a 2, okay,

0:06:02.780,0:06:12.970
x and then until the last one which is
gonna be? okay, there's no dot but sure.

0:06:12.970,0:06:17.810
Like someone is calling that dot product
instead of the cross product but that is

0:06:17.810,0:06:20.389
assumes like you use a different kind of
notation.

0:06:20.389,0:06:26.710
All right so and this is gonna be my, so
how many elements does this vector have?

0:06:26.710,0:06:35.120
m okay so we have z 1, z 2 and so on
until z m and this is my final set, right?

0:06:35.120,0:06:39.710
my vector z, okay? Fantastic. So now
we are going to be focusing a little bit

0:06:39.710,0:06:48.590
about the meaning of this thing over
here okay, other questions so far?

0:06:48.590,0:06:54.530
all right this is very trivial so far, I
hope, I mean let me know if if not, okay

0:06:54.530,0:07:00.140
so let's analyze one of these guys
here so I'd like to figure out what is

0:07:00.140,0:07:10.970
the meaning of writing a T times x right
so my a T is gonna be my generic a i so

0:07:10.970,0:07:24.680
let's assume in this case when n equals 2
okay? So what is a T x? so a T x is going

0:07:24.680,0:07:29.780
to be equal to what? so let me draw here
something so that is gonna be easier for

0:07:29.780,0:07:37.720
you to understand. So this one is gonna
be my a, these gonna be my alpha and

0:07:37.720,0:07:48.620
then here you have like it's gonna be my
x and is this gonna be here my xi, so

0:07:48.620,0:07:57.430
what is the output of this
product here? This one here.

0:07:58.210,0:08:08.470
Say again, sorry. A transpose, let's
call it, let's say that is a row vector

0:08:10.030,0:08:27.880
Can you see? no? what is gonna be the
output of this operation here? no, why?

0:08:29.410,0:08:34.610
This is this is like a generic one of
the many a's, right? there are m a's,

0:08:34.610,0:08:39.169
this is one of those m a's, so I'm
multiplying one of those a's times my x

0:08:39.169,0:08:43.310
right let's assume there are only two
dimensions, so what is gonna be the

0:08:43.310,0:08:50.660
output of this scalar product? can
someone tell me? no no no like normal

0:08:50.660,0:09:02.720
scalar product. Hold on
so you have a here, you have

0:09:02.720,0:09:08.510
here this part here is gonna be a 1 it
is gonna be a 2 okay? then you have

0:09:08.510,0:09:18.550
here x 1 and now you have x 2, right? so how
do you express this scalar product here?

0:09:19.980,0:09:26.730
okay, so I'm just writing, let me know if
it's all clear, so I'm gonna write here: 

0:09:28.890,0:09:37.780
a 1 times x 1 plus a 2 times x 2, right?
this is the definition. Clear, right? So

0:09:37.780,0:09:44.830
far, yeah? no? okay, yeah, question yeah. That
is meant to be a transpose. A row

0:09:44.830,0:09:49.600
times column vector so let's assume a is
a column vector then I have row times

0:09:49.600,0:10:02.230
column. So let's keep writing this
stuff here so what is a 1? how can I

0:10:02.230,0:10:15.220
compute a 1? Say again? ok ok.
So I'm gonna write here the a 1 is going

0:10:15.220,0:10:20.770
to be the length of vector a times
cosine alpha, then what about x 1? someone

0:10:20.770,0:10:30.010
else. The same right? No, wait, what? what
is x 1? same thing, right, different

0:10:30.010,0:10:37.450
letters. so someone say something. You're
following, you're completely confused,

0:10:37.450,0:10:43.210
you're not having any idea, it's too easy?
I have no idea what's going on here. It's

0:10:43.210,0:10:53.770
good right? so far ok? what's gonna be the
second term? This one is gonna be x

0:10:53.770,0:11:00.430
here, right? times cos xi right, and then you
had the second term which is going to be

0:11:00.430,0:11:13.920
what? magnitude of a... shout, I can't hear. Ok
sine of alpha and then

0:11:17.660,0:11:21.290
okay thank you okay

0:11:23.930,0:11:27.870
all right I'm gonna be just putting
together those two guys so you're gonna

0:11:27.870,0:11:40.370
get equal magnitude of a times magnitude
of x times cosine alpha cosine Xi plus

0:11:40.370,0:11:48.210
sine alpha and sine, cosine, sine Xi
I'm sorry.

0:11:48.210,0:11:54.210
what is the stuff in the parentheses? all
right so it's the cosine of the

0:11:54.210,0:11:57.240
difference of the two angle right?
everyone knows trigonometry here, right?

0:11:57.240,0:12:05.160
so, high school stuff so this one is
gonna be equal to a x times the cosine

0:12:05.160,0:12:11.000
of cos xi minus alpha, right? or the
other way around, alpha minus xi.

0:12:11.000,0:12:17.040
So what does this mean? You can think
about each element. so far is clear? I

0:12:17.040,0:12:22.680
didn't do any magic yeah shake your head
like this for yes, this for no, this

0:12:22.680,0:12:27.810
for maybe, no something's working. Okay so
you can think about whenever you

0:12:27.810,0:12:33.600
multiply a matrix times a vector that
basically each output of this operation

0:12:33.600,0:12:39.570
is gonna be measuring, so okay hold on,
what is this cosine? how much is cosine

0:12:39.570,0:12:45.780
of zero? One. So it means if these two
angles, if the two vectors are aligned,

0:12:45.780,0:12:50.460
which means there is a zero angle
between the two vectors you can have the

0:12:50.460,0:12:57.090
maximum value of this element, right?
whenever when you have the minus the

0:12:57.090,0:13:03.240
most negative value? when they are
opposite, right? so when they're in

0:13:03.240,0:13:08.310
opposition of phase you're gonna get the
most negative magnitude but then if you

0:13:08.310,0:13:11.730
apply just let's say a ReLU you're gonna cut
all those negative things you're just

0:13:11.730,0:13:16.200
checking for the positive matches right
so neural net basically just perhaps

0:13:16.200,0:13:20.520
is gonna be figuring out only the
positive matches, right? and so again when

0:13:20.520,0:13:23.620
you multiply a matrix times a column
vector you

0:13:23.620,0:13:31.330
be performing element-wise sorry scalar
product between each column each row of

0:13:31.330,0:13:36.220
the matrix which represents your kernel
right? so whenever you have like a linear

0:13:36.220,0:13:40.000
layer your kernel is gonna be the whole
row of the matrix and now you see what

0:13:40.000,0:13:47.050
is the projection of that input on that
column, I meant, on the input on that row

0:13:47.050,0:13:52.900
right? so each element of this product is
going to tell you the alignment with

0:13:52.900,0:13:57.580
which the input is what is the alignment
of the input with respect to the

0:13:57.580,0:14:04.600
specific row of the matrix okay?
yes? no? this should shape some more like

0:14:04.600,0:14:08.290
intuition while we are using these
linear transformations they are like

0:14:08.290,0:14:13.140
allowing you to see the projection of
the input onto different kind of

0:14:13.140,0:14:22.300
orientations let's say this way. okay? you
can try to you know extrapolate this in

0:14:22.300,0:14:26.140
high dimensions I guess the intuition at
least I can give it to you works

0:14:26.140,0:14:30.580
definitely in two and three dimensions,
in higher dimensions I kind of think it

0:14:30.580,0:14:34.209
works in a similar way. next lesson we
are going to watch we are actually we

0:14:34.209,0:14:38.890
are going to see how what is the
distribution of the projections in a

0:14:38.890,0:14:43.240
higher dimensional space is this sort
gonna be so cool I think. all right so

0:14:43.240,0:14:49.779
this was the first part of I think of
the class oh well there is one more more

0:14:49.779,0:14:54.940
part so actually here this z here we
can also write it in a different way so

0:14:54.940,0:15:00.130
maybe this is maybe it's known maybe
it's not known. when I saw it first time

0:15:00.130,0:15:05.050
I didn't know so you know it's it's cool
that sometimes you see these things once

0:15:05.050,0:15:10.450
again maybe so let's go back here is the
same z there there and so you can express

0:15:10.450,0:15:18.820
this z as being equal the vector a 1 in
this case a 1 is gonna be the first

0:15:18.820,0:15:22.830
column of the a matrix ok and this one
is going to be multiplied by the scalar

0:15:22.830,0:15:29.380
x1 now you have the second column of the
matrix so I have a 2 this is multiplied

0:15:29.380,0:15:34.870
by the second element of the X right
until the last one which is gonna be?

0:15:34.870,0:15:44.930
again? I can't hear if it's m or n? m? like
this? or n? you know sign language? which

0:15:44.930,0:15:52.880
one? n? right, you know sign language? no?
you should learn it's good, you know?

0:15:52.880,0:15:59.630
inclusivity. a n,  right? so the last column
times your x n, of course because x

0:15:59.630,0:16:04.100
has a size of n, there are n items, right?
and so basically when you also when you

0:16:04.100,0:16:07.940
do a you know transformation a linear
or apply a linear operator you

0:16:07.940,0:16:12.890
are going to be basically weighting each column
of the matrix with the coefficient that

0:16:12.890,0:16:16.640
is in a, you know, you have first column
times the first coefficient of the

0:16:16.640,0:16:22.130
vector, second column, and by the second
item, plus third column times the third

0:16:22.130,0:16:26.420
item and so you can see the output of
this DN transformation is a weighted sum

0:16:26.420,0:16:32.180
of the columns of the a matrix okay? so
this is a different kind of intuition

0:16:32.180,0:16:36.620
sometimes you see these as like whenever
you want to express your signal your

0:16:36.620,0:16:45.290
data is a combination of different, you
know, the composition, this is kind of a

0:16:45.290,0:16:50.600
linear composition of your input. alright
so that was the first part it is the

0:16:50.600,0:16:57.530
recap about the linear algebra. a second
part is going to be something even more

0:16:57.530,0:17:06.790
cool I think. questions so far?
no? easy? too easy? you're getting bored?

0:17:06.790,0:17:10.670
sorry okay all right then I'm gonna
speed up I guess.

0:17:10.670,0:17:15.170
alright so let's see how we can extend
the one that the stuff we have seen

0:17:15.170,0:17:19.040
right now to convolutions right so maybe
convolution sometimes are a little bit

0:17:19.040,0:17:28.900
weird let's see how we can do an
extension to convolutions

0:17:31.720,0:17:38.390
all right. so let's say I start with the
same matrix. so I'm gonna have here four

0:17:38.390,0:17:53.660
rows and then three columns. okay. so my
data has to be? if I have, if I have this

0:17:53.660,0:17:59.360
matrix, if I multiply this to a column, my
column vector should be? of size? three,

0:17:59.360,0:18:04.250
thank you. all right so let me draw here
my column vector of size three and this

0:18:04.250,0:18:08.809
is gonna give you an output of size four
okay fantastic

0:18:08.809,0:18:15.740
but then is your data, let's say you're
gonna listen to some nice audio, audio

0:18:15.740,0:18:21.260
file, is your data just three sample long?
how long is gonna be your data? let's say

0:18:21.260,0:18:24.590
you listening to a piece of music that
is like three minutes

0:18:24.590,0:18:32.330
how many sample does three minutes of
audio have? yeah, I guess, what is

0:18:32.330,0:18:40.070
gonna be my sampling rate? let's say
twenty two, okay. twenty two thousand kilo

0:18:40.070,0:18:46.480
Hertz, right? 22 kilohertz so how many
samples three minutes of music have?

0:18:47.799,0:18:58.010
say again? you sure? is monophonic
or stereophonic? just kidding. okay so

0:18:58.010,0:19:02.650
you're gonna multiply the number of
samples, the number of seconds, right? the

0:19:02.650,0:19:08.660
number of seconds times the frame rate,
right? the okay, the frequency in this

0:19:08.660,0:19:12.620
case. anyhow so this signal is gonna be
very very long, right? it's gonna be keep

0:19:12.620,0:19:16.940
going down so if I have a vector that is
very very long I have I have to use a

0:19:16.940,0:19:23.540
matrix which is gonna be very very fat,
wide, right? okay fantastic so this top

0:19:23.540,0:19:27.080
keeps going this direction,
alright, so my question for you is gonna

0:19:27.080,0:19:31.540
be what should I put in this location
here?

0:19:35.570,0:19:48.020
what should I put here? so do we care
about things that are further away?

0:19:48.740,0:19:54.000
no, why not?
because our data has the property of

0:19:54.000,0:20:00.299
locality, fantastic. so what am I gonna go
what am I gonna be putting here? a big

0:20:00.299,0:20:06.690
zero, right, fantastic, well done, okay so we put
a zero here and then what is the other

0:20:06.690,0:20:09.809
property so let me start drawing this
stuff again right so can I have my

0:20:09.809,0:20:16.890
kernel of size three and then here I
have my data which is gonna be very long

0:20:16.890,0:20:31.169
right? and so on. Can't draw hold on. I can't see. All right so
here there are zero, then let's say what

0:20:31.169,0:20:36.960
is the other property my
natural data have? stationarity, which

0:20:36.960,0:20:45.059
means? the pattern that you expect to
find this can be kind of repeating over

0:20:45.059,0:20:49.140
and over again right? and so if I have
here my three values here perhaps I'd

0:20:49.140,0:20:53.789
like to reuse them over and over again
right? and so if these three values let

0:20:53.789,0:21:00.779
me change the color maybe so you can see
in there is the same stuff. so I have

0:21:00.779,0:21:07.230
three values here and then I'm gonna be
using these three same values one step

0:21:07.230,0:21:16.770
futher right? and I keep going further down 
and I keep going this way all

0:21:16.770,0:21:22.799
right. So what should I put here on the
bottom? What should I put here? a zero,

0:21:22.799,0:21:28.529
right? why that why is that? because of
locality of the data. right? so putting

0:21:28.529,0:21:36.350
zeros around is called it's also called
padding but in this case it's called

0:21:36.350,0:21:44.850
sparsity, right? so this is like sparsity
and then the replication of this thing

0:21:44.850,0:21:52.730
that is over and over over again
it's called stationarity was the property

0:21:52.730,0:21:57.430
of the signal, this is called weight
sharing. yeah?

0:22:04.330,0:22:10.090
okay fantastic all right so how many how
many values do we have now? how many

0:22:10.090,0:22:13.530
parameters I have on the right hand side?

0:22:15.150,0:22:22.750
well so we have three parameters. Over 
the left hand side instead we

0:22:22.750,0:22:28.900
had? Twelve, right? so is the right side
is gonna be, is the right side gonna

0:22:28.900,0:22:36.810
work at all? you have three parameters
one side on the other side you have 12.

0:22:36.810,0:22:41.860
Okay? that is good, using locality and 
whatever not like sparsity

0:22:41.860,0:22:45.610
and parameter sharing but then we
end up with just three parameters, isn't

0:22:45.610,0:22:51.190
this too restrictive? how can we have
multiple multiple parameter? what's

0:22:51.190,0:22:56.290
missing here in the big picture? there
are multiple channels, right? so this is

0:22:56.290,0:23:01.570
just one layer here and then you have
this stuff coming out from the 

0:23:01.570,0:23:08.230
board here, so you have the
first kernel down here, now you have some

0:23:08.230,0:23:20.500
second kernel let's say this one, and I
have last one here, right? and so you have

0:23:20.500,0:23:24.760
each plane of these metrics being
containing just one kernel which is

0:23:24.760,0:23:36.190
replicated multiple times. Who knows the
name of this matrix? so this is gonna

0:23:36.190,0:23:40.650
be called Toeplitz matrix

0:23:43.419,0:23:48.219
okay? so what is the main feature of
these Toeplitz matrix? what is the big

0:23:48.219,0:24:01.479
big big thing that you won't notice? it's
a sparse matrix. okay okay what is gonna

0:24:01.479,0:24:16.149
be here, this first item over here? what
is the content of the first guy? yeah? so

0:24:16.149,0:24:21.669
this one here is gonna be the extension
of my linear transformation which was,

0:24:21.669,0:24:26.169
you know, I have a signal that is longer
than three samples therefore I have to

0:24:26.169,0:24:32.229
make this matrix fatter, second part is
going to be given that I don't care about

0:24:32.229,0:24:36.129
what things, like, things that are here
down I don't care about things that are

0:24:36.129,0:24:40.299
here if I look at the points that are up
here, so gonna be putting a big 0 here

0:24:40.299,0:24:45.070
such that everything that is down
here, it gets cleaned up, right? and 

0:24:45.070,0:24:49.899
finally I'm gonna be using the same
kernel over and over again because I

0:24:49.899,0:24:55.839
assume that my data is stationary and
therefore I assume that similar patterns

0:24:55.839,0:24:58.929
are gonna be happening over and over
again therefore I'm gonna be using this

0:24:58.929,0:25:03.429
one which is written here: 
weight sharing.

0:25:03.429,0:25:08.019
Finally given that this one gives you
only three parameters to

0:25:08.019,0:25:12.940
work with I will use several layers in
order to have different, you know,

0:25:12.940,0:25:17.739
channels. So this one is one kernel.
before, one kernel was the whole row of

0:25:17.739,0:25:21.849
the matrix, okay? so when you have a fully
connected layer, the only difference

0:25:21.849,0:25:25.179
between a fully connected layer and a
convolution is that you have the full

0:25:25.179,0:25:37.019
row of the matrix. So what is gonna
be in this first item over here? anyone?

0:25:38.480,0:25:43.230
so the green kernel, let's call the
green kernel just a 1 let me actually

0:25:43.230,0:25:55.200
make it glow green because it's green
kernel. So you have a 1 times... what? it's

0:25:55.200,0:25:58.890
gonna be from the number one to number
three right? and then the second item is

0:25:58.890,0:26:05.640
gonna be same same dude here a 1 and
then you're gonna have the x shifted by

0:26:05.640,0:26:20.039
one and so on right? make sense? yeah and
then we're gonna have this one is going

0:26:20.039,0:26:23.970
to be the green output then you're gonna
have the blue output one layer coming

0:26:23.970,0:26:26.730
out and then you have the further one
the red one coming.

0:26:26.730,0:26:32.929
even one layer out. Okay? Was the
iPad experience cool?

0:26:32.929,0:26:40.520
yes? no? I liked it. Okay. Other questions?

0:26:41.029,0:26:49.049
say again? the blue circle this one? it's a
big zero

0:26:49.049,0:26:53.480
that's the sparsity one the same is here.

0:26:53.899,0:26:58.520
yes? no? yeah yeah.

0:27:01.530,0:27:16.200
so here I just put a lot of
zeros inside here so I killed all the

0:27:16.200,0:27:21.000
values that are away from the little
part and then I repeat the same three

0:27:21.000,0:27:24.480
values over and over again because I
expect to find the same pattern in

0:27:24.480,0:27:33.360
different regions of this, this big big
signal I have. This one here? So I said

0:27:33.360,0:27:36.840
that in this case I'm gonna have just
three values, right? and we started with

0:27:36.840,0:27:41.040
12 values and I ended up with 3 which is
really really little so if I'd like to

0:27:41.040,0:27:44.760
have let's say 6 values then if I want
to have six values and I can have my

0:27:44.760,0:27:49.530
second 3 on a different plane and I
perform the same operation whenever you

0:27:49.530,0:27:55.110
multiply this matrix times a vector you
perform a convolution so it just to tell

0:27:55.110,0:27:58.770
you that a convolution is just a matrix
multiplication with a lot of zeros

0:27:58.770,0:28:09.600
that's it. Yeah so they're gonna have
this one here, then you have a second one,

0:28:09.600,0:28:14.790
then you have a third one so you have three
versions of the input. Alright. So for the

0:28:14.790,0:28:18.000
second part of the class I'm gonna be
showing you some more interactive things

0:28:18.000,0:28:27.810
please do participate to the second part
as well alright? so let's try so I have

0:28:27.810,0:28:37.950
rebranded I have rebranded the website
and now the

0:28:37.950,0:28:42.450
environment is going to be called pDL
so PyTorch deep learning instead of

0:28:42.450,0:28:49.980
deep learning mini course, it was too
long. So let me start by running this one

0:28:49.980,0:28:52.460
so

0:28:55.320,0:29:04.960
PyTorch deep learning so we can do just
Conda activate activate PyTorch deep 

0:29:04.960,0:29:12.940
learning (pDL) and then let's open notebook, the
Jupiter notebook. All right, so now you're

0:29:12.940,0:29:18.520
gonna be watching, going over the
listening to kernels. So I showed you a

0:29:18.520,0:29:22.630
convolution on paper well on my tablet
now you're going to be listening to

0:29:22.630,0:29:25.990
convolution so can such that you can
really appreciate what these convolutions

0:29:25.990,0:29:35.919
are. Here we said, the new kernel right
which is called pDL PyTorch deep

0:29:35.919,0:29:42.520
learning so you're gonna notice the same
kind of, you know, procedure if you update

0:29:42.520,0:29:49.690
your system. Alright so in this case we
can read the top here so let me hide the

0:29:49.690,0:29:52.200
top here.

0:29:52.890,0:29:56.950
Alright so given the assumption of
locality, stationarity, and 

0:29:56.950,0:30:00.280
compositionality we can reduce the
amount of computation for a matrix

0:30:00.280,0:30:05.169
vector multiplication by using a sparse
because local Toeplitz matrix because

0:30:05.169,0:30:09.850
stationary scheme in this way we can
simply end up rediscovering the

0:30:09.850,0:30:14.980
convolution operator, alright? moreover, we
can also recall that a scalar product is

0:30:14.980,0:30:19.150
a simply a normalized cosine distance
which tells us the alignment of two

0:30:19.150,0:30:21.850
vectors,
more specifically we compute the

0:30:21.850,0:30:26.320
magnitude of the orthogonal projection
of two vectors onto the other and vice

0:30:26.320,0:30:29.590
versa.
So let's figure out now how all this

0:30:29.590,0:30:34.270
can make sense by using our ears, okay? so
i'm going to be importing a library that

0:30:34.270,0:30:39.880
professor here at NYU made and here I'm
gonna be just loading my audio data and

0:30:39.880,0:30:43.600
I'm gonna have that in my x, and then my
sampling rate is going to be

0:30:43.600,0:30:48.570
in the other variable. So here I'll just
show you I'm gonna have like 70,000

0:30:48.570,0:30:54.430
samples in this case because I have a
sampling rate of 22 kilo Hertz and then

0:30:54.430,0:31:01.720
my total time is gonna be three seconds,
okay, so three seconds times 22 you get

0:31:01.720,0:31:06.910
what? so it's not 180 you were saying, it
was a hundred and eighty, it was three, right?

0:31:06.910,0:31:11.380
oh it was three minutes, oh you're right
it's three seconds, so you actually are

0:31:11.380,0:31:16.540
correct my bad. So this is three seconds
so times 22 kilo Hertz you get 70

0:31:16.540,0:31:22.390
roughly some 70,000 samples. Here I'm
gonna be importing some libraries to

0:31:22.390,0:31:28.180
show you something and then I'm gonna be
showing you the first chart so this is

0:31:28.180,0:31:37.270
the audio signal I have imported right
now how does it look like? wavy, okay cool.

0:31:37.270,0:31:50.680
Can you tell me how it sounds? 
Student: "aaaaaaaaaaahhhhhh".
That was a good guess. The guess was 'aaah'. Yeah, you can't tell exactly

0:31:50.680,0:31:55.450
what's the content, right? from
this diagram because the amplitude of,

0:31:55.450,0:32:01.240
the y-axis here is gonna be
showing you just the amplitude. can I

0:32:01.240,0:32:05.530
turn off the light? is it okay? or... are you
sure? okay thank you,

0:32:05.530,0:32:16.090
I really dislike these lights. Okay.
Goodnight. Oh see how nice it is? okay

0:32:16.090,0:32:19.930
cool. Alright so you can't tell anything
here right?

0:32:19.930,0:32:26.580
you cannot tell what is the what is
the sound, right? so how can we figure out

0:32:26.580,0:32:31.870
what is the sound inside here? so for
example I can show you a transcript of

0:32:31.870,0:32:37.660
the sound and, actually let me let me
actually force these in your in your

0:32:37.660,0:32:44.810
your head, right? so you're gonna have...
hold on, didn't work. *A sound comes on*

0:32:44.810,0:32:50.610
all right so now we actually heard it,
okay so now you can actually see *mimics the sound*

0:32:50.610,0:32:56.400
you know you can kind of
imagine a little bit but okay so what

0:32:56.400,0:33:00.830
notes did we play there? like how can I
figure out what are the notes that

0:33:00.830,0:33:05.550
they are inside? so I'm gonna be showing
this one, so since this is a bit brighter

0:33:05.550,0:33:13.820
I can see your faces. How many of you can
not read this? Oh, ouch…

0:33:13.820,0:33:20.960
Okay, so let me see if I can ask for some help.

0:33:23.620,0:33:26.620
Maybe someone can help us out here.

0:33:29.400,0:33:32.480
Okay. Let's see.

0:33:40.140,0:33:42.140
Hey, hey Alf!
Oh, hi Alf!

0:33:42.880,0:33:45.420
How's going?
Yeah, yeah, I'm fine, thank you.

0:33:45.420,0:33:47.040
Nice glasses there!
Oh, thank you for the glasses.

0:33:47.040,0:33:49.040
Oh, nice sweater! you too!
Nice sweater!

0:33:49.040,0:33:51.040
Oh, we're wearing the same sweater!

0:33:51.040,0:33:54.480
Can you help us out?
They don't know how to read the

0:33:54.480,0:33:57.120
Oh, the connection…
What the f- hell!

0:33:57.120,0:34:00.380
They cannot read the score!
Can you help us out, please?

0:34:00.380,0:34:02.380
Alright!
Let me try to help you out.

0:34:02.380,0:34:04.380
Thank you!
Let me switch the camera.

0:34:04.380,0:34:06.380
Alright.
Please, do.

0:34:06.380,0:34:08.380
So, here we can go like…

0:34:08.380,0:34:10.860
and hear first how it sounds everything.

0:34:10.860,0:34:14.320
So, it's going to be like that.

0:34:14.440,0:34:23.380
How cool is that? 
*students clap*

0:34:23.380,0:34:27.990
Thank you. It took four
lessons for you to clap me. So now…

0:34:27.990,0:34:32.360
This is very nice of you. Let's keep
going.

0:34:32.360,0:34:36.320
A♭, then we have an E♭, and then a A♭.

0:34:36.320,0:34:40.380
The difference between the first A♭ and the other one in frequencies

0:34:40.380,0:34:46.540
is that the first A♭ is going to be twice the frequency as the other one.

0:34:46.540,0:34:51.400
And instead, in the middle, we have the 5th.
We're going to figure out what is the frequency of that.

0:34:51.400,0:34:55.320
And then, we are gonna be going to a B♭, here.

0:34:55.320,0:34:57.680
On the left hand side, instead, we have the accompagnement,

0:34:57.680,0:35:01.220
and so we're gonna have a A♭ and B♭

0:35:01.220,0:35:05.620
and then B♭ and E♭.

0:35:05.680,0:35:10.900
So, if we put all together, we gonna get
this one.

0:35:11.300,0:35:14.020
Alright? Simple, no?
Yep! Thank you!

0:35:14.020,0:35:17.320
Bye-bye!
Byeeeeee!

0:35:18.820,0:35:23.540
See? This took a whole damn day to
prepare…

0:35:23.540,0:35:27.480
I was so nervous before come here…

0:35:27.480,0:35:32.140
I didn't know if it actually would have worked…
Both of them, tablet and this one.

0:35:32.140,0:35:35.060
I'm so happy!
Now I can actually go to sleep, later.

0:35:35.060,0:35:39.280
Anyhow, so this was like in the first

0:35:39.290,0:35:43.280
part you're gonna have the first note,
there's A♭ that you have a B♭

0:35:43.280,0:35:51.550
A♭ and B♭ so you have *recreates the sound* and the and the
difference between the first pitch and

0:35:51.550,0:35:57.440
is one octave, therefore the first
frequency is gonna be twice the second

0:35:57.440,0:36:02.870
frequency. Okay? so whenever we're gonna
be observing the waveform one sign will

0:36:02.870,0:36:07.730
have a shorter like the half the
period of the other one, right?

0:36:07.730,0:36:12.410
especially the A♭ on top
is gonna have a period which is half of

0:36:12.410,0:36:20.000
the period of the A♭ on the bottom one,
right, so you have *recreates the sound* okay, if you go half of

0:36:20.000,0:36:27.290
this one you get *makes sound* right okay okay so how
do we actually get these notes out from

0:36:27.290,0:36:33.770
that spectra, from the waveform?
who can tell me how can I extract these

0:36:33.770,0:36:40.790
pitches, these frequencies from the other
signal? any guess? okay Fourier transform

0:36:40.790,0:36:45.530
that I think is kind of a good guess.
What happens if I perform now a

0:36:45.530,0:36:50.660
Fourier transform of that signal? anyone
can actually answer me? you cannot raise

0:36:50.660,0:36:54.100
your hand because I don't see, just shout.

0:36:55.120,0:36:59.690
So if you basically perform the Fourier
transform of the whole signal are you

0:36:59.690,0:37:06.470
going to hear *makes sound* like the whole notes together *makes sound*?
all together, right, but then you can't

0:37:06.470,0:37:13.260
figure out which pitch is playing where
or when actually in this case right.

0:37:13.260,0:37:18.210
Ha! So we need kind of a Fourier
transform which is localized and so a

0:37:18.210,0:37:23.190
localized Fourier transform in time or
in space depending on whatever domain

0:37:23.190,0:37:27.390
you are using its called spectrogram.
right, and so on I'm gonna be

0:37:27.390,0:37:30.000
printing for you the spectrogram, oh
sorry.

0:37:30.000,0:37:34.380
and I'll be printing here the
spectrogram of this one here. And so, here

0:37:34.380,0:37:39.960
you can compare the two, alright, on the
first part here on this side here you're

0:37:39.960,0:37:47.970
gonna have this peak here at 1600 which
is the *makes sound*, the pitch is actually higher.  *makes sound* there we go. Now

0:37:47.970,0:37:56.640
you have a second one which is this peak
here *makes sound* and then this one *makes sound*. You can see this peak, right? And you

0:37:56.640,0:38:04.260
see this peak, yeah okay, so these peaks
are gonna be the actual notes I play

0:38:04.260,0:38:07.560
with the right hand,
so let's actually put those together and

0:38:07.560,0:38:13.140
I'm gonna have here the frequencies. So I
have 1600, 1200 and 800, can you see here?

0:38:13.140,0:38:20.550
I have 1600, 800, why is one double of
the other? because they are one octave

0:38:20.550,0:38:27.540
apart, so if this is *makes sound* this gonna be *makes sound* okay
and this is a fifth which also

0:38:27.540,0:38:32.280
has a nice interval. So let me actually
generate these signals here and then I

0:38:32.280,0:38:36.720
gotta be concatenate them all so I'm
gonna be playing both these. the first

0:38:36.720,0:38:42.109
one is actually the original audio, but

0:38:42.109,0:38:45.470
let me try again, whereas if I play 

0:38:45.680,0:38:54.860
the second one, the concatenation, yeah so
it's a bit loud, I can now cannot even

0:38:54.860,0:39:01.720
reduce the volume. Oh I can reduce it
here. Too much. Okay, let me go again. All

0:39:02.020,0:39:05.350
right.
So this is the concatenation of these

0:39:05.350,0:39:12.380
four different pitches so guess what we
are going to be doing next? so how can I

0:39:12.380,0:39:20.420
extract all the notes that I can hear in
a specific given piece? so let's say 

0:39:20.420,0:39:29.360
you play a full score and I'd like to
know which pitch is played at what time. Of

0:39:29.360,0:39:36.230
what? so the answer was convolution, just
for the recording, so I'm asking convolution

0:39:36.230,0:39:43.370
of what? no convolution of the spectrogram, so
you have convolution of your input

0:39:43.370,0:39:49.460
signal with what? with some different
kind of pitches right which pitches will

0:39:49.460,0:39:59.150
you pick? let's say you don't see the
spectrum because let's say I'm just gonna

0:39:59.150,0:40:03.770
play any kind of piece of music so I'd
like to know all possible notes they are

0:40:03.770,0:40:06.220
there, what would you do?

0:40:06.220,0:40:13.430
you don't know all pitches, how would you
try? right, so in which are all the

0:40:13.430,0:40:20.900
pitches you may want to use, if you're if
I'm playing the piano? all the keys of

0:40:20.900,0:40:24.530
the piano, right? so if I play a
concerto with the piano then I want to

0:40:24.530,0:40:28.010
have a piece of audio for each of
those keys and I'm gonna be running

0:40:28.010,0:40:32.690
convolutions of my whole piece with the
old keys, right? and therefore you're

0:40:32.690,0:40:36.470
gonna see peaks which are the alignment
of the cosine similarity

0:40:36.470,0:40:41.349
whenever you get basically 
the audio matching your specific kernel.

0:40:41.349,0:40:46.989
so I'm gonna be doing this but with
these specific tones I actually extract

0:40:46.989,0:40:52.929
it here. So here I'm going to be showing
you first how the two spectrograms look

0:40:52.929,0:40:57.699
like the left side is going to be the
spectrogram of my actual signal X of t

0:40:57.699,0:41:01.630
and on the right hand side I have just
the spectrogram of this concatenation of

0:41:01.630,0:41:10.749
my pitches, so here you can clearly see
this *makes sound* but then here, first of all, what

0:41:10.749,0:41:14.429
are these bars here, these vertical bars?

0:41:15.269,0:41:20.589
you following, right? I can't see you, have to
actually talk back. What are these red

0:41:20.589,0:41:24.160
bars here, vertical bars? Now, the 
horizontal one I already told you, right? 

0:41:24.160,0:41:34.390
*makes sound* and the vertical? what is
it? sampling issues, right, transitions. So

0:41:34.390,0:41:39.099
whenever you have the *makes sound* you actually have
one white waveform one waveform and then

0:41:39.099,0:41:44.019
the other one, one waveform has to stop
so it's no longer periodic and whenever

0:41:44.019,0:41:47.609
you do a Fourier transform of a non
periodic signal you get you know crap.

0:41:47.609,0:41:53.589
That's why whenever you get the 
junction between these the *makes sound* the jump

0:41:53.589,0:41:57.729
here you're gonna have this spike
because you can

0:41:57.729,0:42:01.749
think about the jump is like having a
very high frequency right? because 

0:42:01.749,0:42:05.469
it's like a delta, so you actually
get all the frequencies, that's why you

0:42:05.469,0:42:12.549
get all the frequencies here. Boom. Okay?
makes sense so far? kind of? alright.

0:42:12.549,0:42:18.720
So this is the clean version *makes sound* I
cannot even sign and what

0:42:18.720,0:42:25.800
left side here? why is on the left-hand
side all red down there? Okay

0:42:25.800,0:42:31.440
yes, You knew. so the left side left hand
the cord are the one I show you on the

0:42:31.440,0:42:37.320
bottom left side. Okay so let me finish
this class and then I'll let you go. So

0:42:37.320,0:42:42.990
here I'm gonna show you
first all the kernels, you can tell now

0:42:42.990,0:42:48.090
the red one is going to be the first
chunk of my signal, the real

0:42:48.090,0:42:53.280
one and then you can see the first pitch
has the same frequency,

0:42:53.280,0:42:58.460
can you see? So the *makes sound*
has the same  

0:42:58.460,0:43:04.230
delta t the same interval,
period, can you see? you cannot nod your

0:43:04.230,0:43:08.700
head because again I don't see you, have
to answer me. Can you see or not? Okay,

0:43:08.700,0:43:13.050
thank you, fantastic. And so this one is
that the third one you can see that it

0:43:13.050,0:43:17.130
starts here in the period and it
finishes here, if you go up here you're

0:43:17.130,0:43:20.520
gonna see exactly there were two of
these guys, right, so that's

0:43:20.520,0:43:24.619
how you can see this is like
twice the frequency of the one below.

0:43:24.619,0:43:30.089
Finally I'm gonna be performing the
convolution of these four kernels with

0:43:30.089,0:43:36.839
my input signal, and this is how we look
like, okay, so the first kernel has a high

0:43:36.839,0:43:42.150
match in the first part of the
score. So between zero and zero five

0:43:42.150,0:43:46.830
seconds. The second one starts just after
after the first one, then you have the

0:43:46.830,0:43:50.820
third one starting at zero three I guess
and then you have the last one

0:43:50.820,0:43:56.940
starting at the zero six okay? So
guess what? I'm gonna make you listen to

0:43:56.940,0:44:02.520
convolutions now, are you excited?
oh okay, you actually answering now, good!

0:44:02.520,0:44:07.440
Alright and so these are the outcomes.
Let me lower a little the volumes

0:44:07.440,0:44:14.900
otherwise you will complain, yeah I can't
lower the,

0:44:16.800,0:44:23.690
okay, so the first one, let's try again

0:44:28.880,0:44:37.110
*plays sound* how cool is that? You listen to
convolutions. Okay so basically this was

0:44:37.110,0:44:41.280
almost it, I have one more slide because I
felt like there was some confusion last

0:44:41.280,0:44:45.360
time about what is the different
dimensionality of different type of

0:44:45.360,0:44:50.850
signals, so I'm really recommending to go
and take the class of Joan Bruna which

0:44:50.850,0:44:56.580
is math for deep learning and I stole
one of his, you know, small things he was

0:44:56.580,0:45:04.320
teaching, I just put a one slide here
together for you. So this slide is the

0:45:04.320,0:45:13.140
following. So we have the input
layer or the samples we provide into

0:45:13.140,0:45:18.420
this network and so usually our last
time I define this I have this curly X

0:45:18.420,0:45:23.010
which is gonna be made of those x i's
which are all my data samples right

0:45:23.010,0:45:29.100
and so I usually have m data 
samples so my i goes from m=1 to n, ok, so

0:45:29.100,0:45:34.080
is this clear? on is this notation clear?
because it's a bit more formal I'm usually a bit 

0:45:34.080,0:45:39.030
less formal but then somehow someone was
feeling a little bit not comfortable I

0:45:39.030,0:45:46.470
think so this one is just my input
samples but we can also see this one

0:45:46.470,0:45:52.950
is this curly X which is my input set as
the set of all these functions as x i's

0:45:52.950,0:45:59.850
which are mapping my Omega capital Omega
which is my domain to a RC which are

0:45:59.850,0:46:06.150
going to be basically my channels of
that specific example and so here I'm

0:46:06.150,0:46:14.040
gonna be mapping those lowercase Omega
to these x i's of omega so let's see how

0:46:14.040,0:46:17.550
these different this from the previous
notation. So I'm gonna give you now three

0:46:17.550,0:46:21.300
examples and you should be able to
tell now what is the dimensionality and

0:46:21.300,0:46:24.820
in this example. So the first one,
let's say,

0:46:24.820,0:46:29.560
I have like the one I showed you right now
in the time, just fun piece of you

0:46:29.560,0:46:34.870
audio signal, so my Omega is going to be
just the samples like sample number one

0:46:34.870,0:46:39.550
sample number two like the index, right?
so you have index one, index two, index

0:46:39.550,0:46:44.740
until these 70,000 whatever we just saw
right now, okay? and the last value is

0:46:44.740,0:46:49.330
going to be the T, capital T, which is the
number of seconds divided by the delta T

0:46:49.330,0:46:53.200
which would be the 1 over the
frequency and this is going to be a

0:46:53.200,0:46:57.250
subset of n right? so this is a discrete
number of samples, because you have a

0:46:57.250,0:47:03.220
computer, you always have discrete
samples. So this is my input data, and

0:47:03.220,0:47:09.310
then what about the image of this
function? so when I ask what is the

0:47:09.310,0:47:13.330
dimensionality of this type of signal,
you should answer this is a

0:47:13.330,0:47:19.360
one-dimensional signal because the power
of these n here is 1 ok? so this is like

0:47:19.360,0:47:25.000
a 1 dimensional signal, although you may
have the total time and the

0:47:25.000,0:47:28.900
1 there it was a sampling interval, on
the right hand side you have the number

0:47:28.900,0:47:33.340
of channels can be 1 if you have a mono
signal or you can have 2 if you have a

0:47:33.340,0:47:38.230
stereophonic, so you have mono there, you
have a 2 for stereophonic or what is 5

0:47:38.230,0:47:44.650
plus 1? that's the Dolby like 5.1 how
cool? alright so this is still a one

0:47:44.650,0:47:48.430
dimensional signal which may have
multiple channels, nevertheless is still

0:47:48.430,0:47:53.020
one dimensional signal because there is
only one running variable there ok? is it

0:47:53.020,0:47:58.270
somehow better than last time? yes? no?
better? thank you.

0:47:58.270,0:48:03.070
let's say thanks to Joan. Alright, second
example I have here my Omega is going to

0:48:03.070,0:48:07.630
be the Cartesian product of these two
sets, the first set is going to be going

0:48:07.630,0:48:13.150
from 1 to the height, and also this one
in discrete, and the other one is going

0:48:13.150,0:48:17.020
to be going from 1 to the width, so these
are the actual pixels, and so this one

0:48:17.020,0:48:21.690
is a 2 dimensional signal because I have
2 degrees of freedom in my

0:48:21.690,0:48:28.690
domain. What are the possible channels we
have? So here, the possible channels that

0:48:28.690,0:48:32.440
are very very common are the following:
So you can have a grayscale image and

0:48:32.440,0:48:36.849
therefore you only output one
scalar value or you get the

0:48:36.849,0:48:42.789
rainbow there, the color, and therefore
you get like my X which is a function of

0:48:42.789,0:48:49.299
the coordinates Omega 1 Omega 2
which each point is

0:48:49.299,0:48:52.690
represented by a vector of three
components which is gonna be the R

0:48:52.690,0:48:59.109
component of the point Omega 1 Omega 2,
the G component of Omega 1 Omega 2, and

0:48:59.109,0:49:04.299
the blue component of Omega 1 Omega 2. So
again you can all think about this as a

0:49:04.299,0:49:08.980
big big data point or you can think
about this as function mapping a low

0:49:08.980,0:49:12.640
dimensional domain which is a two
dimensional domain to a three

0:49:12.640,0:49:18.490
dimensional domain, right? finally the
twenty, who who knows the name of the

0:49:18.490,0:49:24.400
twenty channel image? yeah, this is a
hyperspectral image. It's very common to

0:49:24.400,0:49:31.869
have 20 bands.
Finally, who can guess this one?

0:49:31.869,0:49:40.829
if my my domain is r4 x r4, what can
it be?

0:49:41.099,0:49:50.589
No, no, this discrete right?
This is r4, so it's not even computer. Ha!

0:49:50.589,0:49:56.740
who said something there? I heard! Yes
this correct so this is space-time, what

0:49:56.740,0:50:00.849
is the second one?
Yeah, which momentum? It has a special

0:50:00.849,0:50:07.779
name. It's called four momentum
because it has a temporal information as

0:50:07.779,0:50:12.160
well, right?
And so what's gonna be my possible image

0:50:12.160,0:50:24.790
of the X function? let's say it's c equals 1.
What is it? do you know?

0:50:24.790,0:50:29.630
So this could be for example the
Hamiltonian of the system, okay? so this

0:50:29.630,0:50:36.460
was like a bit more mathematical
introduction or mathematical

0:50:37.000,0:50:42.890
procedure, how do you say, you'll make
make a more precise definition. So that

0:50:42.890,0:50:48.980
was pretty much everything for today, let
me turn on the light and I see you

0:50:48.980,0:50:54.969
on next Monday.
Thank you for being with me.

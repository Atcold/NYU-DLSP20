0:00:00.030,0:00:04.730
so since last time okay welcome back
thank you for being here

0:00:04.730,0:00:09.059
last time yang was using the tablet
right and how can you use the tablet I

0:00:09.059,0:00:13.040
don't use the tablet right so I should
be as cool as the Anna at least I think

0:00:13.040,0:00:18.900
one more thing to begin with there is a
spreadsheet where you can decide whether

0:00:18.900,0:00:22.890
you'd like to join the slack channel
where we collaborate over making like

0:00:22.890,0:00:28.529
some drawings for the website fixing
some mathematical notations having some

0:00:28.529,0:00:32.640
kind of you know fixing the error in
English in the English grammar or

0:00:32.640,0:00:37.290
whatever so if you if you're interested
in a help in improving the content of

0:00:37.290,0:00:42.450
this class feel free to fill out the
spreadsheet okay we are already a few of

0:00:42.450,0:00:49.789
us on the slug channel so I mean if you
want to join if we are you're welcome so

0:00:49.789,0:00:53.399
instead of writing on the whiteboard
because it's impossible to see I think

0:00:53.399,0:01:00.270
from the upper side we're gonna be going
to experiment with a new toy here all

0:01:00.270,0:01:03.930
right
first time so you know I'm a little bit

0:01:03.930,0:01:11.250
tense last time I screw up with a
notebook so okay alright so we're gonna

0:01:11.250,0:01:15.659
be talking starting with a small review
about linear algebra I hope I'm not

0:01:15.659,0:01:20.670
offending anyone I'm aware that you
already taken linear algebra and you're

0:01:20.670,0:01:25.920
very strong in it but nevertheless I'd
like to provide you my intuition my

0:01:25.920,0:01:31.320
perspective okay it's just you know one
slide not too much so maybe you'd like

0:01:31.320,0:01:36.290
to take out some paper and pen or you
can just you know whatever follow along

0:01:36.290,0:01:49.850
so so this one is going to be linear and
linear algebra review

0:01:51.170,0:02:06.450
okay am i waiting a little bit let me
wait for a sec ready yes no shake your

0:02:06.450,0:02:12.150
head okay fantastic alright so we were
talking last time we had a neural

0:02:12.150,0:02:15.270
network with the input on the bottom
then we had like an affine

0:02:15.270,0:02:18.510
transformation then we have a hidden
layer right so I'm gonna just write the

0:02:18.510,0:02:23.280
first equation we're gonna have that my
hidden layer and since I'm writing on

0:02:23.280,0:02:26.970
with a pen can you see anything yeah
so since I'm writing with a pen I'm

0:02:26.970,0:02:30.360
going to be putting a underscore
underneath the variable in order to

0:02:30.360,0:02:35.120
indicate it's a vector okay that's how I
write vectors so my H is gonna be a

0:02:35.120,0:02:42.660
nonlinear function f apply to my Z and
Zed is gonna be my linear input

0:02:42.660,0:02:46.230
okay the linear when the output of the
affine transformation so in this case

0:02:46.230,0:02:54.989
I'm gonna be writing here Z is going to
be equal to my matrix a times X okay we

0:02:54.989,0:03:00.630
can imagine there is no bias in this
case it's generic enough because we can

0:03:00.630,0:03:06.030
include it bias inside the matrix and
have the first item of the X equal to B

0:03:06.030,0:03:18.360
1 so if this X here belongs to RN and is
dead here belongs to RM first question

0:03:18.360,0:03:24.989
what is the size of this matrix okay
fantastic right so this matrix here is

0:03:24.989,0:03:30.360
gonna be our M times n you have as many
rows as the dimension where you should -

0:03:30.360,0:03:34.200
and you have as many columns the
dimension where you're shooting from ok

0:03:34.200,0:03:39.930
all right so let's expand this one right
so this matrix here is gonna be equal to

0:03:39.930,0:03:49.549
what do you have a 1 1 a 1 2 so on until
the last one which is gonna be shout

0:03:49.549,0:03:56.269
thank you 1
and yeah 1n then you have second one's

0:03:56.269,0:04:04.249
gonna have 1 a 2 1 a 2 2 so on until the
last one which is 2 n right and then you

0:04:04.249,0:04:14.840
keep going down until the last one which
is going to be what are the indexes M 1

0:04:14.840,0:04:24.740
right okay so you had a and 1 a M 2 and
so on until amen okay thank you all

0:04:24.740,0:04:37.220
right and then we have here our X right
so you have X 1 X 2 and so on until xn

0:04:37.220,0:04:39.490
right

0:04:41.680,0:04:47.479
you're more responsive than last year
good thank you alright so we can also

0:04:47.479,0:04:52.099
rewrite this one in different ways so
the first way I'm gonna write this one

0:04:52.099,0:05:00.020
is gonna be the following so I'm gonna
have here these a 1 then I'm gonna have

0:05:00.020,0:05:12.409
here my a 2 and then I have the last one
which is gonna be my a and ok and then

0:05:12.409,0:05:15.800
here I'm gonna be multiplying this by a
column vector right so my column vector

0:05:15.800,0:05:23.409
I'm gonna be write it like this alright
so what is the output of this operation

0:05:23.409,0:05:28.820
so these are metrics you have a vector
the outcome is going to be a vector so

0:05:28.820,0:05:35.780
what is gonna be the first item of my
vector I don't use dots because I'm not

0:05:35.780,0:05:40.219
a physicist actually I am that we are
doing linear algebra so what should I

0:05:40.219,0:05:42.400
write

0:05:43.810,0:05:47.509
alright so that's already transpose
because those are a row vector so I just

0:05:47.509,0:05:54.979
write a I'm gonna just mean just right
right so I have a 1 X ok so there is no

0:05:54.979,0:06:02.780
transposition here no dots around and so
on second element is gonna be a two okay

0:06:02.780,0:06:12.970
X and then until the last one which is
gonna be D okay there's no dot but sure

0:06:12.970,0:06:17.810
like someone is calling that dot product
instead of the cross product but that is

0:06:17.810,0:06:20.389
assumes like you use a different kind of
notation

0:06:20.389,0:06:26.710
all right so and this is gonna be my so
how many elements does this vector have

0:06:26.710,0:06:35.120
M okay so we have Z 1 Z 2 and so on
until Z M and this is my final set right

0:06:35.120,0:06:39.710
my vector is that okay fantastic so now
we are going to be focusing a little bit

0:06:39.710,0:06:48.590
about the meaning of this thing over
here okay other questions so far

0:06:48.590,0:06:54.530
all right this is very trivial so far I
hope I mean let me know if if not okay

0:06:54.530,0:07:00.140
okay so let's analyze one of these guys
here so I'd like to figure out what is

0:07:00.140,0:07:10.970
the meaning of writing a T times X right
so my a T is gonna be my generic a I so

0:07:10.970,0:07:24.680
let's assume in this case when n equal
to okay so what is ATX so ATX is going

0:07:24.680,0:07:29.780
to be equal to what so let me draw here
something so that is gonna be easier for

0:07:29.780,0:07:37.720
you to understand so this one is gonna
be my a are these gonna be my alpha and

0:07:37.720,0:07:48.620
then here you have like it's gonna be my
X and is this gonna be here my X I so

0:07:48.620,0:07:57.430
what is this what is the output of this
product here these are here

0:07:58.210,0:08:08.470
see you can sorry 8/8 transpose it let's
call it let's say that is a row vector

0:08:10.030,0:08:27.880
can you see know what is gonna be the
output of this operation here no why

0:08:29.410,0:08:34.610
this is this is like a generic one of
the many aids right there are M aids

0:08:34.610,0:08:39.169
this is one of those mas so I'm
multiplying one of those A's times my X

0:08:39.169,0:08:43.310
right let's assume there are only two
dimensions so what is gonna be the

0:08:43.310,0:08:50.660
output of this scalar product can
someone tell me no no no like normal

0:08:50.660,0:09:02.720
scalar product hold on
so you have egg hey here do you have

0:09:02.720,0:09:08.510
here this part here is gonna be a 1 it
is gonna be a two okay then you have

0:09:08.510,0:09:18.550
here x1 and now you have x2 right so how
do you express this scalar product here

0:09:19.980,0:09:26.730
okay so I'm just writing let me know if
you took clear so I'm gonna write here a

0:09:28.890,0:09:37.780
1 times X 1 plus a 2 times X 2 right
this is the definition clear right so

0:09:37.780,0:09:44.830
far yeah no okay yeah question yeah that
is meant to be a transpose Y a role

0:09:44.830,0:09:49.600
times column vector so let's assume a is
a column vector then I have role x

0:09:49.600,0:10:02.230
colouring so let's keep writing this
stuff here so what is a 1 how can i

0:10:02.230,0:10:15.220
compute AI a 1 second ok ok
so I'm gonna write here the a 1 is going

0:10:15.220,0:10:20.770
to be the length of vector a times
cosine alpha then what about X 1 someone

0:10:20.770,0:10:30.010
else the same right now wait what what
is X 1 same thing right different

0:10:30.010,0:10:37.450
letters so someone say something you're
following you're completely confused

0:10:37.450,0:10:43.210
you're not having any idea it's too easy
I have no idea what's going on here it's

0:10:43.210,0:10:53.770
good right so far ok what's gonna be the
second term the this one is gonna be X

0:10:53.770,0:11:00.430
here write x cos x I right and then you
had the second term which is going to be

0:11:00.430,0:11:13.920
what magnitude of a shout I can hear ok
sine of alpha and then

0:11:17.660,0:11:21.290
okay thank you okay

0:11:23.930,0:11:27.870
all right I'm gonna be just putting
together those two guys so you're gonna

0:11:27.870,0:11:40.370
get equal magnitude of a times magnitude
of x times cosine alpha cosine Phi plus

0:11:40.370,0:11:48.210
sana sine alpha and sine cosine a sine X
I sorry

0:11:48.210,0:11:54.210
what is be starting the parentheses all
right so there's the cosine of the

0:11:54.210,0:11:57.240
difference of the two angle right
everyone knows trigonometry here right

0:11:57.240,0:12:05.160
sorry high school stuff so this one is
gonna be equal to a x times the cosine

0:12:05.160,0:12:11.000
of cos x i minus alpha right oh the
other way around right from monoxides

0:12:11.000,0:12:17.040
so what does this mean you can think
about each element so far is clear I I

0:12:17.040,0:12:22.680
didn't do any magic yeah shake your head
like these four yes these four no this

0:12:22.680,0:12:27.810
for maybe no something's working okay so
you can think about whenever you

0:12:27.810,0:12:33.600
multiply a matrix times a vector that
basically each output of this operation

0:12:33.600,0:12:39.570
is gonna be measuring so okay hold on
what is this cosine how much is cosine

0:12:39.570,0:12:45.780
of zero one so it means if these two
angles if the two vectors are aligned

0:12:45.780,0:12:50.460
which means there is a zero angle
between the two vectors you can have the

0:12:50.460,0:12:57.090
maximum value of this element right
whenever when you have the minus the

0:12:57.090,0:13:03.240
most negative value when they are
opposite right so when they're in

0:13:03.240,0:13:08.310
opposition of phase you're gonna get the
most negative magnitude but then if you

0:13:08.310,0:13:11.730
apply just let's a redo you're gonna cut
all those negative things you're just

0:13:11.730,0:13:16.200
checking for the positive matches right
so neural net basically just perhaps

0:13:16.200,0:13:20.520
who's gonna be figuring out only the
positive matches right and so again when

0:13:20.520,0:13:23.620
you multiply a matrix times a column
vector you

0:13:23.620,0:13:31.330
be performing element-wise sorry scalar
product between each column each row of

0:13:31.330,0:13:36.220
the matrix which represents your kernel
right so whenever you have like a linear

0:13:36.220,0:13:40.000
layer your kernel is gonna be the whole
row of the matrix and now you see what

0:13:40.000,0:13:47.050
is the projection of that input on that
column I am on the input on that flow

0:13:47.050,0:13:52.900
right so each element of this product is
going to tell you the alignment with

0:13:52.900,0:13:57.580
which the input is what is the alignment
of the input with respect to the

0:13:57.580,0:14:04.600
specific row of the matrix okay
yes no this should shape some more like

0:14:04.600,0:14:08.290
intuition while we are using these
linear transformations they are like

0:14:08.290,0:14:13.140
allowing you to see the projection of
the input onto different kind of

0:14:13.140,0:14:22.300
orientations let's say this way okay you
can try to you know extrapolate this in

0:14:22.300,0:14:26.140
high dimensions I guess the intuition at
least I can give it to you works

0:14:26.140,0:14:30.580
definitely in two and three dimensions
in higher dimensions I kind of think it

0:14:30.580,0:14:34.209
works in a similar way next lesson we
are going to watch we are actually we

0:14:34.209,0:14:38.890
are going to see how what is the
distribution of the projections in a

0:14:38.890,0:14:43.240
higher dimensional space is this sort
gonna be so cool I think all right so

0:14:43.240,0:14:49.779
this was the first part of I think of
the class oh well there is one more more

0:14:49.779,0:14:54.940
part so actually here this said here we
can also write it in a different way so

0:14:54.940,0:15:00.130
maybe this is maybe it's known maybe
it's not known when I saw it first time

0:15:00.130,0:15:05.050
I didn't know so you know it's it's cool
that sometimes you see these things once

0:15:05.050,0:15:10.450
again maybe so let's go back here is the
things that there and so you can express

0:15:10.450,0:15:18.820
this Z as being equal the vector a 1 in
this case a 1 is gonna be the first

0:15:18.820,0:15:22.830
column of the a matrix ok and this one
is going to be multiplied by the scalar

0:15:22.830,0:15:29.380
x1 now you have the second column of the
matrix so I have a 2 this is multiplied

0:15:29.380,0:15:34.870
by the second element of the X right
until the last one which is gonna be

0:15:34.870,0:15:44.930
again I can't hear if it's M or n m like
this or end you know sign language which

0:15:44.930,0:15:52.880
one n right you know sign language no
you should learn it's good you know

0:15:52.880,0:15:59.630
inclusivity a and write the last column
times your X and of course because n X

0:15:59.630,0:16:04.100
has a size of n there are n items right
and so basically when you also when you

0:16:04.100,0:16:07.940
do a you know transformation a linear
you know apply a linear operator you

0:16:07.940,0:16:12.890
can't be basically waiting each column
of the matrix with the coefficient that

0:16:12.890,0:16:16.640
is in a you know you have first column
times the first coefficient of the

0:16:16.640,0:16:22.130
vector second column and by the second
item plus third column times the third

0:16:22.130,0:16:26.420
item and so you can see the output of
this DN transformation is a weighted sum

0:16:26.420,0:16:32.180
of the columns of the a matrix okay so
this is a different kind of intuition

0:16:32.180,0:16:36.620
sometimes you see these as like whenever
you want to express your signal your

0:16:36.620,0:16:45.290
data is a combination of different you
know the composition this is kind of a

0:16:45.290,0:16:50.600
linear composition of your input alright
so that was the first part it is the

0:16:50.600,0:16:57.530
recap about the linear algebra a second
part is going to be something even more

0:16:57.530,0:17:06.790
cool I think questions so far
no easy too easy you're getting bored

0:17:06.790,0:17:10.670
sorry okay all right then I'm gonna
speed up I guess

0:17:10.670,0:17:15.170
alright so let's see how we can extend
the one that the stuff we have seen

0:17:15.170,0:17:19.040
right now to convolutions right so maybe
convolution sometimes are a little bit

0:17:19.040,0:17:28.900
weird let's see how we can do an
extension to convolutions

0:17:31.720,0:17:38.390
all right so let's say I start with the
same matrix so I'm gonna have here four

0:17:38.390,0:17:53.660
rows and then three columns okay so my
data has to be if I had if I had this

0:17:53.660,0:17:59.360
matrix if I multiply this to a column my
column vector should be of size three

0:17:59.360,0:18:04.250
thank you all right so let me draw here
my column vector of size right and this

0:18:04.250,0:18:08.809
is gonna give you a output of size four
okay fantastic

0:18:08.809,0:18:15.740
but then is your data let's say you're
gonna listen to some nice audio audio

0:18:15.740,0:18:21.260
file is your data just three simple long
how long is gonna be your data let's say

0:18:21.260,0:18:24.590
you listening to a piece of music that
is like three minutes

0:18:24.590,0:18:32.330
how many sample does three minutes of
all your health yeah I guess what is

0:18:32.330,0:18:40.070
gonna be my sampling rate let's say
twenty two okay twenty two thousand kilo

0:18:40.070,0:18:46.480
Hertz right 22 kilohertz so how many
samples three minutes of music have

0:18:47.799,0:18:58.010
second you sure is monophonic
or stereophonic just kidding okay so

0:18:58.010,0:19:02.650
you're gonna multiply the number of
sample the number of seconds right

0:19:02.650,0:19:08.660
number of seconds times the frame rate
right the okay the frequency in this

0:19:08.660,0:19:12.620
case anyhow so this signal is gonna be
very very long right it's gonna be keep

0:19:12.620,0:19:16.940
going down so if I have a vector that is
very very long I have I have to use a

0:19:16.940,0:19:23.540
matrix which is gonna be very very fat
wide right okay fantastic so this top

0:19:23.540,0:19:27.080
keeps going this direction
alright so my question for you is gonna

0:19:27.080,0:19:31.540
be what should I put in this location
here

0:19:35.570,0:19:48.020
what should I put here so do we care
about things that are further away

0:19:48.740,0:19:54.000
no why not
because our data has the property of

0:19:54.000,0:20:00.299
locality fantastic so what am I gonna go
what am I gonna be putting here a big

0:20:00.299,0:20:06.690
zero right fantastic mom okay so we put
a zero here and then what is the other

0:20:06.690,0:20:09.809
property so let me start drawing this
stuff again right so can I have my

0:20:09.809,0:20:16.890
kernel of size three and then here I
have my data which is gonna be very long

0:20:16.890,0:20:31.169
right so hold on I can see all right so
here there are zero then let's say what

0:20:31.169,0:20:36.960
is the other property my my my date my
natural data have stationarity which

0:20:36.960,0:20:45.059
means the pattern that you expect to
find this can be kind of repeating over

0:20:45.059,0:20:49.140
and over again right and so if I have
here my three values here perhaps I'd

0:20:49.140,0:20:53.789
like to reuse them over and over again
right and so if these three values let

0:20:53.789,0:21:00.779
me change the color maybe so you can see
in there is the same stuff so I have

0:21:00.779,0:21:07.230
three values here and then I'm gonna be
using these three same values one step

0:21:07.230,0:21:16.770
for the rain and I keep going further I
don't down and I keep going this way all

0:21:16.770,0:21:22.799
right so what should I put here on the
bottom what should I put here a zero

0:21:22.799,0:21:28.529
right why that why is that because of
locality of the data right so putting

0:21:28.529,0:21:36.350
zeros around is called it's also called
padding but in this case it's called

0:21:36.350,0:21:44.850
sparsity right so this is like sparsity
and then the replication of this thing

0:21:44.850,0:21:52.730
that is over and over over again
it's called session I'd was the property

0:21:52.730,0:21:57.430
of the signal this is called weight
sharing yeah

0:22:04.330,0:22:10.090
okay fantastic all right so how many how
many values do we have now how many

0:22:10.090,0:22:13.530
parameters I have on the right hand side

0:22:15.150,0:22:22.750
well so we have three parameters over
left on the left hand side instead we

0:22:22.750,0:22:28.900
had product right so if the right side
is gonna be easy is the right side gonna

0:22:28.900,0:22:36.810
work at whole you have three parameters
one side on the other side you have 12

0:22:36.810,0:22:41.860
okay that is good using locality and
spiral and whatever not like sparsity

0:22:41.860,0:22:45.610
and parameter sharing but then now we
end up with just three parameters isn't

0:22:45.610,0:22:51.190
this too restrictive how can we have
multiple multiple parameter what's

0:22:51.190,0:22:56.290
missing here in the big picture there
are multiple channels right so this is

0:22:56.290,0:23:01.570
just one layer here and then you have
this stuff coming out from the from the

0:23:01.570,0:23:08.230
from the board here so you have the
first kernel down here now he has some

0:23:08.230,0:23:20.500
second kernel let's say listen and I
have last one here right and so you have

0:23:20.500,0:23:24.760
each plane of these metrics being
containing just one kernel which is

0:23:24.760,0:23:36.190
replicated multiple times who knows the
name of this matrix now so this is gonna

0:23:36.190,0:23:40.650
be called triplets top list matrix

0:23:43.419,0:23:48.219
okay so what is the main feature of
these topless metrics what is the big

0:23:48.219,0:24:01.479
big big thing that you won't notice it's
a sparse matrix okay okay what is gonna

0:24:01.479,0:24:16.149
be here this first item over here what
is the content of the first guy yeah so

0:24:16.149,0:24:21.669
this one here is gonna be the extension
of my linear transformation which was

0:24:21.669,0:24:26.169
you know I have a signal that is longer
than three samples therefore I have to

0:24:26.169,0:24:32.229
make this matrix fatter second part is
might be given that I don't care about

0:24:32.229,0:24:36.129
what things like things that are here
down I don't care about things that are

0:24:36.129,0:24:40.299
here if I look at the points that are up
here so gonna be putting a big 0 here

0:24:40.299,0:24:45.070
such that Oh everything that is down
here it gets cleaned up right and then

0:24:45.070,0:24:49.899
finally I'm gonna be using the same
kernel over and over again because I

0:24:49.899,0:24:55.839
assume that my data is stationary and
therefore I assume that similar patterns

0:24:55.839,0:24:58.929
are gonna be happening over and over
again therefore I'm gonna be using this

0:24:58.929,0:25:03.429
one which is written here parameter
sharing weight sharing

0:25:03.429,0:25:08.019
finally given that this one gives you
only three parameter two parameters to

0:25:08.019,0:25:12.940
work with I will use several layers in
order to have different you know

0:25:12.940,0:25:17.739
channels so this one is one kernel
before one kernel was the whole row of

0:25:17.739,0:25:21.849
the matrix okay so when you have a fully
connected layer the only difference

0:25:21.849,0:25:25.179
between a fully connected layer and a
convolution is that you have the full

0:25:25.179,0:25:37.019
dam row of the matrix so what is gonna
be in this first item over here anyone

0:25:38.480,0:25:43.230
so the green colonel let's call the
green corner just a one let me actually

0:25:43.230,0:25:55.200
make it glow green because it's green
karna so you have a one times what it's

0:25:55.200,0:25:58.890
gonna be from the number one to number
three right and then the second item is

0:25:58.890,0:26:05.640
gonna be same same dude here a 1 and
then you're gonna have the X shifted by

0:26:05.640,0:26:20.039
one and so on right make sense yeah and
then we're gonna have this one is going

0:26:20.039,0:26:23.970
to be the green output then you're gonna
have the blue output one layer coming

0:26:23.970,0:26:26.730
out and then you have the further one
the red one coming

0:26:26.730,0:26:32.929
we've been one layer out okay what's the
iPad experience cool

0:26:32.929,0:26:40.520
yes no I liked it okay other questions

0:26:41.029,0:26:49.049
so again the blue circle this one it's a
big zero

0:26:49.049,0:26:53.480
that's the sparsity one the same is here

0:26:53.899,0:26:58.520
yes no yeah yeah

0:27:01.530,0:27:16.200
yeah green so here I just put a lot of
zeros inside here so I killed all the

0:27:16.200,0:27:21.000
values that are away from the little
part and then I repeat the same three

0:27:21.000,0:27:24.480
values over and over again because I
expect to find the same pattern in

0:27:24.480,0:27:33.360
different regions of this is the big big
signal I have this one here so I said

0:27:33.360,0:27:36.840
that in this case I'm gonna have just
three values right and we started with

0:27:36.840,0:27:41.040
12 values and I ended up with 3 which is
really really little so if I'd like to

0:27:41.040,0:27:44.760
have let's say 6 values then if I want
to have six values and I can have my

0:27:44.760,0:27:49.530
second 3 on a different plane and I
perform the same operation whenever you

0:27:49.530,0:27:55.110
multiply this matrix times a vector you
perform a convolution so it just to tell

0:27:55.110,0:27:58.770
you that a convolution is just a matrix
multiplication with a lot of zeros

0:27:58.770,0:28:09.600
that's it yeah so they're gonna have
this one here then you have a second one

0:28:09.600,0:28:14.790
you have a third one so you have three
versions of the input alright so for a

0:28:14.790,0:28:18.000
second part of the class I'm gonna be
showing you some more interactive things

0:28:18.000,0:28:27.810
please do participate to the second part
as well alright so let's try so I have

0:28:27.810,0:28:37.950
rebranded I have rebranded the website
and now has slightly some like that the

0:28:37.950,0:28:42.450
environment is going to be called P PBL
so pi torch deep learning instead of

0:28:42.450,0:28:49.980
sleep learning mini course it was too
long so let me start by running this one

0:28:49.980,0:28:52.460
so

0:28:55.320,0:29:04.960
my torch deep learning so we can do this
Conda activate activate PI torch deep

0:29:04.960,0:29:12.940
learning and then let's open not the
Jupiter notebook all right so now you're

0:29:12.940,0:29:18.520
gonna be watching the going over the
listening to corners so I show you a

0:29:18.520,0:29:22.630
convolution on paper well on my tablet
now got me making listening to

0:29:22.630,0:29:25.990
convolution so can such that you can
really appreciate what this compositions

0:29:25.990,0:29:35.919
are here we said a the new kernel right
which is called P PBL Python sleep

0:29:35.919,0:29:42.520
learning so you're gonna notice the same
kind of you know procedure if you update

0:29:42.520,0:29:49.690
your system alright so in this case we
can read the top here so let me hide the

0:29:49.690,0:29:52.200
top here

0:29:52.890,0:29:56.950
alright so given the assumption of
locality stationarity and composition

0:29:56.950,0:30:00.280
compositionality we can reduce the
amount of computation for a matrix

0:30:00.280,0:30:05.169
vector multiplication by using a sparse
because local popplets matrix because

0:30:05.169,0:30:09.850
stationary scheme in this way we can
simply end up rediscovering the

0:30:09.850,0:30:14.980
convolution operator array moreover we
can also recall that a scalar product is

0:30:14.980,0:30:19.150
a simply a normalized cosine distance
which tells us the alignment of two

0:30:19.150,0:30:21.850
vectors
more specifically we compute the

0:30:21.850,0:30:26.320
magnitude of the orthogonal projection
of two vectors onto the other and vice

0:30:26.320,0:30:29.590
versa
so let's figure out now how all these

0:30:29.590,0:30:34.270
can make sense by using our ears okay so
i'm going to be importing a library that

0:30:34.270,0:30:39.880
professor here at NYU made and here I'm
gonna be just loading my audio data and

0:30:39.880,0:30:43.600
I'm gonna have that in my X and then my
sampling grade is going to be

0:30:43.600,0:30:48.570
in the other variable so here I'll just
show you I'm gonna have like 70,000

0:30:48.570,0:30:54.430
samples in this case because I have a
simple in rate of 22 kilo Hertz and then

0:30:54.430,0:31:01.720
my total time is gonna be three seconds
okay so three seconds times 22 you get

0:31:01.720,0:31:06.910
what so it's not 180 you were saying it
was a hundred and eighty was three right

0:31:06.910,0:31:11.380
oh it was three minutes oh you're right
it's three seconds so you actually are

0:31:11.380,0:31:16.540
correct my dad so this is three seconds
so times 22 kilo Hertz you get 17

0:31:16.540,0:31:22.390
roughly some 70,000 samples here I'm
gonna be important some libraries to

0:31:22.390,0:31:28.180
show you something and then I'm gonna be
showing you the first chart so this is

0:31:28.180,0:31:37.270
the audio signal I have imported right
now how does it look like wavy okay cool

0:31:37.270,0:31:50.680
can you tell me how it sounds that was a
good guess the guess was you can tell is

0:31:50.680,0:31:55.450
actually what's the content right from
this diagram because the amplitude of

0:31:55.450,0:32:01.240
like the the the y-axis here is gonna be
showing you just the amplitude can I

0:32:01.240,0:32:05.530
turn off the light is okay or you're you
sure okay thank you

0:32:05.530,0:32:16.090
I really dislike this like okay
goodnight Oh see how nice it is okay

0:32:16.090,0:32:19.930
cool alright so you can't tell anything
here right

0:32:19.930,0:32:26.580
you can't not tell what is the what is
the sound right so how can we figure out

0:32:26.580,0:32:31.870
what is the sound inside here so for
example I can show you a transcript over

0:32:31.870,0:32:37.660
the sound and actually let me let me
actually force these in your in your

0:32:37.660,0:32:44.810
your head right so you're gonna have
hold on your network

0:32:44.810,0:32:50.610
all right so now we actually heard it
okay so now you can actually see ting

0:32:50.610,0:32:56.400
ting ting boom you know you can kind of
imagine a little bit but okay so what

0:32:56.400,0:33:00.830
notes did we play there like how can I
figure out what put out the notes that

0:33:00.830,0:33:05.550
they are inside so I'm gonna be showing
this one so since this is a bit brighter

0:33:05.550,0:33:13.820
I can see your faces. How many of you can
not read this? Oh, ouch…

0:33:13.820,0:33:20.960
Okay, so let me see if I can ask for some help.

0:33:23.620,0:33:26.620
Maybe someone can help us out here.

0:33:29.400,0:33:32.480
Okay. Let's see.

0:33:40.140,0:33:42.140
Hey, hey Alf!
Oh, hi Alf!

0:33:42.880,0:33:45.420
How's going?
Yeah, yeah, I'm fine, thank you.

0:33:45.420,0:33:47.040
Nice class there!
Oh, thank you for the class.

0:33:47.040,0:33:49.040
Oh, nice sweater you too!
Nice sweater!

0:33:49.040,0:33:51.040
Oh, we're wearing the same sweater!

0:33:51.040,0:33:54.480
Can you help us out?
They don't know how to read the

0:33:54.480,0:33:57.120
Oh, the connection…
What the f- hell!

0:33:57.120,0:34:00.380
They cannot read the score!
Can you help us out, please?

0:34:00.380,0:34:02.380
Alright!
Let me try to help you out.

0:34:02.380,0:34:04.380
Thank you!
Let me switch the camera.

0:34:04.380,0:34:06.380
Alright.
Please, do.

0:34:06.380,0:34:08.380
So, here we can go like…

0:34:08.380,0:34:10.860
and hear first how it sounds everything.

0:34:10.860,0:34:14.320
So, it's going to be like that.

0:34:14.440,0:34:23.380
How cool is that?

0:34:23.380,0:34:27.990
Thank you. It took four
lessons for you to clap me. So now…

0:34:27.990,0:34:32.360
This is very nice of you. Let's keep
going.

0:34:32.360,0:34:36.320
A♭, then we have an E♭, and then a A♭.

0:34:36.320,0:34:40.380
The difference between the first A♭and the other one in frequencies

0:34:40.380,0:34:46.540
is that the first A♭ is going to be twice the frequency as the other one.

0:34:46.540,0:34:51.400
And instead, in the middle, we have the 5th.
We're going to figure out what is the frequency of that.

0:34:51.400,0:34:55.320
And then, we are gonna be going to a B♭, here.

0:34:55.320,0:34:57.680
On the left hand side, instead, we have the accompagnement,

0:34:57.680,0:35:01.220
and so we're gonna have a A♭ and B♭

0:35:01.220,0:35:05.620
And then B♭ and E♭.

0:35:05.680,0:35:10.900
So, if we put all together, we gonna get
this one.

0:35:11.300,0:35:14.020
Alright? Simple, no?
Yep! Thank you!

0:35:14.020,0:35:17.320
Bye-bye!
Byeeeeee!

0:35:18.820,0:35:23.540
See? This took a whole damn day to
prepare…

0:35:23.540,0:35:27.480
I was so nervous before come here…

0:35:27.480,0:35:32.140
I didn't know if it actually would have worked…
Both of them, tablet and this one.

0:35:32.140,0:35:35.060
I'm so happy!
Now I can actually go to sleep, later.

0:35:35.060,0:35:39.280
Anyhow, so this was like in the first

0:35:39.290,0:35:43.280
part you're gonna have the first note
there's A♭ that you have a B♭

0:35:43.280,0:35:51.550
A♭ and B so you have … and the and the
difference between the first pitch and

0:35:51.550,0:35:57.440
is one octave there for the first
frequency is gonna be twice the second

0:35:57.440,0:36:02.870
frequency okay so whenever we're gonna
be observing the waveform one sign will

0:36:02.870,0:36:07.730
have to a shorter like the half the
period of the other one right the

0:36:07.730,0:36:12.410
especially the the a the a flat on top
is gonna have a period which is half of

0:36:12.410,0:36:20.000
the period of the a on the bottom one
right so you have okay if you go half of

0:36:20.000,0:36:27.290
this one you get right okay okay so how
do we actually get these notes out from

0:36:27.290,0:36:33.770
that spectra from the from the waveform
who can tell me how can I extract these

0:36:33.770,0:36:40.790
pitches these frequencies from the other
signal any guess okay a free transform

0:36:40.790,0:36:45.530
that I think if are kind of a good guess
what does it happen if I perform now a

0:36:45.530,0:36:50.660
Fourier transform of that signal anyone
can actually answer me you cannot raise

0:36:50.660,0:36:54.100
your hand because I don't see just shout

0:36:55.120,0:36:59.690
so if you basically perform the Fourier
transform of the whole scene are you

0:36:59.690,0:37:06.470
gonna hear like the whole notes together
all together right but then you can't

0:37:06.470,0:37:13.260
figure out which pitch is playing where
or when actually in this case right

0:37:13.260,0:37:18.210
ha so we need kind of a Fourier
transform which is localized and so a

0:37:18.210,0:37:23.190
localized Fourier transform in time or
in space depending on whatever domain

0:37:23.190,0:37:27.390
you are using its called spectrograph
right and so on I'm gonna be now

0:37:27.390,0:37:30.000
printing for you the spectrogram oh
sorry

0:37:30.000,0:37:34.380
and I'll be printing here the
spectrogram of this one here and so here

0:37:34.380,0:37:39.960
you can compare the two alright on the
first part here on this side here you're

0:37:39.960,0:37:47.970
gonna have this peak here at 1600 which
is the the higher power there we go now

0:37:47.970,0:37:56.640
you have a second one which is this peak
here you can see this peak right and you

0:37:56.640,0:38:04.260
see this peak yeah okay so these Peaks
are gonna be the actual notes I play

0:38:04.260,0:38:07.560
with the right hand
so let's actually put those together and

0:38:07.560,0:38:13.140
I'm gonna have here the frequencies so I
have 1600 1200 and 800 can you see here

0:38:13.140,0:38:20.550
I have 1600 and 800 Y one is double of
the other because they are one octave

0:38:20.550,0:38:27.540
apart so if this is this gonna be okay
and this is a fifth which is like a also

0:38:27.540,0:38:32.280
has a nice interval so then we actually
generate these signals here and then I

0:38:32.280,0:38:36.720
gotta be concatenate the mouth so I'm
gonna be playing both these the first

0:38:36.720,0:38:42.109
one is actually the original audio but

0:38:42.109,0:38:45.470
let me try again

0:38:45.680,0:38:54.860
the second one the concatenation yeah so
it's a bit loud I can now cannot even

0:38:54.860,0:39:01.720
reduce the volume oh I can reduce it
here too much okay let me go again all

0:39:02.020,0:39:05.350
right
so this is the concatenation of these

0:39:05.350,0:39:12.380
four different pitches so guess what we
are going to be doing next so how can I

0:39:12.380,0:39:20.420
extract all the notes that I can hear in
a specific given piece so let's say I

0:39:20.420,0:39:29.360
you play a full score and I'd like to
know which pitch is play at what time of

0:39:29.360,0:39:36.230
what so the answer was convolution just
for the recording I'm asking convolution

0:39:36.230,0:39:43.370
of what no convolutional spectrogram so
you have convolution of your input

0:39:43.370,0:39:49.460
signal with what with some different
kind of pitches right which pitches will

0:39:49.460,0:39:59.150
you pick let's say you don't see the
spectrum because let's say I'm I just

0:39:59.150,0:40:03.770
play any kind of piece of music so I'd
like to know all possible notes they are

0:40:03.770,0:40:06.220
there what would you do

0:40:06.220,0:40:13.430
you don't know all pitches how would you
try that right so in which are all the

0:40:13.430,0:40:20.900
pitches you may want to use if you're if
I'm playing the piano all the keys of

0:40:20.900,0:40:24.530
the piano right so if I have if I play a
concerto with the piano then I want to

0:40:24.530,0:40:28.010
have like a piece of audio for each of
those keys and I'm gonna be running

0:40:28.010,0:40:32.690
convolutions of my whole piece with the
old kids right and therefore you're

0:40:32.690,0:40:36.470
gonna see hits which are the alignment
of the cosine similarity

0:40:36.470,0:40:41.349
whenever you get basic
the audio matching your specific kernel

0:40:41.349,0:40:46.989
so I'm gonna be doing this but with
these specific tones I actually extract

0:40:46.989,0:40:52.929
it here so here I'm going to be showing
you first how the two spectrograms look

0:40:52.929,0:40:57.699
like the left side is going to be the
spectrogram of my actual signal X of T

0:40:57.699,0:41:01.630
and on the right hand side I have just
the spectrogram of this concatenation of

0:41:01.630,0:41:10.749
my pitches so here you can clearly see
this but then here is first of all what

0:41:10.749,0:41:14.429
are these bars here these vertical bars

0:41:15.269,0:41:20.589
you follow me right I can see you had to
actually talk back what are these red

0:41:20.589,0:41:24.160
bars here vertical bars now it don't
tell van I already told you right

0:41:24.160,0:41:34.390
these are and the vertical what what is
it sampling issues right transitions so

0:41:34.390,0:41:39.099
whenever you have deep you actually have
one white waveform one waveform and then

0:41:39.099,0:41:44.019
the other one one away from his to stop
so it's no longer periodic and whenever

0:41:44.019,0:41:47.609
you do a Fourier transform of a non
periodic signal you get you know crop

0:41:47.609,0:41:53.589
that's why whenever you get the junk to
the junk junction between these the jump

0:41:53.589,0:41:57.729
here you're gonna have this spike
because it's gonna get basically you can

0:41:57.729,0:42:01.749
think about the jump is like having a
very high frequency right because it's a

0:42:01.749,0:42:05.469
it's like a delta right so you actually
get all the frequencies that's why you

0:42:05.469,0:42:12.549
get all the frequencies here boom okay
makes sense right so far kind of alright

0:42:12.549,0:42:18.720
so this is the green version that I
cannot even sign and what

0:42:18.720,0:42:25.800
left side here why is on the left-hand
side all red down there okay

0:42:25.800,0:42:31.440
yes you know so the left side left hand
the cold are the one I show you on the

0:42:31.440,0:42:37.320
bottom left side okay so let me finish
this class and then I'll let you go so

0:42:37.320,0:42:42.990
here I'm gonna be convoyed show you
first all the kernels you can tell now

0:42:42.990,0:42:48.090
the red one is going to be the first
chunk of my signal they want the real

0:42:48.090,0:42:53.280
one and then you can see the first pitch
is gonna be the has the same frequency

0:42:53.280,0:42:58.460
can you see
so the problem have the same same same

0:42:58.460,0:43:04.230
delta T the same interval the same
period can you see you cannot nod your

0:43:04.230,0:43:08.700
head because again I don't see you had
to answer me can you see or not okay

0:43:08.700,0:43:13.050
thank you fantastic and so this one is
that the third one you can see that it

0:43:13.050,0:43:17.130
starts here in the period-- and it
finishes here if you go up here you're

0:43:17.130,0:43:20.520
gonna see exactly there were two of
these guys right so that's why you can

0:43:20.520,0:43:24.619
see that's how you can see this is like
twice the frequency of the one below

0:43:24.619,0:43:30.089
finally I'm gonna be performing the
convolution of these four corners with

0:43:30.089,0:43:36.839
my input signal and this is how we look
like okay so the first kernel has a high

0:43:36.839,0:43:42.150
match in the first part of the of the
score so between zero and zero five

0:43:42.150,0:43:46.830
seconds the second one starts just after
after the first one then you have the

0:43:46.830,0:43:50.820
third one starting at zero three I guess
and then you have the last one

0:43:50.820,0:43:56.940
starting at the zero six okay and so
guess what I'm gonna make you listen to

0:43:56.940,0:44:02.520
convolutions now are you excited
oh okay you actually answering now good

0:44:02.520,0:44:07.440
alright and so these are the outcomes
let me lower a little the volumes

0:44:07.440,0:44:14.900
otherwise you complain yeah I can't
lower the

0:44:16.800,0:44:23.690
okay so the first one let's try again

0:44:28.880,0:44:37.110
how cool is that it's just listen to
convolution okay so basically this was

0:44:37.110,0:44:41.280
on shit I have one more slide because I
felt like there was some confusion last

0:44:41.280,0:44:45.360
time about what is the different
dimensionality of different type of

0:44:45.360,0:44:50.850
signals so I'm really recommending to go
and take the class of young Bruna which

0:44:50.850,0:44:56.580
is math for deep learning and I stole
one of his you know small things he was

0:44:56.580,0:45:04.320
teaching I just put a one slide here
together for you so this slide is the

0:45:04.320,0:45:13.140
following so okay so we have the input
layer or the samples we provide into

0:45:13.140,0:45:18.420
this network and so usually our last
time I define this I have this curly X

0:45:18.420,0:45:23.010
which is gonna be made of those x i's
which are all my data same post right

0:45:23.010,0:45:29.100
and so I usually have em there are
samples so my I goes from n 1 to n ok so

0:45:29.100,0:45:34.080
is this clear on is this notation clear
because it's a bit more formal I use AMA

0:45:34.080,0:45:39.030
less formal but then somehow someone was
feeling a little bit not comfortable I

0:45:39.030,0:45:46.470
think so this one is just my input
samples but we we can also see this one

0:45:46.470,0:45:52.950
is this curly X which is my input set as
the set of all these functions as x i's

0:45:52.950,0:45:59.850
which are mapping my Omega capital Omega
which is my domain to a RC which are

0:45:59.850,0:46:06.150
going to be basically my channels of
that specific example in so here I'm

0:46:06.150,0:46:14.040
gonna be mapping those lowercase Omega
to these x i's of omega so let's see how

0:46:14.040,0:46:17.550
these different this from the previous
notation so I'm gonna give you now three

0:46:17.550,0:46:21.300
examples and I you should be able to
tell now what is the dimensionality and

0:46:21.300,0:46:24.820
is all in this example so the first one
let's say

0:46:24.820,0:46:29.560
I have likely one and show you right now
in the time just fun piece of you know

0:46:29.560,0:46:34.870
audio signal so my Omega is going to be
just the samples like sample number one

0:46:34.870,0:46:39.550
sample number two like the index right
so you have index one index to index

0:46:39.550,0:46:44.740
until these 70,000 whatever we just saw
right now okay and the last value is

0:46:44.740,0:46:49.330
going to be the T capital T which is the
number of seconds divided by the delta T

0:46:49.330,0:46:53.200
which would be the you know 1 over the
frequency and this is going to be a

0:46:53.200,0:46:57.250
subset of n right so this is a discrete
number of samples because you have a

0:46:57.250,0:47:03.220
computer you always have discrete
samples so this is my input data and

0:47:03.220,0:47:09.310
then what about the image of this
function so when I ask what is the

0:47:09.310,0:47:13.330
dimensionality of this type of signal
you should answer this is a

0:47:13.330,0:47:19.360
one-dimensional signal because the power
of these n here is 1 ok so this is like

0:47:19.360,0:47:25.000
a 1 dimensional signal although you may
have the total total time and sorry the

0:47:25.000,0:47:28.900
1 there it was a sampling interval on
the right hand side you have the number

0:47:28.900,0:47:33.340
of channels can be 1 if you have a mono
signal you can have 2 if you have a

0:47:33.340,0:47:38.230
stereophonic so you have mono there you
have a 2/4 stereophonic or what is 5

0:47:38.230,0:47:44.650
plus 1 that should all be like 5.1 how
cool alright so this is still a one

0:47:44.650,0:47:48.430
dimensional signal which may have
multiple channels nevertheless is still

0:47:48.430,0:47:53.020
one dimensional signal because there is
only one running variable there ok is it

0:47:53.020,0:47:58.270
somehow better than last time yes no
better thank you

0:47:58.270,0:48:03.070
let's say thanks to John alright second
example I have here my Omega is going to

0:48:03.070,0:48:07.630
be the Cartesian product of these two
sets the first set is going to be going

0:48:07.630,0:48:13.150
from 1 to the height and also this one
in discrete and the other one is going

0:48:13.150,0:48:17.020
to be going from 1 to the width so these
are the actual pictures and so this one

0:48:17.020,0:48:21.690
is a 2 dimensional signal because I have
2 different 2 degrees of freedom in my

0:48:21.690,0:48:28.690
domain what are the possible channels we
have so here the possible channels that

0:48:28.690,0:48:32.440
are very very common are the following
so you can have a grayscale image and

0:48:32.440,0:48:36.849
therefore you only output one
scrape one scalar value or you get the

0:48:36.849,0:48:42.789
rainbow there the color and therefore
you get like my X which is a function of

0:48:42.789,0:48:49.299
the coordinates w1 sorry Omega 1 Omega 2
which is not be the each point is

0:48:49.299,0:48:52.690
represented by a vector of three
components which is gonna be the R

0:48:52.690,0:48:59.109
component of the point Omega 1 Omega 2
the G component at Omega 1 Omega 2 and

0:48:59.109,0:49:04.299
the blue component of Omega 1 Omega 2 so
again you can all think about this as a

0:49:04.299,0:49:08.980
big big data point or you can think
about this as function mapping a low

0:49:08.980,0:49:12.640
dimensional domain which is a two
dimensional domain to a three

0:49:12.640,0:49:18.490
dimensional domain right finally the
twenty who who knows the name of the

0:49:18.490,0:49:24.400
twenty channel image yeah this is a
hyper spectral image it's very common to

0:49:24.400,0:49:31.869
have a better than 20/20 bandwidth but
then sorry finally cooking gas this one

0:49:31.869,0:49:40.829
my if my my domain is r4 x r4 what can
it be

0:49:41.099,0:49:50.589
no no this discrete right
this is r4 so it's not even computer ha

0:49:50.589,0:49:56.740
who said something that I heard yes is
this correct so this is space-time what

0:49:56.740,0:50:00.849
is the second one
yeah which momentum has a special

0:50:00.849,0:50:07.779
special name it's called for momentum
because it has a temporal information as

0:50:07.779,0:50:12.160
well right
and so what's gonna be my possible image

0:50:12.160,0:50:24.790
of the X function let's say it's equal 1
what is it do you know

0:50:24.790,0:50:29.630
so this could be for example the
Hamiltonian of the system okay so this

0:50:29.630,0:50:36.460
was like a bit more mathematical
you know introduction or mathematical

0:50:37.000,0:50:42.890
procedure how do you say you'll make
make a more precise definition so that

0:50:42.890,0:50:48.980
was pretty much everything for today let
me turn on the light and let I see you

0:50:48.980,0:50:54.969
on next Monday
thank you for being with me


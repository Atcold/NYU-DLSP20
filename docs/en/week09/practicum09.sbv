0:00:00.030,0:00:05.609
today we're going to be talking about genitive address our networks or how to

0:00:05.609,0:00:12.750
actually have them properly made alright so generated other cyanate works

0:00:12.750,0:00:17.850
unsupervised learning generative models so generating models again our models

0:00:17.850,0:00:23.330
that allow you to get something that is in the input space most of the time

0:00:23.330,0:00:28.439
that's you know what is happening in this field is that we assume there is

0:00:28.439,0:00:35.880
like a probability distribution over these samples it doesn't have to for

0:00:35.880,0:00:42.329
example a decoder in a classical encoder can be thought as a generative model in

0:00:42.329,0:00:47.190
my opinion and also for yan many will disagree and they say generative model

0:00:47.190,0:00:53.640
has to have like an input which you know follow a specific distribution we are in

0:00:53.640,0:00:59.129
the realm of unsupervised learning where we don't have labels and so let's get

0:00:59.129,0:01:03.230
started with generative others are networks

0:01:05.310,0:01:10.120
so what is this stuff aha you should know all right so this is

0:01:10.120,0:01:14.110
the variation of the encoder the variation of encoder is basically like a

0:01:14.110,0:01:19.930
normal thing coder where the encoder in this case provides us the parameters for

0:01:19.930,0:01:25.660
a distribution from where we sample our latent input to z okay so the only

0:01:25.660,0:01:31.240
difference between the normal one is the again the sampler which is gonna kick a

0:01:31.240,0:01:36.700
random sample so instead of having one simple code which is like one point you

0:01:36.700,0:01:40.840
have one input here you have one code here instead now you're gonna have like

0:01:40.840,0:01:46.840
got some volume and therefore each like point within this volume will be mapped

0:01:46.840,0:01:53.350
back to the original point yeah that's you know a very important part about the

0:01:53.350,0:01:57.970
variation of the encoder so let's see how these generally generative address

0:01:57.970,0:02:06.040
Arnette look like so we have this stuff which is huh it's actually the same

0:02:06.040,0:02:12.690
right so what's going on here we have the same generator in the same sampler

0:02:12.690,0:02:19.959
okay and then what is we have okay we have another input there so the input

0:02:19.959,0:02:23.590
before it was on the left-hand side on the bottom now the input is halfway

0:02:23.590,0:02:30.340
through and the output is actually also halfway through finally we get that kind

0:02:30.340,0:02:35.170
of switch and then on top of that switch we're gonna have a cost network usually

0:02:35.170,0:02:41.590
in the classical definition in the classical formulation of a gun there we

0:02:41.590,0:02:46.720
have like a discriminator this commentar needs discriminators are just plain

0:02:46.720,0:02:53.590
wrong option at least following young suggestions which I agree with because

0:02:53.590,0:03:00.450
we see soon why we see that in a bit all right now let's focus on the fact that

0:03:00.450,0:03:06.730
we have this cost Network okay so let's we have basically similar models right

0:03:06.730,0:03:10.690
sampler on the right right hand side this sampler on the left hand side we

0:03:10.690,0:03:15.639
have a decoder on the left hand side which is basically generating something

0:03:15.639,0:03:20.950
but since the z is consider a code we have a decoding step whereas on the

0:03:20.950,0:03:25.120
right hand side since z was not a code but was simply an input then we have a

0:03:25.120,0:03:30.310
generator and that the said is simply for example sample sample from Gaussian

0:03:30.310,0:03:34.060
distribution wait you know normal distribution and

0:03:34.060,0:03:40.329
then that X hat will be generated by this initially untrained Network the

0:03:40.329,0:03:46.989
cost network instead has to figure out it has to be a high cost if we feed that

0:03:46.989,0:03:54.819
X hat the blue one because we want to give like we want to say it is a bad

0:03:54.819,0:04:00.159
sample or instead if we sample the pink one we if you get the the switch to

0:04:00.159,0:04:05.260
select the pink one we should have a low cost because that would allow us to

0:04:05.260,0:04:09.489
figure out that we are actually doing you know we are have actually a true

0:04:09.489,0:04:14.980
sample a good sample so summarizing the sequence of operations we had at the

0:04:14.980,0:04:21.039
generator Maps my leighton input set into this ℝⁿ which is the space of the

0:04:21.039,0:04:25.630
input space so we have delayed and input the orange one that is mapped into the

0:04:25.630,0:04:31.870
original input so we had that orange z mapped or x̂ in blue the top one

0:04:31.870,0:04:38.620
instead is a cost network in this case which Maps the input which can be the

0:04:38.620,0:04:45.849
pink X or the blue hat the blue X hat which is mapped to my cost so in this

0:04:45.849,0:04:51.550
case this cost it is a this cost module is actually a it's a cost like in

0:04:51.550,0:04:57.340
Young's diagram is gonna be a square okay which outputs a scalar this scalar

0:04:57.340,0:05:03.159
will be a high high value a large number positive large number if the input is a

0:05:03.159,0:05:10.389
fake input and it should be a low number probably up v-0 if we actually have the

0:05:10.389,0:05:17.740
input coming from the pink side the real side okay and then how do we train the

0:05:17.740,0:05:23.259
system so the system will be trained with different with different gradients

0:05:23.259,0:05:28.000
so the cost network will be trained in order to have

0:05:28.000,0:05:33.970
low cost for inputs that are pink in a high cost for input to dark blue

0:05:33.970,0:05:37.270
okay so for example you can think about this

0:05:37.270,0:05:42.280
if you would have like you know a discriminator in this case you may have

0:05:42.280,0:05:47.320
you may think about these as a you know two classes classification

0:05:47.320,0:05:55.900
problem you try to get 0 for X pink pink eggs and a 1 for the blue X we'll talk a

0:05:55.900,0:06:03.520
bit about why that's bad to use this 0 1 output in a second in a second but

0:06:03.520,0:06:09.250
otherwise we just want this network to learn this cost okay so let's figure out

0:06:09.250,0:06:13.660
how this works in a diagram do you remember how we were starting with a

0:06:13.660,0:06:17.620
variational encoder with a variational encoder we were starting from the left

0:06:17.620,0:06:21.970
hand side right we were picking a input and then we were so we were taking the

0:06:21.970,0:06:27.250
input we were moving to the latent space we were moving this point because we are

0:06:27.250,0:06:31.300
adding some noise and then we were getting back to the original point then

0:06:31.300,0:06:34.950
we were trying to get those points calls together by using the reconstruction

0:06:34.950,0:06:41.020
laws and then we were trying to set some structure in the latent space by using

0:06:41.020,0:06:46.990
that relative entropy term okay instead for the gun the generative adversarial

0:06:46.990,0:06:49.710
Network we're going to be starting from the right-hand side

0:06:49.710,0:06:56.290
so we pick a sample a random number let's say 42 we feed that through a

0:06:56.290,0:07:03.120
generator and we get that blue X hat over there then we're going to be

0:07:03.120,0:07:10.240
training in another network in order to be coming up with a higher value for

0:07:10.240,0:07:15.370
that blue sample then we're gonna pick another X let's say pink X in this case

0:07:15.370,0:07:20.530
on the bottom right of the spiral which is gonna be enforced now to have a low

0:07:20.530,0:07:28.230
cost so this is pretty much like a first initial big picture about how these

0:07:28.230,0:07:35.650
system works so let me try to give you two more interpretation so this is like

0:07:35.650,0:07:42.430
the kind of definitions and this will interpret like mathematical definition

0:07:42.430,0:07:46.720
and then the visual definition now gonna be trying to give you a few

0:07:46.720,0:07:52.810
interpretations which I pretty like and I can't make me sound like a fool but I

0:07:52.810,0:07:59.020
am one so you know I just go for it so you can think about the generator has

0:07:59.020,0:08:05.800
been a Italian and therefore I will be using some proper Italian accent okay so

0:08:05.800,0:08:10.240
I'm a proper Italian now and I am in the south of Italy and I'm gonna be trying

0:08:10.240,0:08:17.290
to make some fake man okay because we are very good at using so we make a

0:08:17.290,0:08:22.870
second money and then we go to Germany to buy something game

0:08:22.870,0:08:27.250
we go to Germany with this fake money and then there is this different people

0:08:27.250,0:08:34.360
look at the US is like oh [ __ ] Italian this is fake money and so we

0:08:34.360,0:08:41.320
can't really manage to we can really manage to buy anything but since we are

0:08:41.320,0:08:45.220
Italian we have spies we have spies in the… Okay there are

0:08:45.220,0:08:52.720
questions hold on maybe I'm offending people now chart what's going on oh okay

0:08:52.720,0:08:59.050
you're enjoying the thing cool okay so I was not offending anyone fantastic okay

0:08:59.050,0:09:05.860
so we have a spy back in Germany when the spy is like calling back home hey

0:09:05.860,0:09:12.520
mom here you give us the wrong good money like it was a so [ __ ] it up it was

0:09:12.520,0:09:18.250
just a you know not to prop better okay okay so yeah chill chill down right we

0:09:18.250,0:09:24.580
are we are like back again home what move is this that's just my own movie so

0:09:24.580,0:09:30.580
we are back in Italy you know we are we are making you are able to make such

0:09:30.580,0:09:36.340
nice art and everything so we must be able to make better money right so we

0:09:36.340,0:09:41.290
try now to fix the things that our spy told us so we make a better money we go

0:09:41.290,0:09:44.760
back to Germany and try to buy other things

0:09:44.760,0:09:49.860
and Germans are like uh-huh it's better it's fake

0:09:49.860,0:09:55.080
okay then again you had a spy call him back down to Italy and say Oh what are

0:09:55.080,0:10:00.660
you doing and they will understand capisce you know and we are fixing it

0:10:00.660,0:10:04.440
the money no we are making several iterations of that Thanks so we try to

0:10:04.440,0:10:10.620
make better and better versions of the money finally we go back to Germany in

0:10:10.620,0:10:13.710
this case Germany because they have money right that we had they have things

0:10:13.710,0:10:18.320
we can buy so we go back there and they are like huh

0:10:18.320,0:10:22.980
it looks very good now I don't know how to make in German accent I'm sorry

0:10:22.980,0:10:29.150
and so they accept the money right okay and this is how pretty much these

0:10:29.150,0:10:33.900
GANs works we had like a generator which are the

0:10:33.900,0:10:37.770
Italian dudes in the south which are making fake money and we are trying to

0:10:37.770,0:10:41.190
purchase something in Germany and Germany is the discriminator and they

0:10:41.190,0:10:50.700
are very strict and very you know German okay politically correct I'm not so

0:10:50.700,0:10:54.300
whatever but then we do have a spy right and what is this spy

0:10:54.300,0:10:59.100
can anyone figure out what the spy analogy here we haven't mentioned that

0:10:59.100,0:11:07.380
so far so the loss function back prop discriminator okay some feedback okay

0:11:07.380,0:11:14.570
it's feedback and how is the feedback coming from so whenever we train the

0:11:14.570,0:11:19.380
whenever we train the discriminator or the coffee network right we have some

0:11:19.380,0:11:27.900
gradient that gradient allow me to do two things right I can either in lower D

0:11:27.900,0:11:35.460
I can either lower the final value and so I can tune my parameters of the cost

0:11:35.460,0:11:39.420
function let me go back to the cost function so we have some gradients of

0:11:39.420,0:11:45.300
the final cost right so we had a final some gradients of the final cost with

0:11:45.300,0:11:50.220
respect to the parameters of the network and and so usually when usually when I

0:11:50.220,0:11:55.320
train the network the cost Network I will try to tune the parameters such

0:11:55.320,0:12:00.870
that I will have a final lower loss right this is a cost Network and there

0:12:00.870,0:12:07.890
is a loss on top of the cost Network right it's a bit confusing so we're

0:12:07.890,0:12:12.149
going to be trying to optimize the parameters of the cost network in order

0:12:12.149,0:12:18.450
to perform well and therefore having a very low loss on the same way we can use

0:12:18.450,0:12:23.970
those ingredients that are computed with respect to this network and you see my

0:12:23.970,0:12:30.330
mouse so I have you know my final loss on top of here will come down with the

0:12:30.330,0:12:35.160
gradients and then you have here some gradients in all these gradients you

0:12:35.160,0:12:41.880
know how if you change this x̂ you're gonna know how these final loss will

0:12:41.880,0:12:48.360
change right therefore you can train now this generator with this gradient in

0:12:48.360,0:12:53.940
order to increase this final loss so when we train this cost network we'd

0:12:53.940,0:12:59.070
like to minimize the final loss given that we input these two different inputs

0:12:59.070,0:13:04.830
right but also we'd like to increase this final loss so we'd like to make

0:13:04.830,0:13:11.640
this final Network perform worse by you know improving the generator okay and so

0:13:11.640,0:13:17.279
these information that comes down here and down this way which is the backward

0:13:17.279,0:13:22.560
pass right the input gradient will be used for tuning the parameter of the

0:13:22.560,0:13:29.279
generator such that it managed to fool the cost Network and so this is the

0:13:29.279,0:13:37.370
analogy with the spy in the German in Germany is the distribution of Z fixed

0:13:37.370,0:13:43.260
so yeah so Z has it actually comes from let's say a normal distribution I

0:13:43.260,0:13:48.870
actually don't really have anything about anything to say about this

0:13:48.870,0:13:54.050
distribution as long as you pick your distribution

0:13:54.050,0:13:59.630
you know the generator will map that distribution into some x̂

0:13:59.630,0:14:07.620
distribution which will hopefully match what is the pink distribution of the X's

0:14:07.620,0:14:12.480
okay so even even the distribution of z is fixed we reach

0:14:12.480,0:14:17.459
be sure that we can change the generator in such a way that we can minimize the

0:14:17.459,0:14:24.889
cost function right so although that distribution is fixed the generator will

0:14:24.889,0:14:30.600
how do you say apply PL a why I think you will apply this kind of distribution

0:14:30.600,0:14:35.130
such that you're going to be music likely flowing into something that looks

0:14:35.130,0:14:41.790
alike the X in the pink X hopefully okay I haven't told you about the

0:14:41.790,0:14:48.060
pitfalls of this system okay but hopefully we'd like to manage to get a

0:14:48.060,0:14:54.510
distribution out of those blue X's X hat such that they resemble the original and

0:14:54.510,0:14:58.769
distribution on the left hand side in the pink one okay did I answer your

0:14:58.769,0:15:05.850
question yeah that makes it okay put on the X produced by the generator be the

0:15:05.850,0:15:13.050
new improved money the blue one okay yeah thank you I actually didn't finish

0:15:13.050,0:15:18.510
that one so the pink one are the true euros we are using in Europe and the

0:15:18.510,0:15:24.410
blue the blue x hat are the money that we make in Italy okay

0:15:24.410,0:15:30.930
Amamiya okay other questions I have a generator was supposed to give negative

0:15:30.930,0:15:38.970
samples so negative samples okay so there are two steps here

0:15:38.970,0:15:44.610
we provide negative samples that are these x̂ to the cost Network so the

0:15:44.610,0:15:50.490
cost network is trained in order to have low values on the pink inputs and higher

0:15:50.490,0:15:55.949
values on the blue input okay and so if the network the cost Network performs

0:15:55.949,0:16:02.430
well then the final loss here on top will be very easier very low okay so if

0:16:02.430,0:16:06.810
the cost network is very is performing very well then you're gonna have a final

0:16:06.810,0:16:13.769
low loss here nevertheless the generator will be trained in order to increase

0:16:13.769,0:16:20.790
that loss because we'd like to fool these Germans the Dozen

0:16:20.790,0:16:25.230
make sense could you just clarify what the spy is in this analogy

0:16:25.230,0:16:30.330
yeah the Spy is the input gradient so whenever here I have my cost network and

0:16:30.330,0:16:34.350
to train this cost Network I'm gonna have a final layer here on top right

0:16:34.350,0:16:41.910
let's say this is an MSC for example let me see with you know zero for whenever I

0:16:41.910,0:16:50.190
have an input eggs pink or some you know value or let's say +10 in this case are

0:16:50.190,0:16:55.560
we try number for the moment value plus 10 for the blue guys right so my cost

0:16:55.560,0:16:59.940
network is a regression regression network you can think about this as just

0:16:59.940,0:17:05.460
one single linear layer so it's like an affine transformation of the input and

0:17:05.460,0:17:12.120
then these basically a final volume I set it to be zero for the pink input I

0:17:12.120,0:17:15.960
have an MSc between the output of the network and zero

0:17:15.960,0:17:21.750
for whenever I input the pink input and instead let's say I choose an arbitrary

0:17:21.750,0:17:27.960
value of 10 to be a reflecting that the input is the blue one right so we have

0:17:27.960,0:17:33.030
cost Network which is a network that is outputting a single skat scalar value

0:17:33.030,0:17:40.020
and this scalar value will go inside the MSC module here on top let me write

0:17:40.020,0:17:49.380
maybe so we can all see what's going on so I have here my M and C this is my

0:17:49.380,0:17:53.640
loss function right so don't get confused between loss and costs there

0:17:53.640,0:18:02.340
are two different things so I have my MSC here and if I have this guy here my

0:18:02.340,0:18:11.190
target is going to be zero there's gonna be my yc for this one and instead

0:18:11.190,0:18:16.860
if I input this guy here to the cost Network I expect to have let's say in

0:18:16.860,0:18:22.980
this case an arbitrary plus ten so my MSC in this case is gonna be you know

0:18:22.980,0:18:27.450
mean square error between the output of the cost Network and zero in the other

0:18:27.450,0:18:33.779
case I'm gonna have the MSE between jobs of the network and ten so the network if

0:18:33.779,0:18:38.279
I just train let's say we forget about all this stuff right we have just a few

0:18:38.279,0:18:42.359
samples with weed with we think for the moment that the generator is not

0:18:42.359,0:18:47.940
improving so we have several pink samples and several blue samples and now

0:18:47.940,0:18:52.739
you train a network such that if I put the input the pink one you're gonna get

0:18:52.739,0:18:57.059
a zero in the output and then if you put the blue one instead you're gonna be

0:18:57.059,0:19:01.649
forcing the network to learn number ten okay so you do some steps in gradient

0:19:01.649,0:19:06.299
descent in the parameter space such that in one case you get zero the other case

0:19:06.299,0:19:12.119
you get then whenever you provide several several samples right now that

0:19:12.119,0:19:17.969
we have this network this cost Network you can think about having the cost

0:19:17.969,0:19:26.099
network to be actually the loss for the generator okay and so if I have my

0:19:26.099,0:19:30.479
generator input-output in something and this cost network will say oh it's a

0:19:30.479,0:19:36.149
very high cost then by trying to minimize this cost you will try to

0:19:36.149,0:19:41.549
basically generate something that was initially making that cost Network

0:19:41.549,0:19:48.779
providing you a low value okay is it making sense you just quickly clarify

0:19:48.779,0:19:51.690
the difference between costs and loss haha

0:19:51.690,0:19:59.039
the loss is what we use in order to train something okay so my loss in this

0:19:59.039,0:20:11.279
case is the MSE loss this is my loss so in order to train my cost network I will

0:20:11.279,0:20:17.399
have a loss function which is the MSE loss function by minimizing the MSE loss

0:20:17.399,0:20:20.389
function I will be training the cost Network

0:20:20.389,0:20:28.440
now the [ __ ] up part comes and I'm gonna say that for my generator the loss

0:20:28.440,0:20:35.820
function that I want to minimize is the cost Network so for this generator the

0:20:35.820,0:20:43.519
loss is the cost and I try to minimize this guy output okay so this is also

0:20:43.519,0:20:47.650
relative to what Yann is teaching with the energy based models you have energy

0:20:47.650,0:20:54.049
and we try to have low energies through minimization of a loss function so the

0:20:54.049,0:20:59.419
loss function is what you use in order to train the parameters of a network

0:20:59.419,0:21:08.120
okay so that's the difference so it's the network okay so another additional

0:21:08.120,0:21:15.130
point is that a cost is like a evaluation of some network performance

0:21:15.130,0:21:23.090
so if my generator outputs a bad X like which is not pretty good-looking then

0:21:23.090,0:21:30.140
you're gonna have a high cost okay like a high energy but in order to minimize

0:21:30.140,0:21:34.610
this energy usually you have to minimize this losses okay so but again the

0:21:34.610,0:21:39.380
definition what we like to use is that the loss is what you minimize in order

0:21:39.380,0:21:48.110
to train the parameters of a network so instead like a cost can be thought as

0:21:48.110,0:21:54.350
you know I take an action and then I have an action I have a cost for taking

0:21:54.350,0:22:00.159
that specific action okay so you take an action which is like writing an email

0:22:00.159,0:22:05.360
about changing things and then the cost is gonna be having every one piece at

0:22:05.360,0:22:12.200
you know makes sense right you always learn something new okay

0:22:12.200,0:22:18.030
other questions so far I'm sorry Alvin but I'm still be

0:22:18.030,0:22:23.190
confused about a constant generator so for generator that generates the blue X

0:22:23.190,0:22:27.750
we want to increase the cost but you just mentioned that we want to minimize

0:22:27.750,0:22:32.730
the cost is like the loss function for the generator and we want to minimize

0:22:32.730,0:22:38.310
the loss so we want to increase the cost or we want to decrease the cost for the

0:22:38.310,0:22:44.700
generator for the generator you want to minimize the cost so we train the

0:22:44.700,0:22:53.760
generator through a minimization of the cost Network value okay so there is two

0:22:53.760,0:22:58.200
parts of this thing let me change color so first part is going to be the

0:22:58.200,0:23:03.000
training of this guy here and the training of the cost network is made

0:23:03.000,0:23:09.240
through the minimization of the MSE on top of here so this is the loss for the

0:23:09.240,0:23:17.580
cost Network so the cost the MSE here is bit made between zero whenever I input a

0:23:17.580,0:23:23.810
pink input and then it's let's say for this example like for sake of example I

0:23:23.810,0:23:30.720
like to have an MSc against 10 whenever I input a blue sample okay so now we

0:23:30.720,0:23:35.280
perform several steps of gradient descent in the parameter space of the

0:23:35.280,0:23:42.240
cost Network such that we minimize these loss okay so now we have a network here

0:23:42.240,0:23:48.360
which is gonna be outputting 0 if I put a pink input and input and output a 10

0:23:48.360,0:23:56.100
if I input a blue input so far are you with me yes it was like cost will the

0:23:56.100,0:24:00.810
network cost will generate a high value for blue X right yeah that's what we

0:24:00.810,0:24:06.780
train this cost to do okay so this cost network will have to generate some large

0:24:06.780,0:24:13.530
value in this case 10 if I input a blue guy and we'll have to generate a small 0

0:24:13.530,0:24:20.490
output if I put a 0 if I put a pink input and in order to do that we do this

0:24:20.490,0:24:26.640
by minimization of MSE loss okay this is first part sorry

0:24:26.640,0:24:31.870
you with me right yeah okay fantastic now we have the second

0:24:31.870,0:24:37.240
part which is the cute version the version that Yun likes that a different

0:24:37.240,0:24:42.220
version that you don't find online which is the following so this cost network

0:24:42.220,0:24:48.550
now will give you values that are close to zero whenever you input something

0:24:48.550,0:24:56.440
that looks like proper okay otherwise it will put a high output let's say number

0:24:56.440,0:25:04.540
a value around 10 if you put inside crappy input so now finally how do we

0:25:04.540,0:25:08.429
train this generator well the generator now will be trained

0:25:08.429,0:25:15.550
through the minimization of the cost Network right so the cost network will

0:25:15.550,0:25:22.630
say 10 here so this output blue guy here it's bad guy right so if the generator

0:25:22.630,0:25:28.929
now switches slightly these acts to make something that looks like this guy over

0:25:28.929,0:25:36.070
here then you get that from 10 we went down to 0 right and therefore you got to

0:25:36.070,0:25:42.460
minimize this cost Network output value and so we're using the cost network as

0:25:42.460,0:25:51.309
the loss for training the generator ok what do you mean by like getting blue

0:25:51.309,0:25:59.800
axe closer to pink X right so right now my generator outputs these blue blue X

0:25:59.800,0:26:05.140
okay and this is like some image that looks bad or is a you know money that

0:26:05.140,0:26:13.210
really looks fake now how do you make better money well the cost network is

0:26:13.210,0:26:20.800
gonna give you a scalar value for each output your generator makes therefore

0:26:20.800,0:26:25.300
you can compute the partial derivative you can look at the gradient you know of

0:26:25.300,0:26:44.070
that cost value I like to compute partial derivative of these : ∂c/∂x̂.

0:26:44.309,0:26:58.950
Sorry it's hard to write properly, it's really ugly. And here it's a lowercase c not a capital one.

0:26:58.950,0:27:13.110
So I compute ∂c/∂x̂. So now I have a gradient this gradient allows me to move right around and I figure out

0:27:13.110,0:27:19.820
whether the cost is going increase or decrease right so this is some kind of

0:27:19.820,0:27:26.940
maybe you know a little bit not standard is in also yesterday Yann was talking

0:27:26.940,0:27:32.100
about this you know you have some inputs in to your network you can decide to do

0:27:32.100,0:27:37.169
grid in the send in the input space I can decide for example there is a

0:27:37.169,0:27:42.090
architecture which doesn't have a generator at all you start with a sample

0:27:42.090,0:27:46.860
here and now you perform gradient descent in this sample space and then

0:27:46.860,0:27:52.100
you move these samples such that you get a lower lower value for the cost network

0:27:52.100,0:27:59.730
in this way you can you know get an input that looks like resembling a good

0:27:59.730,0:28:05.429
input right the pink one does it make that did I kind of explain myself or is

0:28:05.429,0:28:10.289
it still weird there is much clearer thank you you sure

0:28:10.289,0:28:16.740
yeah yeah it's like taking gradients in the input space and make it move towards

0:28:16.740,0:28:22.110
like and then decrease the cost so that means the input actually gets better

0:28:22.110,0:28:27.480
like ass better money right right right and then you can also

0:28:27.480,0:28:33.030
use this one is your gradient here coming down here right and so now you

0:28:33.030,0:28:46.870
can compute with the chain rule also ∂c/∂Wg, g for the generator.

0:28:46.870,0:28:51.940
okay so in this case then I can train the generator I had the parcel of the

0:28:51.940,0:28:59.290
cost over the parameters and therefore I can change now the values of the of the

0:28:59.290,0:29:05.710
parameters on the generator in order to you know improve the network oh it

0:29:05.710,0:29:10.210
totally makes sense of course yeah other train

0:29:10.210,0:29:18.120
simultaneously or train first the cost Network order generator network right

0:29:18.120,0:29:26.110
people try both they say it's better sometimes to keep one fixed while you

0:29:26.110,0:29:29.920
change in the other because otherwise you have always a moving target then

0:29:29.920,0:29:35.500
there are contradictory evidence we are actually gonna be really now some source

0:29:35.500,0:29:40.330
code after we cover the major pitfalls but I'm gonna get back to your question

0:29:40.330,0:29:48.340
in a few minutes we don't need a regularization like kld for saving gun

0:29:48.340,0:29:55.630
because we simple from normal yeah the right directory direct yeah directly you

0:29:55.630,0:30:03.730
simple the orange guy here from normal distribution so that's it right you have

0:30:03.730,0:30:08.260
a simple like a random number and then you send this random number through the

0:30:08.260,0:30:13.140
generator that's it and my Google home just came back to life

0:30:13.140,0:30:18.070
okay again sir your question I think more questions then we have pitfalls and

0:30:18.070,0:30:22.870
then we actually gonna be looking at source code yeah so it seems like we are

0:30:22.870,0:30:31.630
replacing that the reconstruction loss with the differentiator Network oh how

0:30:31.630,0:30:35.830
does that help exactly why can't like how is it bad to just use the

0:30:35.830,0:30:41.830
reconstruction loss Oh what is this is this is this a very very good question I

0:30:41.830,0:30:46.350
mean I it's something I forgot completely to say

0:30:51.740,0:30:56.000
so on the original encoder we were always starting from some point then we

0:30:56.000,0:30:59.419
were getting back to this space we were moving a little bit that point such that

0:30:59.419,0:31:03.020
we could cover some area and then go back to the other side now you try to

0:31:03.020,0:31:07.610
make those two close right but in our case right now in this janati

0:31:07.610,0:31:11.750
bothers our net we're actually starting from the right-hand side so in the

0:31:11.750,0:31:15.399
general tea butter cyanide you start from the right there is no whatsoever

0:31:15.399,0:31:21.919
connection between this guy here and this guy here all you have is a cost

0:31:21.919,0:31:28.159
network which is telling you whether you are on this kind of thing here right I

0:31:28.159,0:31:35.450
can't okay there's a cost networking is gonna tell you in this case plus 10 here

0:31:35.450,0:31:41.899
and then it's gonna tell you let's say zero here okay in the other case you

0:31:41.899,0:31:47.000
have a generative network here which is mapping these input here down to here

0:31:47.000,0:31:52.730
right so one is trained in order to have below values around the manifold and

0:31:52.730,0:31:57.230
then larger values outside and then use some you would like something that is

0:31:57.230,0:32:02.960
like you know you may want some curve levels right like that such that is you

0:32:02.960,0:32:07.899
move further away in the stuff keeps increasing if you have a discriminator

0:32:07.899,0:32:12.860
they will force to have zero here and one outside

0:32:12.860,0:32:20.539
exactly this manifold like very very close by right and so that creates many

0:32:20.539,0:32:26.919
problems so okay let me try another analogy and there is another analogy so

0:32:26.919,0:32:30.500
hold on there are questions more question let me go with the analogy then

0:32:30.500,0:32:36.710
let's see whether this makes more sense let me actually see myself such that I

0:32:36.710,0:32:44.480
can okay I can see now myself all right so you have like some true data points

0:32:44.480,0:32:49.640
here okay and now you have some generated data points over here that

0:32:49.640,0:32:55.149
have been generated by the generator right so points here points down there

0:32:55.149,0:33:00.200
let's assume now we are talking about this discriminator okay so that I can

0:33:00.200,0:33:04.960
illustrate what are the problems there so you have a discriminator which

0:33:04.960,0:33:11.260
has these two kind of data you have true data down here fake data over here and

0:33:11.260,0:33:15.279
so what does the discriminator do the discriminator decision boundary is gonna

0:33:15.279,0:33:22.539
be just a line here right that is cutting this stuff in half so far yeah

0:33:22.539,0:33:27.789
right yes okay good so now you turn the on the second step second step is gonna

0:33:27.789,0:33:32.529
be you turn on gravity on this decision boundary so this point that are here

0:33:32.529,0:33:38.260
will be both falling down okay the point here get attracted by the decision

0:33:38.260,0:33:43.169
boundary so we train first the discriminator we had this kind of

0:33:43.169,0:33:47.710
decision boundary then we train the gist the generator you have these guys

0:33:47.710,0:33:54.190
collapsing down here so then you're gonna be new situation you have through

0:33:54.190,0:33:58.360
data here fake data here you train again the discriminator in this case you're

0:33:58.360,0:34:03.010
gonna have a decision boundary which is gonna be half with here right then you

0:34:03.010,0:34:07.029
want you turn on gravity such that this point here we've collapsed here right

0:34:07.029,0:34:11.440
and now you keep iterating this fry this stuff will be getting closer and closer

0:34:11.440,0:34:17.230
and closer and closer to the true data right so you had these points that are

0:34:17.230,0:34:27.970
like approaching and arriving to the real data location so let's say now

0:34:27.970,0:34:32.800
you're using your discriminator you have those binary cross-entropy

0:34:32.800,0:34:41.050
laws for training the discriminator what is now the main issue let's say I do a

0:34:41.050,0:34:46.810
shifting I bring my true data here such that we can see the data would think

0:34:46.810,0:34:51.609
happens so you have true data here yeah generated data here right they are

0:34:51.609,0:34:57.880
overlapping and now you have a discriminator cutting here so you're

0:34:57.880,0:35:02.589
gonna have overlap of these samples and this discriminate or has no idea what to

0:35:02.589,0:35:07.960
do right so first of all you're gonna get you know misclassifications just

0:35:07.960,0:35:12.490
because you thought you converge like we actually converge right

0:35:12.490,0:35:17.290
if you if you think about that my true data is here my generated data is here

0:35:17.290,0:35:19.760
they are over shopping

0:35:19.760,0:35:27.230
so I actually managed to reach convergence and now my discriminator has

0:35:27.230,0:35:35.530
no whatsoever clue how to split these things apart so huh

0:35:35.530,0:35:45.320
so we don't converge or when we converge we don't we we we get issues right huh

0:35:45.320,0:35:49.790
the discriminator I think this mater just tells apart two classes

0:35:49.790,0:35:53.869
well this community cannot tell apart the two classes because this input are

0:35:53.869,0:36:00.290
you know no more separated right they are gonna be like if you actually manage

0:36:00.290,0:36:05.660
to get the generator to perform very very good samples then these good

0:36:05.660,0:36:10.340
samples are you cannot tell them apart from the actual real samples right you

0:36:10.340,0:36:17.450
know the discriminator has no whatsoever clue about how to how to basically tell

0:36:17.450,0:36:20.840
them apart so whenever the generator works the

0:36:20.840,0:36:26.570
discriminatory not work hmm how nice is that okay

0:36:26.570,0:36:32.119
one other problem let's say again you have the fake data here through data

0:36:32.119,0:36:39.020
over here and now you have a perfect amazing awesome discriminator such that

0:36:39.020,0:36:43.810
here is absolutely zero and then here is absolutely one okay so you have like a

0:36:43.810,0:36:50.480
basically like a step function you don't have a sigmoid what's gonna be not the

0:36:50.480,0:36:55.910
gradient it's saturated right or it's zero or it's one there is no more

0:36:55.910,0:37:01.430
gradient these points will never move right so the the gravity that I was

0:37:01.430,0:37:06.680
showing you before that was attracting these generated data through onto the

0:37:06.680,0:37:12.070
decision boundary was basically the gradients that I saw the gradient of the

0:37:12.070,0:37:17.420
final of the output of the discriminator or the cost network with respect to the

0:37:17.420,0:37:24.770
you know samples generated by the generator but now if these discriminator

0:37:24.770,0:37:28.970
has a perfect is a perfect discriminators zero here one here well

0:37:28.970,0:37:33.329
it's completely flat right if it's like that there is no whatsoever

0:37:33.329,0:37:39.539
a gradient here right and therefore if you're over here so let's say we have

0:37:39.539,0:37:45.259
data in 1 1 X right in 1 one dimension you have 0 0 0 then you have 1 1 1 1 1

0:37:45.259,0:37:49.890
but then if there is just you know there is no gradient this point will never

0:37:49.890,0:37:56.069
know they had to go in that direction do we see oh we are bad guys we have a bad

0:37:56.069,0:37:59.999
value but then we don't know in which direction to move because there is no

0:37:59.999,0:38:04.709
whatsoever direction the gradient is 0 it's a flat region right so this is a

0:38:04.709,0:38:09.269
very big issue right so whenever we train this generator with the cellular

0:38:09.269,0:38:16.019
network you want to make sure that this cost gradually increases as you move

0:38:16.019,0:38:22.739
away from your region of the true data ok such that if there is a smooth or

0:38:22.739,0:38:26.430
it's like a you know a convex thing right so if you keeps going up up up up

0:38:26.430,0:38:32.579
you always know which direction to fall down in order to arrive at the location

0:38:32.579,0:38:41.009
where your true data is ok and my Google home keeps rebooting I'm like tiny

0:38:41.009,0:38:47.989
little shipped off there you go is it clear so far

0:38:47.989,0:39:00.509
yeah rip yeah one final issue was that if we get a generator which gets every

0:39:00.509,0:39:11.849
point here mapped into this point over here you know all the weights are 0 you

0:39:11.849,0:39:17.249
have the final bias be exactly this value over here then that's finished

0:39:17.249,0:39:21.779
because the discriminator over the cost function will say if done a very good

0:39:21.779,0:39:28.859
job and generators say yay and then the generator just outputs one

0:39:28.859,0:39:33.749
image right this is called mode collapse meaning that all points are mapped into

0:39:33.749,0:39:40.529
just one point and you can't do anything about it so the actual full story is

0:39:40.529,0:39:46.430
that if every point here gets mapped to this point here

0:39:46.430,0:39:50.540
then the discriminator will tell that oh this is a fake point right and therefore

0:39:50.540,0:39:55.940
the generator will switch and will say this is the real loud put right and now

0:39:55.940,0:39:58.579
you train the discriminator discriminator say oh this is fake okay

0:39:58.579,0:40:03.470
so the generator we say this is the real one right okay so you basically have a

0:40:03.470,0:40:11.119
network that is just jumping through the samples and you can't fix that unless

0:40:11.119,0:40:15.800
you introduce some you know penalty for not having some kind of diversity in the

0:40:15.800,0:40:20.480
output of the generator vanishing gradients whenever you have like

0:40:20.480,0:40:25.160
saturated discriminators and we don't like discriminators we prefer to learn

0:40:25.160,0:40:32.569
this kind of smooth laws the cost right cost Network mod collapse that's things

0:40:32.569,0:40:37.160
like just describe right now we just fold on one specific point unstable

0:40:37.160,0:40:42.280
convergence yeah and the point is that whenever you get a very cute generator

0:40:42.280,0:40:47.630
the you know the discriminator will have no idea what's going on you may have

0:40:47.630,0:40:53.240
like very big very big loss because you may get you know these points would be

0:40:53.240,0:40:57.319
classified as this one instead is completely classified as something else

0:40:57.319,0:41:01.790
you get some very very large gradient the disk emitter will jump away and then

0:41:01.790,0:41:06.589
they generate this committee will jump you know away and the decision boundary

0:41:06.589,0:41:10.190
will go in all bunkers bunkers and then you're gonna have the generator trying

0:41:10.190,0:41:18.290
to run after the these you know running away decision boundary okay and so there

0:41:18.290,0:41:22.910
is no convergence there is a equilibrium so it's an unstable equilibrium point

0:41:22.910,0:41:30.829
which is very very tricky to catch yeah so I understand we have some sort of

0:41:30.829,0:41:36.020
minimax problem here with a regenerator and a cost but in general when you

0:41:36.020,0:41:40.490
optimize this I don't know if really any straightforward ways to make sure you

0:41:40.490,0:41:45.530
convert you into the right point right I am not sure how you figure out whether

0:41:45.530,0:41:50.180
you converge to a good point but through visual inspection of your outputs of the

0:41:50.180,0:41:57.109
generator or you can train several you can train several guns and then you

0:41:57.109,0:42:04.670
train a discriminator on some image dataset and now you classify

0:42:04.670,0:42:12.020
you evaluate the quality of the of the image right so this is like some kind of

0:42:12.020,0:42:18.619
not good metric we don't like but that's what has been done is called inception

0:42:18.619,0:42:23.000
score so you train a network let's say the inception Network that's why it's

0:42:23.000,0:42:29.090
called inception score on you know image data set and then you can tray you can

0:42:29.090,0:42:35.420
try to see whether these generators are giving you images that look like

0:42:35.420,0:42:43.190
something from you know from from this training dataset again it's not really a

0:42:43.190,0:42:49.040
good metric but someone try to use this for a way to evaluate generative to

0:42:49.040,0:42:55.359
evaluate and generative model before starting before going to the notebooks

0:42:55.359,0:43:01.190
let's have a look to actually a practical example of training laws for

0:43:01.190,0:43:08.210
these two networks we have just seen now okay so the loss function for my cost

0:43:08.210,0:43:17.390
Network given the input X and the latent input z in orange can be the following

0:43:17.390,0:43:27.980
so it can be equal my cost C given my pink input X and then plus this part

0:43:27.980,0:43:35.270
here now which is the positive part of enlarging M minus the cost I'm going to

0:43:35.270,0:43:41.990
give to a generated input which is is outputted by my generator which is fed

0:43:41.990,0:43:45.740
with the input latent input a random number okay

0:43:45.740,0:43:54.410
so G of Z gives me a fake input then C will have to give me a cost and as long

0:43:54.410,0:44:01.970
as this cost will be lower than M this part here will be positive part as soon

0:44:01.970,0:44:07.220
as C the cost Network gives me a cost for this

0:44:07.220,0:44:13.550
generated input which is larger than M then this part here and mine

0:44:13.550,0:44:17.690
some number larger than M is gonna be a negative number then since I take the

0:44:17.690,0:44:23.270
positive part this goes to 0 so this part of the loss goes to 0 whenever the

0:44:23.270,0:44:29.300
cost Network gives me a output that is larger than M for a input that is being

0:44:29.300,0:44:34.670
provided by my generator on the other side here we have simply the cost

0:44:34.670,0:44:39.920
associated to the correct ink input right and so in order to squish this

0:44:39.920,0:44:46.100
down to zero you just have to have your cost network outputting a zero whenever

0:44:46.100,0:44:53.060
the input is the good one ok so in the example in the example was making before

0:44:53.060,0:45:00.710
I was saying that M is 10 and therefore the network is encouraged to output a

0:45:00.710,0:45:06.590
scalar of 10 at least 10 at least and right for inputs are coming from the

0:45:06.590,0:45:11.270
generator and said cause that is equal to 0 is promoted by this term over here

0:45:11.270,0:45:17.900
so this is a example of possible loss we can use for training the cost Network

0:45:17.900,0:45:25.580
known this is done in this paper here by Jake, Michael and Yann from 2016 then how do

0:45:25.580,0:45:30.980
we train the generator well this quite straightforward because you simply

0:45:30.980,0:45:39.290
have the loss for training the generator being equal the cost that the network

0:45:39.290,0:45:45.140
the cost network gives me for a given generated sample right

0:45:45.140,0:45:53.620
and so my generator will simply try to get a low cost and that's so pretty

0:45:53.620,0:46:01.100
all right ok again can we both can we be more specific know what is this cost

0:46:01.100,0:46:06.350
network I have it's I haven't told you yet a specific choice you can make for

0:46:06.350,0:46:11.330
creating a network that is giving you this scalar based on the input but I

0:46:11.330,0:46:19.100
think you may already have some ideas how this network can be made and so a

0:46:19.100,0:46:27.260
possible choice for for this network is going to be in the following it's going

0:46:27.260,0:46:33.290
to be the MSE the quadratic difference between the

0:46:33.290,0:46:40.500
decoding of the encoding of the specific input so this is the reconstruction of a

0:46:40.500,0:46:49.140
out encoder - the input itself squared right the norm squared so how does this

0:46:49.140,0:46:57.210
work well like if the out encoder is being trained only on pink samples it

0:46:57.210,0:47:03.480
will be able to reconstruct pink samples only right and therefore the distance

0:47:03.480,0:47:09.330
between my input the pink input and the reconstruction of the out encoder when I

0:47:09.330,0:47:13.950
provide the pink input will be very small hopefully right if we train this

0:47:13.950,0:47:20.340
nicely instead what happens now if I put an input here that is far from anything

0:47:20.340,0:47:24.990
that is on the data manifold well my out encoder has been trained to output

0:47:24.990,0:47:30.810
things that stays on the on the on the on the data manifold and therefore there

0:47:30.810,0:47:36.300
will be a substantial difference between my actual input and what my out encoder

0:47:36.300,0:47:43.110
can give you right the nice part of this specific choice of course network is

0:47:43.110,0:47:48.450
that you can train these out encoder without the generator right you can

0:47:48.450,0:47:53.130
simply train an autoencoder whatever you can have like an under complete hidden

0:47:53.130,0:47:58.850
layer over complete and you use some kind of regularization and information

0:47:58.850,0:48:03.630
restriction bottleneck but nevertheless you can actually train this guy without

0:48:03.630,0:48:09.150
having a generator right and this one you will simply learn what is the Train

0:48:09.150,0:48:14.820
the data manifold and now you can use this as a proxy to establish the

0:48:14.820,0:48:19.350
difference the distance between your current input and what the network

0:48:19.350,0:48:26.550
thinks the closest input on the training manifold could be okay all right let's

0:48:26.550,0:48:31.890
move on in the last five minutes if there are no questions we are going to

0:48:31.890,0:48:38.640
be reading the source code from PyTorch’s examples together and this I think it's

0:48:38.640,0:48:41.560
going to be the first case we are we actually reading

0:48:41.560,0:48:47.260
programmer developer code I'm not a programmer so whatever you'll be in

0:48:47.260,0:48:51.870
consuming so far where my notebooks which were some kind of you know

0:48:51.870,0:48:57.760
pedagogical educational content which is kind of massaged such that it looks nice

0:48:57.760,0:49:02.230
and pretty and has nice-looking output right now you're gonna be reading

0:49:02.230,0:49:08.230
actually nice code written die by people like that do this is their job right so

0:49:08.230,0:49:13.570
we go get up we don't go on PyTorch Deep Learning  we are going to apply

0:49:13.570,0:49:30.850
PyTorch examples. Ok so let's zoom a little bit ok so

0:49:30.850,0:49:40.540
here we have the DC gun and it's smooth here okay so we can just go through this

0:49:40.540,0:49:45.220
code main things right so we start with you know importing a bunch of crappy

0:49:45.220,0:49:49.360
things as usual you have an argument parser such that you can send some

0:49:49.360,0:49:55.900
specific commands specific parameters in the command line this printouts all the

0:49:55.900,0:50:02.470
options for the current setup this one tries to make a directory otherwise you

0:50:02.470,0:50:06.070
know whatever this is if you choose a manual seed then you're going to be

0:50:06.070,0:50:10.600
actually setting a manual seed in such a way you're gonna have reproducible

0:50:10.600,0:50:19.210
results could a benchmark equal true I think speed up the yes this one allows

0:50:19.210,0:50:27.280
you to have faster GPU routines Karnas if you don't have CUDA you're

0:50:27.280,0:50:33.370
gonna be taking forever to train this stuff data route whatever data set so

0:50:33.370,0:50:38.950
you're gonna be loading here imagenet folders or l fw own data set so with

0:50:38.950,0:50:47.290
this is all things that we already know ok so n GPU is gonna be the number of

0:50:47.290,0:50:55.210
GPU and there's gonna be the size of the latent variable ngf and NDF

0:50:55.210,0:51:03.849
it's gonna be at T and G F and DF um the number I think on the generative futures

0:51:03.849,0:51:09.520
and the number of discriminative features and okay we have some specific

0:51:09.520,0:51:15.970
weight initialization which really helps getting some proper training starting

0:51:15.970,0:51:20.950
and then let's actually have a look to this generator right okay so this is

0:51:20.950,0:51:27.250
classical a classical and then subclass generator you don't need these stuff if

0:51:27.250,0:51:34.270
you're using Python 3 so let's see so we have a sequential right we have the

0:51:34.270,0:51:38.740
generator will be up sampling so such that as you have seen from the last

0:51:38.740,0:51:43.030
homework you want to go from a small dimension to a larger dimension you're

0:51:43.030,0:51:48.190
gonna use this model they have bench norm reloj and so on right and transpose

0:51:48.190,0:51:54.369
convolution batch norm real low and keep going and finally we have a tanh we

0:51:54.369,0:52:00.220
have a tanh because the output in this case is going to be lying within

0:52:00.220,0:52:07.240
minus 1 to +1 forward is simply send through forward through D you send the

0:52:07.240,0:52:14.049
input through the main and the main was this one main main model right this is

0:52:14.049,0:52:19.869
for using data parallel if you want to use several GPUs and then here is how

0:52:19.869,0:52:25.930
how do you initialize with the specific initialization you define above so

0:52:25.930,0:52:30.940
simply just putting in short right what does this thing do you input something

0:52:30.940,0:52:38.280
here that has NZ size right and NZ is the size of the latent which is n Z&Z

0:52:38.280,0:52:45.549
100 so you input a vector of size of size 100 so it's a tensor a one

0:52:45.549,0:52:51.880
dimensional tensor with 100 size the size is 100 and so whenever you input

0:52:51.880,0:52:59.380
this 100 vector the output is going to be something like a 64 by 64 times the

0:52:59.380,0:53:04.950
number of channels in case you have color image or not right

0:53:05.560,0:53:13.180
nc and Symbian the number of channels of the output the input imagery okay it

0:53:13.180,0:53:17.980
should be clear so far I know no crazy things going on let's see the last part

0:53:17.980,0:53:23.230
and then like to see how the Train so the discriminator is the same stuff you

0:53:23.230,0:53:28.120
have a sequential in this case we feed these whatever number of channels times

0:53:28.120,0:53:33.760
64 times 64 and then you go down with LeakyReLU  oh oh this is important so

0:53:33.760,0:53:37.750
LeakyReLU  in the discriminator make sure you're not gonna be killing the

0:53:37.750,0:53:41.680
gradient if you are in the region in a negative region right this is very

0:53:41.680,0:53:45.660
really important if you don't have gradients here then you know you can't

0:53:45.660,0:53:50.500
train the diction the generator so you keep the going down like that and then

0:53:50.500,0:53:55.150
finally they use a sigmoid because they train this stuff is like a discriminator

0:53:55.150,0:54:01.210
like a classifier between two classes and the forward is simply you send stuff

0:54:01.210,0:54:08.200
through the the main branch and they initialize these network so we have a

0:54:08.200,0:54:13.300
net D and Natalie so the this implementations slightly different from

0:54:13.300,0:54:19.030
the from what you were going over before right because the discriminator is just

0:54:19.030,0:54:29.950
one it outputs like the sigmoid the only difference is this line here right so

0:54:29.950,0:54:35.800
far so in the things we were talking in the lecture just before we don't have

0:54:35.800,0:54:40.920
the sigmoid we just have this final convolution later okay okay gotcha

0:54:40.920,0:54:46.870
of course second difference is that we would not be using a binary cross

0:54:46.870,0:54:55.360
entropy loss this is the source of all evils right BCE plus these sigmoid it's

0:54:55.360,0:55:02.580
wrong way of training a generative address our network our generator okay

0:55:02.580,0:55:06.810
so nevertheless we go with the main formulation here so let's see how it

0:55:06.810,0:55:10.680
works fixed noise you just create a you know

0:55:10.680,0:55:16.200
some random stuff with a bad batch size and the correct size here we have two

0:55:16.200,0:55:20.910
optimizers one optimizer for the discriminator one optimizer for the

0:55:20.910,0:55:27.060
generator and let's see what are the five steps that you should all know

0:55:27.060,0:55:33.390
right so let's figure out first of all we zero the gradients of the

0:55:33.390,0:55:41.880
discriminator okay so now we have the real data is going to be the data zero

0:55:41.880,0:55:52.500
that comes from the data loader right so we have real data here and then we're

0:55:52.500,0:55:58.340
going to be having is of labels which are going to be the real

0:55:58.340,0:56:04.490
labels okay so then we have the network the discriminator is going to be fed

0:56:04.490,0:56:08.930
with the real input and then we have some real output right and then you're

0:56:08.930,0:56:12.500
going to be computing the first part which is going to be the criterion which

0:56:12.500,0:56:17.660
is the binary cross-entropy between the output for whenever we put

0:56:17.660,0:56:24.530
the real input and the real label yeah and then we perform the first step so

0:56:24.530,0:56:30.770
here we perform backward in this criterion which is computing the partial

0:56:30.770,0:56:34.849
derivative of this binary cross-entropy with respect to the weights of the

0:56:34.849,0:56:40.970
discriminator when we fed the real data to the discriminator and we output we

0:56:40.970,0:56:45.740
try to match the labels which are the real labels okay this first point number

0:56:45.740,0:56:51.790
one okay keep in mind second part second part is gonna be you get noise and

0:56:51.790,0:56:59.510
therefore you get your network your generator you feed some noise inside the

0:56:59.510,0:57:05.089
generator therefore you get some fake output here I'm gonna be having my

0:57:05.089,0:57:10.310
labels now are filled with the fake label okay therefore you feed this stuff

0:57:10.310,0:57:18.680
inside the discriminator we feed the fake data but we detach right is the

0:57:18.680,0:57:24.080
important part so right now we fade we've fed the fake data but we detach it

0:57:24.080,0:57:29.119
from the generator and then we train again so we have the criterion we

0:57:29.119,0:57:33.740
compute the loss between the output of the discriminator with the labels for

0:57:33.740,0:57:39.260
the fake class okay and then we perform another step of backward so now we have

0:57:39.260,0:57:42.920
two backward right so we have backward here and backward here and we have

0:57:42.920,0:57:49.369
computed the partial derivative of these criterion in the case where we were

0:57:49.369,0:57:56.300
inputting real data and in the case where we were inputting fake data so you

0:57:56.300,0:58:00.710
compute backward here backward here there is no clear gradient right this is

0:58:00.710,0:58:04.400
the important part so we only called clear the gradient at the beginning and

0:58:04.400,0:58:09.349
we compute first the gradient with the real data and then

0:58:09.349,0:58:15.680
gradients for defected now you have that we can compute this one right so we step

0:58:15.680,0:58:20.660
in the optimizer so we computed the back part they did the partial derivatives we

0:58:20.660,0:58:26.150
computed the other parts of derivatives now we step finally we train the

0:58:26.150,0:58:31.999
generator and then we are done so how do we train the generator now you fill the

0:58:31.999,0:58:39.349
labels with the real labels okay but you still feed the discriminator and the

0:58:39.349,0:58:44.630
fake data the one that is generated by my generator this discriminator should

0:58:44.630,0:58:50.210
say oh this is fake data but we say no no this is real data and therefore you

0:58:50.210,0:58:55.749
basically swap the the thing right so now you have the when we compute these

0:58:55.749,0:58:59.210
backpropagation we have this gradients which are going in the opposite

0:58:59.210,0:59:05.359
direction these are trying to make your network perform worse okay but then we

0:59:05.359,0:59:10.160
are going to be just stepping with the generator right so this one computes the

0:59:10.160,0:59:14.029
partial derivative for everyone right the first partial derivative of the

0:59:14.029,0:59:18.499
criterion with respect to the weights of the discriminator and the weights of the

0:59:18.499,0:59:24.229
generator but then we are going to be stepping only with the generator so the

0:59:24.229,0:59:30.410
generator will try to make lower criterion and the criterion has the

0:59:30.410,0:59:35.719
label swapped right these are real label for whenever we feed the discriminator

0:59:35.719,0:59:41.660
fake data and so this one is actually working against the discriminator and

0:59:41.660,0:59:48.920
that was it so you had one backward here you have another backward here and you

0:59:48.920,0:59:55.309
have another backward here and other questions right now um wait what I was

0:59:55.309,0:59:59.599
looking to do the first two backwards because they're both on the same oh yeah

0:59:59.599,1:00:06.190
right right okay so the first backward here it's computed when the network the

1:00:06.190,1:00:12.499
discriminator the cost network has been fed with the real data and the label

1:00:12.499,1:00:18.410
here our will feel our field with the real label okay so this is the first

1:00:18.410,1:00:23.150
part of the backward so you have class true class and then you have class

1:00:23.150,1:00:27.860
of the fake class right in this case a week I generate my fake data through the

1:00:27.860,1:00:33.910
generator which was fed noise and then I feed my discriminator with the fake data

1:00:33.910,1:00:40.190
but I stopped the gradient to go backwards in the generator and this

1:00:40.190,1:00:47.210
criterion still tries to make the output of the discriminator has been close to

1:00:47.210,1:00:52.040
the label and the label in this case are the one the fake label the one that are

1:00:52.040,1:00:56.600
associated to the noise so more than know it may emit maybe we can call this

1:00:56.600,1:01:01.340
noise level or maybe okay it's fake labels finding fake is the data and then

1:01:01.340,1:01:07.490
the blue X Hut that is generated by my generator network and then when I put

1:01:07.490,1:01:12.920
these X Hut here inside the sorry the discriminator

1:01:12.920,1:01:17.990
I will tell the discriminator hey this one should be labeled as fake labels

1:01:17.990,1:01:20.540
right and so you have this criterion yeah

1:01:20.540,1:01:26.660
so in this backward you're going to be getting those partial derivative of the

1:01:26.660,1:01:33.680
loss function with respect to the parameters in the case when in the case

1:01:33.680,1:01:39.710
when we have fed the fake data and we are trying to label them as fake you

1:01:39.710,1:01:45.470
know fake labels right we have faked targets fake labels in the other part

1:01:45.470,1:01:51.110
here we actually were inputting inside the discriminator real data and then we

1:01:51.110,1:01:56.030
tell the you know the network you have a loss with print your output in the

1:01:56.030,1:02:00.580
labels which are supposed to be real label so the first part you try to get

1:02:00.580,1:02:05.780
you get the partial derivatives corresponding to the loss that has been

1:02:05.780,1:02:11.120
computed when real data was fed to the discriminator in the second part is that

1:02:11.120,1:02:16.670
you have the loss of with respect you know the loss of your output of the

1:02:16.670,1:02:22.370
network when we fed fake data right and so here we simply do again another

1:02:22.370,1:02:26.570
backward so in this case this backward this line here in this line here will

1:02:26.570,1:02:30.440
give you them they will accumulate right because PyTorch by default will

1:02:30.440,1:02:34.730
accumulate every time you perform backward so first part you accumulate

1:02:34.730,1:02:38.119
for the first half of the batch and then second time you accumulated

1:02:38.119,1:02:41.750
basically you have the partial derivative for the second part of the

1:02:41.750,1:02:46.160
bed the first part of the bed is the real data second part of the batch is

1:02:46.160,1:02:51.140
the fake data overall you're gonna have you know the partial derivatives of the

1:02:51.140,1:02:54.950
fake that the real data and the fake data and then we use this gradient in

1:02:54.950,1:03:01.579
order to to tune to change the parameters of the network the

1:03:01.579,1:03:07.010
discriminative ray does it make sense so far yeah that makes sense but one of

1:03:07.010,1:03:12.349
them is increasing it another one so this one so far are both trying to

1:03:12.349,1:03:16.790
decrease the criterion okay so this is this is you can see here and this

1:03:16.790,1:03:23.420
criterion here has the output which is fed of the discriminator when it was fed

1:03:23.420,1:03:28.190
with the real CPU data so you have real data and real labels okay

1:03:28.190,1:03:36.130
so the criterion here is trying to match to pay a real data and real labels

1:03:36.130,1:03:43.570
okay so far yes okay second part you try to have the network here try to match

1:03:43.570,1:03:49.480
fake data with fake labor okay because the output comes from this discriminator

1:03:49.480,1:03:55.660
which was input with fake data and then this you know it should force the

1:03:55.660,1:04:00.670
network to say oh these are fake labels right and so first one you had X

1:04:00.670,1:04:07.510
criterion here acting on true data with true with labels that are sailing

1:04:07.510,1:04:13.930
telling you these are true data and then you train you you have the the loss for

1:04:13.930,1:04:18.670
the network which is going to be sainted this help instead should be labeled as

1:04:18.670,1:04:24.370
fake data right so this is still trying to minimize these criteria therefore

1:04:24.370,1:04:29.110
whenever you perform the optimizer step the optimizer step will try to lower

1:04:29.110,1:04:35.440
both this one and this one okay another way to do this one would be to have the

1:04:35.440,1:04:41.080
summation between this one plus this one you perform only one go in the same step

1:04:41.080,1:04:45.700
okay the alternative if you understand what I said would be let me try to open

1:04:45.700,1:04:51.930
item eight this line here right so at 2:26

1:04:51.930,1:05:01.900
and then the other one was down to 235 right to 35 so we performed this one dot

1:05:01.900,1:05:07.570
backward and we did this on dock back right but otherwise we could have done

1:05:07.570,1:05:17.100
226 plus the other one 235 and then we just perform back backward here okay so

1:05:17.100,1:05:21.640
and this was an alternative which is actually exactly the same as right now

1:05:21.640,1:05:26.830
if you perform twice backward on the two different Criterion's is exactly as some

1:05:26.830,1:05:29.950
in the two Criterion's and then performing backward only once

1:05:29.950,1:05:36.550
okay and then below whenever we train the generator here we swap the the

1:05:36.550,1:05:42.220
labels right in this case we try to train the we're going to be training d

1:05:42.220,1:05:48.640
so we step in with the generator optimizer such that the we try to induce

1:05:48.640,1:05:52.569
the network to output labels that are real labels

1:05:52.569,1:05:56.019
when we provide data that is fake data right

1:05:56.019,1:06:01.269
so this stepping here it will not try to untrain the discriminator but we will it

1:06:01.269,1:06:06.159
will train the generator such that it tries to make up the discriminator

1:06:06.159,1:06:13.899
performing poorly so our generator is generating our fake data don't we want

1:06:13.899,1:06:17.229
used a lot of parts don't we want to take a step in the other direction for

1:06:17.229,1:06:20.769
that yeah so you want to take a step in the other direction for the generator

1:06:20.769,1:06:26.079
right you said no for the fake data we want to be able to tell it's fake yeah

1:06:26.079,1:06:31.929
and and that's that's that's where you do that here if you have fake data when

1:06:31.929,1:06:35.469
you when you put fake data inside the discriminator you also say in these

1:06:35.469,1:06:40.179
labels our fake levels right okay fake label doesn't mean they are fake

1:06:40.179,1:06:48.279
the these are the label for the fake data maybe this is weird so these are

1:06:48.279,1:06:53.669
the true label they are not fake label their true label for the fake data okay

1:06:53.669,1:07:00.749
this I guess is seed that's what I does dislike from other people writing code

1:07:00.749,1:07:06.130
that doesn't make sense in this case before these for the generate for the

1:07:06.130,1:07:11.769
discriminator we try to lower this criterion and we put this criterion so

1:07:11.769,1:07:18.219
these two lines are trying to match real data the true datum with a true label I

1:07:18.219,1:07:25.419
in this case you have trying to match the you know generated data with the

1:07:25.419,1:07:31.089
generated labels okay so both of these two parts are trying to train the

1:07:31.089,1:07:35.619
discriminator such that it can tell apart the two things so just to clarify

1:07:35.619,1:07:39.849
so for example like if returns produce cat images then like the generator would

1:07:39.849,1:07:43.989
produce like oh I tried to make a cat image here and here's label that saying

1:07:43.989,1:07:48.339
that it should be cat for since this image I didn't try to make a cat so the

1:07:48.339,1:07:52.539
label is 0 for I didn't try to make it yeah okay so let me go with cats I guess

1:07:52.539,1:07:59.289
it's gonna be easier or is it so here we're gonna have real data

1:07:59.289,1:08:03.550
these are very nice cute pictures of cats right and so we're gonna say

1:08:03.550,1:08:08.320
oh this output should be name is cut right because it's very nice and looking

1:08:08.320,1:08:14.080
cute then I'm gonna be feeding some garbage some noise to the generator this

1:08:14.080,1:08:21.280
looks like a monster okay ugly cut so then we provide these monster looking

1:08:21.280,1:08:24.910
like images to the discriminator and then we are going to be feeding these

1:08:24.910,1:08:30.549
laws with the you know verdict then whatever the discriminator says and

1:08:30.549,1:08:35.980
wheedle able to say these are monsters and so here you perform backward again

1:08:35.980,1:08:40.390
and then step such that you're going to be training the discriminator such that

1:08:40.390,1:08:47.109
they can tell apart cuts from monsters first part second part below we feed the

1:08:47.109,1:08:50.710
monsters in this case we still have we have the gradients right in this case we

1:08:50.710,1:08:55.299
cut off the gradient pay attention to this part here we cut off the gradient

1:08:55.299,1:09:00.280
so gradients don't go down the generator in this case we actually input the fake

1:09:00.280,1:09:03.370
data the monster looking images inside the

1:09:03.370,1:09:08.020
discriminator the discriminator say all monsters monsters but in this case we

1:09:08.020,1:09:14.710
say no these are cute cuts pictures and so now we train the we

1:09:14.710,1:09:17.680
perform backward which is computing the partial derivatives with respect to

1:09:17.680,1:09:23.140
everything and then we step for the generator such that the monsters that

1:09:23.140,1:09:28.080
the generator were was making now they look more cute

1:09:29.350,1:09:37.180
I can be more cute than this sorry why don't we send a gradient of the fake

1:09:37.180,1:09:45.340
data to the discriminator we do in the second case right so let me answer the

1:09:45.340,1:09:52.900
thing so in this case here we when we send when we send the gradients

1:09:52.900,1:09:57.610
backwards but back to the you know to the generator we actually swap the

1:09:57.610,1:10:03.150
correct labels with the you know incorrect labels in this case we input

1:10:03.150,1:10:08.680
monsters the decimator says these are monsters and we say oh these are

1:10:08.680,1:10:15.190
good-looking cuts and then we train the generator such that these monsters will

1:10:15.190,1:10:20.470
look like more nice-looking cut in this case you don't want to send the

1:10:20.470,1:10:24.850
gradients through because in this case you try to minimize the correct

1:10:24.850,1:10:29.170
classification part right so if you would send gradients backward you would

1:10:29.170,1:10:34.510
basically get a worse performing generator right because you don't want

1:10:34.510,1:10:41.320
to minimize this criterion you want to maximize this criterion right so that's

1:10:41.320,1:10:44.710
why we don't have gradients in this first case but we do have gradients in

1:10:44.710,1:10:49.390
this case because we absolutely want to compute the gradients with respect to

1:10:49.390,1:10:58.480
the generator of this criterion it's the combination VC lost and sigmoid because

1:10:58.480,1:11:06.820
I mean it's a problem because the underflow so the problem with the dbca

1:11:06.820,1:11:10.840
thing here is the probability probabilistic approach right so this

1:11:10.840,1:11:15.790
sigmoid if you train this network very well this sigmoid will be giving you

1:11:15.790,1:11:21.850
zero gradients and because if you saturate you know you're gonna have

1:11:21.850,1:11:26.590
you're in the two if you're not exactly on the middle way if you are just away

1:11:26.590,1:11:31.960
from the decision boundary you're gonna have basically or one it's so it's still

1:11:31.960,1:11:36.040
gonna have 0 gradient always going to be the other side here still all 0 but

1:11:36.040,1:11:40.690
there is no gradient so if you're over here you don't know where to go how to

1:11:40.690,1:11:43.390
go down the hill right because there is no here is like a

1:11:43.390,1:11:48.070
clap oh so this is a first problem second problem is that if you want to

1:11:48.070,1:11:55.810
really have a very vertical like a very vertical edge here you will need very

1:11:55.810,1:12:00.100
very very very large weight okay such that if you you know they have

1:12:00.100,1:12:04.090
larger the weight the larger is going to be the final value inside the sigmoid

1:12:04.090,1:12:08.380
and if you want to get like a saturated Sigma you're gonna have like pretty

1:12:08.380,1:12:15.340
large weight leading to that module and this one creates um you know it's gonna

1:12:15.340,1:12:19.990
make your weights and everything kind of explode and so that's why people want to

1:12:19.990,1:12:24.190
do several things like they want to limit the norm of the weights then you

1:12:24.190,1:12:29.740
want to limit the norm of the gradients and there are many many ways to patch

1:12:29.740,1:12:36.730
this architecture but that's dispatching right we don't want patching we'd like

1:12:36.730,1:12:41.980
to know what is proper and what is proper is gonna be basically and using a

1:12:41.980,1:12:49.030
out encoder for example for your final cost network so if you consider the

1:12:49.030,1:12:52.660
reconstruction error of a note encoder the reconstruction era of an auto

1:12:52.660,1:12:57.070
encoder will be zero of small if you provide a data and it is coming from the

1:12:57.070,1:13:00.790
training distribution if you provide a symbol it is away from the training

1:13:00.790,1:13:04.210
distribution to remember the manifold from the last time then the auto encoder

1:13:04.210,1:13:08.890
will do a poor job at the reconstruction and therefore the reconstruction error

1:13:08.890,1:13:14.710
will be larger right so instead of using a discriminator you can use a out

1:13:14.710,1:13:19.570
encoder reconstruction error how can you get more out of this course right

1:13:19.570,1:13:23.530
overall so let me give you a few suggestions first comprehension if

1:13:23.530,1:13:27.400
something was still not clear just as moving the question a section below the

1:13:27.400,1:13:30.820
video I will answer every question so you will get it eventually

1:13:30.820,1:13:36.430
if you'd like to i get more news about the field things i draw on in terms of

1:13:36.430,1:13:40.210
educational content and things I find interesting you can follow up on Twitter

1:13:40.210,1:13:44.740
and there you have my handle I've seen that you should like to have updates

1:13:44.740,1:13:48.400
about newer videos don't forget to subscribe to the channel and activate

1:13:48.400,1:13:52.960
the notification bell if you actually like this video don't forget to put a

1:13:52.960,1:13:56.050
thumbs up it helps as well recommending this video

1:13:56.050,1:14:00.250
to other people if you'd like to search the content of this lesson we have our

1:14:00.250,1:14:04.989
English transcription which is connected directly to this video so every title in

1:14:04.989,1:14:08.530
the transcription is clickable if you click on the title you get the right

1:14:08.530,1:14:12.040
director to the correct location on the video in the same way each section of

1:14:12.040,1:14:15.100
the video is the same in title is in transcription so you can go back and

1:14:15.100,1:14:21.550
forth maybe English is not your first language: Parli italiano? ¿Hablas español?

1:14:21.550,1:14:26.230
你会说中文吗 ? speak Korean have no idea how to speak Korean well we have several

1:14:26.230,1:14:31.239
translations of this material avaible odd at the web site so and we are also

1:14:31.239,1:14:35.770
looking for more translations if you can help as well it's really important that

1:14:35.770,1:14:40.750
you actually try to do some of the exercises and you play around with the

1:14:40.750,1:14:44.530
notebooks and the source code we provided in order to internalize and

1:14:44.530,1:14:49.780
understand better the concepts we explained during the lessons contribute

1:14:49.780,1:14:54.460
this is really giving you the opportunity to show your contribution

1:14:54.460,1:14:58.150
for example you find some typos in the write-up so you find some bugs in the

1:14:58.150,1:15:03.790
notebooks you can fix those and you know be part of this whole project by sending

1:15:03.790,1:15:09.400
me a request on GitHub or letting me know otherwise and that was it so see

1:15:09.400,1:15:12.180
you next time bye bye

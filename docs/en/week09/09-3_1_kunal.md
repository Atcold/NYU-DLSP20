
Introduction to Generative Adversarial Networks (GANs) 

GANs are a type of neural network which are used in unsupervised machine learning. They are essentially comprised of two modules competing with each other, in order to be able to capture and copy variations and features in a dataset. They can be used in tasks such as future predictions or for generating images after being trained on a particular dataset. 

GANs have a slightly different architecture from the variation auto-encoder (VAEs) studied previously. Instead of feeding the input through an encoder and a sampler to generate an estimate through latent variable space $z$; in GANs a scalar valued cost function is used to determine whether the input  is generated from the generator network or whether it is a true sample. It outputs a scalar value which is either max or min when the input is identical to the true input. The cost and the generator networks are successively trained through back propagation using gradient descent. 

![](https://i.imgur.com/9AgyauK.png)

Lets assume that the cost network needs to be trained to have a low cost for inputs which are closer in distribution to the true input, denoted in pink in the above picture. For any other input data distributions, it should have a high cost. A Mean Squared Error function is typically used to calculate the loss of the cost function to measure its accuracy. Also, it is worth noting that the cost function outputs scalar value, within a specified range (0 to some finit maximum value) i.e. $Cost : \mathbb{R}^n \rightarrow \mathbb{R}^+ \cup {0}$. This is unlike a classic discriminator which uses discrete classfication for its outputs. Thus, GANs involve two networks being trained simulataneously, each trying to counter each other's efficiency. The two networks, cost (or discriminator) and generator are adversial in nature. 

Meanwhile, the generator network($G : Z \rightarrow  \mathbb{R}^n$) needs to be trained to improve its mapping of input noise variables ***z*** to the desired data space ***x*** ($G : z \rightarrow x$). If the cost or the discriminator function is denoted by $D$ and the generator by $G$ then  the loss/error function used for this network maximizes $D(G(z))$.

During each training step, the gradients are calculated backward starting from the MSE function. The cost function updates its parameters based on the gradients calculated from the loss(MSE). By doing this, the cost function tries to adjust to the distribution of the original input by changing its parameters to output a low cost for inputs identical to the original input. 

As the gradients are calculated from the cost network to the Generator, the cost network *acts like a loss function* to the generator network. In this  context, 'loss' of a network is something that is used to train the neural network through gradient descent and to measure its accuracy. 

Thus, the training of the cost network is through the minimization of the MSE function, while the training of the Generator network is through minimization of the cost network through gradients, i.e. partial derivatives of cost parameters over the $\hat{x}$ vector generated by the generator network.

Thus the update step for the generator would be of the nature : 

$W_{e_{k+1}}= W_{e_k} + \eta \frac{\partial D}{\partial \hat{x}}$

where $D$ (sometimes denoted by $C$) is the discriminator or the cost function , 

and $\hat{x} = G(z)$, where G is the generator network and z is sampled from a random distribution. 


Comparison to Variational Auto-Encoders : 


![](https://i.imgur.com/bDABBH0.png)

In VAEs, we encode from input space (left) to latent space (right), through encoder and noise. Next, we decode from latent space (right) to output space (left). The reconstruction loss $l(x,\hat{x})$ is used to train the VAE, so that it generates samples in the input space manifold shown in pink in the figure. 

However, in GANs the generator is not calculating any gradients with respect to the true input variables for updates. Thus, the latent variable z is more independently generated in a GAN network at each step, and is not dependant on the encoding of the inputs. 

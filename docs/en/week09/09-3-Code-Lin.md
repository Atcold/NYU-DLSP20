The source code of GAN can be found at <https://github.com/pytorch/examples/blob/master/dcgan/main.py>

#### Generator

1. The generator will be upsampling. `nn.ConvTranspose2d` is the function to go from small dimension to large dimension.
2. At the end of the sequential, use `Tanh()` to generate output from -1 ot 1.
3. The input is the latent vector of size $nz$, the output is of size $nc*64*64$ , where $nc$ is the number of channels.  

```python
class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # state size. (ngf*8) x 4 x 4
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # state size. (ngf*4) x 8 x 8
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # state size. (ngf*2) x 16 x 16
            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # state size. (ngf) x 32 x 32
            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (nc) x 64 x 64
        )

    def forward(self, input):
        if input.is_cuda and self.ngpu > 1:
            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
        else:
            output = self.main(input)
        return output
```

#### Discriminator

1. It's important to use `LeakyReLU` as the activation function to make sure you will not be killing the gradient in negative regions. The generator cannot be trained without gradient.
2. At the end of the sequential, use `Sigmoid()` to generate output because the discriminator is treated as a classifier.

```python
class Discriminator(nn.Module):
    def __init__(self, ngpu):
        super(Discriminator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is (nc) x 64 x 64
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf) x 32 x 32
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*2) x 16 x 16
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*4) x 8 x 8
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 4 x 4
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        if input.is_cuda and self.ngpu > 1:
            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
        else:
            output = self.main(input)

        return output.view(-1, 1).squeeze(1)
```

And these two classes as initialized as `netG` and `netD`.

#### Loss function for GAN

We use Binary Cross Entropy (BCE) between target and output. 

```python
criterion = nn.BCELoss()
```

#### Setup

We set up fixed_noise of size batchsize and size of latent vector and create labels for real data and fake data generated by the generator.

```python
fixed_noise = torch.randn(opt.batchSize, nz, 1, 1, device=device)
real_label = 1
fake_label = 0
```

Then we set up optimizers for discriminator and generator.

```python
optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
```

#### Training

Each epoch of training is divided into two steps.

**Step 1** is to update the Disciminator network. This is done in two parts. First, we feed the discriminator with real datas coming from dataloaders, get the output of it, compute the loss between the output and real labels and then accumulate the gradient. Second, we feed the discriminator with datas generated by the generator using the fixed_noise, get the output of it, compute the loss between the output and fake labels and then accumulate the gradient. Finally, we use the accumulated gradients to update the parameters for the discriminator.

Note that we detach the fake datas to stop gradients from flowing to the generator while we train the discriminator.

Also note that we only need to call `zero_grad()` once in the beginning to clear the gradients. The two `.backward()` accumulate gradients computed from real and fake datas. And eventually we only need to call `optimizerD.step()` once to update the parameters.

```python
# train with real
netD.zero_grad()
real_cpu = data[0].to(device)
batch_size = real_cpu.size(0)
label = torch.full((batch_size,), real_label, device=device)

output = netD(real_cpu)
errD_real = criterion(output, label)
errD_real.backward()
D_x = output.mean().item()

# train with fake
noise = torch.randn(batch_size, nz, 1, 1, device=device)
fake = netG(noise)
label.fill_(fake_label)
output = netD(fake.detach())
errD_fake = criterion(output, label)
errD_fake.backward()
D_G_z1 = output.mean().item()
errD = errD_real + errD_fake
optimizerD.step()
```

**Step 2** is to update the Generator network. This time, we feed in the fake data, but with real labels! The purpose of doing this is that we train the generator to make the discriminator perform poorly.

```python
netG.zero_grad()
label.fill_(real_label)  # fake labels are real for generator cost
output = netD(fake)
errG = criterion(output, label)
errG.backward()
D_G_z2 = output.mean().item()
optimizerG.step()
```


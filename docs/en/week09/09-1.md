---
lang-ref: ch.09-1
title: Motivation of Deep Learning, and Its History and Inspiration
authors: Kelly Sooch, Anthony Tse, Arushi Himatsingka, Eric Kosgey
date: 7 April 2020
---

## Discriminative Recurrent Sparse Auto-Encoder (DrSAE) 
The idea of DrSAE consists of combining sparse coding, or the sparse auto-encoder, with discriminative training.
 <center><img src="{{site.baseurl}}/images/week09/09-1/q7pSvUJ.png" width="400px"/></center> 

**Fig 1:** Discriminative Recurrent Sparse Auto-Encoder Network

The encoder, $W_e$, is similar to the encoder in the LISTA method. The X variable is run through $W_e$, and then through a non-linearity. This result is then multiplied by another learned matrix, S, and added to $W_e$. Then it is sent through another non-linearity. This process can be repeated a number of times, with each repetition as a layer.

We train this neural network with 3 different criteria:
1. $L_1$: Apply $L_1$ criterion on the feature vector X to make it sparse.
2. Reconstruct X: This is done using a decoding matrix that reproduces the input on the output. This is done by minimizing square error, indicated by $W_d$ in Figure 1.
3. Add a Third Term: This third term, indicated by $W_c$, is a simple linear classifier which attempts to predict a category.

The system is trained to minimize all 3 of these criteria at the same time.

The advantage of this is by forcing the system to find representations that can reconstruct the input, then you're basically biasing the system towards extracting features that contain as much information about the input as possible. In other words, it enriches the features.


**Group Sparsity**
The idea here is to generate sparse features, but not just normal features that are extracted by convolutions, but to basically produce features that are sparse after pooling. 
 <center><img src="{{site.baseurl}}/images/week09/09-1/kpDK8Xu.png" width="400px"/></center>

**Fig 2:** Auto-Encoder with Group Sparsity

Figure 2 shows an example of an auto-encoder with group sparsity. Here, instead of the latent variable Z going to an $L_1$, it goes through basically an $L_2$ over groups. So you take the $L_2$ norm for each component in a group of Z, and take the sum of those norms. So now that is what is used as the regularizer, so we can have sparsity on groups of Z. These groups, or pools of features, tend to group together features that are similar to one another.



## AE With Group Sparsity: Questions and Clarification

Q: Can a similar used in the first slide strategy with classifier and regularizer can be applied for VAE?

A: Adding noise and forcing sparity in a VAE are two ways of reducing the information that the latent variable/code. Prevent learning of an identity function.

Q: In slide "AE with Group Sparsity", what is $P_j$?

A: p is a pool of features. For a vector z, it would be a subset of the values in z.

Q: *Clarification on feature pooling.*

A: (Yan draws representation of AE with group sparsity) Encoder produces latent variable z, which is regularized using the $L_2$ norm of pooled features. This z is used by the decoder for image reconstruction.

Q: Does group regularization help with grouping similar features.

A: The answer is unclear, work done here was done before computational power/ data was redily availiable. Techniques have not been brought back to the forefront. 

**Slide: "Image Level Training, local filters but no weight sharing"**

Beginning for discussion of another example of sparse coding (Gregor & Lecun arXiv:1006.044). Model consists of an encoder and decoder. Units/filters of this model operates over local patches of the input, and do not have shared weights so as to be more similar to biological neural networks. (Cont'd in next section)


## Third section???

*Missing contribution.*

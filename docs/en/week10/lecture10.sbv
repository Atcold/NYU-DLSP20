0:00:00.979,0:00:05.730
The recording is running and so as you can see, today we have a guest lecturer,

0:00:05.730,0:00:11.519
we have a Ishan Mishra. Ishan Mishra is a research scientist at Facebook AI

0:00:11.519,0:00:16.320
Researchm (FAIR) where he works on computer vision and machine learning. His research

0:00:16.320,0:00:20.939
interest are in reducing the need for supervision in visual learning.

0:00:20.939,0:00:25.680
He finished his PhD at the Robotics Institute at the Carnegie Mellon

0:00:25.680,0:00:32.369
University, where he worked with Martial Hebert and Abhinav Gupta. His PhD

0:00:32.369,0:00:37.649
thesis was titled Visual learning with minimal human supervision

0:00:37.649,0:00:43.530
for which he received the SCS distinguished dissertation award in

0:00:43.530,0:00:49.440
2018. So with less... how to say? without further ado, I don't know how to speak

0:00:49.440,0:00:54.809
English, let's get... We cannot even have the round of applause! Can we

0:00:54.809,0:00:59.730
have in the chat a round of applause for our speaker. fIshan] So everyone my name is

0:00:59.730,0:01:02.940
Ishan, I'll be talking about  self-supervised learning and computer vision

0:01:02.940,0:01:07.890
today and a lot of the focus is actually going to be sort of more on the

0:01:07.890,0:01:11.790
discriminative style of approaches and it's not really going to be on

0:01:11.790,0:01:15.210
generative set of approaches, and I sort of go about it more and more as I go

0:01:15.210,0:01:21.509
into my talk. So this sort of success story for representation learning or

0:01:21.509,0:01:25.439
like computer vision so far has been clearly this sort of pre training step,

0:01:25.439,0:01:31.409
or the ImageNet moment of like computer vision. So what has worked really well is

0:01:31.409,0:01:35.220
that when we have a large label data set like ImageNet, we can learn a

0:01:35.220,0:01:39.540
representation by performing an image classification task on this large data

0:01:39.540,0:01:44.070
set and what is very useful is not just performing this particular task at hand

0:01:44.070,0:01:48.000
but to take these representations that you learn and they're to use them for

0:01:48.000,0:01:51.810
downstream tasks where you may not have enough label data. And this has worked

0:01:51.810,0:01:57.140
really really well and sort of the standard recipe of success.

0:01:57.140,0:02:03.170
Now, this really involves collecting a large dataset of supervised

0:02:03.170,0:02:08.390
images and you need to get a bunch of these large diverse images and label

0:02:08.390,0:02:12.950
them with a bunch of large diverse concepts. So let's try to first see

0:02:12.950,0:02:19.370
whether we can sort of collect these labels and what are sort of the

0:02:19.370,0:02:25.160
difficulties in doing so. So the ImageNet dataset is a very, sort of, small

0:02:25.160,0:02:30.530
data set in the grander scheme of things. For example, ImageNet just has 14

0:02:30.530,0:02:36.380
million images and it has roughly 22,000 concepts. And just labeling this entire

0:02:36.380,0:02:40.160
thing, if you look at the amount of effort that was spent, it's about 22

0:02:40.160,0:02:45.560
human years to label this entire dataset. For contrast, a lot of people started

0:02:45.560,0:02:49.400
looking at these alternative supervision approaches where you are predicting

0:02:49.400,0:02:54.950
something, like, not really a very sort of pristine nice label but something which

0:02:54.950,0:03:00.170
is more easy to get, for example, predict like hashtags or predict GPS locations

0:03:00.170,0:03:04.880
of images, or what we're going to really focus on in this lecture is going to be

0:03:04.880,0:03:11.420
about self-supervised learning which is going to be using the data itself. So the

0:03:11.420,0:03:15.680
first question that I always like to sort of start out with is, why why don't

0:03:15.680,0:03:19.519
you just like get labels for all your data? why do you even want to invent this

0:03:19.519,0:03:24.650
entire line of research? why not just get all the labels?. So I did this small

0:03:24.650,0:03:28.519
exercise where I plotted the amount of supervision that we have for vision data

0:03:28.519,0:03:33.650
sets. So what I did is basically, I looked at all the images which have

0:03:33.650,0:03:39.110
bounding boxes and so, these are images where you know what kind of concepts are

0:03:39.110,0:03:42.170
in the image and you also have a box drawn around them, and this is sort of

0:03:42.170,0:03:46.070
the standard thing to do for something like an object detection model. So if you

0:03:46.070,0:03:49.160
look at all the data sets in vision that have bounding boxes, you'll get roughly

0:03:49.160,0:03:53.870
about a million or so images. Now, if you relax this constraint and you say that,

0:03:53.870,0:03:57.769
okay I don't really care about where the object is located, all I care about is

0:03:57.769,0:04:01.670
what objects are  present in the image. And so, if you

0:04:01.670,0:04:05.810
relax that constraint you immediately get an order of magnitude more data. So

0:04:05.810,0:04:10.910
you basically get about 40 million images or so.

0:04:10.910,0:04:14.540
Now, if you further sort of relax this constraint and you say that: I don't

0:04:14.540,0:04:19.040
really care about this image level supervision either, all I care about is

0:04:19.040,0:04:25.070
internet pictures that are present, you get basically about five

0:04:25.070,0:04:30.680
orders of magnitude more amount of data. And so if you look at this plot now, you

0:04:30.680,0:04:34.040
can see immediately that the amount of data that we have, which is labeled, even

0:04:34.040,0:04:38.180
at a bounding box or an image level, is basically nothing compared to what

0:04:38.180,0:04:43.610
images exist on in the internet scale. And I haven't really forgotten these

0:04:43.610,0:04:47.480
images, like forgotten the bars on the left hand side, it's just that they

0:04:47.480,0:04:50.750
completely disappear, and you really need to make this plot something like a log

0:04:50.750,0:04:58.010
plot to actually even get these bars to appear. So now, of course internet photos

0:04:58.010,0:05:01.100
do not represent everything about the world, there are things that really

0:05:01.100,0:05:04.640
require motion or things that really require other physical senses to learn,

0:05:04.640,0:05:08.630
so in the real world there are going to be far more things that you actually

0:05:08.630,0:05:13.400
experience, far more sensory inputs that you can get and it's really hard to

0:05:13.400,0:05:17.540
obtain labels for all this dataset. And again, to put things in perspective,

0:05:17.540,0:05:22.310
ImageNet which is just 14m in images and with a very small number of

0:05:22.310,0:05:26.840
concepts that you have required a lot of time the label, so clearly labeling is

0:05:26.840,0:05:30.800
not really going to scale to either all of Internet photos or even the real

0:05:30.800,0:05:38.960
world. So, the other sort of problem with labeling is that for complex concepts

0:05:38.960,0:05:44.210
like video, it's just really hard to scale labeling. The second problem is

0:05:44.210,0:05:50.810
that rare concepts sort of are really hard to label. So for example, this is one

0:05:50.810,0:05:55.580
of the popular image data sets called LabelMe and over here we can see that,

0:05:55.580,0:05:59.330
if you look at the kinds of concepts you observe there are a lot of concepts that

0:05:59.330,0:06:03.710
are so rare that you're going to have the label a lot of data to even get a

0:06:03.710,0:06:09.770
few instances of these concepts. So in this data set, 10% of the classes account

0:06:09.770,0:06:14.660
for more than 93% of the data, which already tells you that in order to sort

0:06:14.660,0:06:19.190
of scale labeling to like more and more concepts you'll need a lot and lot more

0:06:19.190,0:06:23.630
data with very diminishing returns. So this is sort of the standard long-tailed

0:06:23.630,0:06:29.340
problem. And of course pre-training is not right

0:06:29.340,0:06:32.400
always the right thing to do. For example, if you just completely change your

0:06:32.400,0:06:36.330
domain to now move it to, say medical imaging, it's not clear whether the

0:06:36.330,0:06:40.860
Imagenet pretraining is the right sort of thing for this task, or if you do not

0:06:40.860,0:06:44.520
know sort of the downstream tasks a priority, how do you collect a big data

0:06:44.520,0:06:48.060
set? and, how do you do this entire pre training and downstream task fine-tuning

0:06:48.060,0:06:54.570
recipe. So self-supervised learning sort of comes in between and it tries to give

0:06:54.570,0:06:59.729
you an alternate way to pre-train your models, or to learn from data or learn

0:06:59.729,0:07:06.780
from experiences without requiring pristine supervision. So in this case... so

0:07:06.780,0:07:09.870
there are sort of two simple definitions that you can come up with for 

0:07:09.870,0:07:13.770
self-supervised learning The first is more from like a discriminative or like a

0:07:13.770,0:07:17.970
supervised training perspective. So in ImageNet, for example, you have an image

0:07:17.970,0:07:22.110
and it can be classified into one of thousand levels so self-supervised

0:07:22.110,0:07:27.410
learning can be thought of as a way to obtain labels from the data

0:07:27.410,0:07:31.919
using an automatic process. So that automatic process does not really

0:07:31.919,0:07:36.030
require a lot of human intervention and so once you get these automatic

0:07:36.030,0:07:42.060
labels, now you can sort of go ahead and train your model with these labels. The

0:07:42.060,0:07:45.419
other way of thinking about  self-supervised learning is that it's really

0:07:45.419,0:07:49.770
a prediction problem where you are trying to predict a part of the data

0:07:49.770,0:07:54.840
from the other parts of the data. So you have some observed data and you have

0:07:54.840,0:07:58.950
some hidden data and you can now formulate a task where given the

0:07:58.950,0:08:03.060
observed data you try to predict either the hidden data or some property of the

0:08:03.060,0:08:07.740
hidden data. And so, pretty much a lot of sort of the self-supervised techniques

0:08:07.740,0:08:10.930
can be viewed in this particular framework.

0:08:10.930,0:08:16.600
So, the term sort of self-supervised learning, I really like to give this analogy

0:08:16.600,0:08:21.100
which is from Virginia de Sa, where she tries to

0:08:21.100,0:08:24.130
distinguish between the three terms: supervised, unsupervised and self-supervised.

0:08:24.130,0:08:30.430
And so, in supervised learning you have, say, an input cow and you're

0:08:30.430,0:08:35.020
given the exact target for it, which is going to be the label cow. And in 

0:08:35.020,0:08:39.070
unsupervised learning you're given this input and it's not clear what really the

0:08:39.070,0:08:43.599
entire target is, what exactly is the objective function or so on. Self-supervised

0:08:43.599,0:08:49.959
learning is sort of the term which is preferred now more and more and

0:08:49.959,0:08:54.940
the idea is that the label really comes from either a co-occurring modality or

0:08:54.940,0:08:58.779
co-occurring part of the data itself. So really all of the power is in the data

0:08:58.779,0:09:02.410
and you're really trying to predict  either parts with this

0:09:02.410,0:09:09.730
properties of the data. So, some very sort of standard and successful examples of

0:09:09.730,0:09:16.150
this are, say, either the Word2vec model, they're given this, let's say, a sentence.

0:09:16.150,0:09:20.500
For example, the cat sits on the mat. You are given a part of the sentence that

0:09:20.500,0:09:24.490
you observe, so which is, in this case, labeled as the context or the history

0:09:24.490,0:09:29.650
and then you have a part of the sentence, or word in this case, which is not

0:09:29.650,0:09:35.200
observed, which is sort of hidden from this entire model. And given this context

0:09:35.200,0:09:39.820
you ask the model to predict this target. And so you have your self-supervised

0:09:39.820,0:09:44.529
objective, you can minimize it in a particular fashion and now you will

0:09:44.529,0:09:49.839
learn a representation for your input data. And word2vec has been, I mean, it

0:09:49.839,0:09:53.410
has actually shown a lot of promise in a variety of applications and this

0:09:53.410,0:09:57.160
entire sort of predictive model has inspired a lot of work in computer

0:09:57.160,0:10:03.339
vision as well. The success of self-supervised learning is sort of undebatable

0:10:03.339,0:10:07.990
in natural language processing. So, in 2018, there was this really successful

0:10:07.990,0:10:14.440
model called BERT which basically is a form of a masked auto-encoder and this

0:10:14.440,0:10:18.100
modular, sort of, revolutionized the amount of things that you can do in NLP

0:10:18.100,0:10:22.420
with limited amount of data, and a lot of people call this the ImageNet moment of

0:10:22.420,0:10:28.360
NLP (Natural Language Processing). So in this talk, we'll sort of, again, to

0:10:28.360,0:10:32.410
motivate why we want to use self-supervised learning, we are really going

0:10:32.410,0:10:38.980
to focus on how you can look at data and you can use observations and

0:10:38.980,0:10:42.520
interactions of the data to formulate self-supervised tasks, how you can

0:10:42.520,0:10:46.570
leverage multiple modalities and I'll talk a little bit more about what this

0:10:46.570,0:10:51.810
term modalities means or structure in the data to sort of learn

0:10:51.810,0:10:57.760
representations. So let's move on to the context of computer vision and I'll sort

0:10:57.760,0:11:01.450
of now try to define things that I've been talking about in a slightly high

0:11:01.450,0:11:05.800
level in more concrete fashion. [Alfredo] First question: So self-supervised learning is

0:11:05.800,0:11:14.440
basically just unsupervised learning, right? [Ishan] Yes, I mean, yes yes I mean,

0:11:14.440,0:11:21.250
basically, the sort of main differences, like unsupervised is sort of very poorly

0:11:21.250,0:11:26.730
defined term. So there is supervised but, what is unsupervised? So, for example, the

0:11:26.730,0:11:31.240
analogy given by Jitendra Malik is: There is a cat but there is no category

0:11:31.240,0:11:35.530
called un-cat. So that's the reason to sort of like

0:11:35.530,0:11:39.520
refer this term more and more, that it's really about using the data or the

0:11:39.520,0:11:43.480
properties of the data ITSELF to come up with supervision, so that's why 'self-supervision'.

0:11:43.480,0:11:53.980
[Alfredo] So what I am suggesting, is a subset? [Ishan] Yes, I guess. Yes. [Yann] I mean, I guess, you know my

0:11:53.980,0:11:59.560
reason for calling it this is that the algorithms, the algorithms are

0:11:59.560,0:12:02.440
essentially the same as supervised learning algorithms with some

0:12:02.440,0:12:07.450
modifications, because you're kind of training the system to learn part of its

0:12:07.450,0:12:11.830
input from another part of the input, so it's very similar to supervised learning in

0:12:11.830,0:12:17.890
many ways except that you need to handle uncertainty better and the negative

0:12:17.890,0:12:22.830
category, if you want, may be much larger which is kind of an issue but

0:12:22.830,0:12:27.460
unsupervised learning is really not very well defined. So self-supervised learning is kind

0:12:27.460,0:12:31.120
of a very defined concept, it's not entirely clear it's a subset of

0:12:31.120,0:12:33.960
unsupervised learning. [Ishan] So moving ahead I'll now try to

0:12:41.459,0:12:47.470
talk about self-supervised learning more in the context of vision. So in vision a

0:12:47.470,0:12:54.310
lot of these sort of prediction problems have been framed as pretext tasks So a lot

0:12:54.310,0:13:00.060
of the vision algorithms, sort of, this term comes more from 2015 

0:13:00.060,0:13:06.490
from this particular paper by Carl Doersch, and the idea here was that you have a

0:13:06.490,0:13:10.959
text task or the sort of task that you really care about at the end, like image

0:13:10.959,0:13:16.660
classification, but of course you don't have a lot of data for that, so

0:13:16.660,0:13:22.270
you want to solve a task before going to the real task, so a pretext task So this

0:13:22.270,0:13:26.380
pretext task is a prediction task that you are solving but it's not often the

0:13:26.380,0:13:31.570
real task that you really care about, so you solve this particular task to learn

0:13:31.570,0:13:35.649
a representation and then you finally get your downstream task where you want

0:13:35.649,0:13:38.200
to use this representation to perform something meaningful.

0:13:38.200,0:13:45.070
So these pretext tasks are sort of funny, they're often, like people got

0:13:45.070,0:13:51.600
very creative with sort of coming up with these pretext tasks So let's look at

0:13:51.600,0:13:56.740
how you can define a bunch of pretext tasks and what each of these pretext

0:13:56.740,0:14:02.140
tasks are trying to do. And so you can use either images, video, video and sound when

0:14:02.140,0:14:06.070
you're trying to do these things. And in each case you'll have a bunch of observed

0:14:06.070,0:14:09.880
data and you'll try to either predict hidden data or you try to predict a

0:14:09.880,0:14:16.050
property of type in hidden data, and this sort of distinguishes a bunch of approaches.

0:14:16.050,0:14:22.770
So let's look at how you can use images to define something like a pretext task.

0:14:22.770,0:14:28.540
So, the paper that introduced this term pretext task came up with this, a fairly

0:14:28.540,0:14:36.250
sort of funny method, but what you do is you take say two image patches, basically

0:14:36.250,0:14:39.730
take the network and you ask the network to predict what is the relative position

0:14:39.730,0:14:45.220
of each patch with respect to the other. So in this case, say I first sample a

0:14:45.220,0:14:51.170
blue patch, and now I sample another red path, so what I do is I basically 

0:14:51.170,0:14:55.220
feed-forward both of these patches through a ConvNet and I have a classifier that is

0:14:55.220,0:14:59.600
going to solve the a to 8 classification problem, and how do I get the label for

0:14:59.600,0:15:03.530
this classification problem? Well I just look at where the red patch is located

0:15:03.530,0:15:08.030
with respect to the blue patch and that's it. So at the end of it, you're

0:15:08.030,0:15:11.270
just predicting, you're just solving an an a to 8 classification task,

0:15:11.270,0:15:16.220
you got your labels by basically doing this sort of exploiting this property of

0:15:16.220,0:15:22.100
the input data and that's it. Now you can use this to basically train this

0:15:22.100,0:15:28.070
entire ConvNet. So to look at it in a

0:15:28.070,0:15:32.570
different way, it's only solving a very small classification problem,

0:15:32.570,0:15:37.180
it's just solving basically eight possible locations sort of a problem.

0:15:37.180,0:15:42.110
Surprisingly enough doing this sort of pretext task actually learns something

0:15:42.110,0:15:48.020
fairly reasonable. So the one way to look at what this network has learned is to

0:15:48.020,0:15:51.320
look at what it considers our nearest neighbors in spatial

0:15:51.320,0:15:57.680
representation. So to explain this plot a little bit more, on the left-hand side

0:15:57.680,0:16:01.610
you have the input patch so you feed forward this input patch through that

0:16:01.610,0:16:08.150
CNN and you basically extract a bunch of patches on the data, on your data set, so

0:16:08.150,0:16:11.960
in this case ImageNet, and you compute feature representations for each of

0:16:11.960,0:16:16.160
these patches. Now for the particular input patch that you sent through the

0:16:16.160,0:16:20.150
ConvNet, you compute the nearest neighbors of all the patches from the

0:16:20.150,0:16:23.210
data set, and you can use three different networks to compute the feature

0:16:23.210,0:16:27.560
representations. So the first column is the relative positioning pretext task

0:16:27.560,0:16:32.510
and the second column is basically a randomly initialized AlexNet and then

0:16:32.510,0:16:36.320
the third column is basically an ImageNet pre-trained AlexNet.

0:16:36.320,0:16:43.400
So if you sort of look at what this relative positioning task is capturing,

0:16:43.400,0:16:47.510
it's really able to find very good patches, patches that are identical or

0:16:47.510,0:16:52.970
very close to the input patch. And you also see that it's, for example, like in

0:16:52.970,0:16:57.290
the row of the cat, so that's the fourth row, you can see that it's actually able

0:16:57.290,0:17:01.610
to figure out it's slightly invariant to, say, the colors. So the input cat was

0:17:01.610,0:17:04.460
black and white, but it's actually able to pick out cats which are not just

0:17:04.460,0:17:09.050
black and white. So it's really doing something, it's at least able

0:17:09.050,0:17:15.170
to reason about patches as a whole. So, why should this representation do

0:17:15.170,0:17:20.180
anything which is semantic? So the nearest neighbor visualization technique

0:17:20.180,0:17:24.710
is good at telling you what this representation space has captured, so in

0:17:24.710,0:17:29.810
this case what we can confidently say is that this relative patch representation

0:17:29.810,0:17:35.000
has learned to sort of associate a bunch of these local patches together, local

0:17:35.000,0:17:37.890
patches that have roughly the same appearance.

0:17:37.890,0:17:42.030
And so, because it is able to reason about these local patches maybe it's

0:17:42.030,0:17:45.750
actually able to reason about an image because image can sort of be viewed as a

0:17:45.750,0:17:50.550
bunch of local patches together. So it's able to sort of put these patches in one

0:17:50.550,0:17:54.680
part of the representation space. Now people, have like I said, people

0:17:58.280,0:18:02.510
have gotten fairly creative with the kinds of pretext tasks they do, so

0:18:02.510,0:18:07.780
another sort of popular pretext task is predicting rotations of an image, and

0:18:07.780,0:18:13.280
this task is very straightforward. You have an image, you

0:18:13.280,0:18:19.610
can either apply a rotation of 0 degrees, 90 degrees, 180 degrees or 270 degrees to

0:18:19.610,0:18:24.410
it and basically you send in that particular image after applying a

0:18:24.410,0:18:28.370
rotation and you ask the network to predict what was the exact rotation

0:18:28.370,0:18:33.230
applied to the image and it just solves a four-way classification problem. So it

0:18:33.230,0:18:41.960
predicts basically either if the rotation is 0, 90, 180, or 270 (degrees) and this

0:18:41.960,0:18:45.140
pretext task is actually one of the most popular pretext tasks now because it's

0:18:45.140,0:18:50.480
so easy to implement. You basically just take an image, it's very very simple, you

0:18:50.480,0:18:54.049
don't really need to sample too many patches or solve any sort of complicated

0:18:54.049,0:18:57.799
thing it's a very standard architecture and you can solve this and its become

0:18:57.799,0:19:02.150
fairly popular now. [Alfredo] So the network is going to be basically trained, so the

0:19:02.150,0:19:06.410
features are trained in order to solve this problem, right? So the output will be,

0:19:06.410,0:19:11.690
somehow, dependent on the specific task someone is going to be picking somehow,

0:19:11.690,0:19:16.730
right? [Ishan] Yes. So this is, again, this is a pretext task so we are not really

0:19:16.730,0:19:19.970
interested in predicting the rotations of an image, we are just using this task

0:19:19.970,0:19:24.440
as a proxy to learn some features so that on the downstream tasks, say when

0:19:24.440,0:19:28.490
someone gives us a thousand labels images of a cat, we can then use this 

0:19:28.490,0:19:31.299
pre-trained feature representation to do that particular task.

0:19:31.299,0:19:35.210
So these pretext tests are often really not going to make a lot of semantic

0:19:35.210,0:19:40.090
sense and that's sort of the reason for calling them pretext

0:19:40.090,0:19:44.000
because you have a downstream task where you actually have some semantics or some

0:19:44.000,0:19:48.100
label that you actually know is good. [Alfredo] Thanks. [*] Why would predicting

0:19:51.620,0:19:59.480
rotations give us any sort of useful representations? [Ishan] Yes, so in fact, when this

0:19:59.480,0:20:03.770
paper came out this was the question of many many people and it was my question

0:20:03.770,0:20:07.480
as well, empirically this actually works really well.

0:20:07.480,0:20:13.940
And sort of my intuition for this has basically been that in order to predict

0:20:13.940,0:20:18.410
what sort of the rotation of an object is, it needs to it roughly understand

0:20:18.410,0:20:23.120
what the boundaries or what sort of some concepts in this image are. For example,

0:20:23.120,0:20:28.130
to predict that this particular image is rotated by 180 degrees it needs to at

0:20:28.130,0:20:33.800
least recognize or sort of segregate the sky from like the sand, or the sky from

0:20:33.800,0:20:38.810
the water, or at least understand that for a tree the leaves are generally not

0:20:38.810,0:20:44.720
below sort of the bar, trees don't go grow like downwards, they grow upwards. So

0:20:44.720,0:20:49.310
it needs to reason about some things implicitly. It's not super clear

0:20:49.310,0:20:55.000
what it really needs to do but this task empirically works very well. [*] Has this

0:20:55.000,0:21:01.910
only been tried out, or works as a task with like a discrete classification? or has it

0:21:01.910,0:21:05.750
has been tried on like a continuous scale of angles at which the image is

0:21:05.750,0:21:10.790
rotated? [Ishan] Yes, so you can do both versions. So you

0:21:10.790,0:21:14.120
can increase the number of them you want and as you

0:21:14.120,0:21:17.870
basically make it very very large approaching more of a regression problem

0:21:17.870,0:21:23.360
where you have a continuous variable. In practice these four angles work

0:21:23.360,0:21:32.210
pretty well, increasing gives marginal difference. [Alfredo] There is a question about the previous slide: How

0:21:32.210,0:21:36.410
does the nearest neighbor would work in this context? Do you run every patch, each

0:21:36.410,0:21:41.720
patch, through the CNN? [Ishan] Yes, so this is just for visualization, this is just 

0:21:41.720,0:21:46.970
understanding, so although it is sort of expensive to compute

0:21:46.970,0:21:50.300
this, it gives you a very good idea of what the representation has learned. So

0:21:50.300,0:21:54.440
what the authors did was basically extract a bunch of patches from each

0:21:54.440,0:22:00.530
image, for roughly ten to nine patches, and so, on a small set of images so, I

0:22:00.530,0:22:03.120
think in this case it was like 50,000 to a 100,000 images and

0:22:03.120,0:22:06.539
then you basically just compute nearest neighbors on those those patches of

0:22:06.539,0:22:08.960
those images. Is that clear? [Alfredo] Yes, it is. 

0:22:12.780,0:22:22.200
[Ishan] Good. So another task which is also fairly popular is called colorization.

0:22:22.200,0:22:28.950
So in this case, given a grayscale image you basically try to predict the colors

0:22:28.950,0:22:32.490
of that image. So you can really formulate this task for any image. You

0:22:32.490,0:22:36.870
can take an image, you can remove its color and you can ask a network to

0:22:36.870,0:22:40.020
basically predict the color from this black and white or grayscale

0:22:40.020,0:22:47.429
image. And this task by itself is  actually useful in some respect. So,

0:22:47.429,0:22:52.890
a lot of old movies when you  see them colorized, select movies

0:22:52.890,0:22:57.900
short and say the 40s or 30s, when there was not a lot of color technology, you

0:22:57.900,0:23:01.530
can actually have this task  be applied there. So in some way it

0:23:01.530,0:23:07.350
actually is more useful than other pretext tasks. And why does this task

0:23:07.350,0:23:11.549
learn something meaningful? Well, it needs to recognize that trees are

0:23:11.549,0:23:15.990
green, it needs to understand what sort of object categories it is in order to

0:23:15.990,0:23:20.490
color it fairly well. And so, in practice this has now been extended to

0:23:20.490,0:23:24.809
the video domain and there are a lot of follow-up works on polarization

0:23:24.809,0:23:31.260
itself. [Alfredo] It is interesting because, I think, the colour mapping is not 

0:23:31.260,0:23:36.350
deterministic, right? [Ishan] It is not deterministic, yes. [Alfredo] So there are several

0:23:36.350,0:23:43.740
possible true solutions, right? [Ishan] Yes. So the initial paper was basically

0:23:43.740,0:23:47.520
proposing a deterministic mapping, so you were solving either a classification or

0:23:47.520,0:23:52.740
the regression problem. So you only could have, say, a blue colored umbrella and you

0:23:52.740,0:23:58.679
could never predict a gray color umbrella. And so what ended up happening was for a

0:23:58.679,0:24:04.049
lot of categories which have different kinds of colors, so for example let's

0:24:04.049,0:24:07.440
assume, say, you have a ball and that ball can appear either in the red blue or

0:24:07.440,0:24:11.700
green colors, the netbook would  predict that to be grey. Because, I mean,

0:24:11.700,0:24:15.260
that sort of the mean of all of these things, so that's the best it can do.

0:24:15.260,0:24:21.149
There was follow-up work from David Forsyth's group in UIUC which tried to

0:24:21.149,0:24:24.360
come up with variational auto-encoders, so you actually have a

0:24:24.360,0:24:28.240
latent variable tp be able to have diverse colorization. So in

0:24:28.240,0:24:32.320
practice, you can do approaches like that, so you can actually have now a

0:24:32.320,0:24:35.500
green colored ball and because you're doing that for the entire scene you can

0:24:35.500,0:24:38.800
actually have consistent colorings of the entire scene. [Alfredo] Yeah, yeah that's what, I

0:24:38.800,0:24:42.460
think, we've talking with Yann or whenever we have some

0:24:42.460,0:24:48.160
mapping that goes from one to many and then we should choose like a little

0:24:48.160,0:24:51.910
variable model which allow us to choose a multiple solution given that we have

0:24:51.910,0:25:01.360
the same input. [Ishan] so if I think the reason why like people did not really focus on

0:25:01.360,0:25:07.960
a lot on that particular aspect in this case was, at least back in the day: One, it

0:25:07.960,0:25:11.620
was not clear what was working and what was not, and second, this was still a

0:25:11.620,0:25:15.520
pretext task and people were not really concerned about the colorization quality,

0:25:15.520,0:25:19.240
people were more concerned about representation quality. But I think now a

0:25:19.240,0:25:23.290
lot of us understand that both of them are fairly tied to one another,

0:25:23.290,0:25:27.790
that you really need to have this  non-deterministic mapping to get

0:25:27.790,0:25:35.530
something more out of the data. [Alfredo] Thanks. [Ishan] And finally, so this is, 

0:25:35.530,0:25:40.330
and I apologize for this picture, it's from the paper and I think it was low resolution. 

0:25:40.330,0:25:44.410
So this is another task which is  context auto-encoders. So the idea is

0:25:44.410,0:25:49.330
basically borrowed pretty much from say  word2vec. So you hide a particular

0:25:49.330,0:25:53.230
part of the image and now given the surrounding part of the image you need

0:25:53.230,0:25:56.530
to predict what was hidden. So it's really sort of the fill in the blanks

0:25:56.530,0:26:02.090
task. And why should this work? Well, it's at

0:26:02.090,0:26:07.250
least trying to reason about what objects are present, so cars can run on

0:26:07.250,0:26:11.539
roads or like buildings basically consist of windows and

0:26:11.539,0:26:15.470
closer to the ground they're supposed to have doors and so on. So it needs to

0:26:15.470,0:26:19.460
learn something more about like the implicit structure of the data by

0:26:19.460,0:26:26.840
performing this task. So this was just about images and now I'll talk

0:26:26.840,0:26:34.159
about what are the other tasks that you can do in video. So in video, the

0:26:34.159,0:26:40.390
main source of supervision is this notion of sequentiality of frames. So

0:26:40.390,0:26:45.080
frames basically have an inherent order in them and you want to use that

0:26:45.080,0:26:49.070
order to get something. For example, say, predict the order of frames or fill in

0:26:49.070,0:26:53.240
the blanks and a bunch of other pretext tasks that are all dependent on

0:26:53.240,0:26:58.309
sequential nature. So here I'll talk about one of

0:26:58.309,0:27:03.440
the works that I did in 2016 which was about predicting the temporally correct

0:27:03.440,0:27:08.539
or incorrect order of frames. This is very much inspired from earlier work that

0:27:08.539,0:27:13.250
Yann and basically others did on sequential ordering of frames through

0:27:13.250,0:27:16.760
contrastive learning, and I'll talk about those towards the end when I

0:27:16.760,0:27:22.429
actually talk about contrasting learning. So in this particular work, we were very

0:27:22.429,0:27:26.960
much inspired by the pretext tasks again, and we saw the binary

0:27:26.960,0:27:32.120
classification problem. So given a bunch of frames we extract three frames and if

0:27:32.120,0:27:36.529
we extract them in the right order we label them plus one and if we shuffle

0:27:36.529,0:27:40.520
them basically we label them as zero. And so, now we need to solve a binary

0:27:40.520,0:27:44.700
classification problem to predict whether something is shuffled or not.

0:27:44.700,0:27:52.070
And the reason this sort of works is because, so, given three frames, or let's

0:27:52.070,0:27:57.090
think of them as basically, start, middle and end, this network really tries to

0:27:57.090,0:28:02.640
learn given a start and end point is this point of valid 

0:28:02.640,0:28:05.850
interpolation of the start and end points? So it really tries to

0:28:05.850,0:28:13.710
interpolate smoothly these features given this visual input. So

0:28:13.710,0:28:17.760
the network is fairly straightforward, it's a triplets siammese network.

0:28:17.760,0:28:22.800
You have three frames, you feed forward each one of them independently, you

0:28:22.800,0:28:25.980
concatenate the features that you obtain from these three frames and then you

0:28:25.980,0:28:29.640
perform a binary classification problem, so you predict whether this thing is

0:28:29.640,0:28:33.240
correct or incorrect, whether it's a shuffle or

0:28:33.240,0:28:37.410
not shuffled. You can basically minimize this with

0:28:37.410,0:28:41.900
cross-entropy loss and you can train this entire network end-to-end.

0:28:42.140,0:28:48.060
So, again, like I had mentioned earlier, nearest neighbor is a good way

0:28:48.060,0:28:52.890
to visualize what these networks are learning. So we further prior work and we

0:28:52.890,0:28:58.260
basically looked at the nearest neighbors of frames. So on the left hand

0:28:58.260,0:29:02.070
side you have a query frame and you feed forward that frame, you get a feature and

0:29:02.070,0:29:05.730
then you basically look at the nearest neighbors in that feature representation.

0:29:05.730,0:29:11.850
So we will do that for ImageNet, shuffle and then learn random features. So

0:29:11.850,0:29:16.260
what you observe is there's a very stark difference between what

0:29:16.260,0:29:22.500
ImageNet, shuffle and random give you. So the first row, if you look at the

0:29:22.500,0:29:26.880
gym scene, ImageNet is really good at figuring out that it's a gym scene. it's

0:29:26.880,0:29:30.930
the nearest neighbor it retrieves, looks is very different from the initial scene,

0:29:30.930,0:29:35.670
the initial image that we've given. So, like, the floor is much

0:29:35.670,0:29:42.240
better lit, in the query the floor was actually black, and the exact exercise

0:29:42.240,0:29:45.780
being performed is not really the same, but ImageNet is really good at

0:29:45.780,0:29:50.760
collapsing this entire semantic category and really bringing in various

0:29:50.760,0:29:56.130
different gym scenes together close by in the representation space. The same

0:29:56.130,0:30:00.660
thing goes for the row below. So, you have an outdoor scene and

0:30:00.660,0:30:04.140
ImageNet is immediately able to  pick up on that outdoor part,

0:30:04.140,0:30:08.190
it's able to figure out there is grass and so on, and it brings these

0:30:08.190,0:30:14.640
two points together in the feature space. If you look at, say, the rightmost,

0:30:14.640,0:30:19.890
the nearest neighbors retrieve by the random network, you see that it really

0:30:19.890,0:30:24.270
focuses on the color. So in the top row it's a sort of focusing on the black

0:30:24.270,0:30:29.160
floor it's really looking at, maybe,  the black color in this image

0:30:29.160,0:30:32.970
and that's how it's retrieving its nearest neighbor. Now, if you look at the

0:30:32.970,0:30:37.080
shuffle and learn, the nearest neighbors are fairly odd. It's not immediately

0:30:37.080,0:30:40.470
clear whether it's focusing on the color or whether it's focusing on that entire

0:30:40.470,0:30:46.260
semantic concept. And so on, on further inspection and after looking at

0:30:46.260,0:30:50.190
a lot of these examples, we figured out that it was really looking at the pose

0:30:50.190,0:30:54.299
of the person. So if you look at, in the top row, the person is 

0:30:54.299,0:30:57.990
upside down and that's sort of the nearest neighbor retrieved as well.

0:30:57.990,0:31:03.299
And in the second row, also the person is sort of have their

0:31:03.299,0:31:07.440
feet in the particular way and it's really trying to get there with

0:31:07.440,0:31:11.549
its nearest neighbor and it's ignoring the entire scene. It's not

0:31:11.549,0:31:16.139
really focused on the background. And when we were thinking about this, why

0:31:16.139,0:31:20.940
would a network even try to do something of this sort? Well we thought

0:31:20.940,0:31:26.429
back to our pretext task. So the pretext task was predicting the order, or

0:31:26.429,0:31:30.480
basically predicting whether things are in the right order or not, and to do this

0:31:30.480,0:31:35.700
you really need to focus on what is moving in the same way, or in

0:31:35.700,0:31:39.840
this case the people. So if you focus on the background you will never be able to

0:31:39.840,0:31:42.450
answer this question fairly well because the

0:31:42.450,0:31:46.409
background doesn't change a lot between three frames that are taken 

0:31:46.409,0:31:49.919
close by in a video. The only thing that sort of changes is the person or the

0:31:49.919,0:31:54.749
things that are moving in that video. So sort of accidentally, we

0:31:54.749,0:31:58.740
basically trained a network that was really trying to look at things

0:31:58.740,0:32:03.029
that are moving and then ended up focusing on the pose of 

0:32:03.029,0:32:09.419
people. Now, of course this is my interpretation, we wanted to verify this

0:32:09.419,0:32:15.749
quantitatively, so what we did was we took our representation and we fine-tuned it

0:32:15.749,0:32:19.679
on this task of human key point estimation. So this task is basically,

0:32:19.679,0:32:25.529
given up human you need to  predict where certain key points are. So

0:32:25.529,0:32:30.779
the key points are  defined as basically: The nose, the

0:32:30.779,0:32:36.840
neck, the left shoulder, right shoulder, right elbow, left elbow, wrist, and so on.

0:32:36.840,0:32:40.440
So you basically have these bunch of predefined key points and you train the

0:32:40.440,0:32:44.399
network to predict this. So this is really useful for something like

0:32:44.399,0:32:51.509
tracking or pose estimation of a person. So we took our shuffle and learn 

0:32:51.509,0:32:56.220
self-supervised method and we fine-tuned it on these two data sets called FLICK and

0:32:56.220,0:33:01.710
MPII, and we did the same thing for an ImageNet and supervised network, and this

0:33:01.710,0:33:04.919
was back in the days of AlexNet so AlexNet was the architecture that we used.

0:33:04.919,0:33:12.210
And fairly surprisingly, what we found was that the self-supervised

0:33:12.210,0:33:16.320
representation was very competitive or even slightly better than ImageNet

0:33:16.320,0:33:21.509
supervised representation, at this task of key point estimation. So in this case

0:33:21.509,0:33:25.200
what I'm measuring is a you see which is AUC (area under the curve), so higher is better.

0:33:25.200,0:33:28.350
And you can see that it's performing fairly well

0:33:28.350,0:33:32.490
which was very surprising to us because we hadn't any thought about this task

0:33:32.490,0:33:37.710
when we were designing a pretext task, we really thought that this pretext

0:33:37.710,0:33:42.540
task will help us understand actions better, but it turns out that you

0:33:42.540,0:33:45.990
you can have surprising outcomes depending on what you end up

0:33:45.990,0:33:50.880
creating as a pretext task. So in this case that was pose estimation. [*] So

0:33:50.880,0:33:56.580
for this example, you said you fine-tuned it on human key

0:33:56.580,0:34:02.880
point estimation, so is that kind of like a supervised step, like once you have

0:34:02.880,0:34:10.110
your pretext representations? [Ishan] Yes, so so the pipeline basically generally goes

0:34:10.110,0:34:15.360
like, you have you do a pre-training step, so that can basically be, say, ImageNet supervision,

0:34:15.360,0:34:18.510
which is predicting one of a thousand classes, and then you have a downstream

0:34:18.510,0:34:22.530
task where you have a few amount of labels. So you basically in this

0:34:22.530,0:34:28.020
case, that's predicting the human key points. So the this way of evaluation,

0:34:28.020,0:34:32.580
what it does is it basically  takes a bunch of pre-trained networks

0:34:32.580,0:34:36.810
and then it fine-tunes them using the same supervised data at the end. And so

0:34:36.810,0:34:42.030
what you're evaluating is how good was it if I started from, say, ImageNet

0:34:42.030,0:34:45.570
supervised network or a shuffle and learn network to perform this task of

0:34:45.570,0:34:54.030
key point estimation. [*] Okay, thank you. [*] Is it strange that it does well since the shuffle and learn

0:34:54.030,0:34:59.700
all focuses on the background? [Ishan] So it actually focuses a lot on the foreground.

0:34:59.700,0:35:04.080
So that's what I was trying to sort of come up with this talk about in this

0:35:04.080,0:35:07.860
example. If you look at the nearest neighbor, it is really looking

0:35:07.860,0:35:11.310
at the person to come up with this, right? It's looking at the upside-down person

0:35:11.310,0:35:16.710
to sort of come up with its nearest neighbor. And the reason is, if you

0:35:16.710,0:35:20.970
want to talk about ordering of frames, actually need to focus on things that

0:35:20.970,0:35:25.260
move, and in these videos people are the things that move. So if it focuses on the

0:35:25.260,0:35:30.270
background it actually will not be able to solve the shuffle and learn tasks.

0:35:30.270,0:35:35.750
All right, so this was sort of surprising

0:35:35.750,0:35:41.060
and it goes to show that if you design your pretext task well it will

0:35:41.060,0:35:47.270
work well for a certain set of downstream tasks. And there have been

0:35:47.270,0:35:52.670
fairly nice methods since then, which have been basically about

0:35:52.670,0:35:57.590
using sequentiality and predicting

0:35:57.590,0:36:01.160
whether things are in the correct order or not, so this is odd-one-out networks

0:36:01.160,0:36:04.940
which basically rather than solving a binary classification problem, it

0:36:04.940,0:36:10.970
actually tries to predict which of the frames is the unlike one or the one

0:36:10.970,0:36:16.369
odd one out, the one that is shuffled. And this, because you're

0:36:16.369,0:36:19.130
increasing the amount of information that you're predicting at

0:36:19.130,0:36:23.869
the output, this sort of network ends up doing better and better. And it also 

0:36:23.869,0:36:27.550
reasons about more frames at a time. So now you've seen images and

0:36:31.060,0:36:36.460
video, there's been a lot of creative work at the multimodal, so where

0:36:36.460,0:36:42.220
you have to two modalities: Video and sound, or two sensory inputs, and these two

0:36:42.220,0:36:47.680
have been very popular and fairly nice work has been coming out of this

0:36:47.680,0:36:54.190
regime. So the key signaling in these works is predicting whether an

0:36:54.190,0:37:00.490
image or, say a video clip, corresponds to an audio clip. So the way you can

0:37:00.490,0:37:06.700
construct these tasks is to take a video and you can basically just sample any

0:37:06.700,0:37:11.800
frame from it, and similarly, take an audio track and sample any part of that.

0:37:11.800,0:37:15.040
And now the problem is basically to predict whether these things are

0:37:15.040,0:37:23.520
corresponding or not. So essentially, given this entire video of a drum,

0:37:23.520,0:37:27.910
you can sample the frame and the corresponding audio and call that the

0:37:27.910,0:37:33.550
positive. And in this case you basically take a different video, and you take the

0:37:33.550,0:37:37.210
audio from the drum video and that becomes your negative. And so again, you

0:37:37.210,0:37:40.150
can solve a binary classification problem by taking these bunch of

0:37:40.150,0:37:43.200
positives and negatives. So the architecture for this is fairly

0:37:46.920,0:37:49.980
straightforward. You take in an image, you pass it through

0:37:49.980,0:37:54.570
a vision subnetwork. Now you have your audio, you pass it through an audio 

0:37:54.570,0:38:00.990
subnetwork and get 128 dimensional features for them, so also embeddings. Then you

0:38:00.990,0:38:05.610
fuse them together and have a binary classification problem, saying

0:38:05.610,0:38:09.360
whether these things correspond or not. So at the end of it is just solving a

0:38:09.360,0:38:18.420
single binary problem. What it shows is that you can actually do a

0:38:18.420,0:38:22.770
bunch of nice things when you train networks this way. So you can

0:38:22.770,0:38:26.610
answer the question what is making a sound? because the network really needs

0:38:26.610,0:38:31.050
to focus on, say, to predict whether the sound is coming from this video. It also

0:38:31.050,0:38:36.330
needs to identify what in the video might be making the sound, so if it's the

0:38:36.330,0:38:39.810
sound of a guitar, it needs to  understand what a guitars roughly

0:38:39.810,0:38:43.650
looks like. Or if it's a drum it needs to roughly identify what a

0:38:43.650,0:38:51.680
drum is. So in this particular case, the author looked at visualizations

0:38:51.680,0:38:57.960
for, in this case, two instruments: So you have a piano and a flute, and you look at

0:38:57.960,0:39:03.840
just the radio information and nothing else, the network already 

0:39:03.840,0:39:10.170
puts a very high visual importance on the piano and on the flute.

0:39:10.170,0:39:14.400
And this is because when you feed-forward this image, it knows

0:39:14.400,0:39:17.720
that there are going to be these two kinds of things that can produce sounds.

0:39:17.720,0:39:24.650
So it really learns to identify these kinds of objects automatically.

0:39:25.360,0:39:30.710
[Alfredo] Do you know about the the slide before, whenever you had the convolutional net

0:39:30.710,0:39:35.420
over the spectrogram, do you know what is the kernel size for that audio

0:39:35.420,0:39:39.220
ConvNet? I'm interested to know whether it makes sense to have like

0:39:39.220,0:39:47.810
rectangular or a square kernel size?  [Ishan] So these are square kernels, now there are modern 

0:39:47.810,0:39:52.190
proof models so this is basically operating on the log spectrogram, so it still

0:39:52.190,0:39:55.990
needs handcraft, you need to decide how your computing that spectrogram exactly,

0:39:55.990,0:40:00.050
people have now figured out that you can actually use the raw audio and you can

0:40:00.050,0:40:04.850
actually apply convolutions directly on the audio signal. [Alfredo] Yeah yeah, sure sure. [Ishan] And for

0:40:04.850,0:40:08.540
that it's generally a small window, it really depends on what the corresponding

0:40:08.540,0:40:13.220
video that you're using, so roughly about a second worth of audio in a second

0:40:13.220,0:40:23.840
worth of video. So now that I've shown you how there are multiple

0:40:23.840,0:40:30.410
different creative ways of defining what a pretext task is, let's try to see what a

0:40:30.410,0:40:35.270
pretext task learns and how can you,  if I give you 25 different

0:40:35.270,0:40:40.430
pretext tasks, how can you a priori decide which one is the one that you want to

0:40:40.430,0:40:47.930
use and what are they going to learn. So the first thing is: pretext tasks

0:40:47.930,0:40:52.610
are actually complementary. So there was this really nice paper in 2017 that

0:40:52.610,0:40:58.280
looked at two of these tasks, so relative position was the first pretext

0:40:58.280,0:41:01.760
asked I talked about, where you take two patches and you try to predict what

0:41:01.760,0:41:05.660
their relative position with respect to one another is, and colorization is

0:41:05.660,0:41:10.100
basically taking a grayscale image and trying to predict its colors. And so what

0:41:10.100,0:41:14.810
these authors showed is basically that if you train a single network to do both

0:41:14.810,0:41:18.920
of these tasks, to predict both the colorize colorize output as well as

0:41:18.920,0:41:23.210
relative position, you can actually get gains in performance. So again, this is

0:41:23.210,0:41:26.000
evaluated the same way I was talking about earlier. You have a pretrained

0:41:26.000,0:41:29.120
network, then you're basically evaluating it on

0:41:29.120,0:41:34.210
an n task, in this case ImageNet classification and detection benchmark.

0:41:34.210,0:41:39.980
And in both cases you can get gains by performing both of these tasks,

0:41:39.980,0:41:44.840
so you get best of both words. So in some way what this also shows you is that a

0:41:44.840,0:41:49.220
single pretext task may not be the right answer, so predicting just coloured or

0:41:49.220,0:41:53.750
predicting just relative position may not be the right answer to learn

0:41:53.750,0:41:59.090
self-supervised representations. In fact, if you reason about what

0:41:59.090,0:42:04.790
information is being predicted, it really varies a lot across tasks. So starting

0:42:04.790,0:42:08.030
with the relative position task, you're predicting a fairly low level

0:42:08.030,0:42:12.980
information, you're predicting just  eight possible locations. So just a

0:42:12.980,0:42:16.580
to a classification problem. Or for that shuffle and learn problem you're

0:42:16.580,0:42:19.580
predicting whether things are shuffled or not. So it's just a simple binary

0:42:19.580,0:42:23.560
problem. So it's less amount of information that's being predicted

0:42:23.560,0:42:28.310
whereas if you look at on the extreme right, if you are trying to predict what

0:42:28.310,0:42:31.580
is missing in an image and you're trying to reconstruct the pixels you're

0:42:31.580,0:42:35.870
predicting a lot of information because that entire box contains, I mean

0:42:35.870,0:42:40.070
it can have a very  different appearance space, right? So if

0:42:40.070,0:42:44.180
you have, in pixels, then you can basically have a lot of different values

0:42:44.180,0:42:47.450
for that entire predictive region, so you're predicting a lot of information

0:42:47.450,0:42:52.070
there. So essentially this is one simple way of thinking about pretext tasks, how

0:42:52.070,0:42:56.830
much information are you predicting, and that can give you already a good idea of

0:42:56.830,0:43:01.070
whether you're actually predicting a lot of information, so

0:43:01.070,0:43:08.300
probably that representation is actually going to be better. So, in general this is

0:43:08.300,0:43:13.670
going to guide the next part of my talk, you can think of this 

0:43:13.670,0:43:17.270
predicting more information part as on an axis, and I'll talk about three

0:43:17.270,0:43:21.470
different sort of categories and this, actually two different categories. So

0:43:21.470,0:43:24.950
pretext tasks is what I've been talking about till now, which is just predicting

0:43:24.950,0:43:30.260
simple classification problems like different degrees of rotation or so on. I

0:43:30.260,0:43:34.970
moved to contrastive methods which actually predict say

0:43:34.970,0:43:39.980
more information than these pretext tasks and in this particular talk I'm

0:43:39.980,0:43:43.130
actually not going to talk about generative models but generative models

0:43:43.130,0:43:48.559
predict more information than, say, a typical contrastive method and so this

0:43:48.559,0:43:53.480
is basically one way of thinking about these classes of methods. [Alfredo] Question: How do

0:43:53.480,0:43:59.510
we train multiple pre-training tasks? do we shuffle data for both tasks?

0:43:59.510,0:44:06.200
if training dividually, would it lead to catastrophic forgetting? [Ishan] So, the

0:44:06.200,0:44:10.400
simple way of doing that is  that so you can 

0:44:10.400,0:44:14.660
alternate batches, so you can have the same network and in one batch you

0:44:14.660,0:44:17.990
basically feed it black-and-white images and you ask it to predict the colored

0:44:17.990,0:44:23.390
part of it, and in the second batch you basically feed it

0:44:23.390,0:44:26.480
patches and you ask it to do the relative position tasks, you basically

0:44:26.480,0:44:33.829
have two different, like head, like fully connected layers at the top. So you can

0:44:33.829,0:44:38.150
basically alternate between these tasks. What the authors of the paper did was

0:44:38.150,0:44:44.619
actually slightly more sophisticated, they basically had 

0:44:44.619,0:44:50.569
multi-task network which was three or four depending on the number of pretext

0:44:50.569,0:44:54.470
tasks you have and you actually solve all of them at once but they there was

0:44:54.470,0:44:58.819
more weight sharing involved across these three or four different

0:44:58.819,0:45:10.910
networks. [Student] Hi, so about the pretext tasks, what performance should we aim

0:45:10.910,0:45:15.349
for in a pretext task? when do we know that this is enough or when can we stop?

0:45:15.349,0:45:19.940
and because ultimately we care about the performance on the downstream, that's

0:45:19.940,0:45:23.930
question one. And question two is you were speaking about low information and

0:45:23.930,0:45:29.299
more information, for example in the case where you mentioned, where you were

0:45:29.299,0:45:32.780
predicting whether it's in the correct sequence or not, you could have also

0:45:32.780,0:45:38.510
predicted the actual permutation of the images, right? So how do you decide

0:45:38.510,0:45:45.470
between which task to follow and based on what? [Ishan] So both parts, the second part of

0:45:45.470,0:45:49.700
the question actually that's going to be in a couple of slides, so I'll defer

0:45:49.700,0:45:53.390
to that question or that one later, but the first part How much do you train

0:45:53.390,0:45:58.200
this model on a pretext task? So, a good sign of a

0:45:58.200,0:46:02.970
pretext task is that as your accuracy on the pretext task improves, so as you

0:46:02.970,0:46:05.970
get better at predicting whether things are shuffled or not or as you get better

0:46:05.970,0:46:10.830
at predicting rotations, the  accuracy on the downstream semantic

0:46:10.830,0:46:15.870
tasks will also improve. So a good rule of thumb for using these

0:46:15.870,0:46:21.570
pretext tasks is, we have a very difficult pretext task or try to make it

0:46:21.570,0:46:26.340
as difficult as possible and then  optimize or reduce the loss on

0:46:26.340,0:46:31.260
that pretext task so that your final, downstream accuracy improves, so it's

0:46:31.260,0:46:34.580
very correlated. [Student] Right, so in practice you'll actually

0:46:36.760,0:46:41.079
train the entire pipeline, each line each time, like the pretext and the downstream

0:46:41.079,0:46:44.740
and measure the performance, right? so it's not like you stop the pretext at a

0:46:44.740,0:46:47.950
certain point and then switch over to only like downstream or something? [Ishan] So

0:46:47.950,0:46:51.550
that's generally how these methods are evaluated but I guess when you're

0:46:51.550,0:46:55.690
developing, you would probably do this pipeline multiple times.

0:46:55.690,0:47:00.579
So these methods are trained like you do you're pretext task then you

0:47:00.579,0:47:04.329
stop and then you perform your downstream evaluation task and that gives

0:47:04.329,0:47:08.289
you the final measurement of how good your pretext task was, and that's

0:47:08.289,0:47:15.210
it, you do do this entire thing once. [Student] Right, thank you.

0:47:16.339,0:47:19.849
[Ishan] And about the the second part of your question, the more information part, I'll come

0:47:19.849,0:47:24.609
to it later... the permutation and so on. Good, so these are sort of the three main

0:47:27.770,0:47:34.520
buckets and the first two are going to be covered now. So

0:47:34.520,0:47:39.170
this was another work we did which was basically about scaling self-supervised

0:47:39.170,0:47:45.140
learning. So in this particular work we focused on two problems: One was the

0:47:45.140,0:47:49.930
colorization problem that I had talked about earlier, and the second is this

0:47:49.930,0:47:56.180
more information variant of the relative position task. So

0:47:56.180,0:48:00.770
this task is called Jigsaw puzzles. The idea is that you take an image and you

0:48:00.770,0:48:05.299
split it into multiple different patches and you try to predict exactly and even

0:48:05.299,0:48:09.049
if shuffled these patches by a permutation, and then you predict which

0:48:09.049,0:48:13.309
permutation was applied to the input, so that's very similar to what the student was

0:48:13.309,0:48:16.480
suggesting earlier. Alright, so, the way you solve this

0:48:19.880,0:48:24.799
problem is you take, say, in this case three patches. You feed forward each one

0:48:24.799,0:48:28.609
of these patches independently. You concatenate their features. And then, you

0:48:28.609,0:48:34.039
classify which permutation was  used to permute these input patches. Now,

0:48:34.039,0:48:38.900
the authors used nine patches to solve this problem and that's basically going

0:48:38.900,0:48:44.650
to be nine factorial, which is like 360 thousand number of permutations. Of

0:48:44.650,0:48:48.799
course, when you're trying to perform this classification at the end, this

0:48:48.799,0:48:52.730
means that your fully-connected layer should have 360 thousand output neurons,

0:48:52.730,0:48:57.319
which is a fairly large number. So, in practice, what the authors did was

0:48:57.319,0:49:03.950
have a subset of permutations that they use, so say they

0:49:03.950,0:49:08.030
sample one hundred permutations from the nine factorial permutations and then

0:49:08.030,0:49:13.579
just have this perform this hundred weight classification, right? So in some

0:49:13.579,0:49:18.380
way, you can look at this the size of this subset as the problem complexity or

0:49:18.380,0:49:21.770
the amount of information that you're predicting. If you predict the full nine

0:49:21.770,0:49:25.670
factorial thing, you're actually predicting a lot of

0:49:25.670,0:49:29.539
information at the output. If you only subsample, say, two or three permutations,

0:49:29.539,0:49:32.510
then you're basically not predicting a lot of information. So the problem

0:49:32.510,0:49:38.750
basically gets harder and harder as the size of the subset increases. So in this

0:49:38.750,0:49:43.819
paper we basically wanted to study the entire role of how much information

0:49:43.819,0:49:48.170
that you predict and how good is the final representation that you learn. So

0:49:48.170,0:49:53.329
in terms of evaluation, there are two ways to sort of evaluate once you have a

0:49:53.329,0:49:57.589
self-supervised pre-trained network. And there is a lot of debate on which one is

0:49:57.589,0:50:02.750
exactly the right method to evaluate networks. So the first way is to

0:50:02.750,0:50:06.710
basically fine-tune all the layers of a network: So you have a downstream

0:50:06.710,0:50:11.510
task, a pose estimation, or say image classification... you train this network

0:50:11.510,0:50:15.069
and you update all the parameters of this network for the downstream task.

0:50:15.069,0:50:20.329
The second way is to just use your network as a feature extractor. So you

0:50:20.329,0:50:23.930
basically run your images through it, you get your feature representation and now

0:50:23.930,0:50:27.579
you only train a linear classifier on top of that fixed feature representation.

0:50:27.579,0:50:32.120
So in this particular work we said that a good representation

0:50:32.120,0:50:36.590
should transfer with little amount of training. So we opted basically for the

0:50:36.590,0:50:41.090
second part, which is just to train a linear classifier on top of a network

0:50:41.090,0:50:45.200
treated as a feature extractor. So there are of course different pros and cons of

0:50:45.200,0:50:50.620
using both methods. So the first method, that is fine tuning all the layers, is

0:50:50.620,0:50:55.580
treating the self-supervised network as an initialization because you're

0:50:55.580,0:50:59.690
basically updating the entire network. So if your downstream tasks has,

0:50:59.690,0:51:03.710
say, 1 million images, your weights are updating your entire network for that 1 million

0:51:03.710,0:51:08.090
images; whereas in the second case, you're just training very limited number of

0:51:08.090,0:51:12.320
parameters on the fixed-feature extractor. So in some way, basically the

0:51:12.320,0:51:18.500
second one is measuring how good of a feature is that that you've done.

0:51:18.500,0:51:24.030
All right, so, the other thing that is sort of critical in evaluating 

0:51:24.030,0:51:28.860
self-supervised methods is to evaluate them on a bunch of different tasks. So earlier,

0:51:28.860,0:51:32.100
when I talked about that shuffle and learn work, I just showed you results in

0:51:32.100,0:51:36.900
pose estimation. So on pose estimation it was doing really well but it actually

0:51:36.900,0:51:40.920
did not do really well on other tasks like say action recognition. So in this

0:51:40.920,0:51:44.910
particular evaluation, we wanted to correct that mistake and we wanted to

0:51:44.910,0:51:49.260
focus on multiple different tasks, so a variety of different tasks like:

0:51:49.260,0:51:53.790
Image classification, few-short learning, object detection, 3d understanding,

0:51:53.790,0:51:58.080
navigation and so on. Ao we defined a set of nine different

0:51:58.080,0:52:05.580
tasks. So the way to evaluate the representations is basically to extract

0:52:05.580,0:52:09.210
fixed features and you can extract these fixed features from different parts of

0:52:09.210,0:52:12.810
the network, so they can come from a layer which is very close to the

0:52:12.810,0:52:16.770
input or from a very high-level layer which is very close to the output. So in

0:52:16.770,0:52:19.590
this way you are measuring the semanticness of each of these different

0:52:19.590,0:52:21.920
layers. And the standard thing we did

0:52:25.109,0:52:29.430
floor a lot of these experiments was to use an image classification task to 

0:52:29.430,0:52:34.769
understand what is going on. So the image classification task is on this

0:52:34.769,0:52:37.980
data set called the VOC which is fairly standard for detection and

0:52:37.980,0:52:44.099
classification, and the idea is to predict whether an image has one of

0:52:44.099,0:52:49.650
twenty classes. So an image can actually have more than one class, for

0:52:49.650,0:52:54.480
example, like that picture of a person with a dog that has both person and dog,

0:52:54.480,0:52:57.809
so this network now needs to recognize both the objects in it, so it's 

0:52:57.809,0:53:02.759
slightly harder than ImageNet, where you need to only certified one

0:53:02.759,0:53:07.039
of the key objects in the image. So the first thing we did was

0:53:10.390,0:53:13.510
to verify the hypothesis: Whether increasing the amount of

0:53:13.510,0:53:18.670
information predicted actually results in better representations. So in the

0:53:18.670,0:53:23.440
x-axis, we are increasing the number of permutations that we are using to

0:53:23.440,0:53:28.089
train our network, so that's going from 100 to 10,000. And on the

0:53:28.089,0:53:31.630
y-axis we are measuring the downstream transferred performance of

0:53:31.630,0:53:35.769
these pre-trained representations, and it's measured using a metric called mAP,

0:53:35.769,0:53:41.339
which is mean average precision. So essentially, because this is a 

0:53:41.339,0:53:45.339
multi-label classification problem, you're going to

0:53:45.339,0:53:48.849
measure average precision for each of the different 20 classes and then you're

0:53:48.849,0:53:52.480
going to compute the mean of that of precision, so higher is better in

0:53:52.480,0:53:57.250
this case. So we do that for two different architectures AlexNet, which

0:53:57.250,0:54:02.289
was originally used in the Jigsaw paper and then the ResNet-50. And what you

0:54:02.289,0:54:07.869
observe is for AlexNet, increasing the amount of permutations is useful up to a

0:54:07.869,0:54:12.609
certain point but the gain is overall limited; whereas for ResNet, if you

0:54:12.609,0:54:16.690
increase the amount of permutations, the representation quality gets better and

0:54:16.690,0:54:22.240
better. And our hypothesis was basically that the ResNet model has enough

0:54:22.240,0:54:26.170
capacity that it can actually solve a very difficult permutation problem, and

0:54:26.170,0:54:30.279
when it solves a difficult permutation problem it's able to learn much

0:54:30.279,0:54:38.740
better representations that generalize to different downstream tasks. So, the

0:54:38.740,0:54:42.910
next thing we did was to evaluate our method on the object detection task. So

0:54:42.910,0:54:47.349
object detection is where you try to identify what objects are present

0:54:47.349,0:54:51.910
in an image, we try to draw a box around them and your measure is based

0:54:51.910,0:54:56.170
on how good the boxes are placed around the object and whether you were able to

0:54:56.170,0:55:01.000
identify all the objects in an image. And again for this one we use the same VOC

0:55:01.000,0:55:05.490
dataset. So this was the setting where we

0:55:05.490,0:55:09.900
fine-tuned all the layers of a network because that's what is standard in

0:55:09.900,0:55:15.930
detection. And what we observed was, on two different splits of

0:55:15.930,0:55:21.200
this VOC data set, the Jigsaw method was actually fairly comparable

0:55:21.200,0:55:27.060
within the margin of error to  training an ImageNet supervised method. So

0:55:27.060,0:55:30.600
you have a ImageNet supervised Network, you fine-tune that on the task of

0:55:30.600,0:55:35.369
detection and you get a mean average precision of 70.5 or 

0:55:35.369,0:55:39.240
76.2 two, and the Jigsaw method is basically within the margin of error of

0:55:39.240,0:55:45.660
these methods, which in itself  shows that it actually had some 

0:55:45.660,0:55:53.880
nice semantic property and it was able to localize objects fairly well. And to put

0:55:53.880,0:55:59.340
this in context, for semantic feature learning, in computer

0:55:59.340,0:56:03.480
vision especially, object detection is considered the benchmark dataset

0:56:03.480,0:56:08.580
to reach something like really belong and this result basically when

0:56:08.580,0:56:12.030
we published it, was the closest anyone had ever come to

0:56:12.030,0:56:17.150
supervised pre-training in terms of detection. 

0:56:17.490,0:56:24.890
Right. Is there a question? [Student] Is pretext tasks

0:56:24.890,0:56:28.500
similar to what we could try achieving with transfer learning or is it like a

0:56:28.500,0:56:33.570
subset of that? [Ishan] Yes, so the way you evaluate these pretext tasks is by

0:56:33.570,0:56:37.260
transfer learning, so you perform your original pretext tasks and then you 

0:56:37.260,0:56:41.790
fine-tune it on a data set for a particular task, like detection, so the evaluation is

0:56:41.790,0:56:44.810
always transfer learning. So the next task we looked at was

0:56:51.190,0:56:56.350
surface normal evaluation. So this is basically, given input you try to

0:56:56.350,0:57:01.330
estimate what are the 3D properties  at each pixel

0:57:01.330,0:57:06.730
location in the input. You try to predict what is the surface orientation, so in 3D

0:57:06.730,0:57:13.060
basically the x, y & z vectors at each particular surface, so it's a sort of

0:57:13.060,0:57:16.690
dense prediction problem where you need to assign that xyz vector to each

0:57:16.690,0:57:22.109
location in the input, and for that to use this nice dataset created by NYU.

0:57:22.109,0:57:29.050
We measured the prediction properties of our method and

0:57:29.050,0:57:33.940
compare it to an ImageNet supervised method. And so in this case we measured

0:57:33.940,0:57:38.770
the median error and the percentage correct predictions. So the median error

0:57:38.770,0:57:42.070
means that lower is better and percentage correct means higher is

0:57:42.070,0:57:47.230
better. So it turned out that the Jigsaw pre-training task was actually really

0:57:47.230,0:57:51.670
good in this case and it provided significant improvements over 

0:57:51.670,0:57:56.080
ImageNet pre-training, so it was basically across some multiple different sets,

0:57:56.080,0:58:00.160
multiple different splits, it was able to really easily outperform the ImageNet

0:58:00.160,0:58:05.740
supervised pre-trained method. So, again, it goes on to show that evaluating

0:58:05.740,0:58:09.850
a pretext task on multiple different tasks and multiple different data sets

0:58:09.850,0:58:14.770
is really important to understand what is really going on in a pretext task. So,

0:58:14.770,0:58:20.470
somehow, Jigsaw is really incorporating something about like geometry and

0:58:20.470,0:58:23.830
something pixel-level information much better than ImageNet

0:58:23.830,0:58:26.640
supervised methods. So finally, we found the Achilles

0:58:30.690,0:58:35.610
heel of this method, like the Jigsaw pre-training tasks. So, to do this we

0:58:35.610,0:58:40.170
evaluated on like the setting called  few shot learning. So, in few shot learning

0:58:40.170,0:58:45.060
you have very limited number of training examples and you are training

0:58:45.060,0:58:49.830
your classifier just on these very limited number of training examples. So

0:58:49.830,0:58:54.030
on the x-axis I have the number of training examples that were used to

0:58:54.030,0:58:59.670
train a method, so that goes from 1 to 96. And I'm showing you curves

0:58:59.670,0:59:03.660
for 2 different self-supervised methods, Jigsaw

0:59:03.660,0:59:07.740
methods trained on two different datasets, ImageNet which is on the top and a

0:59:07.740,0:59:13.110
random has ResNet-50. So what you can observe is that there is a significant

0:59:13.110,0:59:16.380
gap in performance between a  self-supervised method and a supervised

0:59:16.380,0:59:21.240
method and that gap just does not seem to reduce as you increase the

0:59:21.240,0:59:25.890
number of labeled examples, which  sort of shows that self-supervised

0:59:25.890,0:59:29.700
representations, although they may be good at tasks like say pose estimation

0:59:29.700,0:59:35.040
or particular tasks like surface normal estimation, there is still a lot of

0:59:35.040,0:59:38.850
difference between what sort of semantic aspect of the data they capture, because

0:59:38.850,0:59:43.890
this in this few shot learning tasks, if I give you one image and if

0:59:43.890,0:59:47.250
you're able to say something about it your feature representation needs to be

0:59:47.250,0:59:50.750
really good to solve that task. the other way evaluated this method was

0:59:55.730,1:00:00.349
to basically look at what it learns at each different layer. So we basically

1:00:00.349,1:00:06.770
trained linear classifiers on different clear representations in

1:00:06.770,1:00:11.240
a ResNet-50, so from the conv1 which is going to be the layer closest to the

1:00:11.240,1:00:15.500
input, to the output, say, of the res2 block,  the res3 block and the res5 block

1:00:15.500,1:00:19.010
so the res5 is basically the highest level representation

1:00:19.010,1:00:23.510
that you get out from a ResNet-50, and after that representation is where you

1:00:23.510,1:00:28.040
perform this entire Jigsaw, like predicting the permutation task. And so,

1:00:28.040,1:00:32.780
you look at in this case the x-axis represents where the feature

1:00:32.780,1:00:38.000
is coming from conv1 or res5, and on the y-axis we are looking at the, again, mean

1:00:38.000,1:00:44.000
average precision of image classification on VOC. And funnily enough, what you see

1:00:44.000,1:00:48.859
is basically that the representation quality improves when you go from conv1

1:00:48.859,1:00:53.619
to res4, so it steadily  increases in the mean average precision,

1:00:53.619,1:00:59.930
but towards the end there's a sharp drop, so for res4 to res5 there is a sharp drop in

1:00:59.930,1:01:04.339
performance which... [Alfredo] Is that due to the fact that it specializes to the specific task?

1:01:04.339,1:01:10.490
[Ishan] Yes, exactly. So this was very worrying because if you were to plot this

1:01:10.490,1:01:15.920
thing for a supervised network, you observe that from conv1 to res5 the

1:01:15.920,1:01:19.940
representation quality always improves. And this is true for like pretty much

1:01:19.940,1:01:24.650
any good supervised network, whereas for a lot of the self-supervised networks, we

1:01:24.650,1:01:28.910
repeated this experiment for the Rotation network, for colorization, for

1:01:28.910,1:01:33.650
relative position... we would always observe this very sharp gap from

1:01:33.650,1:01:40.220
res4 to res5. And so, this says that the end task that we are solving, the

1:01:40.220,1:01:45.680
pretext task, is probably not very nice because it's not very well aligned to

1:01:45.680,1:01:50.860
the downstream semantic tasks that we really want to solve,

1:01:50.860,1:01:55.780
which basically brings me to the next part which is to understand what is

1:01:55.780,1:02:01.200
missing from these pretext or these proxy tasks.

1:02:01.470,1:02:07.080
So, recap: Pretext tasks are basically something like predicting rotation or to

1:02:07.080,1:02:12.720
predict, say, Jigsaw puzzles and  if you look at it in the bigger

1:02:12.720,1:02:16.890
picture of things, they're very surprising, and the fact that they even

1:02:16.890,1:02:22.080
work is super surprising. So for pretext tasks we have this

1:02:22.080,1:02:26.010
pre-training step where which is self-supervised and then we have our transfer

1:02:26.010,1:02:30.900
tasks which are imaged classification or detection. And it's really a lot of

1:02:30.900,1:02:34.980
wishful thinking and a lot of hope that the pre-training task and the transfer

1:02:34.980,1:02:40.080
task are super aligned, and there is no evidence really, it's a lot of just

1:02:40.080,1:02:43.770
wishing really really hard that whatever pretext task we've come up with is

1:02:43.770,1:02:47.580
really well aligned with our transfer task and solving that pretext task will do

1:02:47.580,1:02:51.840
really well in transfer tasks. So a lot of research basically goes into designing

1:02:51.840,1:02:58.800
these pretext tasks and implementing them really well. But it's not clear why

1:02:58.800,1:03:03.119
solving something like Jigsaw puzzles should teach us anything about semantics

1:03:03.119,1:03:08.640
or, for example, even the case of say, weak-supervised learning, where you are trying to

1:03:08.640,1:03:13.050
predict hashtags from an image. It's not clear why predicting hashtags of an image

1:03:13.050,1:03:18.210
is actually going to do something well for learning a good classifier

1:03:18.210,1:03:25.140
on transfer tasks. So this question remains that, how do you design good 

1:03:25.140,1:03:32.160
pre-training tasks which are well aligned with your transfer tasks? So this hope of

1:03:32.160,1:03:37.770
generalization, and the way we can evaluate this is

1:03:37.770,1:03:41.280
basically by looking at the representation that each layer and if

1:03:41.280,1:03:45.089
the last layer we do not see representations that are well aligned

1:03:45.089,1:03:49.650
with the transfer task, then that is a red flag in that  telling us that

1:03:49.650,1:03:54.040
maybe this pre-training task is not really the right task to solve.

1:03:54.040,1:03:58.330
So like I mentioned earlier, this basically is the pattern that we

1:03:58.330,1:04:03.370
get for Jigsaw and this shows us that probably the last layers are very much

1:04:03.370,1:04:07.410
specialized towards that Jigsaw problem. Right, so, in general what we really want

1:04:11.339,1:04:15.750
from pre-trained features is that they should represent how images are related

1:04:15.750,1:04:20.940
to one another. So, feature representation should, and this basically goes back to

1:04:20.940,1:04:24.569
say the nearest neighbor visualizations that I had, it should really be able to

1:04:24.569,1:04:30.059
group together images that are semantically related in some way. And the

1:04:30.059,1:04:35.460
second property is a property that has been the backbone of designing

1:04:35.460,1:04:39.809
vision features, so even before the deep learning features were popular, the

1:04:39.809,1:04:43.500
handcrafted features were always all about invariance, about being

1:04:43.500,1:04:47.520
invariant to things like lighting or things like exact color or exact

1:04:47.520,1:04:51.690
location. So these are the two properties that we really want in our pre-trained 

1:04:51.690,1:04:58.530
features. And there are two ways of achieving these things: one is

1:04:58.530,1:05:03.720
clustering and the other is contrastive learning, and both these methods have

1:05:03.720,1:05:09.410
promise because they are  trying to get these

1:05:09.410,1:05:13.859
properties when they're trying to learn representations. And I believe

1:05:13.859,1:05:17.970
that's why they've actually now started performing so much better than whatever

1:05:17.970,1:05:24.779
pretext tasks that were hand designed so far. So now, I focus on two

1:05:24.779,1:05:29.130
recent works that we have  which fall into this pocket of

1:05:29.130,1:05:33.359
clustering and invariances, so one is called ClusterFit the other is called

1:05:33.359,1:05:40.079
PIRL and both of them will be presented at CVPR this year. So the first work is

1:05:40.079,1:05:45.869
ClusterFit, it's a method which we think is very good to improve generalization

1:05:45.869,1:05:52.380
of visual representations. So  clustering is basically a good way to

1:05:52.380,1:05:56.609
understand what images are grouped together, what images go together and

1:05:56.609,1:06:01.200
whatever it is do not go together, and you are basically

1:06:01.200,1:06:05.160
performing clustering on the feature space, you can get these nice buckets of

1:06:05.160,1:06:11.579
images that are related and images that are not related. So the main idea of this

1:06:11.579,1:06:17.069
paper is extremely simple. There are just two steps: One is the cluster of step, the

1:06:17.069,1:06:21.000
other is the predict step. So what we do is we take any 

1:06:21.000,1:06:25.800
pre-trained network, and this can be any pre-trained networth, it it does not really have to

1:06:25.800,1:06:30.270
be just a self-supervised network, it can either be a ImageNet pre-trained network or a

1:06:30.270,1:06:34.440
network pre-trained say using hashtags or a self-supervised network like the one

1:06:34.440,1:06:38.970
trained to predict Jigsaw permutations. And you take this pre-trained network

1:06:38.970,1:06:45.060
and you extract a bunch of features from it, on a set of images, and of course

1:06:45.060,1:06:49.440
these images have no labels. You extract these features when you perform k-means

1:06:49.440,1:06:55.260
clustering and what you now get is basically, for each image

1:06:55.260,1:07:00.690
you know which cluster it belongs to and that becomes its label. So in the second

1:07:00.690,1:07:06.210
Fit step, what you do is you train a network from scratch, so from random

1:07:06.210,1:07:12.660
bits, and you train this network to predict just these pseudo labels. So they're

1:07:12.660,1:07:15.480
pseudo because they were basically obtained using clustering, so they're not

1:07:15.480,1:07:19.290
really hard labels which were given by, say, a human annotator.

1:07:19.290,1:07:23.550
And so now this second network is just trying to predict these cluster

1:07:23.550,1:07:27.960
assignments. So it takes our image and it tries to predict which one of the 

1:07:27.960,1:07:34.070
k-clusters that you got from your k-means does this image belong to. So a standard

1:07:34.070,1:07:38.160
pre-trained and transfer task is to basically perform your pre-training, so

1:07:38.160,1:07:42.000
that's the top row, is to perform your pre-training on an objective like

1:07:42.000,1:07:46.830
predicting hashtags or predicting GPS locations, and then to evaluate this

1:07:46.830,1:07:54.150
feature by learning, say, linear prop (?). In the ClusterFit world, we basically do

1:07:54.150,1:07:57.260
not touch the pre-training so you perform your free training as you would,

1:07:57.260,1:08:03.000
you just insert a step in between which is the ClusterFit step where you take a

1:08:03.000,1:08:07.230
dataset D and you take your pre-trained network and you learn a new network from

1:08:07.230,1:08:11.910
scratch on this data. And finally you basically use this like green network

1:08:11.910,1:08:20.220
for all your downstream tasks. So the reason we believe that this method works

1:08:20.220,1:08:24.900
is because the clustering step, and then you are sort of clustering just these

1:08:24.900,1:08:29.940
images, you are only capturing the essential information which is basically

1:08:29.940,1:08:34.469
what images go together and what images do not go together, so you are throwing

1:08:34.469,1:08:38.369
away all the other information that is present in the original network but just

1:08:38.369,1:08:43.440
capturing the inter-image relationships that were

1:08:43.440,1:08:50.130
modeled by the initial network. And to understand this, we

1:08:50.130,1:08:54.719
performed a fairly simple experiment: We added label noise, so synthetic label

1:08:54.719,1:08:59.339
noise, to ImageNet, and we trained a network on this noisy ImageNet.

1:08:59.339,1:09:06.059
So just flip a bunch of image labels and train a network and now you evaluate

1:09:06.059,1:09:10.199
the feature representation from this network on a downstream task, which is

1:09:10.199,1:09:14.190
again ImageNet, but it's a much larger version of ImageNet, so its 9,000

1:09:14.190,1:09:21.599
way classification. So the x-axis have the amount of label

1:09:21.599,1:09:27.449
noise added to the images, so that's going from 0% to 75% and on the y-axis

1:09:27.449,1:09:31.650
we are looking at the transfer performance on the larger ImageNet, the

1:09:31.650,1:09:37.799
ImageNet 9,000 dataset. So the pink line is showing you the pre-trained

1:09:37.799,1:09:42.690
network which, as the amount of label noise increases the

1:09:42.690,1:09:46.769
pre-trained networks performance on the downstream tasks decreases. And well, this

1:09:46.769,1:09:51.329
is not surprising because as your labels become less and less reliable of course

1:09:51.329,1:09:55.679
your representation quality is going to suffer, so that sort of goes down very

1:09:55.679,1:10:02.610
quickly. In the blue line, we experimented with this technique called

1:10:02.610,1:10:07.019
model distillation, where you take your initial network and you use that to

1:10:07.019,1:10:11.940
generate labels, so you look at the output of that network and you look at

1:10:11.940,1:10:15.809
the sort of confidence in the outputs to generate labels for a second network and

1:10:15.809,1:10:20.309
that's called model distillation. So model distillation generally performs

1:10:20.309,1:10:24.150
better than the pre-trained network and you can see that all across so, as

1:10:24.150,1:10:27.989
the amount of label noise increases, the distillation model actually is much

1:10:27.989,1:10:33.389
better than the original model. And finally towards the end we have ClusterFit, so

1:10:33.389,1:10:36.510
that's the green line, and you can see that the classified model is

1:10:36.510,1:10:41.880
consistently better than any of these methods, either distillation or

1:10:41.880,1:10:47.099
pre-training. And consistently gives better results, including when you have

1:10:47.099,1:10:49.929
zero level noise, which is basically when you have a

1:10:49.929,1:11:00.850
pre-trained ImageNet network. So we applied... [Student] Question, could you elaborate 

1:11:00.850,1:11:08.920
on the difference between distillation and ClusterFit once more? [Ishan] Yes, so in 

1:11:08.920,1:11:13.510
distillation what you would do is, so in this first step

1:11:13.510,1:11:16.660
you would take the pre-trained network and you would use the labels this

1:11:16.660,1:11:23.260
network is predicting, so say the the network basically predicts 1,000 classes,

1:11:23.260,1:11:28.870
so you basically use those labels in a softer fashion to generate

1:11:28.870,1:11:34.960
labels for your images. So say the network was trained to predict 100

1:11:34.960,1:11:40.000
different types of dogs, so you take your images and you get a distribution over

1:11:40.000,1:11:43.780
100 different types of dogs and use that distribution to train your second

1:11:43.780,1:11:48.910
network; whereas in clusterFit you don't really care about the label space or the

1:11:48.910,1:11:52.989
output space of the pre-trained network, you only look at the

1:11:52.989,1:11:56.560
features, you don't even look at the last fully-connected layer, you just look at the

1:11:56.560,1:12:03.580
previous features. [student] Got it, also why would the softer distribution help

1:12:03.580,1:12:07.989
with training, like would training on this be helped, what's like the intuition

1:12:07.989,1:12:12.610
behind distillation? [Ishan] distillation's main intuition is basically that if your

1:12:12.610,1:12:17.699
network was trained really well, so suppose you had no label noise,

1:12:17.699,1:12:23.620
because a lot of  images really don't belong

1:12:23.620,1:12:28.360
in this sort of same classes, so suppose your data set actually had hundred two

1:12:28.360,1:12:32.440
hundred different types of dogs but you had a hundred of them labeled, and so

1:12:32.440,1:12:37.330
for a lot of these images, say,  if had to pick

1:12:37.330,1:12:41.350
which one of the dogs it was, a softer distribution is basically going

1:12:41.350,1:12:46.690
to help you discover hidden categories. So it's basically 0.5 this 

1:12:46.690,1:12:52.090
type of dog, and 0.5 this kind of dog. So basically having these 

1:12:52.090,1:12:57.850
softer labels helps you enhance  the initial class distribution that

1:12:57.850,1:13:00.120
you have. [Student] Okay, thank you. [Ishan] So we applied this method

1:13:10.449,1:13:15.699
to the self-supervised learning, so the Jigsaw task that I talked about

1:13:15.699,1:13:19.420
earlier, and we were able to see surprising amounts of gains across a

1:13:19.420,1:13:25.570
bunch of data sets. So the Jigsaw method is in the top row, which in each

1:13:25.570,1:13:29.949
of those columns you're looking at the transfer performance of

1:13:29.949,1:13:34.960
this Jigsaw method on a bunch of different data sets. If you apply clusterFit

1:13:34.960,1:13:40.329
to this Jigsaw method, you actually can see gains across all of these data

1:13:40.329,1:13:44.769
sets and they're fairly consistent. And we perform this test on a bunch of

1:13:44.769,1:13:49.659
different pre-training methods like  RotNet, so predicting rotations, and

1:13:49.659,1:13:54.389
again we could see fairly nice gains across these four different data sets.

1:13:54.389,1:13:59.829
And surprisingly enough, ClusterFit really works on any pre-trained network,

1:13:59.829,1:14:04.179
so it can be either a fully-supervised network or a weekly-supervised network,

1:14:04.179,1:14:10.150
so say a network that was trained to predict hashtags or a weekly-supervised

1:14:10.150,1:14:15.280
video network or basically any  seld-supervised network. And in each of these

1:14:15.280,1:14:19.780
cases we can observe fairly consistent and large gains when you have ClusterFit

1:14:19.780,1:14:23.590
so it's actually able to improve the generalization of most of

1:14:23.590,1:14:28.110
these methods. [Alfredo] I think you're dragging your microphone around it's very noisy.

1:14:28.110,1:14:41.079
[Ishan] So the second thing is basically these gains were possible without extra

1:14:41.079,1:14:44.559
data, labels or changes in the architecture. So in some ways you can

1:14:44.559,1:14:50.679
think of this as being self-supervised fine-tunning step: So you have your 

1:14:50.679,1:14:53.800
pre-trained network and then you basically perform this cluster step, which is

1:14:53.800,1:14:58.989
ClusterFit step which is completely self-supervised or unsupervised, and then you

1:14:58.989,1:15:07.750
can observe that the representation quality improves. [Student] I had a question, in the

1:15:07.750,1:15:12.050
slide that you showed the improvement with Jigsaw and

1:15:12.050,1:15:17.240
by using ClusterFit, so in this clusterFit, is it separated, right?

1:15:17.240,1:15:21.950
it is not using Jigsaw at all? [Ishan] So it is applied on top of the Jigsaw method,

1:15:21.950,1:15:27.710
right? So there was a pre-trained network from which you extract features, so

1:15:27.710,1:15:33.100
in this case that pre-trained network is the Jigsaw pre-trained network. [Student] Oh okay.

1:15:33.100,1:15:36.500
[Ishan] So you take the Jigsaw pre-trained network  and then you basically perform ClusterFit on top of it.

1:15:36.500,1:15:49.130
[Student] Okay, thank you. [Student] Is there a logical way to think that cluster fit is a good idea?

1:15:47.000,1:15:52.430
[Ishan] I think the main sort of intuition is that when you perform the Jigsaw task, the last layer becomes

1:15:52.430,1:15:56.180
very much fine-tuned for that particular Jigsaw task, right? So we saw that

1:15:56.180,1:16:00.470
accuracy go down. Now when you take those features and you perform clustering on

1:16:00.470,1:16:04.700
it you can think of this as basically, you're reducing the amount of

1:16:04.700,1:16:09.230
information, right? If I train the second network to directly regress the

1:16:09.230,1:16:13.760
features of the first network, I would basically get the same exact network. But

1:16:13.760,1:16:17.660
if I train the second network only to predict what images are grouped together

1:16:17.660,1:16:23.150
in the first one, I'm actually predicting lesser information. And that thinking is

1:16:23.150,1:16:27.290
that clustering is some kind of a noise removal technique, so it's

1:16:27.290,1:16:32.180
really removing all the artifacts that are specific to Jigsaw from that

1:16:32.180,1:16:35.210
feature space, and so the second network is actually learning something slightly

1:16:35.210,1:16:41.180
more generic. []Student] All right, thanks. [Ishan] That's the reason for this experiment as

1:16:41.180,1:16:45.200
well, so in this case we  empirically validate that hypothesis by

1:16:45.200,1:16:49.820
actually ingesting amount of label noise, so the last layer is going to

1:16:49.820,1:16:53.210
get more and more moisy and when you do ClusterFit on top of this you

1:16:53.210,1:16:57.020
actually again see improvement, so that's our validation of this

1:16:57.020,1:17:03.800
hypothesis. [Student] I had another question, so did you measure the performance of

1:17:03.800,1:17:07.880
ClusterFit on object detection, like did it perform as well? or it was it just

1:17:07.880,1:17:13.430
great in classification? [Ishan] So it performs well in detection as well.

1:17:13.430,1:17:19.850
So there were initial experimental detection where it

1:17:19.850,1:17:25.539
actually does perform well, we did not really push a lot of the

1:17:25.539,1:17:29.530
detection aspect of it in this particular paper, we were more

1:17:29.530,1:17:33.579
interested in the retrieval or like linear classification kind of

1:17:33.579,1:17:39.010
experiments. [Student] Okay, because I was thinking it feels like making these pseudo labels

1:17:39.010,1:17:44.349
we were basically making it unable to do classification tasks instead of

1:17:44.349,1:17:48.099
detection task, maybe we could lose one of some of those features that Jigsaw

1:17:48.099,1:17:55.840
got. [Ishan] right that is possible, at least the initial experiments that I had run did

1:17:55.840,1:17:59.530
not seem to suggest this there was improvement in detection, it was minor,

1:17:59.530,1:18:04.119
but detection improvements overall, like the gap in performance is already so

1:18:04.119,1:18:08.280
small that the improvements  are generally very small.

1:18:08.280,1:18:18.789
[Student] Okay, thank you. I have a question about  ClusterFit algorithm, so will the final layer of ClusterFit

1:18:18.789,1:18:24.219
algorithm not get again covariant to the to the labels that were used for

1:18:24.219,1:18:29.769
training it on that task? [Ishan] It becomes less covariant. So what we found was if you

1:18:29.769,1:18:32.860
were to sort of... the paper has this plot, I don't have it at the slide

1:18:32.860,1:18:40.119
unfortunately. The paper has the plot where, okay, this particular plot where we

1:18:40.119,1:18:43.989
were looking at conv1 to res5, ClusterFit is much better. So there res5 to res4 gap 

1:18:43.989,1:18:48.010
for ClusterFit is   much smaller than it is for, say, Jigsaw

1:18:48.010,1:18:55.150
or RotNet. [Student] But was it better than res4? [Ishan] It was slightly worse. So it was on

1:18:55.150,1:19:00.400
VOC, on classification, it was better but for other tasks like ImageNet it was

1:19:00.400,1:19:07.510
slightly worse. So it did not completely fix the problem. [Student] Okay, thank you. [Ishan] which was 

1:19:07.510,1:19:13.570
sort of the motivation for PIRL. So basically I've not talked about PIRL. So PIRL was

1:19:13.570,1:19:17.679
sort of born from the hypothesis that you need to be invariant to these

1:19:17.679,1:19:24.579
pretext tasks. So before I get into the details of PIRL, I will talk really a

1:19:24.579,1:19:29.260
little bit about general contrastive learning. How many minutes do I have by

1:19:29.260,1:19:38.679
the way? [Alfredo] 15 minutes more or less. [Ishan] Okay so contrastive learning is basically a 

1:19:38.679,1:19:44.500
general framework that tries to learn a feature space that can combine

1:19:44.500,1:19:49.300
together, or sort of put together points that are related, and push apart points

1:19:49.300,1:19:53.830
that are not related. So in this case imagine the blue boxes are the

1:19:53.830,1:19:56.650
related points, the greens are the related and the purple are the related

1:19:56.650,1:20:02.199
points. You will extract features for each of these data points

1:20:02.199,1:20:06.460
through a shared network which is called a siamese network. You'll get a

1:20:06.460,1:20:12.429
bunch of image features for each of these data points and then you apply

1:20:12.429,1:20:17.520
a loss function, which is a contrastive loss function which is going to try to

1:20:17.520,1:20:23.020
minimize the distance between the blue points, as opposed to,

1:20:23.020,1:20:28.300
the distance between the blue point and the green point. Or the distance

1:20:28.300,1:20:31.750
between the blue points should be less than the distance between the

1:20:31.750,1:20:36.280
blue point and the green point, or the blue point and the purple point. So

1:20:36.280,1:20:40.780
embeddings from the related samples should be much closer than embeddings

1:20:40.780,1:20:44.980
from the unrelated samples. So that's sort of the general idea of contrastive

1:20:44.980,1:20:48.520
learning and, of course, Yann was one of the first people to propose

1:20:48.520,1:20:54.280
this method and his earlier paper  with Raia Hadsell that was called DrLIM.

1:20:54.280,1:20:57.940
And so, contrastive learning has now made a resurgence in self-supervise learning,

1:20:57.940,1:21:01.389
pretty much a lot of the  self-supervised state-of-the-art methods are

1:21:01.389,1:21:07.960
really based on contrastive learning. And the main question is, How do you define

1:21:07.960,1:21:13.000
what is related and unrelated? So in the case of supervised learning that's

1:21:13.000,1:21:18.130
fairly clear: All of the dog images are related images, and any image that is

1:21:18.130,1:21:21.940
not a dog image is an unrelated image. But it's not so clear

1:21:21.940,1:21:24.580
how to define this related and unrelatedness

1:21:24.580,1:21:30.489
in this case of self-supervised learning. The other main difference from

1:21:30.489,1:21:35.860
something like a pretext task is that contrastive learning really reasons about

1:21:35.860,1:21:42.670
a lot of data at once. So, going back to my previous slide, 

1:21:42.670,1:21:47.170
if you look at the loss function, it always involves multiple images, right? 

1:21:47.170,1:21:50.290
So in the  first row it involves

1:21:50.290,1:21:54.430
the blue images and the green images. In the second row it involves the

1:21:54.430,1:21:58.930
blue images and the purple images, whereas if you look at a task like, say,

1:21:58.930,1:22:03.220
Jigsaw or a task like rotation, you're always reasoning about a single

1:22:03.220,1:22:08.500
image independently. So that's  another difference with contrastive learning.

1:22:08.500,1:22:11.440
contrastive learning always reason about for multiple data points

1:22:11.440,1:22:18.130
at once. So now coming to the question, how do you define related or unrelated

1:22:18.130,1:22:24.100
images? You can actually use similar techniques to what I was talking about

1:22:24.100,1:22:28.690
earlier. You can use frames of a video,  you can use the sequential

1:22:28.690,1:22:35.860
nature of data. So to understand that frames that are nearby in a video

1:22:35.860,1:22:39.910
are related and frames from a different video, or which are further

1:22:39.910,1:22:44.760
away in time, are unrelated. And that  has formed the basis of a lot of

1:22:44.760,1:22:49.390
self-supervised learning methods in this area. So if you know of this popular method

1:22:49.390,1:22:53.350
called CPC, which is Contrastive Predictive Coding, that really relies on

1:22:53.350,1:22:58.660
the sequential nature of a signal, and it says that samples that are

1:22:58.660,1:23:03.310
closed by in time space are related, and samples that are further

1:23:03.310,1:23:09.450
apart in the time space are unrelated. And it's a fairly large amount of work

1:23:09.450,1:23:14.830
exploiting this. It can either be in the speech the domain, it can

1:23:14.830,1:23:21.130
either be in video, it can be in text or it can be in regular images. And

1:23:21.130,1:23:25.720
recently we've also been working on video and audio, so basically saying

1:23:25.720,1:23:30.490
that a video and it's corresponding audio are relate examples, and video and

1:23:30.490,1:23:38.560
audio from a different image video are underrated samples. And some of

1:23:38.560,1:23:44.160
the early work in  self-supervised learning also used this

1:23:44.160,1:23:48.790
contrastive learning method and the way they defined related samples was fairly

1:23:48.790,1:23:54.610
interesting. So you run an object tracker over a video and that

1:23:54.610,1:23:59.170
gives you a   moving patch, and what you say is that

1:23:59.170,1:24:04.230
any patch that was tracked by tracker is related to my original patch

1:24:04.230,1:24:09.420
whereas any patch from a different video is not a related patch. And so,

1:24:09.420,1:24:14.040
that gives you these bunch of related and unrelated samples. So if you

1:24:14.040,1:24:19.670
look at, in this case, Figure C where you have this distance notation,

1:24:19.670,1:24:23.610
what this network tries to learn is that patches that are coming

1:24:23.610,1:24:27.840
from the same video are related and patches that are coming from 

1:24:27.840,1:24:33.390
different videos are not related. And so, in some way it automatically learns about

1:24:33.390,1:24:38.370
different poses of an object. So a cycle viewed from different viewing

1:24:38.370,1:24:44.100
angles, or like different poses of a dog, and it tries to group them

1:24:44.100,1:24:54.320
together. So in general, if you just talk about images, a lot of work is done on

1:24:54.320,1:25:00.030
looking at nearby image patches versus distant patches. So most of the 

1:25:00.030,1:25:04.590
CPC version one and CPC version two methods are really exploiting

1:25:04.590,1:25:09.420
this property of images. So what you do is you have image patches that are close

1:25:09.420,1:25:15.360
by you call them as positives, and image patches that are further apart, like

1:25:15.360,1:25:19.290
further away in the image are considered as negatives. And then you basically just

1:25:19.290,1:25:23.580
minimize a contrastive loss using this definition of positives and

1:25:23.580,1:25:31.670
negatives. The more popular, or performant way of doing this is to

1:25:31.670,1:25:36.720
look at patches coming from an image and contrast them with patches coming from a

1:25:36.720,1:25:41.790
different image. So this forms the basis of a lot of popular methods

1:25:41.790,1:25:48.660
like instance discrimination, MoCo, PIRL SimCLR. The idea is what's

1:25:48.660,1:25:53.880
shown in the image. To into more detail, what these methods do is they

1:25:53.880,1:25:57.840
extract two completely random patches from an image, so these patches can be

1:25:57.840,1:26:01.890
overlapping, they can actually be contained within one another or they can

1:26:01.890,1:26:06.840
be completely far apart, and it then applies some sort of data augmentation.

1:26:06.840,1:26:12.120
So in this case, say a color jittering or removing the color, or so on, and then

1:26:12.120,1:26:15.110
you define these two patches to be your sort of positive

1:26:15.110,1:26:19.760
examples. You extract another patch from a different image, and this is again a

1:26:19.760,1:26:25.580
random patch, and that becomes just negative. And a lot of these methods

1:26:25.580,1:26:30.170
will extract a lot of negative patches and then they will perform

1:26:30.170,1:26:34.850
contrastive learning. So you are relating to positive samples but you have a sort

1:26:34.850,1:26:39.010
of negative sample that you are contrasting this against.

1:26:39.840,1:26:46.679
Now moving to PIRL a little bit, let's try to understand what the

1:26:46.679,1:26:50.579
main difference of pretext tasks is and how contrastive learning is 

1:26:50.579,1:26:54.840
very different from pretext tasks. So the one thing that I've already mentioned was

1:26:54.840,1:26:59.820
pretext tasks always reason about a single image at once. So, the idea is that

1:26:59.820,1:27:03.869
given an image you apply a transform to that image, so in this case they are

1:27:03.869,1:27:10.800
Jigsaw transform, and then you  input this transformed image

1:27:10.800,1:27:14.219
into a ConvNet and you try to predict the property of the transform that you

1:27:14.219,1:27:18.239
applied. So the permutation that you applied or the rotation that you applied

1:27:18.239,1:27:24.329
or the kind of color that you removed and so on. So the pretext task always

1:27:24.329,1:27:28.500
reason about a single image. And the second thing is that the task that

1:27:28.500,1:27:33.989
you're performing, in this case, really has to capture some property of the

1:27:33.989,1:27:38.099
transform. So it really needs to capture the exact permutation that you applied,

1:27:38.099,1:27:42.449
or the kind of rotation that you applied, which means that the last layer

1:27:42.449,1:27:47.010
representations are actually going to  covary or vary a lot as the

1:27:47.010,1:27:52.199
transform t changes. And that is by design, because you are really trying to

1:27:52.199,1:27:57.270
solve that pretext task. But unfortunately what this means is that

1:27:57.270,1:28:02.010
the last layer representations capture a very low-level property of the signal, so

1:28:02.010,1:28:07.469
they capture things like rotation or so on. Whereas what is

1:28:07.469,1:28:11.579
expected of these representations is that they are 

1:28:11.579,1:28:15.119
invariant to these things, which means you should be able to recognize a cat, no

1:28:15.119,1:28:18.449
matter whether the cat is upright or that the cat is rotated

1:28:18.449,1:28:21.690
by 90 degrees. Whereas when you're solving that particular

1:28:21.690,1:28:25.079
pretext task you're imposing the exact opposite thing, you're saying that I

1:28:25.079,1:28:28.590
should be able to recognize whether this picture is upright or whether this

1:28:28.590,1:28:36.659
picture is basically tilted sideways. So, there are many exceptions in which you

1:28:36.659,1:28:42.030
really want these low level representations to be covariant, and 

1:28:42.030,1:28:46.199
a lot of it really has to do on the tasks that you're performing, and quite a

1:28:46.199,1:28:50.519
few tasks in simply really want to be predictive. So you want to

1:28:50.519,1:28:54.990
predict what camera transforms you have when you're looking at two views of the same

1:28:54.990,1:28:59.760
object or so on. But unless you have that kind of a specific application for a lot

1:28:59.760,1:29:03.780
of semantic tasks you really want to be invariant to the transform that are 

1:29:03.780,1:29:11.940
used as input. So invariance has been the workhorse for feature

1:29:11.940,1:29:18.540
learning. So something like SIFT, which is a fairly popular handcrafted feature, the

1:29:18.540,1:29:23.100
i in SIFT stands for invariant. And supervised networks for example,

1:29:23.100,1:29:27.090
supervised AlexNets or supervised ResNets, they are trained to be invariant

1:29:27.090,1:29:32.400
to data augmentation. You want this network to classify different crops, or

1:29:32.400,1:29:37.350
different rotations of this image as a tree, rather than ask it to predict what

1:29:37.350,1:29:43.110
exactly was the transformation applied to the input. So this is what inspired PIRL.

1:29:43.110,1:29:48.660
so PIRL stands for Pretext-Invariant Representation Learning, where the idea

1:29:48.660,1:29:52.740
is that you want the representation to be invariant, or capture as little

1:29:52.740,1:29:59.220
information as possible of the input transform. So you have the image, you have

1:29:59.220,1:30:03.030
the transform version of the image, you feed-forward both of these images

1:30:03.030,1:30:06.860
through a ConvNet, you have get a representation and then you 

1:30:06.860,1:30:12.600
encourage these representations to be similar. So in terms of the notation I

1:30:12.600,1:30:18.570
was talking about earlier, you basically say that the image I and any pretext 

1:30:18.570,1:30:23.670
transformed version of this image I are related samples, and any other image is

1:30:23.670,1:30:29.430
unrelated sample. So in this way, when you train the network, this

1:30:29.430,1:30:33.690
representation hopefully contains very little information about this

1:30:33.690,1:30:39.720
transform t and then you training it using contrastive learning, so the contrastive

1:30:39.720,1:30:44.970
learning part is to basically you have, say, feature V_I coming from the

1:30:44.970,1:30:50.010
original image I and you have the feature V_It coming from the transform

1:30:50.010,1:30:54.450
version and you want both of these representations to be the same. And the

1:30:54.450,1:30:57.750
paper we looked at two different state-of-the-art

1:30:57.750,1:31:01.320
pretext transforms, so that is the Jigsaw and the Rotation method that I

1:31:01.320,1:31:04.830
talked about earlier, and we also explored combinations of these

1:31:04.830,1:31:09.690
transforms, so apply both Jigsaw amd  Rotation at the same time, so in some way,

1:31:09.690,1:31:13.290
this is like multi-task learning but they're not really trying to predict

1:31:13.290,1:31:19.970
both Jigsaw and Rotation, they are trying to be invariant to both Jigsaw and Rotation.

1:31:19.970,1:31:25.830
The key thing that has made contrastive learning work well in the

1:31:25.830,1:31:30.560
past, like successful attempts, is really using a large number of negatives.

1:31:30.560,1:31:37.020
And one of the good papers that introduced this was this instance

1:31:37.020,1:31:42.750
discrimination paper from 2018 which introduced this concept of a memory bank.

1:31:42.750,1:31:47.520
And this has powered, I would say, most of the recent methods which are

1:31:47.520,1:31:51.720
state-of-the-art including MoCo, PIRL, and they all build 

1:31:51.720,1:31:55.950
on this idea of memory bank. [Alfredo] I asked you to unplug your headphones from

1:31:55.950,1:31:59.220
the computer because it's very noisy, because it's the microphone is picked

1:31:59.220,1:32:04.760
from the headphones... [Ishan] Oh, is it better now? [Alfredo] Maybe, I don't know, let's try.

1:32:09.180,1:32:17.040
[Ishan] So the memory bank is a nice way to get a large number of negatives

1:32:17.040,1:32:22.770
without really increasing the  compute requirement. So what you do is

1:32:22.770,1:32:28.860
you store a feature vector per image in memory and then you use that

1:32:28.860,1:32:34.860
feature vector in your contrastive learning. So okay, let's first talk about

1:32:34.860,1:32:40.650
how you would do this entire PIRL setup without using a memory bank. So you have

1:32:40.650,1:32:45.560
an image I, you have an image I t, you feed-forward both of these images, you

1:32:45.560,1:32:52.350
get a feature vector f of V_I from the original image I, you get a feature g of

1:32:52.350,1:32:58.290
VI t from the transformed versions, the patches in this case, and what you want

1:32:58.290,1:33:02.700
is the features f and g to be similar, and you want features from any other

1:33:02.700,1:33:10.740
image, so an unrelated image, to be this negative sample. So in this case, what we

1:33:10.740,1:33:15.870
now can do is, rather than if we want a lot of negatives, we would really want a

1:33:15.870,1:33:19.350
lot of these negative images to be feed-forward at the same time which

1:33:19.350,1:33:24.390
really means that you need a large batch size to be able to do this. And

1:33:24.390,1:33:29.130
of course a large batch size means, it not not possible to have

1:33:29.130,1:33:33.450
unlimited amount of GPU memory, so the way to do that is to use

1:33:33.450,1:33:37.530
something called a memory bank. So what this memory bank does is that it

1:33:37.530,1:33:42.630
stores a feature vector for each of the images in your dataset and when you're

1:33:42.630,1:33:46.470
doing contrastive learning, rather than using feature vectors from a

1:33:46.470,1:33:50.580
different or negative image in your batch, you can

1:33:50.580,1:33:55.500
just retrieve these features from memory. So you can just retrieve features

1:33:55.500,1:33:59.070
of any other unrelated image from the memory and you can just substitute that

1:33:59.070,1:34:04.650
to perform contrastive learning. So in PIRL we divided the objective into two parts,

1:34:04.650,1:34:10.040
there was a contrastive term to bring the feature vector from the

1:34:10.040,1:34:15.480
transformed image, so g of V_I, similar to the representation that we have in the

1:34:15.480,1:34:21.180
memory, so M of I, and similarly we have a second contrastive term that tries to

1:34:21.180,1:34:25.860
bring the feature f of V_I close to the feature representation that we have in

1:34:25.860,1:34:31.620
memory. So essentially g is being pulled close to M_I and f is being pulled close

1:34:31.620,1:34:36.360
to M_I. So by transitivity f and g are being pulled close to one

1:34:36.360,1:34:41.460
another. And the reason for separating this out was because it stabilized

1:34:41.460,1:34:46.290
training and we were able to train without doing this the

1:34:46.290,1:34:50.220
training would not really converge. And so by separating this out into two forms

1:34:50.220,1:34:54.960
rather than doing direct contrastive learning between f and g, we were able

1:34:54.960,1:35:01.620
to stabilize training and actually get it working. So the way to evaluate this

1:35:01.620,1:35:07.080
is by standard  pre-training evaluation setup, so

1:35:07.080,1:35:12.390
transfer learning, where we can pre-train on images without labels, so the standard

1:35:12.390,1:35:16.100
way of doing this is to take ImageNet throw away the labels and pretend it is

1:35:16.100,1:35:21.810
unsupervised, and then evaluate using simple fine-tuning or training

1:35:21.810,1:35:26.190
a linear classifier. The second thing we did was also a test PIRL and its

1:35:26.190,1:35:31.230
robustness to image distributions by training it on in-the-wild images. So

1:35:31.230,1:35:34.810
we just took one million images randomly from Flickr, so this is the 

1:35:34.810,1:35:39.610
YFCC dataset, and then we basically perform pre-training

1:35:39.610,1:35:43.930
on these images and then perform transfer learning on different

1:35:43.930,1:35:51.420
datasets. [Student] I had a question about the PIRL method, about the memory bank, where

1:35:51.420,1:35:57.610
the m, wouldn't those like feature representation stored in the memory bank

1:35:57.610,1:36:04.720
be out of date? [Ishan] Yeah, so they do go a little bit out of date but in practice

1:36:04.720,1:36:09.070
it really does not make that much of a difference. So the 

1:36:09.070,1:36:14.050
particular way of updating them, so m of I is a moving average of the

1:36:14.050,1:36:19.900
representation f, and that moving average although it's still, it actually

1:36:19.900,1:36:25.630
does not matter a lot in practice, you can still continue to use them. [Student] So

1:36:25.630,1:36:31.030
I recently read the paper PIRL, where you you huge batch size

1:36:31.030,1:36:37.540
like 8,000 or something, so using  the memory bank approach and getting

1:36:37.540,1:36:43.000
these 8,000 examples and one loss function, is that possible? [Ishan] Yes, sort

1:36:43.000,1:36:47.980
of simplier way of doing it really requires a large batch size because you're getting

1:36:47.980,1:36:52.330
negatives from different images in the same batch, whereas if you use something

1:36:52.330,1:36:55.660
like the memory bank you really do not need a large batch size. So you can train

1:36:55.660,1:37:00.160
this with like 32 images in a batch because all the negatives are really

1:37:00.160,1:37:03.790
coming from the memory bank, which does not really require you to do multiple

1:37:03.790,1:37:10.520
feed-forwards. [Student] Okay, thank you. [Student] if you are 

1:37:10.520,1:37:14.330
using memory bank then you can't back propagate to the negative example, so is that not a

1:37:14.330,1:37:20.510
problem? [Ishan] It does not create that much of a problem

1:37:20.510,1:37:25.010
really, so that was one thing I was worried about as well, so in the initial

1:37:25.010,1:37:29.150
versions we did try something which was using a larger batch size

1:37:29.150,1:37:34.159
but then when we switched to something like the memory bank, it did not really reduce

1:37:34.159,1:37:41.150
performance, very very little, very minor reduction in performance. [Student] Any

1:37:41.150,1:37:47.480
intuition on why that's the case? [Ishan] so I think overall contrastive learning is fairly

1:37:47.480,1:37:53.090
slow to converge, so all methods like the latest version of MoCo and

1:37:53.090,1:37:56.390
so on, all of them train for very large number of epochs

1:37:56.390,1:38:00.890
anyway, so the number of backprops that you're getting or the number of memory, or

1:38:00.890,1:38:04.219
parameter updates that you're doing are very large in general, so the

1:38:04.219,1:38:07.699
fact that you miss out on one of them, in this particular case, probably does not

1:38:07.699,1:38:16.340
have that much of an effect. [Student] Thanks. [Alfredo] That's five minutes. [Ishan] Good, almost there.

1:38:16.340,1:38:21.050
So yeah we evaluated those on a bunch of different tasks, so the first

1:38:21.050,1:38:27.770
thing was object detection, again sort of standard tasks in vision. And in this

1:38:27.770,1:38:32.210
case, PIRL was able to outperform ImageNet supervised training on detection for

1:38:32.210,1:38:37.730
both the VOC07 and VOC07+12 datasets and it outperforms on

1:38:37.730,1:38:42.020
this most strict evaluation  criterion which is AP all which is now

1:38:42.020,1:38:46.820
introduced by COCO which was already a positive sign and then it was

1:38:46.820,1:38:51.620
able to do this. The second thing we looked at was evaluating PIRL

1:38:51.620,1:38:55.730
on semi-supervised learning and, once again, PIRL was performing fairly well,

1:38:55.730,1:39:01.159
it was actually better than, say, the pretext task of Jigsaw. So the only

1:39:01.159,1:39:05.150
difference between the top row and the bottom row is the fact that PIRL is an

1:39:05.150,1:39:09.510
invariant version whereas Jigsaw is a covariant version.

1:39:09.510,1:39:14.230
And in terms of linear classification, when PIRL came out, it was basically at

1:39:14.230,1:39:19.060
par with CPC is latest version and was performing fairly well on a bunch of

1:39:19.060,1:39:22.720
different parameter settings and a bunch of different architectures. Of

1:39:22.720,1:39:27.190
course, now you can have like fairly good performance by methods like SimCLR,

1:39:27.190,1:39:31.980
so that number for SimCLR corresponding would basically be about 69 or 70

1:39:31.980,1:39:37.020
compared to PIRL 63-ish number. The other thing we looked at was

1:39:39.280,1:39:44.620
how PIRL generalizes across data distributions, so for this we looked

1:39:44.620,1:39:50.380
at just Flickr images from the YFCC dataset, and PIRL was able to 

1:39:50.380,1:39:54.310
outperform methods that were trained using hundred times more data. So the

1:39:54.310,1:39:58.690
Jigsaw in the second row was trained on

1:39:58.690,1:40:02.530
100 million images whereas PIRL was trained on

1:40:02.530,1:40:07.390
1 million images and despite that it's actually able to outperform the

1:40:07.390,1:40:11.890
Jigsaw method fairly easily. This again shows you the power of

1:40:11.890,1:40:15.310
breaking invariance into your representation rather than 

1:40:15.310,1:40:21.660
predicting pretext tasks. And finally, what I started 

1:40:21.660,1:40:25.560
out with, which is whether this thing is actually semantic. So if you

1:40:25.560,1:40:29.790
look at different layers of representations, so conv1 to res5, Jigsaw

1:40:29.790,1:40:34.199
basically shows a drop in performance from res4 to res5, but as for PIRL you

1:40:34.199,1:40:38.340
see a nicely increasing graph, where res4 and res5 get

1:40:38.340,1:40:45.719
increasingly more and more semantic. In terms of problem complexity, PIRL was

1:40:45.719,1:40:49.140
very good at handling that because you're never predicting the number of

1:40:49.140,1:40:52.980
permutations, you're just using them at input as like sort of data augmentation.

1:40:52.980,1:40:58.710
So PIRL can scale very well to all the 360,000 possible permutations in

1:40:58.710,1:41:02.340
the nine patches. Whereas for Jigsaw, because you're predicting that, you're

1:41:02.340,1:41:09.239
very limited by the size of your output space. And the paper also shows that we

1:41:09.239,1:41:13.380
can extend PIRL, it is not  limited to Jigsaw, you can do that on

1:41:13.380,1:41:17.010
rotation, you can have a  combination of Jigsaw and rotation and

1:41:17.010,1:41:24.690
you can get more and more gains if you start doing this. So 

1:41:24.690,1:41:28.860
if you look at these methods, starting from pretext tasks to clustering

1:41:28.860,1:41:33.300
to PIRL, as you go from the left to the right you get more and more

1:41:33.300,1:41:38.580
invariance and in some way you also see an increase in performance, which 

1:41:38.580,1:41:41.640
suggests that birnging in more and more  invariance to your methods is actually going

1:41:41.640,1:41:47.580
to be more helpful in the longer term. There are some shortcomings, 

1:41:47.580,1:41:50.219
we really do not understand what is the set of data

1:41:50.219,1:41:54.719
transforms that matter. So Jigsaw works really well, but it's not very clear why

1:41:54.719,1:42:00.390
this is happening. So some future work, or if you want to spend your spare

1:42:00.390,1:42:04.380
cycles thinking about something, is really understanding what invariances

1:42:04.380,1:42:08.760
really matter when you are trying to solve a supervised task, what invariances

1:42:08.760,1:42:13.800
really matter for something like ImageNet. And that's it.

1:42:13.800,1:42:17.520
So basically predict more and more information and try to be as invariant

1:42:17.520,1:42:24.690
as possible. Thank you. [Student] So I had a question, these contrastive networks,

1:42:24.690,1:42:28.530
they can't use the batch norm layer, right? Because then information would

1:42:28.530,1:42:31.080
pass from one sample to the other and then

1:42:31.080,1:42:35.910
network might learn a very trivial way of separating the negatives from the

1:42:35.910,1:42:42.360
positive? [Ishan] So for PIRL for example, we really did not observe that

1:42:42.360,1:42:45.810
phenomenon at all so we did not really have to do any special tricks with

1:42:45.810,1:42:52.260
batch norm, we were able to use batch norm as it is. [Ishan] Okay and it's not necessary for

1:42:52.260,1:42:57.150
all the contrastive networks to not use batch norm? it's okay to have the

1:42:57.150,1:43:02.760
batch norm layer? [Ishan] Yeah, I mean for example for SimCLR and so on,

1:43:02.760,1:43:06.510
they try and move to same batch norm because they want to emulate a large batch

1:43:06.510,1:43:12.000
size, so you might have to do some tweaks in batch norm, but basically you

1:43:12.000,1:43:15.150
cannot avoid it really, because if you completely remove batch norm then

1:43:15.150,1:43:20.970
training these very deep networks is generally very hard anyway. [Student] Okay, do

1:43:20.970,1:43:27.090
you think that PIRL paper works with the batch norm layers because it uses a memory

1:43:27.090,1:43:32.580
bank, and all the representations are not taken at the same time? Whereas I think

1:43:32.580,1:43:37.710
MoCo, they specifically mentioned not to use the batch norm layer or use it

1:43:37.710,1:43:42.450
spread across multiple GPUs. [Ishan] So that I think is one difference for sure

1:43:42.450,1:43:46.020
because the negatives that  you are contrasting against and the

1:43:46.020,1:43:50.460
positive were from different time steps which makes it harder for batch norm to sort

1:43:50.460,1:43:56.520
of cheat it, but as for the other methods like MoCo and SimCLR, they're very

1:43:56.520,1:44:00.810
correlated to the particular batch that you're evaluating right now. [Student] Okay, so, is

1:44:00.810,1:44:05.730
there any suggestion if we are using a  n-pair loss rather than a memory bank, is

1:44:05.730,1:44:09.120
there any suggestion how to go about this? Whether we should just stick to

1:44:09.120,1:44:14.970
AlexNet and VGG which don't use a batch norm layer, or is there any way to

1:44:14.970,1:44:23.600
turn it off? or? [Ishan] Can you describe the setting a little bit more?

1:44:23.600,1:44:30.750
[Student] So basically what I'm trying to do is train on a frames of videos and I'm

1:44:30.750,1:44:35.910
using a N-pair setting where I'm trying to contrast between n-samples rather

1:44:35.910,1:44:41.040
than two or three samples, and what I'm worried about is whether I should be

1:44:41.040,1:44:44.730
using batch norm or not, and if I am not using batch norm at all then which 

1:44:44.730,1:44:49.910
pre-trained, sorry, pre-architecture models can I use?

1:44:51.199,1:44:55.590
[Ishan] That's tricky. So the one problem with video frames is that they are fairly

1:44:55.590,1:45:01.050
correlated, so in general  the performance of batch norm 

1:45:01.050,1:45:05.520
degrades and you have fairly correlated samples. So with video that becomes more and

1:45:05.520,1:45:11.250
more a problem. The unfortunate, the sad

1:45:11.250,1:45:15.330
news is basically that even if you look at a typical implementation of AlexNet

1:45:15.330,1:45:19.980
these days, it will include batch norm. It's just because it's much more stable

1:45:19.980,1:45:23.280
to train with that, you can train with a higher learning rate and you can

1:45:23.280,1:45:26.699
basically use it for a bunch of different downstream tasks. So I think

1:45:26.699,1:45:31.710
you may still have to use batch norm. If not, you can give other variants like

1:45:31.710,1:45:38.370
group norm a try, which basically do not really depend on the batch size. [Student] Ok, it makes

1:45:38.370,1:45:44.600
sense, thank you. [Yann] Okay thank you so much Ishan,

1:45:44.600,1:45:52.230
there's a lot of no interesting details. [Alfredo] I think we still have like eight minutes

1:45:52.230,1:45:58.739
if people are, I think they're still many left in class, any questions? [Student] Yep, I

1:45:58.739,1:46:02.699
had one question. Which I had also put forward in the lecture when we were

1:46:02.699,1:46:08.760
discussing PIRL. So this question is about the loss function. Can I ask it

1:46:08.760,1:46:15.449
right now? [Ishan] Yes, go for it. [Student] When I read the paper there was a probability term that we

1:46:15.449,1:46:20.969
were computing after computing the V_I m to  V_I t representation for image and

1:46:20.969,1:46:26.670
the transformed version, and after getting those probabilities then we were

1:46:26.670,1:46:33.030
using a noise contrastive estimation loss, so I was kind of confused that, wouldn't

1:46:33.030,1:46:37.170
it had been better if just a negative log of that probability had been

1:46:37.170,1:46:44.550
minimized? [Ishan] So you can use both really, so the reason to use NCE was basically

1:46:44.550,1:46:50.130
more to do with how the memory bank paper was set up. So NCE, if

1:46:50.130,1:46:54.030
you have K negatives you are basically solving k + 1 problems, so 

1:46:54.030,1:46:58.830
you basically have k + 1 different binary problems that

1:46:58.830,1:47:02.250
you are solving. So that's one way of doing it, the other

1:47:02.250,1:47:05.970
way of doing it is basically what is now called info NC, which is really the

1:47:05.970,1:47:10.320
softmax, so you just apply a softmax and you minimize the negative log-likelyhood

1:47:10.320,1:47:16.290
for that. [Student] It's because that edge, the probability function looked

1:47:16.290,1:47:25.380
like softmax. [Ishan] Yes, so at the time when I had tied it out, it actually gave

1:47:25.380,1:47:29.880
me slightly worse results and so that's basically why I used NCE, and this was

1:47:29.880,1:47:34.320
just initial experiments. Now when I'm trying it out it actually gives me similar

1:47:34.320,1:47:39.050
results so I guess in the end it does not make that much of a difference.

1:47:41.150,1:47:46.260
[Student] This is more related to the course, but we are gonna have a project on 

1:47:46.260,1:47:52.020
self-supervised learning. Ishan, can you give us information on how to get 

1:47:52.020,1:47:59.790
the self-supervised learning model working? As in the implementation details, like

1:47:59.790,1:48:05.280
this has been a lecture on high-level idea, so how   to get it

1:48:05.280,1:48:12.690
working quickly? [Ishan] So, there are certain class of techniques that are

1:48:12.690,1:48:16.740
going to be much easier to get working from the get-go, right? So for example, if

1:48:16.740,1:48:20.930
you were looking at just pretext tasks, then you would basically look at

1:48:20.930,1:48:26.010
something like rotation because it's a very easy task to implement, you

1:48:26.010,1:48:29.850
really cannot go wrong with it, I mean, there are very few things to implement.

1:48:29.850,1:48:34.710
So just the number of moving pieces is a good indicator. The other thing to

1:48:34.710,1:48:41.310
remain remember is if you're implementing an existing method,

1:48:41.310,1:48:46.050
then there are going to be lots of tiny details the authors talk about. So for

1:48:46.050,1:48:49.440
example, the exact learning rate that they used or the way they used batch norm

1:48:49.440,1:48:51.630
and so on. If there are lots of these things, then

1:48:51.630,1:48:55.710
it's going to be harder and harder for you to reproduce, more

1:48:55.710,1:49:01.320
and more things for you to get wrong. The second thing to remember is data

1:49:01.320,1:49:06.390
augmentations. Data augmentations are really critical, so if you get anything

1:49:06.390,1:49:10.650
working you would try to sort of add molded augmentations to it.

1:49:10.650,1:49:17.070
[Student] Do you recommend us trying PIRL or do you think that would be too difficult to

1:49:17.070,1:49:25.980
do in one month? [Ishan] I'm not sure what the setting is really, so I'm not sure I can comment on that. 

1:49:25.980,1:49:30.780
[Student] Okay, thanks. One more thing, did you try using momentum contrast on PIRL instead

1:49:30.780,1:49:35.550
of memory bank? [Ishan] I haven't, so we basically move to the end-to-end

1:49:35.550,1:49:41.520
version, which is similar to what SimCLR is, so the thing is, I mean you

1:49:41.520,1:49:44.670
can gather a bunch of negatives from different GPUs, so increase

1:49:44.670,1:49:49.410
your batch size, that actually generally helps a lot, I would suspect

1:49:49.410,1:49:54.030
MoCo would helped a lot as well. [Student] I think MoCo would improve performance over

1:49:54.030,1:50:00.720
SimCLR by replacing end-to-end training with MoCo. [Ishan] But I think numbers

1:50:00.720,1:50:05.820
are still fairly similar, and there are small differences in evaluation protocol

1:50:05.820,1:50:11.340
that you would see across these papers, so we're planning

1:50:11.340,1:50:15.210
to release a more standardized evaluation benchmark. So we did that last

1:50:15.210,1:50:18.810
year, unfortunately that was in Caffe2, so we're trying to release something

1:50:18.810,1:50:22.950
PyTorch now and provide a lot of standardized implementations, so like

1:50:22.950,1:50:30.720
PIRL and a bunch of these, and a standardized evaluation protocol for everything.

1:50:30.720,1:50:36.720
[Student] I had a question about above  self-supervised learning. What do you think

1:50:36.720,1:50:42.200
is the state of generative methods? And did you think about combining like

1:50:42.200,1:50:48.210
contrastive methods with generative methods, SimCLR actually like has a

1:50:48.210,1:50:51.780
different space, so they have like a linear layer on top of the feature

1:50:51.780,1:50:55.830
representation where they compute the actual feature the representation where

1:50:55.830,1:51:01.380
they did the contrastive loss, the NCE stuff, so you think having another

1:51:01.380,1:51:07.830
head, given that a crop of image, you just try to scale out

1:51:07.830,1:51:13.290
that crop of image, and you have that information because you crop that image,

1:51:13.290,1:51:21.330
right? [Ishan] It is definitely a good idea, I

1:51:21.330,1:51:24.449
think this is the tricky part, it is getting these things to train is

1:51:24.449,1:51:29.280
just non-trivial. So I haven't really tried any generative

1:51:29.280,1:51:33.539
approaches. In my experience that's slightly harder to

1:51:33.539,1:51:37.409
get to work, but I do agree, I think in the

1:51:37.409,1:51:42.820
longer term they are the sort of the things to focus on.

1:51:42.820,1:51:56.030
[Student] Thank you. [Alfredo] Last question? No? That's it? I guess... [Student] oh I can actually ask the question,

1:51:56.030,1:52:01.130
so this is regarding distillation actually, so you were

1:52:01.130,1:52:05.860
telling me how predicting softer distributions gives gives a richer

1:52:05.860,1:52:11.930
target, right? So can you elaborate on that? Because it sort of increases the

1:52:11.930,1:52:16.490
uncertainty of our model, right? We are predicting from one-hot distribution and

1:52:16.490,1:52:19.340
then making it softer and then we are predicting on that and so more uncertainty,

1:52:19.340,1:52:23.540
and moreover, why do they call it distillation? Because I feel like

1:52:23.540,1:52:31.220
you need more parameters to account for this richer target. [Ishan] So 

1:52:31.220,1:52:36.170
if you train on one-hot labels your models tend to be

1:52:36.170,1:52:40.220
very overconfident in general. So if you have heard of these tricks called

1:52:40.220,1:52:45.440
label smoothing, which is now being used by a bunch of methods, label

1:52:45.440,1:52:49.490
smoothing, you can think of it like the simplest version of

1:52:49.490,1:52:51.740
distillation. So you have a one-hot vector that you

1:52:51.740,1:52:54.920
were trying to predict but rather than trying to predict that one-hot vector,

1:52:54.920,1:52:59.150
what you do is you take some probability mass out of that... So you

1:52:59.150,1:53:03.130
would predict 1 and a bunch of 0s, so rather than doing that, you predict say

1:53:03.130,1:53:07.490
0.97 and you add 0.01, 0.01, 0.01, like the remaining three labels,

1:53:07.490,1:53:10.610
so you just start a uniform distribution to the remainder. So

1:53:10.610,1:53:14.810
distillation is a more informed way of doing this. So rather than

1:53:14.810,1:53:18.860
randomly increasing the probability of a random unrelated class,

1:53:18.860,1:53:25.010
to actually have a network which was  pre-trained which is pretty good to this. In

1:53:25.010,1:53:29.800
general, softer distributions are very useful for pre-training methods because

1:53:29.800,1:53:33.860
models tend to be overconfident, pre-training on like softer

1:53:33.860,1:53:37.370
distribution is actually slightly easier than optimization problems, you converge

1:53:37.370,1:53:42.380
slightly faster as well, so both of these benefits are present in the solution and

1:53:42.380,1:53:48.710
also something like label smoothing. [Alfredo] Also because  smooth labels allow you to have like a dog looking cat

1:53:48.710,1:53:52.670
or a cat looking dog, right? So if you have a very big network that has been

1:53:52.670,1:53:55.270
trained on very many samples, it will actually

1:53:55.270,1:54:00.040
have you an idea, a proper idea of what is an unambiguous-perhaps-image,

1:54:00.040,1:54:04.450
right? And therefore, if you can actually learn that soft idea you're gonna be

1:54:04.450,1:54:12.240
learning more than if you just give that one-hot label. I think we are out of time,

1:54:12.240,1:54:16.420
I think we run out of time like half an hour ag,o but this was the

1:54:16.420,1:54:20.230
question and answers session.

1:54:20.230,1:54:27.700
If there are no you know really really urgent questions still pending, I will 

1:54:27.700,1:54:35.170
call it the end of the today's lesson. So thank you for tuning in. I'll see

1:54:35.170,1:54:41.070
you tomorrow at the practical session, don't forget to come,

1:54:41.070,1:54:47.590
and it was it. Thank you so much Ishan and I see you around. [Yann] Thank you Ishan,

1:54:47.590,1:54:52.020
thank you everyone, take care everyone. [Alfredo] Right bye-bye.

0:00:04.690,0:00:09.459
in this case we have a network which has

0:00:07.060,0:00:11.230
an input on the left-hand side usually

0:00:09.459,0:00:14.260
you have the input on the bottom side or

0:00:11.230,0:00:17.020
on the left they are pink in my slides

0:00:14.260,0:00:18.400
so if you take notes make them pink not

0:00:17.020,0:00:20.140
just kidding

0:00:18.400,0:00:22.420
and then we have what how many

0:00:20.140,0:00:25.060
activations how many hidden layers do

0:00:22.420,0:00:27.160
you count there four hidden layers so

0:00:25.060,0:00:28.320
overall how many layers does the network

0:00:27.160,0:00:30.789
have here

0:00:28.320,0:00:34.149
six right because we have four hidden

0:00:30.789,0:00:36.340
plus one input plus one output layer so

0:00:34.149,0:00:38.980
in this case I have two neurons per

0:00:36.340,0:00:40.899
layer right so what does it mean how

0:00:38.980,0:00:44.649
what are the dimensions of the matrices

0:00:40.899,0:00:49.749
we are using here two by two so what

0:00:44.649,0:00:51.940
does that 2 by 2 matrix does come on you

0:00:49.749,0:00:56.409
have you know the answer of this

0:00:51.940,0:01:00.280
question rotation yeah then scaling then

0:00:56.409,0:01:03.309
sharing and reflection fantastic right

0:01:00.280,0:01:06.070
so we constrain our network to perform

0:01:03.309,0:01:08.770
all the operations on the plane we have

0:01:06.070,0:01:11.020
seen the first time if I allow the FIR

0:01:08.770,0:01:16.119
the hidden layer to be a hundred neurons

0:01:11.020,0:01:21.670
long we can Wow okay we can easily we

0:01:16.119,0:01:24.939
can affect our fantastic what is it we

0:01:21.670,0:01:28.030
are watching movies now I see see a

0:01:24.939,0:01:35.530
fantastic what is it mandalorian is so

0:01:28.030,0:01:41.289
cool now ok ok how nice is this lesson

0:01:35.530,0:01:49.890
is even recorded ok we have no idea ok

0:01:41.289,0:01:55.450
give me a sec ok so we go here done

0:01:49.890,0:01:57.280
listen all right so we started from this

0:01:55.450,0:01:59.409
network here right which we had this

0:01:57.280,0:02:02.499
intermediate layer and we forced them to

0:01:59.409,0:02:04.780
be two-dimensional right such that all

0:02:02.499,0:02:06.729
the transformations are enforced to be

0:02:04.780,0:02:09.640
on a plane so this is what the network

0:02:06.729,0:02:12.069
does to our plane it folds it on

0:02:09.640,0:02:12.740
specific regions right in those folders

0:02:12.069,0:02:14.990
for

0:02:12.740,0:02:16.670
folding are very very abrupt this is

0:02:14.990,0:02:19.370
because all the transformations are

0:02:16.670,0:02:21.740
performing the 2d layer right so this

0:02:19.370,0:02:23.930
training took me really really a lot of

0:02:21.740,0:02:27.080
effort because the optimization is

0:02:23.930,0:02:29.510
actually quite hard whenever I had the

0:02:27.080,0:02:31.400
hundred neuron into a hidden layer that

0:02:29.510,0:02:34.070
was very very easy to train this one

0:02:31.400,0:02:36.260
really really took a lot of effort and

0:02:34.070,0:02:38.330
you had to tell me why okay if you don't

0:02:36.260,0:02:41.060
know the answer right now better you

0:02:38.330,0:02:42.950
know the answer for the midterm so you

0:02:41.060,0:02:45.310
can take note of what are the questions

0:02:42.950,0:02:48.170
to the meter right so this is the final

0:02:45.310,0:02:51.800
output of the network which is also that

0:02:48.170,0:02:54.800
2d layer to the embedding so I have no

0:02:51.800,0:02:58.310
non-linearity out my last layer these

0:02:54.800,0:03:00.170
are the final classification regions so

0:02:58.310,0:03:02.240
let's see what each layer does this is

0:03:00.170,0:03:04.370
the first layer affine transformation so

0:03:02.240,0:03:06.470
it looks like it's a 3d rotation but

0:03:04.370,0:03:10.370
it's not right it's just to be a

0:03:06.470,0:03:13.010
rotation reflection scaling and shearing

0:03:10.370,0:03:18.530
and then what is this part ah what's

0:03:13.010,0:03:20.900
happening right now you see all we had

0:03:18.530,0:03:24.290
like the video part which is killing all

0:03:20.900,0:03:27.580
the negative sides of the network right

0:03:24.290,0:03:30.950
sorry all the negative sides of this

0:03:27.580,0:03:32.990
space right it is the second affine

0:03:30.950,0:03:35.660
transformation and then here you apply

0:03:32.990,0:03:38.720
again the areevo

0:03:35.660,0:03:40.910
you can see all the negative subspaces

0:03:38.720,0:03:43.280
have been erased and they've been set to

0:03:40.910,0:03:46.430
zero then we keep going with a third

0:03:43.280,0:03:48.290
affine transformation it's zoom it zoom

0:03:46.430,0:03:50.750
in a lot and then again you're gonna

0:03:48.290,0:03:54.080
have now the redo layer which is gonna

0:03:50.750,0:03:55.460
be killing one of those all the three

0:03:54.080,0:03:57.800
quadrants right only one quadrant

0:03:55.460,0:04:00.290
survives every time and then we go with

0:03:57.800,0:04:03.350
the fourth affine transformation we

0:04:00.290,0:04:05.270
along gating a lot because again given

0:04:03.350,0:04:06.770
that we confine all the transformation

0:04:05.270,0:04:09.140
to be living in this space it really

0:04:06.770,0:04:12.670
needs to stretch and you know use all

0:04:09.140,0:04:15.140
the power it can write again this is the

0:04:12.670,0:04:16.790
second last then we had the last affined

0:04:15.140,0:04:19.820
transformation which is the final one

0:04:16.790,0:04:23.479
and then we reach finally linearly

0:04:19.820,0:04:26.120
separable regions here finally we're

0:04:23.479,0:04:26.360
gonna see how each a final summation can

0:04:26.120,0:04:28.849
be

0:04:26.360,0:04:32.840
a split in each component so we had

0:04:28.849,0:04:34.400
rotation we have now squashing like the

0:04:32.840,0:04:36.740
wind then we have rotation reflection

0:04:34.400,0:04:38.659
because the determinant is minus 1 and

0:04:36.740,0:04:40.490
then we had the final bias again you

0:04:38.659,0:04:43.150
have the positive part of the areeda

0:04:40.490,0:04:45.590
rectified linear unit again rotation

0:04:43.150,0:04:49.389
flipping because again we had a negative

0:04:45.590,0:04:51.949
a minus 1 determinant zoom in rotation

0:04:49.389,0:04:53.689
one more reflection and then the final

0:04:51.949,0:04:55.370
bias this was the second a fine

0:04:53.689,0:04:57.530
transformation then we have here the

0:04:55.370,0:05:01.279
positive part again we are third layers

0:04:57.530,0:05:03.650
or rotation reflection zooming and then

0:05:01.279,0:05:06.080
we have the SVD decomposition right you

0:05:03.650,0:05:08.360
should be aware of that right you should

0:05:06.080,0:05:11.449
know and then final is the translation

0:05:08.360,0:05:13.460
and again the third riddle then we had

0:05:11.449,0:05:15.669
the fourth layers or rotation reflection

0:05:13.460,0:05:19.310
because the determinant was negative

0:05:15.669,0:05:21.879
assuming again the other rotation icon a

0:05:19.310,0:05:24.759
small rotate after reflection and bias

0:05:21.879,0:05:28.789
finally read oh and then we had the last

0:05:24.759,0:05:30.169
the fifth layer so rotation zooming we

0:05:28.789,0:05:32.300
didn't have reflection because the

0:05:30.169,0:05:33.860
determinant was plus one again

0:05:32.300,0:05:36.050
reflection in this case because the

0:05:33.860,0:05:38.300
terminal was negative and then finally

0:05:36.050,0:05:42.099
the final bias right and so this was

0:05:38.300,0:05:46.659
pretty much how this network which was

0:05:42.099,0:05:49.190
just made of thickness of layers of

0:05:46.659,0:05:51.289
neurons that are only two neurons per

0:05:49.190,0:05:56.449
layer is being performing the

0:05:51.289,0:05:58.129
classification task and all those

0:05:56.449,0:06:01.039
transformation had been constrained to

0:05:58.129,0:06:03.199
be living on the on the on the plane ok

0:06:01.039,0:06:05.120
so this was really really hard to train

0:06:03.199,0:06:07.460
can you figure out why it was really

0:06:05.120,0:06:11.839
really hard to train what does it happen

0:06:07.460,0:06:15.110
if my if my bias of one of the five

0:06:11.839,0:06:22.490
layers put my points in away from the

0:06:15.110,0:06:25.370
top right quadrant exactly so if you

0:06:22.490,0:06:28.279
will have one of the four biases putting

0:06:25.370,0:06:30.680
my initial point away from the top right

0:06:28.279,0:06:32.210
quadrant then the videos are going to be

0:06:30.680,0:06:35.060
completely killing everything and

0:06:32.210,0:06:37.550
everything gets collapsed into zero okay

0:06:35.060,0:06:38.070
and so there you can't do any more or

0:06:37.550,0:06:40.890
anything

0:06:38.070,0:06:43.140
so this network here was really hard

0:06:40.890,0:06:44.910
again to train if you just make it a

0:06:43.140,0:06:47.970
little bit fatter than instead of

0:06:44.910,0:06:49.530
constraining to be two neurons for each

0:06:47.970,0:06:51.930
of the hidden layer then it much easier

0:06:49.530,0:06:53.190
to train or you can do a combination of

0:06:51.930,0:06:55.080
the two alright so instead of having

0:06:53.190,0:06:57.270
just a fat Network you can have a

0:06:55.080,0:06:59.700
network that is less fat but then you

0:06:57.270,0:07:03.270
have a few hidden layers okay

0:06:59.700,0:07:05.310
so that was pretty much yeah question so

0:07:03.270,0:07:08.760
the fatness is how many neurons you

0:07:05.310,0:07:10.590
helper hidden layer right okay so the

0:07:08.760,0:07:13.410
question is that how do we determine the

0:07:10.590,0:07:15.150
structure or the configuration of our

0:07:13.410,0:07:18.000
network right how do we design Network

0:07:15.150,0:07:19.560
and the answer is going to be that's

0:07:18.000,0:07:22.710
what the N is gonna be teaching across

0:07:19.560,0:07:25.290
the semester right so keeping keep keep

0:07:22.710,0:07:26.700
keep it like keep your attention high

0:07:25.290,0:07:29.130
because you know that's what we're gonna

0:07:26.700,0:07:33.150
be teaching here that's a good question

0:07:29.130,0:07:35.210
right there is no like mathematical rule

0:07:33.150,0:07:37.980
like there is a lot of experimental

0:07:35.210,0:07:39.570
empirical evidence and you know a lot of

0:07:37.980,0:07:41.130
people trying different configurations

0:07:39.570,0:07:42.930
we found something that actually works

0:07:41.130,0:07:44.460
pretty well now again we're gonna be

0:07:42.930,0:07:49.290
covering these architectures in the

0:07:44.460,0:07:54.150
following lessons other questions don't

0:07:49.290,0:07:55.740
be shy no okay so I guess then we can

0:07:54.150,0:07:59.430
switch the sort of second part of the

0:07:55.740,0:08:04.440
class okay so we're gonna talk about

0:07:59.430,0:08:07.620
commercial nets today and is that right

0:08:04.440,0:08:09.240
in so I'll start with something that's

0:08:07.620,0:08:11.460
relevant to commercial Nets but not just

0:08:09.240,0:08:13.920
which is the idea of transforming the

0:08:11.460,0:08:15.300
parameters of a neural net so here we

0:08:13.920,0:08:18.900
have a diagram that we've seen before

0:08:15.300,0:08:20.310
except with a small twist the diagram

0:08:18.900,0:08:23.670
we're seeing here is that we have known

0:08:20.310,0:08:25.560
that G of X and W W being the parameters

0:08:23.670,0:08:26.880
X being the input that you know makes a

0:08:25.560,0:08:28.290
prediction about an output and that goes

0:08:26.880,0:08:31.230
into a cost function we've seen this

0:08:28.290,0:08:35.330
before but the twist here is that the

0:08:31.230,0:08:37.320
weight vector instead of being a

0:08:35.330,0:08:39.210
parameter that's being optimized is

0:08:37.320,0:08:42.630
actually itself the output of some other

0:08:39.210,0:08:45.060
function possibly parametrized in this

0:08:42.630,0:08:46.530
case this function is no parameterized

0:08:45.060,0:08:48.270
function or it's a branch wise function

0:08:46.530,0:08:51.200
but the only input is another parameter

0:08:48.270,0:08:53.930
you okay so essentially

0:08:51.200,0:08:56.150
what we've done here is made the weights

0:08:53.930,0:08:58.520
of that neural net the function of

0:08:56.150,0:09:02.440
someone inventory some more elementary

0:08:58.520,0:09:05.300
parameters you through a function and

0:09:02.440,0:09:07.010
you realize really quickly that that

0:09:05.300,0:09:10.250
caucus works there right if you back

0:09:07.010,0:09:11.870
propagate gradients for you know through

0:09:10.250,0:09:14.120
the the G function to get the gradient

0:09:11.870,0:09:16.850
of whatever objective function realizing

0:09:14.120,0:09:18.800
with respect to the white parameters you

0:09:16.850,0:09:20.780
can keep back propagating through the H

0:09:18.800,0:09:25.030
function here to get the gradients with

0:09:20.780,0:09:31.720
respect to you so in the end you sort of

0:09:25.030,0:09:34.160
you know propagating things like this so

0:09:31.720,0:09:36.050
when you're auditing you you're

0:09:34.160,0:09:37.700
multiplying the Jacobian of the

0:09:36.050,0:09:43.370
objective function with respect to the

0:09:37.700,0:09:45.500
parameters and then by the Jacobian of

0:09:43.370,0:09:47.720
the H function with respect to its own

0:09:45.500,0:09:49.640
parameters okay so you get the product

0:09:47.720,0:09:51.290
of two jacobians here which is the just

0:09:49.640,0:09:52.460
what you get from back propagating you

0:09:51.290,0:09:54.050
don't have to do anything in pi torch

0:09:52.460,0:09:59.960
for this as well happen automatically as

0:09:54.050,0:10:02.330
you define the network and and that's

0:09:59.960,0:10:06.710
kind of the old date that that occurs

0:10:02.330,0:10:09.470
now of course W being a function of U

0:10:06.710,0:10:13.910
through the function H the the change in

0:10:09.470,0:10:15.590
W will will be the change in U

0:10:13.910,0:10:19.220
multiplied by the Jacobian of H

0:10:15.590,0:10:22.310
transpose and so this is the kind of

0:10:19.220,0:10:24.860
thing you get here the effective change

0:10:22.310,0:10:28.310
in W that you get without updating W you

0:10:24.860,0:10:31.790
actually are dating you is the OData you

0:10:28.310,0:10:33.770
multiplied by the Jacobian of H and you

0:10:31.790,0:10:36.350
know we had a transpose here we don't we

0:10:33.770,0:10:40.160
have the opposite there this is a square

0:10:36.350,0:10:41.860
matrix which is n w by n W which is the

0:10:40.160,0:10:46.850
number of the dimension of W squared

0:10:41.860,0:10:51.020
okay so this matrix here has as many

0:10:46.850,0:10:52.370
rows as W has components and then the

0:10:51.020,0:10:54.770
number of columns is the number of

0:10:52.370,0:10:55.970
components of U and then this guy of

0:10:54.770,0:10:58.520
course is the other way around

0:10:55.970,0:11:00.320
so it's an u by n W so when you probably

0:10:58.520,0:11:01.920
make the product do the product of those

0:11:00.320,0:11:04.560
two matrices you get an N W

0:11:01.920,0:11:07.920
interview matrix and then you multiply

0:11:04.560,0:11:09.450
this by this NW vector and you get an NW

0:11:07.920,0:11:14.190
vector which which is what you need for

0:11:09.450,0:11:15.660
dating the weights okay so that's kind

0:11:14.190,0:11:18.990
of a general form of transforming the

0:11:15.660,0:11:21.570
parameter space and there's you know

0:11:18.990,0:11:24.889
many ways you can use this and a

0:11:21.570,0:11:27.690
particular way of using it is when H is

0:11:24.889,0:11:29.670
what's called a you know what we talked

0:11:27.690,0:11:31.800
about last week which is a it's a white

0:11:29.670,0:11:34.290
connector so imagine the only thing that

0:11:31.800,0:11:36.029
H does is that it takes one component of

0:11:34.290,0:11:38.430
U and it copies it multiple times

0:11:36.029,0:11:40.529
so that you have the same value the same

0:11:38.430,0:11:42.779
weight replicated across the G function

0:11:40.529,0:11:46.889
the G function we use the same value

0:11:42.779,0:11:49.860
multiple times so let's say this would

0:11:46.889,0:11:53.639
look like this so let's imagine you is

0:11:49.860,0:11:56.970
two dimensional u1 u2 and then W is four

0:11:53.639,0:12:01.560
dimensional but w1 and w2 are equal to

0:11:56.970,0:12:03.600
u1 and W 3 w 4 are equal to u2 so

0:12:01.560,0:12:05.160
basically you only have two free

0:12:03.600,0:12:06.959
parameters and when you're changing one

0:12:05.160,0:12:09.600
component of you changing two components

0:12:06.959,0:12:11.820
of W at the same time in a very simple

0:12:09.600,0:12:14.279
manner and that's called weight sharing

0:12:11.820,0:12:15.750
okay when two weights are forced to be

0:12:14.279,0:12:18.029
equal they actually afford they are

0:12:15.750,0:12:20.070
actually equal to a momentary parameter

0:12:18.029,0:12:24.170
that controls both that's where sharing

0:12:20.070,0:12:26.339
and that's kind of the basis of a lot of

0:12:24.170,0:12:29.490
ideas you know commercial Nets among

0:12:26.339,0:12:30.990
others but but but that that you can

0:12:29.490,0:12:35.610
think of this as a very very simple form

0:12:30.990,0:12:37.800
of H of U so again you don't need to do

0:12:35.610,0:12:39.959
anything for this in the sense that when

0:12:37.800,0:12:42.540
you have which sharing if you do it

0:12:39.959,0:12:44.220
explicitly with a module that does kind

0:12:42.540,0:12:45.810
of a white connection on the way back

0:12:44.220,0:12:48.390
when the ingredients are back propagated

0:12:45.810,0:12:50.040
the the gradients are summed up so the

0:12:48.390,0:12:51.930
gradient of some cost function with

0:12:50.040,0:12:53.699
respect to u 1 for example will be the

0:12:51.930,0:12:56.360
sum of the gradient so that consumption

0:12:53.699,0:12:59.459
with respect to W 1 and W 2 and

0:12:56.360,0:13:01.019
similarly for the gradient respect to u

0:12:59.459,0:13:03.959
2 would be the sum of the gradients with

0:13:01.019,0:13:05.370
respect to W 3 and W 4 ok that's just

0:13:03.959,0:13:08.329
the effect of that propagating through

0:13:05.370,0:13:08.329
the two white connectors

0:13:12.810,0:13:17.110
okay here is a slightly more general

0:13:14.920,0:13:18.730
view of this parameter transformation

0:13:17.110,0:13:22.630
here that some people have called hyper

0:13:18.730,0:13:25.810
networks so hyper network is a network

0:13:22.630,0:13:27.970
where the weights of one network are

0:13:25.810,0:13:30.069
computed as the output of another

0:13:27.970,0:13:33.250
network okay so you have a network H

0:13:30.069,0:13:36.910
that looks at the input it has its own

0:13:33.250,0:13:40.500
parameters U and it computes the weights

0:13:36.910,0:13:43.630
of a second network okay so the

0:13:40.500,0:13:45.850
advantage of doing this are these values

0:13:43.630,0:13:48.040
name for it the idea is very old goes

0:13:45.850,0:13:49.690
back to the 80s people using what's

0:13:48.040,0:13:51.399
called multiplicative interactions or

0:13:49.690,0:13:55.209
three-way network with signal PI units

0:13:51.399,0:13:57.550
and they're basically this idea and this

0:13:55.209,0:14:02.079
is maybe a slightly more general general

0:13:57.550,0:14:05.199
formulation of it that you have sort of

0:14:02.079,0:14:09.550
a dynamically your function that's going

0:14:05.199,0:14:12.550
to dynamically defined in in G of X and

0:14:09.550,0:14:15.689
W because W is really a complex function

0:14:12.550,0:14:18.459
of the input and some other parameter

0:14:15.689,0:14:20.380
this is particularly interesting

0:14:18.459,0:14:22.120
architecture when what you're doing what

0:14:20.380,0:14:24.639
you're doing to X is transforming it in

0:14:22.120,0:14:27.189
some ways right so the you can think of

0:14:24.639,0:14:28.870
W as being the parameters of that

0:14:27.189,0:14:34.750
transformation so Y would be a

0:14:28.870,0:14:36.610
transformed version of X and and the X I

0:14:34.750,0:14:39.730
mean the the function H basically

0:14:36.610,0:14:43.329
computes that transformation okay but

0:14:39.730,0:14:44.529
welcome back to that in a few weeks just

0:14:43.329,0:14:47.079
wanted to mention this because it's

0:14:44.529,0:14:48.850
basically a small modification of of

0:14:47.079,0:14:51.550
this right you just have one more wire

0:14:48.850,0:14:54.870
that goes from X to H and that's how you

0:14:51.550,0:14:54.870
get those hyper networks

0:14:56.069,0:15:02.130
okay so we're showing that's you know

0:14:59.889,0:15:07.630
the idea that you can have one parameter

0:15:02.130,0:15:10.540
controlling multiple multiple projects

0:15:07.630,0:15:14.649
50 parameters in another network and one

0:15:10.540,0:15:18.279
reason that's useful is if you want to

0:15:14.649,0:15:19.809
detect a motif on an input and you want

0:15:18.279,0:15:21.939
to detect this motif regardless of where

0:15:19.809,0:15:22.420
the peers okay so let's say you have an

0:15:21.939,0:15:24.760
input

0:15:22.420,0:15:26.500
sequence but it could be a cool you be

0:15:24.760,0:15:28.930
an image this case is a sequence

0:15:26.500,0:15:31.329
sequence of vectors let's say and you

0:15:28.930,0:15:32.829
have a network that takes a collection

0:15:31.329,0:15:36.510
of three of those vacu three successive

0:15:32.829,0:15:39.160
vectors this network G of X and W and

0:15:36.510,0:15:41.620
it's tried to detect a particular motif

0:15:39.160,0:15:44.250
of those three vectors maybe this is I

0:15:41.620,0:15:46.089
don't know the power consumption

0:15:44.250,0:15:48.519
electrical power consumption and

0:15:46.089,0:15:51.100
sometimes you know you might want to be

0:15:48.519,0:15:53.649
able to detect like a blip or a trend or

0:15:51.100,0:15:58.649
something like that or maybe it's you

0:15:53.649,0:16:01.149
know financial instruments of some kind

0:15:58.649,0:16:02.709
some sort of time series maybe it's a

0:16:01.149,0:16:05.550
speech signal and you want to detect a

0:16:02.709,0:16:08.440
particular sound that consists in three

0:16:05.550,0:16:10.510
vectors that define the the sort of

0:16:08.440,0:16:15.010
audio content of that of that speech

0:16:10.510,0:16:17.320
signal and so you'd like to be able to

0:16:15.010,0:16:18.790
detect if it's a speech signal and

0:16:17.320,0:16:20.769
there's a particular Sonya you need to

0:16:18.790,0:16:22.680
detect for during speech recognition you

0:16:20.769,0:16:26.410
might want to detect you know the sound

0:16:22.680,0:16:28.810
the vowel P right the the sound P

0:16:26.410,0:16:30.130
wherever it occurs in a sequence you

0:16:28.810,0:16:34.870
want some you know some detector that

0:16:30.130,0:16:36.670
fires when the sound P is is pronounced

0:16:34.870,0:16:38.769
and so it should like to have is

0:16:36.670,0:16:41.260
detector you can you can slide over

0:16:38.769,0:16:44.170
right and regardless of where this motif

0:16:41.260,0:16:46.329
occurs detected so what you need to have

0:16:44.170,0:16:49.899
is some sun networks and prioritize

0:16:46.329,0:16:51.279
function that you know you have multiple

0:16:49.899,0:16:53.740
copies of that function that you can

0:16:51.279,0:16:55.209
apply to various regions on the input

0:16:53.740,0:16:56.680
and they all share the same weight but

0:16:55.209,0:17:01.959
you'd like to train this entire system

0:16:56.680,0:17:05.069
and to n so for example let's say let's

0:17:01.959,0:17:14.339
talk about starting more sophisticated

0:17:05.069,0:17:17.669
scene here where you have let's see a

0:17:14.339,0:17:20.919
keyword that's being being pronounced so

0:17:17.669,0:17:21.880
the the system listens to sound and

0:17:20.919,0:17:26.020
wants to detect when a particular

0:17:21.880,0:17:28.329
keyword a wakeup word has been as been

0:17:26.020,0:17:31.659
pronounced right so this is Alexa right

0:17:28.329,0:17:33.900
and you say Alexa and exit wakes up it

0:17:31.659,0:17:37.050
goes bong right

0:17:33.900,0:17:39.330
so what you like to have is you know

0:17:37.050,0:17:41.390
some network that kind of takes a window

0:17:39.330,0:17:43.740
over the sound and then sort of keeps

0:17:41.390,0:17:46.110
you know in the background sort of

0:17:43.740,0:17:48.090
detecting but you'd like to be able to

0:17:46.110,0:17:50.070
detect you know wherever the sound

0:17:48.090,0:17:51.600
occurs within the frame that is being

0:17:50.070,0:17:53.670
looked at who's been listened to I

0:17:51.600,0:17:55.290
should say so you can have a network

0:17:53.670,0:17:57.179
like this where you have you know

0:17:55.290,0:17:59.220
replicated detectors they all share the

0:17:57.179,0:18:00.870
same weight and then the output which is

0:17:59.220,0:18:02.700
you know the score as to whether

0:18:00.870,0:18:04.980
something has been detected on that goes

0:18:02.700,0:18:07.200
to a max function okay and that's the

0:18:04.980,0:18:09.480
output and the way you train a system

0:18:07.200,0:18:13.350
like this you know you will have a bunch

0:18:09.480,0:18:15.630
of samples are you examples where the

0:18:13.350,0:18:17.040
key word has been pronounced and a bunch

0:18:15.630,0:18:19.500
of audio samples with the key word was

0:18:17.040,0:18:22.230
not pronounced and then you train a 2

0:18:19.500,0:18:24.210
class classifier turn on when a tech

0:18:22.230,0:18:26.790
size somewhere in this frame to an art

0:18:24.210,0:18:28.950
when it's not but nobody tells you where

0:18:26.790,0:18:30.900
the word lxl occurs within the the

0:18:28.950,0:18:32.130
window that you train the system on okay

0:18:30.900,0:18:34.050
because it's really expensive for

0:18:32.130,0:18:36.059
laborers to like look at the audio

0:18:34.050,0:18:37.559
signal and tell you exactly well this is

0:18:36.059,0:18:39.150
the word is Alexa is being pronounced

0:18:37.559,0:18:40.380
the only thing they know is that you

0:18:39.150,0:18:42.390
know within this segment of a few

0:18:40.380,0:18:45.000
seconds the word has been pronounced

0:18:42.390,0:18:46.950
somewhere okay so you'd like to apply a

0:18:45.000,0:18:49.140
network like this that has those

0:18:46.950,0:18:51.000
replicated detectors you don't know

0:18:49.140,0:18:52.500
exactly where it is but you run through

0:18:51.000,0:18:54.960
this max and you want to train the

0:18:52.500,0:18:56.820
system to you know you want to back

0:18:54.960,0:19:00.030
propagate gradient to it so that it

0:18:56.820,0:19:04.830
learns to detect you know Alexa whatever

0:19:00.030,0:19:06.990
wake up word occurs and so they are what

0:19:04.830,0:19:09.210
happens is you have those multiple

0:19:06.990,0:19:10.950
copies five copies in this in this

0:19:09.210,0:19:12.660
example of this network and they all

0:19:10.950,0:19:15.300
share the same weight you can see this

0:19:12.660,0:19:18.270
as just one weight vector is sending its

0:19:15.300,0:19:19.850
value to five different instances of the

0:19:18.270,0:19:23.760
same network and so we back propagate

0:19:19.850,0:19:25.950
back propagate through the to the five

0:19:23.760,0:19:29.179
copies of the network you get five

0:19:25.950,0:19:32.309
radians so those gradients get added up

0:19:29.179,0:19:34.380
for the parameter now this is a slightly

0:19:32.309,0:19:36.840
strange way this is implemented in pie

0:19:34.380,0:19:39.390
charts and other deep running framework

0:19:36.840,0:19:41.340
which is that this accumulation or

0:19:39.390,0:19:43.860
gradient in a single parameter is done

0:19:41.340,0:19:45.610
implicitly and it's one reason why

0:19:43.860,0:19:47.340
before you do a back project or

0:19:45.610,0:19:50.010
you have to zero add the gradient

0:19:47.340,0:19:51.880
because there's sort of implicit

0:19:50.010,0:20:00.100
accumulation of gradients when you do

0:19:51.880,0:20:02.140
back propagation okay so here's another

0:20:00.100,0:20:05.260
situation where that would be useful and

0:20:02.140,0:20:06.730
this is for really the real motivation

0:20:05.260,0:20:10.350
behind conditional Nets in the first

0:20:06.730,0:20:13.420
place which is the the problem of

0:20:10.350,0:20:16.510
training system to recognize the shape

0:20:13.420,0:20:18.910
independently of the position of whether

0:20:16.510,0:20:21.059
the shape occurs and whether there are

0:20:18.910,0:20:25.660
distortions of that shape in the input

0:20:21.059,0:20:27.850
so here this is a very simple type of

0:20:25.660,0:20:29.650
convolutional net that is has been built

0:20:27.850,0:20:33.040
by hand it's not being trained okay it's

0:20:29.650,0:20:35.679
been designed by hand and it's designed

0:20:33.040,0:20:38.290
explicitly to distinguish scenes from

0:20:35.679,0:20:41.320
genes okay so you can draw a C on the

0:20:38.290,0:20:44.980
input image which is you know very low

0:20:41.320,0:20:48.100
resolution and what this equation is

0:20:44.980,0:20:49.900
from these is that C's have n points

0:20:48.100,0:20:51.820
right there is the the stroke kind of

0:20:49.900,0:20:54.190
ends and you can imagine designing a

0:20:51.820,0:20:57.160
detector for that whereas these have

0:20:54.190,0:20:58.600
corners so if you have an endpoint

0:20:57.160,0:21:01.830
detector or something that detects the

0:20:58.600,0:21:04.299
end of a segment and a corner detector

0:21:01.830,0:21:06.250
wherever you have corners are detected

0:21:04.299,0:21:08.710
it's a C it's a D and whatever wherever

0:21:06.250,0:21:14.230
you have segments that end

0:21:08.710,0:21:16.540
it's a C so here's an example of over C

0:21:14.230,0:21:19.450
you take the first detector so the

0:21:16.540,0:21:23.230
little black and white would he's here

0:21:19.450,0:21:26.230
at the top is an endpoint detector okay

0:21:23.230,0:21:30.240
it's it detects the end of a of a

0:21:26.230,0:21:33.250
segment and the way this this is

0:21:30.240,0:21:37.390
represented here is that the the black

0:21:33.250,0:21:39.850
pixels here so think of this as some

0:21:37.390,0:21:41.080
sort of template okay you're going to

0:21:39.850,0:21:45.010
take this template are you going to

0:21:41.080,0:21:47.200
swipe it over the input image and you're

0:21:45.010,0:21:49.809
going to compare that template to the

0:21:47.200,0:21:53.919
little image that is placed underneath

0:21:49.809,0:21:55.270
okay and if it's to match the way you're

0:21:53.919,0:21:56.010
going to determine whether they match is

0:21:55.270,0:21:57.690
that you're going to do

0:21:56.010,0:22:00.150
products so you're gonna think of those

0:21:57.690,0:22:01.740
back in white pixels as value of plus

0:22:00.150,0:22:05.610
one or minus one say plus one for Greg

0:22:01.740,0:22:07.590
minus one for for white and you're gonna

0:22:05.610,0:22:09.240
think of those pixels also as being plus

0:22:07.590,0:22:11.910
one for blacks and minus one for white

0:22:09.240,0:22:17.100
and when you compute the dot product of

0:22:11.910,0:22:20.220
a little window with that template if

0:22:17.100,0:22:21.630
they are similar you're gonna you're

0:22:20.220,0:22:24.510
gonna get a large positive value with

0:22:21.630,0:22:28.520
that dissimilar you're gonna get a zero

0:22:24.510,0:22:32.160
or negative value or smaller value okay

0:22:28.520,0:22:33.810
so you take that little detector here

0:22:32.160,0:22:35.370
and you compute the dot product with the

0:22:33.810,0:22:37.260
first window of second window that we

0:22:35.370,0:22:39.840
know etc you shift by one pixel every

0:22:37.260,0:22:41.970
time for every location and you recall

0:22:39.840,0:22:44.640
the result and what you what you get is

0:22:41.970,0:22:49.700
this right so this is here the the

0:22:44.640,0:22:52.800
grayscale is an indication of the

0:22:49.700,0:22:54.990
matching which is actually the dot

0:22:52.800,0:22:57.960
product between the vector of formed by

0:22:54.990,0:23:00.360
those values and the vac and the the

0:22:57.960,0:23:03.420
patch of the corresponding location on

0:23:00.360,0:23:07.140
the input so this image here is roughly

0:23:03.420,0:23:10.610
the same size as that image - border

0:23:07.140,0:23:13.230
effects and which is you see there is a

0:23:10.610,0:23:16.310
whenever the output is dark there is a

0:23:13.230,0:23:18.900
match so when you see a match here

0:23:16.310,0:23:23.160
because this endpoint detector here

0:23:18.900,0:23:24.660
matches the you know the endpoint you

0:23:23.160,0:23:27.720
see sort of a match here at the bottom

0:23:24.660,0:23:30.060
and and the other kind of values are not

0:23:27.720,0:23:33.870
as dark okay

0:23:30.060,0:23:36.180
not as strong if you want now if you

0:23:33.870,0:23:38.490
threshold those those values you set the

0:23:36.180,0:23:41.570
output to plus one if if it's about the

0:23:38.490,0:23:43.800
special zero if it's below the threshold

0:23:41.570,0:23:45.420
you get those maps here to that the

0:23:43.800,0:23:47.550
threshold appropriately but what you get

0:23:45.420,0:23:50.310
is that you know this little guy here

0:23:47.550,0:23:53.370
detected a match at the two end points

0:23:50.310,0:23:57.120
of the see okay so now if you take this

0:23:53.370,0:24:00.270
map you saw me sum it up just add all

0:23:57.120,0:24:01.950
the values you get a positive number

0:24:00.270,0:24:03.540
that's a through threshold and that's

0:24:01.950,0:24:05.340
your C detector it's not a very good C

0:24:03.540,0:24:06.930
detector it's not a very good detector

0:24:05.340,0:24:08.429
of anything but for those particular

0:24:06.930,0:24:11.249
examples of C's

0:24:08.429,0:24:13.019
and maybe those these it will work

0:24:11.249,0:24:14.850
you'll be enough now for the D is

0:24:13.019,0:24:17.179
similar those other detectors here are

0:24:14.850,0:24:20.730
meant to detect the corners of the D

0:24:17.179,0:24:22.860
right so this guy here this detector as

0:24:20.730,0:24:27.659
you swipe it over the the input will

0:24:22.860,0:24:29.159
will detect the upper left corner and

0:24:27.659,0:24:31.529
that guy will detect the lower right

0:24:29.159,0:24:33.210
corner once you threshold you will get

0:24:31.529,0:24:36.149
those two maps where the corners are

0:24:33.210,0:24:39.330
detected and then you can sum those up

0:24:36.149,0:24:41.369
and the the detector will turn on now

0:24:39.330,0:24:43.950
what you see here is an example of why

0:24:41.369,0:24:45.899
this is good because that detection now

0:24:43.950,0:24:48.119
is shift invariance so if I take the

0:24:45.899,0:24:50.340
same input D here and I've shifted by a

0:24:48.119,0:24:52.139
couple pixels

0:24:50.340,0:24:55.259
and I ran this detector again it will

0:24:52.139,0:24:57.990
detect the motif wherever there here the

0:24:55.259,0:25:00.480
output will be shifted okay so this is

0:24:57.990,0:25:04.080
called equivalence to shift so the

0:25:00.480,0:25:05.610
output of that network is equivalent to

0:25:04.080,0:25:07.350
shift which means that if I shift the

0:25:05.610,0:25:09.749
input the output gets shifted but

0:25:07.350,0:25:13.710
otherwise unchanged okay that's

0:25:09.749,0:25:15.299
equivalence invariance would be if I

0:25:13.710,0:25:16.919
shift it into the output will be

0:25:15.299,0:25:19.019
completely unchanged but here it is

0:25:16.919,0:25:25.769
modified it just went inside the same

0:25:19.019,0:25:28.080
way as the input and so if I just sum up

0:25:25.769,0:25:30.629
the activate the activities in the

0:25:28.080,0:25:33.389
future Mads here it doesn't matter where

0:25:30.629,0:25:36.480
they occur my ID detector will still

0:25:33.389,0:25:38.309
will still activate right if I just

0:25:36.480,0:25:41.070
compute the sum so this is sort of a

0:25:38.309,0:25:43.649
handcrafted pattern recognizer that use

0:25:41.070,0:25:45.119
users local feature detectors and then

0:25:43.649,0:25:48.210
kind of sums up their activity and what

0:25:45.119,0:25:50.309
you get is an invariant detection okay

0:25:48.210,0:25:51.659
this is a very classical way actually of

0:25:50.309,0:25:54.210
building certain types of pattern

0:25:51.659,0:25:59.249
recognition systems going back many

0:25:54.210,0:26:00.539
years but the trick here what's

0:25:59.249,0:26:02.309
important of course what's interesting

0:26:00.539,0:26:05.909
would be to learn those those those

0:26:02.309,0:26:08.309
templates you know can we you know can

0:26:05.909,0:26:09.929
we view this as just a neural net and we

0:26:08.309,0:26:14.669
back propagate to it and we learn those

0:26:09.929,0:26:16.320
templates as weights of a neural net you

0:26:14.669,0:26:17.789
know after all we're using them to do

0:26:16.320,0:26:21.710
that product which are weighted sum so

0:26:17.789,0:26:25.760
you know basically the

0:26:21.710,0:26:28.460
this layer here to go from the input to

0:26:25.760,0:26:31.580
those so-called future maps that are

0:26:28.460,0:26:32.750
weighted sums is a linear operation okay

0:26:31.580,0:26:37.220
and we know how to back propagate

0:26:32.750,0:26:39.020
through that we'd have to use you know a

0:26:37.220,0:26:40.580
kind of a soft threshold value with

0:26:39.020,0:26:44.180
something like this here because

0:26:40.580,0:26:46.820
otherwise we can do back cause okay so

0:26:44.180,0:26:49.880
this operation here of taking the dot

0:26:46.820,0:26:52.310
product of a bunch of coefficients with

0:26:49.880,0:26:58.880
an input window and then swiping it over

0:26:52.310,0:27:00.200
that's a convolution okay so that's the

0:26:58.880,0:27:03.020
definition of a convolution it's

0:27:00.200,0:27:07.760
actually the one up there so this is in

0:27:03.020,0:27:14.920
a one dimensional case where imagine you

0:27:07.760,0:27:25.070
have an input X J so X indexed by the J

0:27:14.920,0:27:29.690
in the index you take a window of X at a

0:27:25.070,0:27:35.360
particular location I okay and then you

0:27:29.690,0:27:38.510
sum you do a weighted sum of the window

0:27:35.360,0:27:42.110
of the X values and you multiply those

0:27:38.510,0:27:45.790
by the weights W J's okay and the Sun

0:27:42.110,0:27:48.950
presumably runs over a kind of a small

0:27:45.790,0:27:52.220
small window so J here would go from 1

0:27:48.950,0:27:53.990
to I don't know 5 something like that

0:27:52.220,0:27:59.420
which is the case in the little example

0:27:53.990,0:28:04.430
I showed earlier okay and that gives you

0:27:59.420,0:28:07.400
one why I okay so take the first window

0:28:04.430,0:28:09.380
of five values of X computer weighted

0:28:07.400,0:28:12.110
sum with the weights that gives you y1

0:28:09.380,0:28:14.660
then shifts that window by one compute

0:28:12.110,0:28:16.640
the weighted sum of the dot product of

0:28:14.660,0:28:23.840
that window by the devil use that gives

0:28:16.640,0:28:25.129
you Y to shift again etc okay now

0:28:23.840,0:28:27.109
in fact in practice where people

0:28:25.129,0:28:29.239
implement and things like by torch there

0:28:27.109,0:28:31.070
is a confusion between two things that

0:28:29.239,0:28:32.629
mathematicians think are very different

0:28:31.070,0:28:34.909
but in fact they're pretty much the same

0:28:32.629,0:28:36.559
its composition and cross correlation so

0:28:34.909,0:28:40.999
in composition the convention is that

0:28:36.559,0:28:42.499
the the index goes backwards in the in

0:28:40.999,0:28:44.929
the window when it goes forwards in the

0:28:42.499,0:28:46.580
weights in cross correlation they both

0:28:44.929,0:28:48.559
go forward in the end it's just a

0:28:46.580,0:28:51.519
convention you know it depends on how

0:28:48.559,0:28:51.519
you lay you know

0:28:51.529,0:28:55.460
organize the data in your weights you

0:28:53.809,0:28:58.099
can interpret this as a convolution if

0:28:55.460,0:29:01.820
you read the weights backwards so

0:28:58.099,0:29:03.559
readers make any difference but for

0:29:01.820,0:29:04.969
certain mathematical property is a

0:29:03.559,0:29:07.940
convolution if you want everything to be

0:29:04.969,0:29:09.950
consistent you you have to have the the

0:29:07.940,0:29:13.039
J in the W having an opposite sign to

0:29:09.950,0:29:18.289
the J in the X so the two dimensional

0:29:13.039,0:29:21.049
version of this if you have an image X

0:29:18.289,0:29:25.339
that has two indices in this case I and

0:29:21.049,0:29:27.169
J you do a weighted sum over two indices

0:29:25.339,0:29:28.940
K and L and so you have a window a

0:29:27.169,0:29:31.369
two-dimensional window index by K and L

0:29:28.940,0:29:36.019
and you compute the dot product of that

0:29:31.369,0:29:39.169
window over X with the weight and that

0:29:36.019,0:29:44.809
gives you one value in y IJ which is the

0:29:39.169,0:29:47.899
the output so the the vector W or the

0:29:44.809,0:29:50.479
matrix W in this in the 2d version there

0:29:47.899,0:29:54.289
is obvious extensions of this to 3-d and

0:29:50.479,0:29:57.639
4-d etc is called a kernel it's got a

0:29:54.289,0:29:57.639
competition kernel okay

0:29:59.880,0:30:12.610
so clear I'm sure this is known to many

0:30:02.799,0:30:15.490
of you but so what we're going to do

0:30:12.610,0:30:18.010
with this is that we're going to

0:30:15.490,0:30:21.970
organize you know build a network as a

0:30:18.010,0:30:24.840
succession of coalition's where you know

0:30:21.970,0:30:27.940
you know regular neural net you have

0:30:24.840,0:30:29.620
alternation of linear operators and

0:30:27.940,0:30:30.700
point wise non-linearity in

0:30:29.620,0:30:32.320
convolutional nets we're going to have

0:30:30.700,0:30:33.730
an alternation of linear operators that

0:30:32.320,0:30:35.980
will happen to be compositions or

0:30:33.730,0:30:37.690
multiple collisions then also a point

0:30:35.980,0:30:39.850
where is non-linearity and there's going

0:30:37.690,0:30:44.970
to be a third type of operation called

0:30:39.850,0:30:49.240
pooling which is actually optional

0:30:44.970,0:30:53.049
before before I go further I should

0:30:49.240,0:30:55.899
mention that there are twists you can

0:30:53.049,0:30:57.190
make to this completion so one twist is

0:30:55.899,0:30:59.440
what's called a stride

0:30:57.190,0:31:03.610
so it's tried to do composition consists

0:30:59.440,0:31:05.679
in giving the window from one you know

0:31:03.610,0:31:07.510
from one position to another instead of

0:31:05.679,0:31:11.019
moving it moving it by just one value

0:31:07.510,0:31:13.240
you move it by two or three or four okay

0:31:11.019,0:31:16.210
let's call this tried of a convolution

0:31:13.240,0:31:20.799
and so if you have an input of a certain

0:31:16.210,0:31:23.169
length and so let's say you have an

0:31:20.799,0:31:28.559
input which is kind of a one-dimensional

0:31:23.169,0:31:34.409
and size 100 and you have a combination

0:31:28.559,0:31:40.120
kernel size five okay and you convolve

0:31:34.409,0:31:42.700
the this kernel with the input and you

0:31:40.120,0:31:47.860
make sure that the window stays within

0:31:42.700,0:31:51.190
the input of size 100 the output you get

0:31:47.860,0:31:54.580
has 96 outputs okay it's got the number

0:31:51.190,0:31:59.080
of inputs - the size of the kernel which

0:31:54.580,0:32:02.889
is 5 minus 1 okay so that makes it 4 so

0:31:59.080,0:32:06.100
you get 100 minus 4 that's 96 that's the

0:32:02.889,0:32:08.350
number of you know windows of size 5

0:32:06.100,0:32:10.710
that fit within this big input of size

0:32:08.350,0:32:10.710
100

0:32:11.260,0:32:18.170
now if I use this tried so what I do now

0:32:14.660,0:32:20.210
is I take my window f5 where I applied

0:32:18.170,0:32:23.030
the kernel and I shift it not by one

0:32:20.210,0:32:25.810
pixel but by 2 pixels or two values

0:32:23.030,0:32:27.980
let's say they're not necessarily pixels

0:32:25.810,0:32:31.310
okay the number of that points I'm gonna

0:32:27.980,0:32:34.250
get is gonna be you know divided by by

0:32:31.310,0:32:36.580
two roughly okay

0:32:34.250,0:32:40.760
instead of 96 I'm gonna have you know a

0:32:36.580,0:32:42.800
little less than 50 now 48 or something

0:32:40.760,0:32:48.260
like that the number is not exact you

0:32:42.800,0:32:49.880
can figure out in your head very often

0:32:48.260,0:32:51.710
when people run collisions in commercial

0:32:49.880,0:32:53.870
Nets they actually pad the completion so

0:32:51.710,0:32:56.570
they sometimes like to have the output

0:32:53.870,0:32:59.090
being the same size as the input and so

0:32:56.570,0:33:01.820
they actually displace the input window

0:32:59.090,0:33:04.970
past the end of the vector assuming that

0:33:01.820,0:33:17.330
it's padded with zeros usually on both

0:33:04.970,0:33:19.670
sides does it have any effect on

0:33:17.330,0:33:22.790
performance or is it just for

0:33:19.670,0:33:25.460
convenience if it has an effect on

0:33:22.790,0:33:29.510
performance is bad okay very nice

0:33:25.460,0:33:34.760
convenient that's pretty much here

0:33:29.510,0:33:36.410
answer the the assumption that's bad is

0:33:34.760,0:33:38.390
assuming that when you don't have data

0:33:36.410,0:33:40.160
its equal to zero so when you're

0:33:38.390,0:33:44.150
nonlinearities are you it's not

0:33:40.160,0:33:46.370
necessarily completely unreasonable but

0:33:44.150,0:33:51.850
it sometimes creates like funny border

0:33:46.370,0:33:56.900
effects you know boundary effects okay

0:33:51.850,0:34:01.550
everything clear so far right okay so

0:33:56.900,0:34:01.730
what we're going to build is a neuron

0:34:01.550,0:34:04.970
that

0:34:01.730,0:34:06.980
composed of those coalition's that are

0:34:04.970,0:34:09.740
going to be used as feature detectors

0:34:06.980,0:34:11.570
local feature detectors followed by

0:34:09.740,0:34:15.070
nonlinearities and then we're gonna

0:34:11.570,0:34:17.510
stack multiple layers of those okay and

0:34:15.070,0:34:21.590
the reason for stacking multiple layers

0:34:17.510,0:34:23.589
is because we want to build hierarchical

0:34:21.590,0:34:26.679
representations of the

0:34:23.589,0:34:28.659
of the visual world of the data it's not

0:34:26.679,0:34:30.579
conditional ads are not necessarily

0:34:28.659,0:34:32.799
applied to images they can be applied to

0:34:30.579,0:34:34.419
speech and all the signals they

0:34:32.799,0:34:35.499
basically can be applied to any signal

0:34:34.419,0:34:39.099
that comes to you in the form of an

0:34:35.499,0:34:41.349
array and I'll come back to on to the

0:34:39.099,0:34:47.139
the properties that this array has to

0:34:41.349,0:34:48.039
verify so what you want is why do you

0:34:47.139,0:34:49.929
want to build hierarchical

0:34:48.039,0:34:52.479
representations because the the wall

0:34:49.929,0:34:53.709
disk is compositional and I alluded to

0:34:52.479,0:34:57.640
this I think you the first lecture if

0:34:53.709,0:35:02.229
remember correctly the fact that you

0:34:57.640,0:35:04.930
know pixels assemble to form simple

0:35:02.229,0:35:07.509
motifs like oriented edges oriented

0:35:04.930,0:35:10.509
edges kind of assemble to form local

0:35:07.509,0:35:13.719
features like corners and T junctions

0:35:10.509,0:35:17.380
and things like that gratings you know

0:35:13.719,0:35:19.869
and then those assembled to form motifs

0:35:17.380,0:35:22.269
that are slightly more abstract the

0:35:19.869,0:35:23.709
those assembled to form parts of objects

0:35:22.269,0:35:25.689
and those assembled to form objects so

0:35:23.709,0:35:28.630
there is a sort of natural compositional

0:35:25.689,0:35:31.119
hierarchy in the natural world and this

0:35:28.630,0:35:33.869
natural compositional hierarchy in the

0:35:31.119,0:35:37.329
natural world is not just because of

0:35:33.869,0:35:42.069
perception visual perception is true at

0:35:37.329,0:35:45.670
a physical level right you know you

0:35:42.069,0:35:48.219
started at the lowest level of the

0:35:45.670,0:35:50.229
description you know you have elementary

0:35:48.219,0:35:52.809
particles and they form you know the

0:35:50.229,0:35:54.549
clump to form less elementary particle

0:35:52.809,0:35:56.199
and they come to form atoms and it come

0:35:54.549,0:35:59.259
to form molecules and molecules come to

0:35:56.199,0:36:02.049
form materials and materials you know

0:35:59.259,0:36:04.170
parts of objects and parts of objects

0:36:02.049,0:36:07.739
into objects and things like that right

0:36:04.170,0:36:10.539
or macromolecules or polymer polymers

0:36:07.739,0:36:12.429
and then you have this this natural

0:36:10.539,0:36:17.619
composition or hierarchy the world is

0:36:12.429,0:36:19.509
built this way and it may be why the

0:36:17.619,0:36:22.179
world is understandable right so this is

0:36:19.509,0:36:24.699
famous quote from Einstein that says the

0:36:22.179,0:36:25.539
most incomprehensible thing about the

0:36:24.699,0:36:27.369
world is that the world is

0:36:25.539,0:36:28.869
comprehensible and it seems like a

0:36:27.369,0:36:32.259
conspiracy that we live in a world that

0:36:28.869,0:36:34.059
we are able to comprehend but we can't

0:36:32.259,0:36:36.470
comprehend it because the world is

0:36:34.059,0:36:39.260
compositional and

0:36:36.470,0:36:42.540
you know it happens to be easy to build

0:36:39.260,0:36:44.160
brains in a compositional world that

0:36:42.540,0:36:46.980
actually can interpret compositional

0:36:44.160,0:36:53.150
world it still seems like a conspiracy

0:36:46.980,0:36:56.880
to me so there's a famous quote from

0:36:53.150,0:36:59.130
from a not that famous but somewhat

0:36:56.880,0:37:02.310
famous from a statistician at Brown

0:36:59.130,0:37:04.170
called Stu given and and he says you

0:37:02.310,0:37:09.900
know that sounds like like a conspiracy

0:37:04.170,0:37:11.310
like magic but you know if if the world

0:37:09.900,0:37:13.890
were not compositional we would need

0:37:11.310,0:37:18.780
some some even more magic to be able to

0:37:13.890,0:37:20.970
understand it and he the way he he says

0:37:18.780,0:37:23.540
this is the world is compositional or

0:37:20.970,0:37:23.540
there is a God

0:37:25.140,0:37:31.110
you need to appeal to superior powers if

0:37:29.700,0:37:36.510
the world was not compositional to

0:37:31.110,0:37:39.540
explain how we can understand it ok so

0:37:36.510,0:37:42.180
this idea hierarchy and local feature

0:37:39.540,0:37:43.320
detection comes from biology so the

0:37:42.180,0:37:46.890
whole idea of conversation that comes

0:37:43.320,0:37:50.370
from biology has been so inspired by

0:37:46.890,0:37:52.620
biology and what you see here on the on

0:37:50.370,0:37:57.180
the right is a diagram by Simon thought

0:37:52.620,0:37:59.100
with psychophysicists and did some

0:37:57.180,0:38:02.190
relatively famous experiments where he

0:37:59.100,0:38:04.440
showed that the the way we recognize

0:38:02.190,0:38:08.400
everyday objects seems to be extremely

0:38:04.440,0:38:10.610
fast so if you show you flash the image

0:38:08.400,0:38:13.980
of an everyday object to a person and

0:38:10.610,0:38:15.650
you flash one of them every 100

0:38:13.980,0:38:19.740
milliseconds or so you realize that the

0:38:15.650,0:38:21.360
the the time it takes for a person to

0:38:19.740,0:38:23.130
identify in the wrong sequence whether

0:38:21.360,0:38:28.950
there was a particular object let's say

0:38:23.130,0:38:31.170
Tiger is about 100 milliseconds so the

0:38:28.950,0:38:32.940
time it takes for brain to interpret an

0:38:31.170,0:38:36.360
image and recognize basic objects in

0:38:32.940,0:38:40.590
them is about 100 milliseconds the tenth

0:38:36.360,0:38:43.920
of a second right and that's just about

0:38:40.590,0:38:47.550
the time it takes for the nerve signal

0:38:43.920,0:38:49.890
to propagate from the retina

0:38:47.550,0:38:53.190
where you know image our form on the in

0:38:49.890,0:38:56.850
the eye to what's called the LGN lateral

0:38:53.190,0:38:59.400
geniculate nucleus which is a small you

0:38:56.850,0:39:01.920
know piece of brain that basically those

0:38:59.400,0:39:04.680
contrast enhancement and gain control

0:39:01.920,0:39:06.450
and things like that and then that

0:39:04.680,0:39:09.990
signal goes to the back of your brain v1

0:39:06.450,0:39:12.330
that's the primary visual cortex area in

0:39:09.990,0:39:14.850
humans and then v2 which is very close

0:39:12.330,0:39:19.230
to v1 there's a fold that sort of makes

0:39:14.850,0:39:22.080
v1 sort of right in front of v2 and

0:39:19.230,0:39:24.330
there is lots of wires between them and

0:39:22.080,0:39:26.430
then v4 and then the info temporal

0:39:24.330,0:39:28.380
cortex which is sort of on the side here

0:39:26.430,0:39:30.570
and that's where object categories are

0:39:28.380,0:39:33.240
represented so there are neurons in your

0:39:30.570,0:39:38.850
temporal cortex that represent you know

0:39:33.240,0:39:40.590
general generic object categories and

0:39:38.850,0:39:44.820
you know people have done experiments

0:39:40.590,0:39:47.820
with this where you know epileptic

0:39:44.820,0:39:50.580
patients are in hospital and have this

0:39:47.820,0:39:54.240
skull open because they we need to

0:39:50.580,0:40:02.580
locate the exact position of the source

0:39:54.240,0:40:04.050
of their epilepsy seizures and because

0:40:02.580,0:40:07.320
they have electrodes or the surface of

0:40:04.050,0:40:09.060
their brain you can show the movies and

0:40:07.320,0:40:11.040
then observe if a particular neuron

0:40:09.060,0:40:12.630
turns on for particular movies and you

0:40:11.040,0:40:15.300
show them a movie with Jennifer Aniston

0:40:12.630,0:40:17.850
and there is this neuron that only turns

0:40:15.300,0:40:19.440
on when Jennifer Aniston is there okay

0:40:17.850,0:40:22.620
it doesn't turn on for anything else as

0:40:19.440,0:40:24.600
far as we could tell okay so you seem to

0:40:22.620,0:40:27.000
have very selective neurons in the info

0:40:24.600,0:40:32.040
temporal cortex that react to its small

0:40:27.000,0:40:34.200
number of categories there's a joke kind

0:40:32.040,0:40:36.090
of a running joke in neuroscience of a

0:40:34.200,0:40:38.310
concept called the grandmother sale so

0:40:36.090,0:40:39.750
this is the the one neuron and you're in

0:40:38.310,0:40:41.700
for temporal cortex that turns on when

0:40:39.750,0:40:43.890
you see your grandmother regardless of

0:40:41.700,0:40:47.010
what position what she's wearing how far

0:40:43.890,0:40:48.720
whether it's a photo or not nobody

0:40:47.010,0:40:50.190
really really using this concept where

0:40:48.720,0:40:52.110
people believe in or distributed

0:40:50.190,0:40:53.790
representation so there is no such thing

0:40:52.110,0:40:54.970
as the grand as a cell that just turns

0:40:53.790,0:40:56.710
on for you grandmother

0:40:54.970,0:40:58.720
there are this collection of cells that

0:40:56.710,0:41:00.370
turn on for various things and they

0:40:58.720,0:41:02.290
serve to represent you know general

0:41:00.370,0:41:06.420
categories but the important thing is

0:41:02.290,0:41:08.260
that they are invariant to position size

0:41:06.420,0:41:10.900
illumination all kinds of different

0:41:08.260,0:41:14.640
things and the the real motivation

0:41:10.900,0:41:17.140
behind congressional Nets is to build

0:41:14.640,0:41:18.670
neural nets that are invariant to in

0:41:17.140,0:41:21.670
relevant transformation of the inputs

0:41:18.670,0:41:23.470
okay you can still recognize a C or D or

0:41:21.670,0:41:25.390
your grandmother regardless of the

0:41:23.470,0:41:31.540
position and to some extent the

0:41:25.390,0:41:33.370
orientation the style etc so this idea

0:41:31.540,0:41:35.860
that the signal only takes 100

0:41:33.370,0:41:37.810
milliseconds to go from the retina to

0:41:35.860,0:41:40.350
the four temporal cortex seems to

0:41:37.810,0:41:42.840
suggest that if you count like the delay

0:41:40.350,0:41:46.870
to go through every neuron or every

0:41:42.840,0:41:48.550
stage in that in that pathway there's

0:41:46.870,0:41:50.170
barely enough time for shoe spikes to

0:41:48.550,0:41:52.270
get through so there's no time for

0:41:50.170,0:41:54.220
complex recurrent connections you know

0:41:52.270,0:41:57.430
recurrent computation is basically a

0:41:54.220,0:41:58.870
feed-forward process it's very fast okay

0:41:57.430,0:42:00.370
and we need it to be fast because that's

0:41:58.870,0:42:02.620
a question of survival for us there's a

0:42:00.370,0:42:03.910
lot of for most animal you know you need

0:42:02.620,0:42:07.350
to be able to recognize really quickly

0:42:03.910,0:42:12.250
what what what's going on particularly

0:42:07.350,0:42:19.330
you know fast-moving predators or price

0:42:12.250,0:42:22.420
for that matter so that kind of suggests

0:42:19.330,0:42:23.710
the idea that you know we can do perhaps

0:42:22.420,0:42:25.120
we could come up with some sort of

0:42:23.710,0:42:27.940
neuron that architecture that is

0:42:25.120,0:42:34.930
completely feed-forward and still can do

0:42:27.940,0:42:37.090
recognition the diagram on the right is

0:42:34.930,0:42:40.420
from Garant and von essen so this is a

0:42:37.090,0:42:43.180
type of sort of abstract conceptual

0:42:40.420,0:42:44.560
diagram of the two pathways in the

0:42:43.180,0:42:45.970
visual cortex there is the ventral

0:42:44.560,0:42:48.130
pathway and the dorsal pathway the

0:42:45.970,0:42:51.040
ventral pathway is you know basically

0:42:48.130,0:42:52.960
the v1 v2 v4 IT hierarchy which is sort

0:42:51.040,0:42:55.780
of from the back of the brain and goes

0:42:52.960,0:42:58.560
to the bottom and to the side and then

0:42:55.780,0:43:01.630
the dorsal pathway kind of goes you know

0:42:58.560,0:43:05.320
through the top also towards the info

0:43:01.630,0:43:06.860
temporal cortex and there is this idea

0:43:05.320,0:43:08.630
somehow that the

0:43:06.860,0:43:11.090
ventral pathway is there to tell you

0:43:08.630,0:43:14.890
what you're looking at right the dorsal

0:43:11.090,0:43:18.920
pathway basically identifies locations

0:43:14.890,0:43:21.290
geometry and motion okay so there is a

0:43:18.920,0:43:24.080
pathway for what and another pathway for

0:43:21.290,0:43:27.530
where if you want and that seems fairly

0:43:24.080,0:43:33.530
separate in the human or primate visual

0:43:27.530,0:43:40.910
cortex and of course there are

0:43:33.530,0:43:43.580
interactions between them so various

0:43:40.910,0:43:45.580
people had the idea of kind of using so

0:43:43.580,0:43:47.750
what is that idea come from there is

0:43:45.580,0:43:51.140
classic work in neuroscience from the

0:43:47.750,0:43:53.990
late 50s early 60s by Rizzo

0:43:51.140,0:43:55.730
there on the picture here the one got

0:43:53.990,0:43:58.790
Nobel Prize for it so it's really

0:43:55.730,0:43:59.270
classic work and what they showed with

0:43:58.790,0:44:01.280
cats

0:43:59.270,0:44:05.480
basically by poking electrodes into cat

0:44:01.280,0:44:10.790
brains is that neurons in the in the cat

0:44:05.480,0:44:12.410
brain in v1 detect are only sensitive to

0:44:10.790,0:44:15.800
a small area of the visual field and

0:44:12.410,0:44:18.800
they detect oriented edges contours if

0:44:15.800,0:44:20.780
you want in that particular area okay so

0:44:18.800,0:44:23.200
the area to which a particular neuron is

0:44:20.780,0:44:26.960
sensitive is called a receptive field

0:44:23.200,0:44:30.560
and you take a particular neuron and you

0:44:26.960,0:44:32.630
you sure it kind of oriented bar if you

0:44:30.560,0:44:36.770
want that you rotate and at one point

0:44:32.630,0:44:38.150
the no one will will fire and as the for

0:44:36.770,0:44:39.980
a particular angle and as you move away

0:44:38.150,0:44:44.690
from that angle the activation of the

0:44:39.980,0:44:48.410
neuron kind of diminishes okay so that's

0:44:44.690,0:44:52.220
called orientation selective neurons and

0:44:48.410,0:44:54.050
when visual close simple cells if you

0:44:52.220,0:44:56.210
move the bar little bit you go out of

0:44:54.050,0:44:58.550
the receptive field that neuron doesn't

0:44:56.210,0:45:00.800
fire anymore doesn't react to it this

0:44:58.550,0:45:04.160
could be another known almost exactly

0:45:00.800,0:45:07.070
identical to it just a little bit you

0:45:04.160,0:45:09.080
know away from the first one that does

0:45:07.070,0:45:11.120
exactly the same function you will react

0:45:09.080,0:45:15.290
to a slightly different receptive field

0:45:11.120,0:45:16.520
but with the same orientation so you

0:45:15.290,0:45:17.390
start getting this idea that you have

0:45:16.520,0:45:20.720
local feature

0:45:17.390,0:45:22.550
detectors that are positioned replicated

0:45:20.720,0:45:27.370
all over the visual field which is

0:45:22.550,0:45:29.810
basically this idea of completion okay

0:45:27.370,0:45:34.600
so there are simple cells and then

0:45:29.810,0:45:37.520
another idea that or discovery that

0:45:34.600,0:45:40.070
Wiesel did is the idea of complex cells

0:45:37.520,0:45:43.070
so a complex cell is is another type of

0:45:40.070,0:45:44.960
neuron that integrates the output of

0:45:43.070,0:45:48.530
multiple simple cells within a certain

0:45:44.960,0:45:51.860
area okay so they will take different

0:45:48.530,0:45:53.660
simple cells that all detect contours at

0:45:51.860,0:45:57.290
a particular orientation edges at a

0:45:53.660,0:45:58.730
particular orientation and compare you

0:45:57.290,0:46:00.680
know an aggregate of all those

0:45:58.730,0:46:04.400
accusations they will either do a max or

0:46:00.680,0:46:05.900
a Sun or some a square or square root of

0:46:04.400,0:46:08.000
some square some sort of function that

0:46:05.900,0:46:11.150
does not depend on the order of the

0:46:08.000,0:46:13.910
arguments okay let's say max for the

0:46:11.150,0:46:16.670
sake of simplicity so basically a

0:46:13.910,0:46:20.420
complex cell will turn on if any of the

0:46:16.670,0:46:24.560
simple cells within its input group

0:46:20.420,0:46:26.630
turns on I said okay so that complex

0:46:24.560,0:46:28.190
cell will detect the negative particular

0:46:26.630,0:46:31.130
orientation regardless of his position

0:46:28.190,0:46:34.750
within that little region so it builds a

0:46:31.130,0:46:36.680
little bit of shift invariance of the

0:46:34.750,0:46:39.410
representation coming out of the complex

0:46:36.680,0:46:47.630
cells with respect to small variation of

0:46:39.410,0:46:51.740
positions of features in the input so a

0:46:47.630,0:46:55.850
gentleman by the name of cuckoo kunihiko

0:46:51.740,0:46:59.480
fukushima no real relationship with a

0:46:55.850,0:47:01.520
nuclear power plant in the late 70s

0:46:59.480,0:47:03.650
early 80s experimented with computer

0:47:01.520,0:47:05.660
models that sort of implemented this

0:47:03.650,0:47:07.040
idea of simple circle complex cell and

0:47:05.660,0:47:10.810
you had the idea of sort of replicating

0:47:07.040,0:47:13.940
this with multiple layers so basically

0:47:10.810,0:47:16.370
you know the oversize he did was very

0:47:13.940,0:47:19.070
similar to the one I showed earlier here

0:47:16.370,0:47:21.950
with this sort of handcrafted feature

0:47:19.070,0:47:24.050
detector some of those feature detectors

0:47:21.950,0:47:26.300
in his model were handcrafted but some

0:47:24.050,0:47:28.580
of them were around they were

0:47:26.300,0:47:30.200
an unsupervised method they were you

0:47:28.580,0:47:32.390
didn't have backdrop right back prop

0:47:30.200,0:47:34.670
didn't exist so I mean it existed but he

0:47:32.390,0:47:39.320
didn't it wasn't really popular and

0:47:34.670,0:47:42.410
people you know didn't use it so so he

0:47:39.320,0:47:44.720
trained those filters basically with

0:47:42.410,0:47:49.330
something that amounts to a sort of

0:47:44.720,0:47:52.970
clustering algorithm a little bit and

0:47:49.330,0:47:57.920
you know separately for each layer and

0:47:52.970,0:47:59.330
so he would you know train the filters

0:47:57.920,0:48:01.550
for the first layer train this with

0:47:59.330,0:48:04.580
Harrington digits he also had a dataset

0:48:01.550,0:48:06.440
of handwritten digits and then feed this

0:48:04.580,0:48:07.940
to complex cells that we kind of you

0:48:06.440,0:48:10.250
know pool the activity of simple cells

0:48:07.940,0:48:13.490
together and then that would consist

0:48:10.250,0:48:15.200
that would form the input to the next

0:48:13.490,0:48:17.300
layer and it would repeat the same

0:48:15.200,0:48:19.339
running algorithm his model neuron was

0:48:17.300,0:48:21.140
very complicated it was kind of inspired

0:48:19.339,0:48:24.260
by biology so we had separate inhibitory

0:48:21.140,0:48:25.880
neurons the the other neurons or you

0:48:24.260,0:48:28.910
know only have positive weights and I

0:48:25.880,0:48:31.369
growing weights etc we managed to get

0:48:28.910,0:48:37.339
this thing to kind of work okay

0:48:31.369,0:48:42.770
not very well but sort of worked then a

0:48:37.339,0:48:45.280
few years later I basically kind of got

0:48:42.770,0:48:47.540
inspired by similar architectures but

0:48:45.280,0:48:50.089
between them supervised with backprop

0:48:47.540,0:48:52.910
okay so that's the the genesis of

0:48:50.089,0:48:58.849
commercial nets if you want and then in

0:48:52.910,0:49:01.010
apparently more or less max reasonably

0:48:58.849,0:49:02.990
into the poachers lab at MIT he kind of

0:49:01.010,0:49:05.560
rediscovered this architecture also but

0:49:02.990,0:49:09.190
also didn't use backdrop for some reason

0:49:05.560,0:49:09.190
equals this H max

0:49:12.150,0:49:16.680
this is sort of early experiments I did

0:49:15.540,0:49:18.990
with commercial Nets when I was

0:49:16.680,0:49:20.940
finishing my postdoc in University of

0:49:18.990,0:49:24.570
Toronto in 1988 so that goes back a long

0:49:20.940,0:49:26.340
time and I was trying to figure out you

0:49:24.570,0:49:27.750
know does this work better on the small

0:49:26.340,0:49:29.070
data set so if you have a tiny amount of

0:49:27.750,0:49:30.600
data you're trying to fully connect to

0:49:29.070,0:49:32.850
network or linear network with just one

0:49:30.600,0:49:34.670
layer or network with local connections

0:49:32.850,0:49:36.960
with no shadow it or compare this with

0:49:34.670,0:49:38.400
what was not yet called accomplish on

0:49:36.960,0:49:40.470
that where you have shared weights and

0:49:38.400,0:49:42.950
local connections which one works best

0:49:40.470,0:49:44.820
and it turned out that in terms of

0:49:42.950,0:49:49.800
generalization ability which are the

0:49:44.820,0:49:52.470
curves on the bottom left the which you

0:49:49.800,0:49:55.170
see here the the top the top curve here

0:49:52.470,0:49:57.630
is basically the baby convolutional net

0:49:55.170,0:49:59.220
architecture trained with very simple

0:49:57.630,0:50:01.080
data set of handwritten digits that were

0:49:59.220,0:50:03.140
drawn with a mouse right we didn't have

0:50:01.080,0:50:06.990
any way of collecting images basically

0:50:03.140,0:50:08.640
at that time and then if you have real

0:50:06.990,0:50:09.990
connections without shared weights it

0:50:08.640,0:50:13.970
doesn't you know it works a little worse

0:50:09.990,0:50:16.590
and then if you have fully connected

0:50:13.970,0:50:19.740
networks it works worse and if you have

0:50:16.590,0:50:22.610
a linear network it not only works worse

0:50:19.740,0:50:25.500
but but it also over hits it over trains

0:50:22.610,0:50:29.910
so the test error goes down after a

0:50:25.500,0:50:32.550
while and this was trained with 320 320

0:50:29.910,0:50:35.250
training samples which is really real

0:50:32.550,0:50:38.790
all those networks add you know on the

0:50:35.250,0:50:41.430
order of five thousand connections 1000

0:50:38.790,0:50:43.490
parameters so this is you know a billion

0:50:41.430,0:50:48.870
times smaller than what we do today a

0:50:43.490,0:50:51.240
million times I would say and then I

0:50:48.870,0:50:53.340
think my postdoc I went to Bell Labs and

0:50:51.240,0:50:55.290
Bell Labs had you know slightly bigger

0:50:53.340,0:50:57.630
computers but what they had was a data

0:50:55.290,0:51:00.330
set that came from the Postal Service so

0:50:57.630,0:51:02.040
they had zip codes for envelopes and we

0:51:00.330,0:51:04.050
built a data set out of those zip codes

0:51:02.040,0:51:08.280
and then trying a slightly bigger a

0:51:04.050,0:51:10.950
neural net for three weeks and got

0:51:08.280,0:51:14.460
really results so this this commission

0:51:10.950,0:51:17.540
on that did not have separate Commission

0:51:14.460,0:51:19.800
and pooling it had

0:51:17.540,0:51:21.450
strided Commission so coefficients where

0:51:19.800,0:51:25.090
the window is shifted by more than one

0:51:21.450,0:51:26.860
pixel so that's what's a result of this

0:51:25.090,0:51:29.080
the result of this is that the output

0:51:26.860,0:51:32.230
map when you do a composition that where

0:51:29.080,0:51:33.790
the stride is is more than one you get

0:51:32.230,0:51:35.950
an output whose resolution is smaller

0:51:33.790,0:51:39.190
than the input and you see an example

0:51:35.950,0:51:42.400
here so here the the input is 16 by 16

0:51:39.190,0:51:45.220
pixels that's what we could afford the

0:51:42.400,0:51:52.450
columns are 5 by 5 but they are shifted

0:51:45.220,0:51:56.770
by 2 pixels every time and so the the

0:51:52.450,0:51:58.920
the output here is smaller because of

0:51:56.770,0:51:58.920
that

0:52:11.130,0:52:15.840
okay and then when you're later this was

0:52:12.990,0:52:18.720
the next generation comes from that this

0:52:15.840,0:52:22.590
one had separate commercial and pulling

0:52:18.720,0:52:24.180
so where's the premium operation at that

0:52:22.590,0:52:27.480
time the breeding operation was just

0:52:24.180,0:52:28.920
another neuron except that all the

0:52:27.480,0:52:33.330
weights of that neuron were equal okay

0:52:28.920,0:52:34.980
so a putting unit was basically you need

0:52:33.330,0:52:37.980
that computed an average of its inputs

0:52:34.980,0:52:39.690
and then cut I did a bias and then

0:52:37.980,0:52:41.400
passed it to a non-linearity which in

0:52:39.690,0:52:44.220
this case was a hyperbola tangent

0:52:41.400,0:52:45.780
function okay all the non-linearity is

0:52:44.220,0:52:48.420
in this network where hyperbola tangents

0:52:45.780,0:52:55.880
at the time that's what people were

0:52:48.420,0:53:00.660
doing and the printing operation was

0:52:55.880,0:53:04.320
performed by shifting the window over

0:53:00.660,0:53:07.500
which you compute the the aggregate of

0:53:04.320,0:53:11.760
the IP to the previous layer by by 2

0:53:07.500,0:53:15.360
pixels okay so here you get a 32 by 32

0:53:11.760,0:53:18.270
input input window convolve this with

0:53:15.360,0:53:19.620
filters that are 505 yeah I should

0:53:18.270,0:53:24.150
mention that a combination kernel

0:53:19.620,0:53:30.020
sometimes is also called a filter and so

0:53:24.150,0:53:35.190
what you get here are outputs that are I

0:53:30.020,0:53:39.000
guess minus 4 is 28 by 28 okay and then

0:53:35.190,0:53:43.650
there is a pooling which computes an

0:53:39.000,0:53:48.870
average of pixels here over a 2x2 window

0:53:43.650,0:53:51.840
and then shifts that window by 2 so how

0:53:48.870,0:53:54.660
many such windows do you have since the

0:53:51.840,0:53:59.610
image is 28 by 28 you divide by 2 is 14

0:53:54.660,0:54:03.210
by 14 okay so those images here are 14

0:53:59.610,0:54:05.580
by 14 pixels okay and they are basically

0:54:03.210,0:54:11.070
half the resolution as the previous

0:54:05.580,0:54:12.840
window because of this stride okay now

0:54:11.070,0:54:14.370
it becomes interesting because what you

0:54:12.840,0:54:15.960
want by is you want the next layer to

0:54:14.370,0:54:21.360
detect combinations of features from the

0:54:15.960,0:54:24.960
previous layer and so the way to do this

0:54:21.360,0:54:29.450
is you have different completion filters

0:54:24.960,0:54:32.340
apply to each of those feature maps okay

0:54:29.450,0:54:34.170
and you sum them up you send the results

0:54:32.340,0:54:35.610
of those full completions and you pass a

0:54:34.170,0:54:39.000
result through non-linearity and that

0:54:35.610,0:54:41.670
gives you one feature map of the next

0:54:39.000,0:54:45.060
layer so because those filters are 5x5

0:54:41.670,0:54:48.960
and those images are 14 by 14

0:54:45.060,0:54:53.070
those guys are ten by ten okay to not

0:54:48.960,0:54:55.530
have border effects so each of these

0:54:53.070,0:55:00.360
feature maps for which there are sixteen

0:54:55.530,0:55:07.980
if I remember correctly uses a different

0:55:00.360,0:55:11.910
set of kernels to convolve the previous

0:55:07.980,0:55:15.150
layers in fact the the connection

0:55:11.910,0:55:16.320
pattern between the feature map the

0:55:15.150,0:55:18.330
future wrapper this way you're on the

0:55:16.320,0:55:19.980
future mapper the next layer is actually

0:55:18.330,0:55:21.330
not full so not every feature mask

0:55:19.980,0:55:24.180
connected to every feature map is a

0:55:21.330,0:55:25.500
particular scheme of different

0:55:24.180,0:55:29.400
combinations of feature map from the

0:55:25.500,0:55:31.080
previous layer forming combining two for

0:55:29.400,0:55:33.030
future master the next layer and the

0:55:31.080,0:55:35.190
reason for doing this is just to save

0:55:33.030,0:55:36.840
computer time we just could not afford

0:55:35.190,0:55:39.150
to connect everything to everything it

0:55:36.840,0:55:42.960
would have taken twice the time to to

0:55:39.150,0:55:45.210
run for more nowadays were kind of

0:55:42.960,0:55:47.040
forced more or less to actually have a

0:55:45.210,0:55:50.040
complete connection between feature maps

0:55:47.040,0:55:51.330
in the commercial net because of the way

0:55:50.040,0:55:52.940
that multiple competitions are

0:55:51.330,0:55:58.050
implemented in GPUs

0:55:52.940,0:56:00.000
which is sad and then the next layer up

0:55:58.050,0:56:02.340
so again those maps are ten by ten those

0:56:00.000,0:56:05.940
feature maps are ten by ten and the next

0:56:02.340,0:56:06.830
layer up again is produced by putting in

0:56:05.940,0:56:10.320
subsampling

0:56:06.830,0:56:13.530
by a factor of two and so those are five

0:56:10.320,0:56:15.510
oh five okay and then again there is a

0:56:13.530,0:56:16.980
five oh five competition here of course

0:56:15.510,0:56:18.990
you can't move the window five by five

0:56:16.980,0:56:20.610
over five of our image so it looks like

0:56:18.990,0:56:24.960
a full connection but it's actually a

0:56:20.610,0:56:26.750
convolution okay keep this in mind but

0:56:24.960,0:56:29.910
you basically just only one location

0:56:26.750,0:56:33.480
okay and those feature maps at the top

0:56:29.910,0:56:34.510
here are really outputs and so you have

0:56:33.480,0:56:37.030
one spatial okay

0:56:34.510,0:56:39.960
because you want you can only place one

0:56:37.030,0:56:42.040
fiber five-window within a fiber 5 image

0:56:39.960,0:56:43.090
and you have 10 of those feature match

0:56:42.040,0:56:44.770
each of which corresponds to a

0:56:43.090,0:56:47.200
categories when you train the system to

0:56:44.770,0:57:01.210
classify digits from 0 to 9 you attend

0:56:47.200,0:57:05.070
categories this is a little animation

0:57:01.210,0:57:07.900
that I borrowed from aundrea Kapiti

0:57:05.070,0:57:11.200
spent the time to build this really nice

0:57:07.900,0:57:12.490
real animation which is the two

0:57:11.200,0:57:14.170
represents or multiple combinations

0:57:12.490,0:57:18.150
right so you have three feature mats

0:57:14.170,0:57:20.560
here on the input and you have 6

0:57:18.150,0:57:23.140
completion kernels and to feature maps

0:57:20.560,0:57:25.240
on the output so here the first group of

0:57:23.140,0:57:29.260
three feature mats are convolve with the

0:57:25.240,0:57:30.490
three input of three kernels are

0:57:29.260,0:57:33.670
involved with the three input feature

0:57:30.490,0:57:36.520
maps to produce the first group the

0:57:33.670,0:57:42.010
first of the two feature mats the green

0:57:36.520,0:57:45.670
one at the top okay and then and the

0:57:42.010,0:57:48.190
animation starts ok so this is the first

0:57:45.670,0:57:50.500
group of three Kaunas convolved with

0:57:48.190,0:57:52.450
this future mats and they produced the

0:57:50.500,0:57:56.020
green map at the top and then you switch

0:57:52.450,0:57:58.680
to the second group of of coalition

0:57:56.020,0:58:01.260
calls you've got you convolve with the

0:57:58.680,0:58:06.310
two input feature maps to produce the

0:58:01.260,0:58:11.320
map at the bottom ok so that's an

0:58:06.310,0:58:13.810
example of if I'm teaching upon the

0:58:11.320,0:58:16.540
input and feature an eye upon the output

0:58:13.810,0:58:19.710
and end times and composition kernels to

0:58:16.540,0:58:19.710
get all combinations

0:58:25.000,0:58:30.280
of animation which I made a long time

0:58:26.500,0:58:32.260
ago that shows commercial net after it's

0:58:30.280,0:58:35.830
been trained in action trying to

0:58:32.260,0:58:38.380
recognize digits and so what's

0:58:35.830,0:58:42.580
interesting to look at here is you you

0:58:38.380,0:58:47.650
have an input here which is I believe 32

0:58:42.580,0:58:49.360
rows by 64 columns and after doing six

0:58:47.650,0:58:51.310
compositions with six conversion kernels

0:58:49.360,0:58:53.350
passing it through a hyperbola tangent

0:58:51.310,0:58:55.510
non-linearity after bias you get those

0:58:53.350,0:58:57.580
sixty two maps here each of which kind

0:58:55.510,0:59:00.880
of activates for a different type of

0:58:57.580,0:59:03.460
feature so for example the feature map

0:59:00.880,0:59:08.110
at the top here turns on when there is

0:59:03.460,0:59:09.550
some sort of a horizontal edge this guy

0:59:08.110,0:59:12.610
here it turns on whenever there is a

0:59:09.550,0:59:13.720
vertical edge okay and those coefficient

0:59:12.610,0:59:15.220
kernels have been learning through back

0:59:13.720,0:59:18.280
pop the the thing has been just training

0:59:15.220,0:59:22.120
with with back prop no it said by hand

0:59:18.280,0:59:23.500
they said you know randomly usually so

0:59:22.120,0:59:27.000
you see this notion of equal variance

0:59:23.500,0:59:30.370
here if I if I shift the input image the

0:59:27.000,0:59:32.040
activations on the teacher mats shift

0:59:30.370,0:59:37.450
but otherwise stay unchanged

0:59:32.040,0:59:38.860
all right that's 50 quid variance okay

0:59:37.450,0:59:40.000
and then we go to the breeding operation

0:59:38.860,0:59:41.830
so this first feature mat here

0:59:40.000,0:59:44.200
corresponds to a putting a pooled

0:59:41.830,0:59:45.610
version of this first one the second one

0:59:44.200,0:59:47.680
to the second one third went to the

0:59:45.610,0:59:49.750
third one and the feeding operation here

0:59:47.680,0:59:53.920
again is an average then it is then a

0:59:49.750,0:59:58.540
similarly non-linearity and so if this

0:59:53.920,1:00:03.340
map shifts by one pixel this map will

0:59:58.540,1:00:06.850
shift by one half pixel okay so you

1:00:03.340,1:00:09.550
still have equal variance but you know

1:00:06.850,1:00:12.100
shifts are reduced by you know in fact

1:00:09.550,1:00:14.410
you have to essentially okay and then

1:00:12.100,1:00:16.810
you have the second stage where each of

1:00:14.410,1:00:19.510
those mats here is a result of doing a

1:00:16.810,1:00:21.310
convolution on each or a subset of the

1:00:19.510,1:00:22.900
previous maps with different kernels

1:00:21.310,1:00:26.140
summing up the resulting the results

1:00:22.900,1:00:29.860
were a sigmoid and so you get those kind

1:00:26.140,1:00:31.300
of abstract features here that are kind

1:00:29.860,1:00:33.360
of you know hard to interpret visually

1:00:31.300,1:00:35.730
but it's still equal value in two shifts

1:00:33.360,1:00:38.430
okay and then again you do prove

1:00:35.730,1:00:40.230
and subsampling so the pooling also has

1:00:38.430,1:00:44.340
you know this tried about a factor of

1:00:40.230,1:00:46.020
two so what you get here are or maps so

1:00:44.340,1:00:49.230
that those maps shift by one quarter

1:00:46.020,1:00:51.300
pixel if the input is by one pixel okay

1:00:49.230,1:00:52.859
so we reduce to shift and it becomes you

1:00:51.300,1:00:54.480
know might become easier and easier for

1:00:52.859,1:00:58.040
following layers to kind of interpret

1:00:54.480,1:01:01.710
what the shape is because you exchange

1:00:58.040,1:01:02.970
spatial resolution for future type

1:01:01.710,1:01:04.680
resolution you increase the number of

1:01:02.970,1:01:07.859
feature types as you go up the layers

1:01:04.680,1:01:10.290
the spatial resolution goes down because

1:01:07.859,1:01:12.510
of the pruning and subsampling but the

1:01:10.290,1:01:13.920
number of Chitra Maps increases and so

1:01:12.510,1:01:15.990
you make the representation a little

1:01:13.920,1:01:20.240
more abstract but less sensitive to

1:01:15.990,1:01:22.619
shift and distortions and the next layer

1:01:20.240,1:01:24.330
again performs completions but now the

1:01:22.619,1:01:26.160
size of the coalition kernel is equal to

1:01:24.330,1:01:29.130
the height of the image and so what you

1:01:26.160,1:01:31.320
get is a single band for this feature

1:01:29.130,1:01:35.190
map it's basically it becomes one

1:01:31.320,1:01:37.380
dimensional and so now the vertical

1:01:35.190,1:01:38.670
shift is basically eliminated right it's

1:01:37.380,1:01:41.760
turning into some variation of

1:01:38.670,1:01:44.640
activation but it's not it's not a shift

1:01:41.760,1:01:47.250
anymore it's some sort of simpler or

1:01:44.640,1:01:51.660
hopefully transformation of the input in

1:01:47.250,1:01:57.690
fact you can show it's simpler it's

1:01:51.660,1:01:59.550
flatter in some ways okay so that's the

1:01:57.690,1:02:02.609
sort of generic condition that

1:01:59.550,1:02:04.040
architecture we have this is a slightly

1:02:02.609,1:02:10.100
more modern version of it where you have

1:02:04.040,1:02:14.130
some form of normalization batch norm

1:02:10.100,1:02:17.160
good norm whatever if your kbank does

1:02:14.130,1:02:19.340
those are the multiple combinations in

1:02:17.160,1:02:21.840
signal processing vehicle filter banks

1:02:19.340,1:02:24.540
point where is non-linearity generally a

1:02:21.840,1:02:27.830
value and then some pooling generally

1:02:24.540,1:02:30.210
maximally okay in the most common

1:02:27.830,1:02:31.590
implementations of compliments because

1:02:30.210,1:02:33.930
of course imagine all the types of

1:02:31.590,1:02:36.140
proving I talked about the average but

1:02:33.930,1:02:40.740
the more generic version is the LP norm

1:02:36.140,1:02:42.510
which is you know take all the inputs

1:02:40.740,1:02:42.800
through a complex a little bit then to

1:02:42.510,1:02:45.740
some

1:02:42.800,1:02:50.570
power and then take the you know sum

1:02:45.740,1:02:53.240
them up and then take the little bit

1:02:50.570,1:02:56.290
that to one over the pallet

1:02:53.240,1:03:00.490
yeah this should be a sum inside of the

1:02:56.290,1:03:00.490
of the piece root here

1:03:00.530,1:03:06.770
another way to pool and again you know

1:03:04.460,1:03:08.810
any pruning up a good pruning operation

1:03:06.770,1:03:11.120
is an operation that is invariant to the

1:03:08.810,1:03:13.490
to a a permutation of the inputs it

1:03:11.120,1:03:15.280
gives you the same result regardless of

1:03:13.490,1:03:17.600
the order in which you put the input

1:03:15.280,1:03:19.700
here's another example we talked about

1:03:17.600,1:03:24.670
this function before one of the be log

1:03:19.700,1:03:24.670
some of our inputs of e to the X

1:03:25.420,1:03:30.350
exponential DX again that's a kind of

1:03:28.580,1:03:33.830
symmetric aggregation operation that you

1:03:30.350,1:03:35.030
can use so that's going to a stage and

1:03:33.830,1:03:37.880
accomplish on that and then you can

1:03:35.030,1:03:40.310
repeat that these sort of various ways

1:03:37.880,1:03:42.650
of positioning the normalization some

1:03:40.310,1:03:46.090
people put it after the non-linearity

1:03:42.650,1:03:54.830
before the pooling you know it depends

1:03:46.090,1:03:56.570
but it's typical so how do you do this

1:03:54.830,1:03:58.040
in pi torched there's no different ways

1:03:56.570,1:04:00.170
you can do it by you know writing it

1:03:58.040,1:04:03.520
explicitly writing a class so this is a

1:04:00.170,1:04:06.320
example of a complex on that class

1:04:03.520,1:04:12.100
particular one here when you do

1:04:06.320,1:04:15.170
completion value and and and Max fully

1:04:12.100,1:04:17.120
okay so the constructor here creates

1:04:15.170,1:04:21.260
coalition's coalition layers which have

1:04:17.120,1:04:22.970
parameters in them and this one has

1:04:21.260,1:04:27.230
what's called three connected layers I

1:04:22.970,1:04:30.140
hate that okay so there is this idea

1:04:27.230,1:04:33.860
somehow that the the last layer of a

1:04:30.140,1:04:39.050
convolutional net like like this one is

1:04:33.860,1:04:41.150
fully connected because every unit in

1:04:39.050,1:04:42.560
this layer is connected to every unit in

1:04:41.150,1:04:45.590
that layer so that looks like a full

1:04:42.560,1:04:51.060
connection but it's actually useful to

1:04:45.590,1:04:53.310
think of it as a convolution okay

1:04:51.060,1:04:57.990
for efficiency reasons or maybe some

1:04:53.310,1:05:00.120
others bad reasons they called you know

1:04:57.990,1:05:02.130
pretty connected layers they we used the

1:05:00.120,1:05:04.290
class linear here but it kind of breaks

1:05:02.130,1:05:07.710
the whole idea that your network is a

1:05:04.290,1:05:10.260
conditional network so it's much better

1:05:07.710,1:05:12.240
actually to view them as compositions in

1:05:10.260,1:05:14.220
this case one by one convolution which

1:05:12.240,1:05:16.920
is sort of a weird concept okay so here

1:05:14.220,1:05:20.160
we have four layers - compositional

1:05:16.920,1:05:24.270
layers and two so-called fully connected

1:05:20.160,1:05:25.860
layers and then the way we so we need to

1:05:24.270,1:05:29.540
create them in the constructor and the

1:05:25.860,1:05:31.800
way we use them in the four pass is that

1:05:29.540,1:05:33.390
you know we do a composition of the

1:05:31.800,1:05:36.210
input and then we apply the value and

1:05:33.390,1:05:38.070
then we do max pooling and then we run

1:05:36.210,1:05:39.990
the second layer and apply the value and

1:05:38.070,1:05:41.970
do max filling again and then we

1:05:39.990,1:05:43.950
initiate the output because we it's a

1:05:41.970,1:05:46.590
pretty connected layer so we want to

1:05:43.950,1:05:51.660
make this a vector so that's where the X

1:05:46.590,1:05:57.270
View - one does and then apply a value

1:05:51.660,1:05:58.950
to it and you know and this the second

1:05:57.270,1:06:00.060
free connected layer and then apply your

1:05:58.950,1:06:02.670
soft max if you want to do

1:06:00.060,1:06:04.230
classification and so this is somewhat

1:06:02.670,1:06:06.060
similar to the architecture you see at

1:06:04.230,1:06:07.470
the bottom the numbers might be

1:06:06.060,1:06:10.110
different in terms of you know feature

1:06:07.470,1:06:13.290
maps and stuff but but the general

1:06:10.110,1:06:24.540
architecture is it's pretty much what

1:06:13.290,1:06:29.130
we're talking about yes this again you

1:06:24.540,1:06:32.340
know whatever bred in descent decides we

1:06:29.130,1:06:35.730
can look at them but if you train with a

1:06:32.340,1:06:37.170
lot of examples or natural images the

1:06:35.730,1:06:39.180
kind of filters you will see that the

1:06:37.170,1:06:41.310
first layer basically will end up being

1:06:39.180,1:06:44.370
mostly oriented edge detectors very much

1:06:41.310,1:06:48.710
similar to what people - what no

1:06:44.370,1:06:53.630
scientists observe in in the context of

1:06:48.710,1:06:53.630
animals individual contacts of animals

1:06:55.280,1:06:58.080
the will change when you train the model

1:06:57.390,1:07:07.890
that's the whole point

1:06:58.080,1:07:10.110
yes okay so it's pretty simple here's

1:07:07.890,1:07:13.320
another way of defining those this is I

1:07:10.110,1:07:13.950
guess it's kind of a outdated way of

1:07:13.320,1:07:17.970
doing it right

1:07:13.950,1:07:19.860
not many people do this anymore but it's

1:07:17.970,1:07:22.080
kind of a simple way also there is this

1:07:19.860,1:07:25.500
class in invite torch called

1:07:22.080,1:07:27.420
and then sequential and it's basically a

1:07:25.500,1:07:30.420
container and you keep putting modules

1:07:27.420,1:07:32.970
in it and it just you know automatically

1:07:30.420,1:07:35.430
kind of use them as being kind of

1:07:32.970,1:07:41.760
connected in sequence right and so then

1:07:35.430,1:07:44.340
you just have to call you know forward

1:07:41.760,1:07:47.010
on it and and and it will just it will

1:07:44.340,1:07:49.380
just compute the right thing in this

1:07:47.010,1:07:51.480
particular form here you pass it kind of

1:07:49.380,1:07:53.340
a bunch of pairs it's like a dictionary

1:07:51.480,1:07:56.210
so you can give a name to each of the

1:07:53.340,1:07:59.420
layers and you can later a kind of

1:07:56.210,1:07:59.420
access them

1:08:07.579,1:08:21.060
it's the same architecture we're talking

1:08:09.660,1:08:23.909
about earlier yeah I mean the the

1:08:21.060,1:08:29.190
backdrop is automatic right you you get

1:08:23.909,1:08:34.040
it by default you just called backward

1:08:29.190,1:08:34.040
and it knows how to back propagate to it

1:08:43.500,1:08:47.740
well the class kind of encapsulates

1:08:45.730,1:08:49.180
everything into you know into an object

1:08:47.740,1:08:51.010
you know where the parameters are

1:08:49.180,1:08:54.630
there's a particular way of sort of you

1:08:51.010,1:08:57.730
know getting the parameters out and and

1:08:54.630,1:08:59.650
kind of feeding them to dog feeding them

1:08:57.730,1:09:00.790
to an optimizer and so the optimizer

1:08:59.650,1:09:02.590
doesn't need to know what your network

1:09:00.790,1:09:04.150
looks like it just knows that there is a

1:09:02.590,1:09:06.460
function and there is a bunch of

1:09:04.150,1:09:07.750
parameters and it gets a gradient and

1:09:06.460,1:09:11.560
you know it doesn't need to know what

1:09:07.750,1:09:13.180
your network looks like yeah you'll

1:09:11.560,1:09:14.340
you'll you'll hear more about this

1:09:13.180,1:09:28.450
[Music]

1:09:14.340,1:09:30.100
tomorrow okay so here's a very

1:09:28.450,1:09:31.330
interesting aspect of congressional nets

1:09:30.100,1:09:35.110
and it's one of the reasons why they

1:09:31.330,1:09:39.940
where they've become so successful in in

1:09:35.110,1:09:41.620
many applications it's the fact that if

1:09:39.940,1:09:44.260
you every that you're in a commercial

1:09:41.620,1:09:48.760
net as a convention so there is no cool

1:09:44.260,1:09:51.340
connection so to speak you don't need to

1:09:48.760,1:09:53.110
have a fixed size input you can value

1:09:51.340,1:09:59.010
the size of the input and the network

1:09:53.110,1:10:01.650
will varied size accordingly because

1:09:59.010,1:10:03.970
when you apply a convolution to an image

1:10:01.650,1:10:07.120
you fit it an image of a certain size

1:10:03.970,1:10:10.120
you do a composition with the kernel you

1:10:07.120,1:10:11.980
get an image whose size is really you

1:10:10.120,1:10:13.630
know related to the size of the input

1:10:11.980,1:10:15.190
but you can change the size of the input

1:10:13.630,1:10:18.190
and it just changes the size of the

1:10:15.190,1:10:19.570
output and this is true for every

1:10:18.190,1:10:22.090
college every a conditional like

1:10:19.570,1:10:24.070
operation right so if your network is

1:10:22.090,1:10:25.090
composed only of convolutions and it

1:10:24.070,1:10:27.340
doesn't matter what the size of the

1:10:25.090,1:10:29.350
input is it's going to go through the

1:10:27.340,1:10:30.970
network and the the size of every layer

1:10:29.350,1:10:32.980
will change according to the size of the

1:10:30.970,1:10:35.470
input and the sum of the output will

1:10:32.980,1:10:39.370
also change accordingly so here is a

1:10:35.470,1:10:42.040
little example here where you know I

1:10:39.370,1:10:44.440
want to do cursive handwriting

1:10:42.040,1:10:46.000
recognition and it's very hard because I

1:10:44.440,1:10:47.890
don't know where the letters are so I

1:10:46.000,1:10:49.070
can't you know just have a character

1:10:47.890,1:10:52.390
recognizer that

1:10:49.070,1:10:57.220
I mean a system that will first cut the

1:10:52.390,1:10:59.600
word into two letters letters R and then

1:10:57.220,1:11:01.730
apply the conditional net to each of the

1:10:59.600,1:11:03.140
letters so the best I can do is take the

1:11:01.730,1:11:06.380
converse on that and swipe it over the

1:11:03.140,1:11:08.000
input and then record the output okay

1:11:06.380,1:11:09.890
and so you would take that to do this

1:11:08.000,1:11:12.560
you would have to take the course on

1:11:09.890,1:11:15.620
that like this that has a window large

1:11:12.560,1:11:17.690
enough to see a single character okay

1:11:15.620,1:11:19.940
and then you take your input image and

1:11:17.690,1:11:21.160
you compute your neural net your

1:11:19.940,1:11:23.660
commercial net at every location

1:11:21.160,1:11:25.160
shifting it by one pixel or two pixels

1:11:23.660,1:11:26.990
or four pixels or something like this

1:11:25.160,1:11:29.270
you know a small enough number of pixels

1:11:26.990,1:11:31.520
that we got less of where the character

1:11:29.270,1:11:34.400
occurs in the input you will still get a

1:11:31.520,1:11:38.120
score on the output whenever it used to

1:11:34.400,1:11:43.940
recognize one but it turns out that will

1:11:38.120,1:11:45.560
be extremely wasteful because you will

1:11:43.940,1:11:48.170
be entering the same competition

1:11:45.560,1:11:49.760
multiple times and so the proper way to

1:11:48.170,1:11:52.310
do this and this is very important to

1:11:49.760,1:11:53.750
understand is that you don't do what I

1:11:52.310,1:11:55.970
just described where you have a small

1:11:53.750,1:12:02.030
commercial net that you apply to every

1:11:55.970,1:12:04.040
window what you do is you take a large

1:12:02.030,1:12:06.950
input and you apply the combinations to

1:12:04.040,1:12:08.660
the input image since its larger you're

1:12:06.950,1:12:10.520
gonna get a larger output you cry the

1:12:08.660,1:12:12.380
second layer composition to that or the

1:12:10.520,1:12:15.620
pooling whatever it is you're gonna get

1:12:12.380,1:12:17.510
a larger input again etc all the way to

1:12:15.620,1:12:19.310
the top and whereas in the original

1:12:17.510,1:12:20.750
design you were getting only one output

1:12:19.310,1:12:23.360
now you're going to get multiple outputs

1:12:20.750,1:12:30.100
because you know it's a compositional

1:12:23.360,1:12:33.560
layer this is super important because

1:12:30.100,1:12:37.610
this way of applying a convolutional

1:12:33.560,1:12:39.500
nets with a sliding window is much much

1:12:37.610,1:12:44.650
much cheaper than recomputing the

1:12:39.500,1:12:47.180
commercial method every location okay

1:12:44.650,1:12:50.060
you wouldn't you would not believe how

1:12:47.180,1:12:53.620
many decades it took to convince people

1:12:50.060,1:12:53.620
that this was a good thing

1:12:58.460,1:13:05.220
so here's an example of how you can use

1:13:01.470,1:13:06.510
this this is a this is a conventional

1:13:05.220,1:13:08.610
net that was trained on individual

1:13:06.510,1:13:13.350
digits 32 by 32 it was trying on a list

1:13:08.610,1:13:14.700
okay 32 by 32 input Windows it's loaded

1:13:13.350,1:13:17.280
5 so it's very similar to the

1:13:14.700,1:13:19.290
architecture I just showed the code for

1:13:17.280,1:13:22.470
okay it's trained on individual

1:13:19.290,1:13:23.880
characters to just classify the

1:13:22.470,1:13:25.680
character in the center of the image and

1:13:23.880,1:13:27.480
the way was trying to said there was a

1:13:25.680,1:13:28.860
little bit of data augmentation where

1:13:27.480,1:13:30.090
the character in the center was kind of

1:13:28.860,1:13:33.690
shifted a little bit in various

1:13:30.090,1:13:36.540
locations changed in size and then there

1:13:33.690,1:13:39.090
were two other characters you know kind

1:13:36.540,1:13:43.440
of that was gonna add it to the side to

1:13:39.090,1:13:45.660
confuse it okay in many samples and then

1:13:43.440,1:13:48.150
it was also trained with a 11th category

1:13:45.660,1:13:50.100
which was none of the above and the way

1:13:48.150,1:13:51.780
it's trained is either you shoot a blank

1:13:50.100,1:13:53.190
image when you shoot an image where

1:13:51.780,1:13:56.520
there is no character in the center with

1:13:53.190,1:13:58.530
our characters on the side okay so that

1:13:56.520,1:14:01.440
it would detect when whenever it's in

1:13:58.530,1:14:03.870
between two characters and then you do

1:14:01.440,1:14:05.400
this thing of you know computing the

1:14:03.870,1:14:07.020
commercial net at every location in the

1:14:05.400,1:14:08.250
input without actually shifting the

1:14:07.020,1:14:12.240
commercial net but just applying the

1:14:08.250,1:14:16.170
convolutions to the entire image and

1:14:12.240,1:14:18.000
that's what you get so so here the input

1:14:16.170,1:14:20.070
image is 64 by 32 even though the

1:14:18.000,1:14:23.780
network was trained on 32 by 32 with

1:14:20.070,1:14:26.370
those kind of generated examples and

1:14:23.780,1:14:27.870
what you see is the activity of some of

1:14:26.370,1:14:30.630
the layers here not all of them are

1:14:27.870,1:14:34.050
represented and what you see at the top

1:14:30.630,1:14:36.630
here those kind of funny shapes you see

1:14:34.050,1:14:39.600
threes and fives popping up and they

1:14:36.630,1:14:40.860
basically are the dedication of the

1:14:39.600,1:14:46.110
winning category for every location

1:14:40.860,1:14:49.020
right so the the the 8 the 8 outputs

1:14:46.110,1:14:50.750
that you see at the top are basically

1:14:49.020,1:14:54.240
the output question into eight different

1:14:50.750,1:14:56.430
positions of the 32 by 32 input window

1:14:54.240,1:15:01.000
on the input shifted by 4 pixels every

1:14:56.430,1:15:02.860
time and what is

1:15:01.000,1:15:04.810
presentin is the winning category within

1:15:02.860,1:15:06.720
that window and the grayscale indicates

1:15:04.810,1:15:09.190
the score okay

1:15:06.720,1:15:11.680
so what you see is you know there's two

1:15:09.190,1:15:13.450
detector is detecting the five until the

1:15:11.680,1:15:15.190
three kind of starts overlapping and

1:15:13.450,1:15:20.380
then two detector is detecting the three

1:15:15.190,1:15:24.190
that kind of move around because you

1:15:20.380,1:15:26.620
know within 32 left 32 we go the three

1:15:24.190,1:15:28.030
adheres to the left of that 32 by 32

1:15:26.620,1:15:30.550
window and then to the right of that

1:15:28.030,1:15:33.490
other 32 by 32 windows shifted by 4 and

1:15:30.550,1:15:37.510
so those two detectors detect that 3 or

1:15:33.490,1:15:39.760
that 5 so then what you do is you take

1:15:37.510,1:15:41.620
all those scores here at the top and you

1:15:39.760,1:15:43.330
do a little bit of post-processing very

1:15:41.620,1:15:45.910
simple and you figure out it's a 3 and a

1:15:43.330,1:15:49.150
5 and what's interesting about this is

1:15:45.910,1:15:51.010
that you don't need to do prior

1:15:49.150,1:15:53.200
segmentation so something that people

1:15:51.010,1:15:55.150
had to do before in computer vision was

1:15:53.200,1:15:56.410
if you wanted to recognize an object you

1:15:55.150,1:15:57.820
had to separate the object from his

1:15:56.410,1:16:01.300
background because the recognition

1:15:57.820,1:16:03.160
system you know we get confused by by

1:16:01.300,1:16:05.020
the background but here with this

1:16:03.160,1:16:06.760
commercial net it's been trained with

1:16:05.020,1:16:09.250
overlapping characters and it knows how

1:16:06.760,1:16:11.050
to tell them apart and so it's not

1:16:09.250,1:16:12.430
confused by characters that overlap I

1:16:11.050,1:16:14.950
have a whole bunch of those on my web

1:16:12.430,1:16:39.550
website by the way those animations from

1:16:14.950,1:16:42.120
the early nineties know that there was

1:16:39.550,1:16:45.460
an issue that's one of the reasons why

1:16:42.120,1:16:49.350
you know computation wasn't working very

1:16:45.460,1:16:52.150
well it's because the very problem of

1:16:49.350,1:16:55.540
figure one separation detecting an

1:16:52.150,1:16:57.400
object and recognizing it is the same

1:16:55.540,1:16:58.900
you can't recognize the object until you

1:16:57.400,1:17:00.850
segment it but you can segment it until

1:16:58.900,1:17:02.620
you recognize it it's the same for

1:17:00.850,1:17:08.050
cursive handwriting recognition right

1:17:02.620,1:17:10.320
you can't so here's an example we have

1:17:08.050,1:17:10.320
pens

1:17:10.429,1:17:14.409
doesn't look like we have pens right oh

1:17:14.469,1:17:27.820
here we go that's true I'm sorry maybe I

1:17:21.199,1:17:27.820
should use the if this works

1:17:34.500,1:17:38.510
oh of course

1:17:42.909,1:17:45.909
okay

1:17:51.880,1:17:57.850
hey you guys read this okay

1:17:56.380,1:17:59.260
I mean that's it's horrible handwriting

1:17:57.850,1:18:01.480
but it's also because you know I'm

1:17:59.260,1:18:03.990
writing on the screen okay now can you

1:18:01.480,1:18:03.990
read it

1:18:07.740,1:18:15.250
minimum yeah okay you know where you can

1:18:13.600,1:18:17.440
segment the letters out of this right I

1:18:15.250,1:18:20.140
mean it's kind of random number of waves

1:18:17.440,1:18:22.000
but just the fact that the two eyes are

1:18:20.140,1:18:25.270
identified then it's basically not

1:18:22.000,1:18:28.930
ambiguous at least in English so that's

1:18:25.270,1:18:32.260
a good example of you know the

1:18:28.930,1:18:34.390
interpretation of individual objects you

1:18:32.260,1:18:35.320
know depending on their context and what

1:18:34.390,1:18:37.150
you need is some sort of high-level

1:18:35.320,1:18:39.490
language model to know what words are

1:18:37.150,1:18:41.650
possible if you don't know English or

1:18:39.490,1:18:44.140
similar languages that have the same

1:18:41.650,1:18:47.110
word there's no way you can you can read

1:18:44.140,1:18:52.240
this spoken language is very similar to

1:18:47.110,1:18:53.500
this so we all of you who have had the

1:18:52.240,1:18:55.780
experience of learning a foreign

1:18:53.500,1:18:58.780
language probably have the experience

1:18:55.780,1:19:01.180
that you have a hard time segmenting

1:18:58.780,1:19:02.980
words from a new language and then

1:19:01.180,1:19:06.070
recognizing the words because you don't

1:19:02.980,1:19:07.660
have the vocabulary right so if I speak

1:19:06.070,1:19:08.890
in front sees your comments about your

1:19:07.660,1:19:10.780
phone say whenever you need a was

1:19:08.890,1:19:13.330
already need mo except if you speak

1:19:10.780,1:19:15.280
French so I said let's focus on tense

1:19:13.330,1:19:16.600
its words but you can tell the boundary

1:19:15.280,1:19:19.210
between the words right because it is

1:19:16.600,1:19:20.920
basically you know key a seizure between

1:19:19.210,1:19:22.870
the words unless you know where the

1:19:20.920,1:19:24.580
words are in advance right so that's the

1:19:22.870,1:19:25.780
program of segmentation you can't

1:19:24.580,1:19:27.400
recognize until the segment you can

1:19:25.780,1:19:29.740
segment until you recognize you have to

1:19:27.400,1:19:31.360
do both at the same time and you know

1:19:29.740,1:19:34.380
early computer vision systems are really

1:19:31.360,1:19:34.380
really hard time doing this

1:19:40.370,1:19:44.640
so that that's why you know this this

1:19:43.230,1:19:46.050
kind of stuff is big progress because

1:19:44.640,1:19:48.840
you don't have to do segmentation in

1:19:46.050,1:19:50.429
advance it just just train your system

1:19:48.840,1:19:52.410
to be robust to kind of overlapping

1:19:50.429,1:19:57.239
objects and things like that yes in the

1:19:52.410,1:20:01.110
back yes there is a bug run class so

1:19:57.239,1:20:02.699
when you see a blank response it means

1:20:01.110,1:20:06.090
the system says none of the above

1:20:02.699,1:20:08.550
basically right so it's been trained to

1:20:06.090,1:20:10.710
produce none of the above either when

1:20:08.550,1:20:15.150
the input is blank or when there is one

1:20:10.710,1:20:16.739
character that's too you know outside of

1:20:15.150,1:20:18.870
the center or when you have two

1:20:16.739,1:20:20.280
characters there's nothing in the center

1:20:18.870,1:20:21.750
or when your two characters are

1:20:20.280,1:20:25.260
overlapped but there is no central

1:20:21.750,1:20:26.790
character right so it's you know trying

1:20:25.260,1:20:29.730
to detect boundaries between characters

1:20:26.790,1:20:34.170
essentially here's another example so

1:20:29.730,1:20:35.850
this this is the this is an example that

1:20:34.170,1:20:38.130
shows that even a very simple commercial

1:20:35.850,1:20:39.600
net with just two stages right coalition

1:20:38.130,1:20:43.170
pooling coercion pooling and then two

1:20:39.600,1:20:46.140
layers of you know two more layers

1:20:43.170,1:20:48.630
afterwards can solve what's called a

1:20:46.140,1:20:50.850
future banding problem so visual

1:20:48.630,1:20:53.969
neuroscientists and computer vision

1:20:50.850,1:20:57.989
people had the the issue there was it

1:20:53.969,1:20:59.730
was kind of a puzzle how is it that we

1:20:57.989,1:21:01.500
perceive objects as objects you know

1:20:59.730,1:21:02.969
objects are collections of features but

1:21:01.500,1:21:06.960
how do we bind all the features together

1:21:02.969,1:21:08.340
of an object to form this object is

1:21:06.960,1:21:14.630
there some kind of magical way of doing

1:21:08.340,1:21:20.910
this and and they did you know

1:21:14.630,1:21:23.060
psychologists did experiments like you

1:21:20.910,1:21:23.060
know

1:21:24.210,1:21:34.170
draw this and then that and you perceive

1:21:30.239,1:21:36.630
the the bar as a single bar because

1:21:34.170,1:21:39.420
you're used to bars being you know

1:21:36.630,1:21:42.440
obstructed by occluded by other objects

1:21:39.420,1:21:46.020
and so you just assume it's an occlusion

1:21:42.440,1:21:47.400
and then there are experiments that you

1:21:46.020,1:21:50.580
know figure out like how much do I have

1:21:47.400,1:21:51.930
to shift the two bars where to kind of

1:21:50.580,1:21:55.410
make me perceive them as two separate

1:21:51.930,1:21:58.020
bars but in fact you know the minute

1:21:55.410,1:22:01.320
they perfectly line and if you if you do

1:21:58.020,1:22:02.850
this you know exactly identical to what

1:22:01.320,1:22:07.050
you see here but but now you perceive

1:22:02.850,1:22:10.590
them as two different objects so you

1:22:07.050,1:22:14.150
know how is it that we do the we seem to

1:22:10.590,1:22:18.870
be solving the future bonding problem

1:22:14.150,1:22:20.940
and what this shows is that you don't

1:22:18.870,1:22:23.160
need any specific mechanism for it it

1:22:20.940,1:22:25.800
just happens if you have enough

1:22:23.160,1:22:29.850
nonlinearities and you train with enough

1:22:25.800,1:22:31.110
data then as a side effect you get the

1:22:29.850,1:22:33.090
system that solve the future binding

1:22:31.110,1:22:39.690
problem without any particular mechanism

1:22:33.090,1:22:42.260
for it so here you have two shapes and

1:22:39.690,1:22:42.260
you move a single

1:22:42.560,1:22:55.640
stroke and you know he goes from the 601

1:22:46.739,1:22:55.640
to a 3 to a 5 and a 1 to a 7 & a 3 etc

1:22:55.660,1:22:58.790
[Music]

1:22:59.770,1:23:04.450
right good question so the question is

1:23:02.170,1:23:06.790
how do you distinguish between the two

1:23:04.450,1:23:09.970
situations we have two fives next to

1:23:06.790,1:23:11.470
each other and the fact that you have a

1:23:09.970,1:23:13.750
single fight being detected by two

1:23:11.470,1:23:16.150
different frames right two different

1:23:13.750,1:23:19.030
framing of of that five well there is

1:23:16.150,1:23:21.400
this explicit training so that when you

1:23:19.030,1:23:23.320
have two characters that are touching

1:23:21.400,1:23:24.550
and none of them is really centered you

1:23:23.320,1:23:27.520
train the system to say none of the

1:23:24.550,1:23:30.670
above right so it's it's always going to

1:23:27.520,1:23:32.200
have you know five blank five it's all

1:23:30.670,1:23:33.910
it's always gonna have even like one

1:23:32.200,1:23:35.500
blank one even the ones can be very

1:23:33.910,1:23:40.900
close it will you'll tell you the

1:23:35.500,1:24:11.010
difference okay so what are common is

1:23:40.900,1:24:13.110
good for so what you look at is this so

1:24:11.010,1:24:16.510
every now your here is a convolution

1:24:13.110,1:24:17.800
okay including the last layer so it

1:24:16.510,1:24:20.350
looks like a full connection because

1:24:17.800,1:24:22.030
every units in the second layer goes

1:24:20.350,1:24:23.470
into the outputs but in fact it is a

1:24:22.030,1:24:25.600
convolution it just happens to be

1:24:23.470,1:24:27.550
applied to a single location so when I

1:24:25.600,1:24:30.430
imagine that this failure at the top

1:24:27.550,1:24:35.230
here now is bigger okay which is

1:24:30.430,1:24:36.790
represented here okay now the size of

1:24:35.230,1:24:38.830
the kernel is the size of the image you

1:24:36.790,1:24:40.060
had here previously but now it's a

1:24:38.830,1:24:43.030
convolution that has multiple locations

1:24:40.060,1:24:48.430
right and so what you get is multiple

1:24:43.030,1:24:49.840
outputs that's right that's right each

1:24:48.430,1:24:52.360
of which corresponds to you know

1:24:49.840,1:24:54.600
classification over an input window of

1:24:52.360,1:24:58.060
size 32 by 32 in the example I showed

1:24:54.600,1:25:00.520
and those those windows are shifted by 8

1:24:58.060,1:25:05.010
by 4 pixels the reason being that the

1:25:00.520,1:25:08.890
the network architecture I showed here

1:25:05.010,1:25:10.540
has a combination which tried one then

1:25:08.890,1:25:13.740
pruning which tried to composition which

1:25:10.540,1:25:13.740
tried one pudding which tried to

1:25:13.849,1:25:22.039
stride is for right and so to get a new

1:25:19.960,1:25:26.179
output you need to shift the input

1:25:22.039,1:25:28.969
window by for two to get to get one of

1:25:26.179,1:25:32.809
those because of the the tool putting

1:25:28.969,1:25:34.070
layers with and maybe I should be a

1:25:32.809,1:25:42.369
little more explicit about this let me

1:25:34.070,1:25:50.119
draw a picture that would be clearer so

1:25:42.369,1:25:52.210
so you have an input like this

1:25:50.119,1:25:58.280
you didn't really complement let's say

1:25:52.210,1:26:02.570
combination of size three okay yeah

1:25:58.280,1:26:05.960
asteroid one ok I'm not I'm not gonna

1:26:02.570,1:26:07.909
draw all of them then you have playing

1:26:05.960,1:26:10.159
with subsampling sighs - so you pull

1:26:07.909,1:26:14.889
over - and you know so let's try this -

1:26:10.159,1:26:14.889
so you shaped by - no overlap

1:26:18.550,1:26:25.650
okay so here the input is this size one

1:26:23.140,1:26:28.540
two three four five six seven eight

1:26:25.650,1:26:31.530
because the combination is of size three

1:26:28.540,1:26:33.430
you get an output here of size six and

1:26:31.530,1:26:36.010
then when you know pulling with

1:26:33.430,1:26:37.960
subsampling of whist tried to you get

1:26:36.010,1:26:39.880
three outputs because you that divides

1:26:37.960,1:26:47.290
the output by two okay

1:26:39.880,1:26:49.530
let me add another one actually two okay

1:26:47.290,1:26:53.020
so now the output is 10

1:26:49.530,1:26:58.300
this guy is eight this guy is four

1:26:53.020,1:27:03.040
I can't do cognitions now also let's say

1:26:58.300,1:27:05.200
three only get two I only get two

1:27:03.040,1:27:10.320
outputs okay

1:27:05.200,1:27:12.670
oops hmm not sure way it doesn't draw

1:27:10.320,1:27:19.560
doesn't to draw you one that's

1:27:12.670,1:27:19.560
interesting aha

1:27:23.610,1:27:28.380
it doesn't react to clicks that's

1:27:25.750,1:27:28.380
interesting

1:27:33.960,1:27:38.830
okay

1:27:35.200,1:27:41.250
oh sure what's going on Oh sure is not

1:27:38.830,1:27:46.050
responding

1:27:41.250,1:27:49.410
all right I guess it crashed on me

1:27:46.050,1:27:49.410
well that's annoying

1:27:52.650,1:28:12.430
yeah definitely crashed and of course

1:28:02.950,1:28:18.730
you forgot it so okay so we're ten then

1:28:12.430,1:28:25.620
eight because of competition with three

1:28:18.730,1:28:31.210
then we have putting of size two with

1:28:25.620,1:28:33.930
stride tune so we get four then we have

1:28:31.210,1:28:38.950
competition with three so we get two

1:28:33.930,1:28:43.950
okay and then maybe putting again sighs

1:28:38.950,1:28:58.720
- and so simply until we get one okay so

1:28:43.950,1:29:02.920
ten input eight four two and then one

1:28:58.720,1:29:10.180
for the pooling this is condition three

1:29:02.920,1:29:15.360
you're right this is two and those are

1:29:10.180,1:29:19.780
three etc right now let's assume I add

1:29:15.360,1:29:23.440
15 units here okay so that's going to

1:29:19.780,1:29:30.120
add let's say four units here two

1:29:23.440,1:29:30.120
minutes here then

1:29:33.020,1:29:36.169
[Music]

1:29:41.150,1:29:47.990
yeah this one is like this and like that

1:29:44.840,1:29:51.050
so a good four and I get another one

1:29:47.990,1:29:54.470
here okay so now I had I had only one

1:29:51.050,1:29:57.890
I'd put in by adding four four inputs

1:29:54.470,1:30:03.470
here which is not 14 I've got two

1:29:57.890,1:30:06.350
outputs why for because I have to stride

1:30:03.470,1:30:10.100
of - okay so the overall subsampling

1:30:06.350,1:30:15.830
ratio from input to output is 4 it's 2

1:30:10.100,1:30:21.920
by 2 times 2 so now this is 12 and this

1:30:15.830,1:30:25.400
is 6 and this is 4 so that's a you know

1:30:21.920,1:30:26.810
a demonstration of the fact that you

1:30:25.400,1:30:28.340
know you can increase the size of the

1:30:26.810,1:30:30.170
input it will increase the size of every

1:30:28.340,1:30:31.850
layer and if you have a layer that has

1:30:30.170,1:30:33.320
size 1 and the it's a compositional

1:30:31.850,1:30:45.370
layer that it's size is going to be

1:30:33.320,1:30:45.370
increased yes

1:30:46.750,1:30:52.520
change the size of a letter like you

1:30:50.090,1:30:55.850
vertically horizontally yeah so there's

1:30:52.520,1:30:57.170
gonna be so first you have to train for

1:30:55.850,1:30:59.150
it if you want the system to have so

1:30:57.170,1:31:00.980
many variants to size you have to train

1:30:59.150,1:31:02.390
it with characters of various sizes you

1:31:00.980,1:31:03.380
can do this with the documentation if

1:31:02.390,1:31:06.380
you saw if your characters are

1:31:03.380,1:31:10.940
normalized that's the first thing second

1:31:06.380,1:31:13.190
thing is empirically simple conditional

1:31:10.940,1:31:15.470
Nets are only invariant - sighs - within

1:31:13.190,1:31:18.440
a factor of Rogersville factor like you

1:31:15.470,1:31:20.300
can increase the size by you know maybe

1:31:18.440,1:31:22.250
40 percent or something I mean change

1:31:20.300,1:31:25.600
the size about 40 percent plus minus 20

1:31:22.250,1:31:25.600
percent - something like that right

1:31:25.750,1:31:31.220
beyond that you you know you might have

1:31:29.480,1:31:32.630
kind of more trouble getting invariance

1:31:31.220,1:31:33.980
but people are trained with like you

1:31:32.630,1:31:36.320
know input input

1:31:33.980,1:31:40.430
I mean objects of sizes that vary by a

1:31:36.320,1:31:43.040
lot so the way to handle this is if you

1:31:40.430,1:31:44.660
want to handle variable size is that if

1:31:43.040,1:31:47.690
you have an image and you don't know

1:31:44.660,1:31:49.670
what size the objects are that are in

1:31:47.690,1:31:52.100
this image you apply to accomplish on

1:31:49.670,1:31:52.539
that to that image and then you take a

1:31:52.100,1:31:55.360
second

1:31:52.539,1:31:56.769
resisted by a factor of two to scale the

1:31:55.360,1:31:58.619
image by a factor to run the same

1:31:56.769,1:32:01.150
condition that on that new image and

1:31:58.619,1:32:02.530
then reduce it by a factor of two again

1:32:01.150,1:32:04.900
and run the same Congress on that again

1:32:02.530,1:32:06.309
on that image okay so the first

1:32:04.900,1:32:09.159
condition that we'll be able to detect

1:32:06.309,1:32:10.630
small objects within the image so let's

1:32:09.159,1:32:13.269
say your network has been trained to

1:32:10.630,1:32:15.099
detect objects of size I don't know 20

1:32:13.269,1:32:18.670
pixels like faces for example right

1:32:15.099,1:32:20.380
there are 20 pixels it will detect faces

1:32:18.670,1:32:22.659
are roughly 20 pixels within this image

1:32:20.380,1:32:24.039
and then when you search sample by a

1:32:22.659,1:32:27.219
factor of 2 and you apply the same

1:32:24.039,1:32:29.110
network it will detect faces that are 20

1:32:27.219,1:32:30.760
pixels within within the new image which

1:32:29.110,1:32:33.610
means there were 40 pixels in the

1:32:30.760,1:32:35.679
original image okay which the first

1:32:33.610,1:32:37.750
network will not see because you know

1:32:35.679,1:32:40.809
the face would be bigger than its input

1:32:37.750,1:32:44.139
window and then the next network over

1:32:40.809,1:32:46.539
will detect cases that are 80 pixels etc

1:32:44.139,1:32:47.980
right so then by combining the scores

1:32:46.539,1:32:49.659
from all of those and doing something

1:32:47.980,1:32:51.730
called on maximum suppression you can

1:32:49.659,1:32:53.769
actually do detection and localization

1:32:51.730,1:32:55.210
of objects people use considerably more

1:32:53.769,1:32:57.489
sophisticated techniques for detection

1:32:55.210,1:32:59.889
now and for localization that we'll talk

1:32:57.489,1:33:04.539
about next week but that's the basic

1:32:59.889,1:33:07.090
idea so let me conclude what our colony

1:33:04.539,1:33:08.590
is good for they're good for signals

1:33:07.090,1:33:10.809
that come to you in the form of a

1:33:08.590,1:33:15.699
multi-dimensional right but that

1:33:10.809,1:33:17.289
multi-dimensional array has to have two

1:33:15.699,1:33:20.170
characteristics at least the first one

1:33:17.289,1:33:24.449
is there is strong local correlations

1:33:20.170,1:33:28.090
between values so if you take an image

1:33:24.449,1:33:30.579
random image take two pixels within this

1:33:28.090,1:33:32.320
image two pixels are on your body those

1:33:30.579,1:33:34.780
two pixels are very likely to have very

1:33:32.320,1:33:37.119
similar colors okay take a picture of

1:33:34.780,1:33:40.119
this class for example two pixels on the

1:33:37.119,1:33:41.559
wall basically of the same color okay it

1:33:40.119,1:33:46.269
looks like there is a ton of object here

1:33:41.559,1:33:48.190
but animate objects but in fact most

1:33:46.269,1:33:52.199
means statistically your neighboring

1:33:48.190,1:33:55.360
pixels are essentially the same color as

1:33:52.199,1:33:57.190
you move the distance from two pixels

1:33:55.360,1:33:59.889
away and you compute the statistics of

1:33:57.190,1:34:02.579
has similar pixels are as a function of

1:33:59.889,1:34:04.720
distance they're less and less similar

1:34:02.579,1:34:08.650
so what does that mean is

1:34:04.720,1:34:10.870
because nearby pixels are likely to have

1:34:08.650,1:34:12.940
similar colors that means that when you

1:34:10.870,1:34:16.750
take a patch of pixels said father five

1:34:12.940,1:34:19.420
or eight by eight or something the type

1:34:16.750,1:34:21.010
of patch you're willing to observe is

1:34:19.420,1:34:25.270
very likely to be kind of a smoothly

1:34:21.010,1:34:27.280
varying color or maybe with an edge but

1:34:25.270,1:34:29.590
among all the possible combinations of

1:34:27.280,1:34:31.810
25 pixels the ones that you actually

1:34:29.590,1:34:35.590
observed in natural images is a tiny

1:34:31.810,1:34:38.020
subset what that means is that it's

1:34:35.590,1:34:41.980
advantageous to represent the content of

1:34:38.020,1:34:43.810
that patch by a vector with perhaps less

1:34:41.980,1:34:45.460
than 25 values that represent the

1:34:43.810,1:34:46.690
content of that patch is there an edge

1:34:45.460,1:34:48.070
is it uniform

1:34:46.690,1:34:50.050
what color is it you know things like

1:34:48.070,1:34:51.460
that right and that's basically what the

1:34:50.050,1:34:54.730
compositions in the first row you

1:34:51.460,1:34:56.380
accomplish on that are doing okay so yes

1:34:54.730,1:34:58.480
you have if you have local correlations

1:34:56.380,1:35:00.250
there is an advantage in detecting local

1:34:58.480,1:35:01.420
features that's what we observe in the

1:35:00.250,1:35:06.070
brain that's what congressional Nets are

1:35:01.420,1:35:09.520
doing the idea of locality if you feed a

1:35:06.070,1:35:11.380
commercial met with commuted pixels it's

1:35:09.520,1:35:13.990
not going to be able to do a good job at

1:35:11.380,1:35:18.760
recognizing your image images even if

1:35:13.990,1:35:21.910
the permutation is fixed right if we

1:35:18.760,1:35:27.280
connected that doesn't care about

1:35:21.910,1:35:30.730
permutations then the second

1:35:27.280,1:35:32.650
characteristics is that features are

1:35:30.730,1:35:34.660
important may appear anywhere on the

1:35:32.650,1:35:35.130
image so that's what justifies shared

1:35:34.660,1:35:38.080
weights

1:35:35.130,1:35:41.200
okay the correlation justifies local

1:35:38.080,1:35:43.720
connections the the fact that features

1:35:41.200,1:35:48.310
can appear anywhere that the statistics

1:35:43.720,1:35:50.980
of images or the signal is uniform means

1:35:48.310,1:35:53.590
that you need to have repeated feature

1:35:50.980,1:35:58.740
detectors for every location and that's

1:35:53.590,1:35:58.740
where shared weights come into play

1:36:01.490,1:36:06.260
it does certify the pudding because the

1:36:04.530,1:36:08.550
pudding is if you want invariance to

1:36:06.260,1:36:10.440
variations in the location of those

1:36:08.550,1:36:12.840
characteristic features and so if the

1:36:10.440,1:36:14.640
objects you're trying to recognize don't

1:36:12.840,1:36:18.620
change their nature by kind of being

1:36:14.640,1:36:18.620
slightly distorted then you want pooling

1:36:20.660,1:36:28.080
so people are his comrade for cancer

1:36:22.950,1:36:30.150
stuff image video text stitch so speech

1:36:28.080,1:36:33.000
actually is pretty sweet recognition

1:36:30.150,1:36:36.720
comm units are used a lot time series

1:36:33.000,1:36:39.120
prediction you know things like that and

1:36:36.720,1:36:41.610
you know biomedical image analysis so if

1:36:39.120,1:36:45.450
you want to you know analyze an MRI for

1:36:41.610,1:36:47.490
example MRI or CT scan is a 3d image as

1:36:45.450,1:36:49.020
humans we can't because we don't have a

1:36:47.490,1:36:51.510
good visualization technology we can't

1:36:49.020,1:36:53.910
really apprehend like you know

1:36:51.510,1:36:56.160
understand a kind of a 3d value but it's

1:36:53.910,1:36:57.840
really emotional image the ComNet is

1:36:56.160,1:37:00.330
fine you know if you did a 3d image and

1:36:57.840,1:37:01.440
it will deal with it that's a big

1:37:00.330,1:37:04.700
advantage because you don't have to go

1:37:01.440,1:37:11.880
through slices to kind of figure out the

1:37:04.700,1:37:13.230
object in the image and then the last

1:37:11.880,1:37:14.520
thing here at the bottom I don't know if

1:37:13.230,1:37:16.680
you guys know where hyperspectral image

1:37:14.520,1:37:18.480
images are so hyper spectral image is an

1:37:16.680,1:37:20.670
image where you know almost natural

1:37:18.480,1:37:22.050
color images I mean images that you

1:37:20.670,1:37:26.610
collect with a normal camera you get

1:37:22.050,1:37:29.160
three color components RGB but we can

1:37:26.610,1:37:32.340
build cameras with way more spectral

1:37:29.160,1:37:34.110
bands than this and that's particularly

1:37:32.340,1:37:38.220
the case for satellite imaging where

1:37:34.110,1:37:40.500
some cameras have many many spectral

1:37:38.220,1:37:43.080
bands going from infrared to ultraviolet

1:37:40.500,1:37:46.260
and that gives you a lot of information

1:37:43.080,1:37:49.050
about what you see in each pixel some

1:37:46.260,1:37:51.450
tiny animals I have small brains find it

1:37:49.050,1:37:53.640
easier to process hyper spectral images

1:37:51.450,1:37:56.250
of low resolution than high resolution

1:37:53.640,1:37:58.770
images with just three colors for

1:37:56.250,1:38:01.130
example the is particularly type of

1:37:58.770,1:38:03.690
shrimp right they have those beautiful

1:38:01.130,1:38:05.580
eyes and they have like 17 spectral

1:38:03.690,1:38:06.960
bands or something but super low

1:38:05.580,1:38:09.270
resolution and they have a tiny brain to

1:38:06.960,1:38:14.850
process it

1:38:09.270,1:38:14.850
okay that's one for today see you


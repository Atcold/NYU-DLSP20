---
lang-ref: ch.03-1
title: Visualization of neural networks parameter transformation and fundamental concepts of convolution
authors: Jiuhong Xiao, Trieu Trinh, Elliot Silva, Calliea Pan
date: 10 February 2020
typora-root-url: 03-1
---


## Visualization of Neural Networks and Parameter Transformations


### Visualization of Neural Networks

In this section we will show the visualization of neural network and the results of linear transformation and ReLU operator.

<center><img src="Network.png" alt="Network" style="zoom:35%;" /><br>
Fig. 1 Network Structure</center>

Figure 1 is a neural network for visualization. Usually, the input of neural networks is in the bottom side or on the left, and the output is in the top side or on the right. In Figure 1, pink neurons are inputs, and blue neurons are outputs. In this network, we have 4 hidden layers, which means we have 6 layers in total (4 hidden layers + 1 input layer + 1 output layer). We use 2-by-2 matrix as our weight matrix $W$. This is because we want to transform input plane into another plane. The linear transformation includes shearing, rotation, reflection and scaling.

<center><img src="Visual1.png" alt="Network" style="zoom:35%;" /><br>
Fig. 2 Visualization of folding space</center>

The transformation of each layer is like folding our plane into some specific regions as shown in Figure 2. In the experiment, we find that if we have only 2 neurons in each hidden layer, the optimization will take longer; the optimization is easier if we have more neurons in hidden layers. So we have one question: Why is it hard to train if we have few neurons? We will answer after the visualization of $\texttt{ReLU}$.

| <img src="Visual2a.png" alt="Network" style="zoom:45%;" /> | <img src="Visual2b.png" alt="Network" style="zoom:45%;" /> |
|(a)|(b)|

<center>Fig. 3 Visualization of ReLU operator</center>

When we split each hidden layers, we can see each layers do some steps of linear transformation, and then do ReLU operation, which eliminates the negative values to zero. In Figure 3(a)(b), we can see the visualization of ReLU operator. ReLU operator helps us do nonlinear transformation. After some transformations, we eventually obtain linearly separable data in Figure 4.

<center><img src="Visual3.png" alt="Network" style="zoom:30%;" /><br>
Fig. 4 Visualization of Outputs</center>

So now we know the reason why 2-neuron hidden layers are hard to train. Our 6-layer network has one bias in each hidden layers. Therefore if one of these biases moves points out of top-right quadrant, then ReLU will eliminate these points to zero. After that, no matter how the latter layers transform, the values will remain zero. To make a neural network easier to train we can make either the network "fat" by adding more neurons in hidden layers, or we can add more hidden layers, or a combination of the two methods. For the rest of the semester we will dig further into what is the best architecture for the network, stay tuned.


### Parameter Transformations

In this section, we will introduce different forms of parameter transformations and its application.


#### General Form of Parameter Transformation

General parameter transformation means that our parameter vector $w$ is the output of a function. By this transformation, we can map original parameter space into another space. In Figure 5, $w$ is actually the output of $H$ with the parameter $u$. $G(x,w)$ is a network and $C(y,\bar y)$ is a cost function. The backprop formula is also changed as follows,

$$
u \leftarrow u - \eta\frac{\partial H}{\partial u}^\top\frac{\partial C}{\partial w}^\top
$$

$$
w \leftarrow w - \eta\frac{\partial H}{\partial u}\frac{\partial H}{\partial u}^\top\frac{\partial C}{\partial w}^\top
$$

These formula is applied in a matrix form. Note that the dimension of right terms should be consistent. The dimension of $u$,$w$,$\frac{\partial H}{\partial u}^\top$,$\frac{\partial C}{\partial w}^\top$ are $[N_u \times 1]$,$[N_w \times 1]$,$[N_u \times N_w]$,$[N_w \times 1]$, respectively. Therefore, the dimension of backprop formula is consistent.

<center><img src="PT.png" alt="Network" style="zoom:35%;" /><br>
Fig. 5 General Form of Parameter Transformations</center>


#### A Simple Parameter Transformation: Weight Sharing

A Weight Sharing Transformation means $H(u)$ just replicates one component of $u$ into multiple components of $w$. $H(u)$ is like a **Y** branch to copy $u_1$ to $w_1$, $w_2$. This can be expressed as,

$$
w_1 = w_2 = u_1, w_3 = w_4 = u_2
$$

We force shared parameters to be equal, so the gradient w.r.t. shared parameters will be summed in the backprop. So for example the gradient of the cost function $C(y, \bar y)$ with respect to $u_1$ will be the sum of the gradient of the cost function $C(y, \bar y)$ with respect to $w_1$ and the gradient of the cost function $C(y, \bar y)$ with respect to $w_2$.


#### Hypernetwork

Hypernetwork is a network where the weights of one network is the output of another network. Figure 6 shows a framework of "hypernetwork". Here the function $H$ as a network with the parameter vector $u$ and the input $x$. As a result, the weights of $G(x,w)$ are dynamically configured by network $H(x,u)$. This idea is old but very powerful even at present.

<center><img src="HyperNetwork.png" alt="Network" style="zoom:35%;" /><br>
Fig. 6 "Hypernetwork"</center>


#### Motif Detection in Sequential Data

Weight sharing transformation can be applied to motif detection. Motif detection means to find some motifs in sequential data like keywords in speech or text. As Figure 7 shows, one solution is to use a sliding window on data, which moves the weight-shared function to detect a particular motif (i.e. a particular sound in speech signal), and the outputs (i.e. a score) goes into a maximum function.

<center><img src="Motif.png" alt="Network" style="zoom:30%;" /><br>
Fig. 7 Motif Detection for Sequential Data</center>

In this example we have 5 of those functions. As a result of this solution, we sum up five gradients and backpropagate to the parameter $w$. We do not want the implicit accumulation of these gradients, so we need to use `zero_grad()` in PyTorch to initialize the gradient.


#### Motif Detection in Images

The other useful situation is motif detection in images. We usually swipe our "templates" over images to detect the shapes independent of position and distortion of shapes. A simple example is to distinguish between "C" and "D",  as Figure 8 shows. The difference between "C" and "D" is that "C" has two endpoints and "D" has two corners. So we can design "endpoint templates" and "corner templates". If the shape is similar to the "templates", it will have thresholded outputs. Then we can distinguish letters from these outputs by summing them up. In Figure 8, the network detects two endpoints and zero corners, so it activates "C".

<center><img src="MotifImage.png" alt="Network" style="zoom:35%;" /><br>
Fig. 8 Motif Detection for Images</center>

It is also important that our "template matching" should be shift-invariant - when we shift the input, the output or the letter detected shouldn't change. This can be perfectly solved with weight sharing transformation. As Figure 9 shows, when we change the location of "D", we can still detect the corner motifs even though they are shifted. When we sum up the motifs, it will activate the "D" detection.

<center><img src="ShiftInvariance.png" alt="Network" style="zoom:35%;" /><br>
Fig. 9 Shift Invariance</center>

This using local detectors and sum up detection to detect which letter the image represent is a hand-crafted method we use for many years. So it comes a problem: How can we design these "templates" automatically? Can we use neural networks to learn these "templates"? Next, We will introduce **convolution** , that is, the operation which we match images with "templates".


## Discrete Convolution


### Convolution

The precise mathematical definition of convolution in 1-dimensional case between input $x$ and $w$ is:

$$y_i = \sum_j w_j x_{i-j}$$

In words, the $i$-th output is computed as the dot product between the **reversed** $w$ and a window of the same size in $x$. To compute the full output, start the window at the beginning, shift this window by one entry each time and repeat until $x$ is exhausted.


### Cross-correlation

In practice, the convention adopted in deep learning frameworks such as PyTorch is slightly different. Convolution in PyTorch is implemented where $w$ is **not reversed**:

$$y_i = \sum_j w_j x_{i+j}$$

Mathematicians call this formulation "cross-correlation". In our context, this difference is just a difference in convention. Practically, cross-correlation and convolution can be interchangeable if one reads the weights stored in memory forward or backward.

Being aware of this difference is important, for example, when one want to make use of certain mathematical properties of convolution/correlation from mathematical texts.


### Higher dimensional convolution

For two dimensional inputs such as images, we make use of the two dimensional version of convolution:

$$y_{ij} = \sum_{kl} w_{kl} x_{i+k, j+l}$$

This definition can be extended beyond two dimensions to three, four dimensions similarly. Here $w$ is called the *convolution kernel*


### Regular twists that can be made with the convolutional operator in DCNNs

1. **Striding**: instead of shifting the window in $x$ one entry at a time, one can do so with larger step (two, three entries at a time).
Example: Suppose the input $x$ is one dimensional and has size of 100 and $w$ has size 5. The output size with striding 1 or 2 is shown in the table below:

| Stride       | 1                          | 2                          |
| ------------ | -------------------------- | -------------------------- |
| Output size: | $\frac{100 - (5-1)}{1}=96$ | $\frac{100 - (5-1)}{2}=48$ |

2. **Padding**: Very often in designing the Deep Neural Networks architecture, we want the output of convolution to be of the same size as the input. This can be achieved by padding the input ends with a number of (typically) zero entries, usually in both sides. Padding is done mostly for convenience. It can sometimes impact performance and create funny border effects, that said, when the non-linearity is ReLU, zero padding is not unreasonable.


## Deep Convolution Neural Networks (DCNNs)

As previously described, deep neural networks are typically organized as repeated alternation between linear operator and point-wise nonlinearity layers. In convolutional neural networks, the linear operator will be the convolution operator described above. There is also an optional third type of layer called the pooling layer.

The reason for stacking multiple such layers is that we want to build a hierarchical representation of the data. Convolution nets does not have to be limited to processing images, they are also successfully applied to speech and languages. Technically they can be applied to any type of data that comes in the form of arrays, although we also expect certain properties that this array has to satisfy.

Why would we want to capture the hierarchical representation of the world? Because the world we live in is compositional. This point is alluded to in previous sections. Such hierarchical nature can be observed from the fact that local pixels assembled to form simple motifs such as oriented edges. These edges in turn assembled to form local features such as corners, T-junction, etc. Those assembled to form motifs that are even more abstract. This keeps going until eventually parts of object and objects we observed in the real world are formed.

<center><img src="cnn_features.png" alt="CNN Features" style="zoom:35%;" /><br>
Figure 10. Feature visualization of convolutional net trained on ImageNet from [Zeiler & Fergus 2013]</center>


This compositional hierarchical nature we observed in the natural world is there not just because of our visual perception, but also because it is true at the physical level. At the lowest level of description, we have elementary particles, which assembled to form atoms, atoms together form molecules, and so on this process builds up materials, parts of objects and objects in the physical world.

The compositional nature of the world might be the answer to Einstein rhetorical question on how human understand the world they live in:

> The most incomprehensible thing about the universe is that it is comprehensible.

The fact that human understands the world thanks to this compositional nature still seems like a conspiracy to Yann. It is, however, argued that without compositionality, it will take even more magic for human to comprehend the world they live in. Quoting our great mathematician Stuart Geman:

> The world is compositional or God exists.


## Inspirations from Biology

So why should Deep Learning be rooted in the idea that our world is comprehensible and has a compositional nature? Research conducted by Simon Thorpe helped motivate this further. He showed that the way we recognize everyday objects is extremely fast. His experiments involved flashing a set of images every 100ms, and then asking users to identify these images, which they were able to do successfully. This demonstrated that it takes about 100ms for humans to detect objects. Furthermore, consider the diagram below, illustrating parts of the brain annotated with the time it takes for neurons to propagate from one area to the next:

<center><img src="Simon_Thorpe.png" alt="Simon_Thorpe" style="zoom:55%;" /></center>

<div align="center">Figure 11. Simon Thorpe's model of visual information flow in the brain <div>

Signals pass from the retina to the LGN (helps with contrast enhancement, gate control, etc.), then to the V1 primary visual cortex, V2, V4, then to the inferotemporal cortex (PIT), which is the part of the brain where categories are defined. Observations from open-brain surgery showed that if you show a human a film, neurons in the PIT will fire only when they detect certain images -- such as Jennifer Aniston or a person's grandmother -- and nothing else. The neural firings are invariant to things such as position, size, illumination, your grandmother's orientation, what she's wearing, etc.

Furthermore, the fast reaction times with which humans were able to categorize these items -- barely enough time for a few spikes to get through -- demonstrates that it's possible to do this without additional time spent on complex recurrent computations. Rather, this is a single feed-forward process.

These insights suggested that we could develop a neural network architecture which is completely feed-forward, yet still able to solve the problem of recognition, in a way which is invariant to irrelevant transformations of the input.

One further insight from the human brain comes from Gallant & Van Essen, whose model of the human brain illustrates two distinct pathways:

<center><img src="Gallant_and_Van_Essen.png" alt="Gallant_and_Van_Essen" style="zoom:55%;" /></center>

<div align="center">Figure 12. Gallen & Van Essen's model of dorsal & ventral pathways in the brain <div>

The right side shows the ventral pathway, which tells you what you're looking at, while the left side shows the dorsal pathway, which identifies locations, geometry, and motion. They seem fairly separate in the human (and primate) visual cortex (with a few interactions between them of course).


### Hubel & Weisel's contributions (1962)

<center><img src="Hubel_and_Weisel.png" alt="Hubel_and_Weisel" style="zoom:55%;" /></center>

<div align="center">Figure 13. Hubel & Weisel's experiments with visual stimuli in cat brains <div>

Hubel and Weisel experiments used electrodes to measure neural firings in cat brains in response to visual stimuli. They discovered that neurons in the V1 region are only sensitive to certain areas of a visual field (called "receptive fields"), and detect oriented edges in that area. For example, they demonstrated that if you showed the cat a vertical bar and start rotating it, at a particular angle the neuron will fire. Similarly, as the bar moves away from that angle, the activation of the neuron diminishes. These activation-selective neurons Hubel & Weisel named "simple cells", for their ability to detect local features.

They also discovered that if you move the bar out of the receptive field, that particular neuron doesn't fire any more, but another neuron will. There are local feature detectors corresponding to all areas of the visual field, hence the idea that the human brain processes visual information as a collection of "convolutions".

Another type of neuron, which they named "complex cells", aggregate the output of multiple simple cells within a certain area. We can think of these as computing an aggregate of the activations using a function such as maximum, sum, sum of squares, or any other function not depending on the order. These complex cells detect edges and orientations in a region, regardless of where those stimuli lie specifically within the region. In other words, they are shift-invariant with respect to small variations in positions of the input.


### Fukushima's contributions (1982)

<center><img src="Fukushima.png" alt="Fukushima" style="zoom:55%;" /></center>

<div align="center">Figure 14. Fukushima's CNN model <div>

Fukushima was the first to implement the idea of multiple layers of simple cells and complex cells with computer models, using a dataset of handwritten digits. Some of these feature detectors were hand-crafted or learned, though the learning used unsupervised clustering algorithms, trained separately for each layer, as backpropagation was not yet in use.

Yann LeCun came in a few years later (1989, 1998) and implemented the same architecture, but this time trained them in a supervised setting using backpropagation. This is widely regarded as the genesis of modern convolutional neural networks. (Note: Riesenhuber at MIT in 1999 also re-discovered this architecture, though he didn't use backpropagation.)

0:00:00.399,0:00:03.840
uh okay there's a number of topics i

0:00:02.159,0:00:05.200
want to talk about today uh this is our

0:00:03.840,0:00:06.560
last lecture and i want to keep some

0:00:05.200,0:00:08.240
time at the end for

0:00:06.560,0:00:09.920
like you know random questions on random

0:00:08.240,0:00:11.519
topics that you might want to ask

0:00:09.920,0:00:13.599
you know maybe sort of general questions

0:00:11.519,0:00:16.560
in general about like approaches to

0:00:13.599,0:00:18.400
machine learning ai deep learning etc uh

0:00:16.560,0:00:19.840
you know maybe questions really

0:00:18.400,0:00:21.840
you know kind of you know maybe that may

0:00:19.840,0:00:24.640
be a little more philosophical but

0:00:21.840,0:00:25.760
um but let me start with uh something

0:00:24.640,0:00:27.119
more concrete so

0:00:25.760,0:00:28.800
um i want to talk about structure

0:00:27.119,0:00:30.000
prediction i alluded to this topic a

0:00:28.800,0:00:33.360
number of times during the

0:00:30.000,0:00:34.960
previous lectures but i think not enough

0:00:33.360,0:00:36.880
in depth for

0:00:34.960,0:00:38.719
for most people to understand so i want

0:00:36.880,0:00:42.079
to come back to this

0:00:38.719,0:00:45.200
so structure prediction is basically

0:00:42.079,0:00:47.200
the problem of uh predicting a variable

0:00:45.200,0:00:48.800
that itself is not just like a single

0:00:47.200,0:00:50.719
category or

0:00:48.800,0:00:51.920
uh you know a single object but

0:00:50.719,0:00:54.079
basically a sort of

0:00:51.920,0:00:55.760
a combinatorial object so for for

0:00:54.079,0:00:57.199
example things like a sentence

0:00:55.760,0:00:59.199
you're doing speech recognition you're

0:00:57.199,0:01:01.039
doing handwriting recognition you're

0:00:59.199,0:01:04.400
doing natural language

0:01:01.039,0:01:07.200
generation or translation and

0:01:04.400,0:01:08.479
what you need to output is a sort of

0:01:07.200,0:01:12.640
grammatically correct

0:01:08.479,0:01:14.960
consistent sequence of symbols

0:01:12.640,0:01:16.159
and there is no you can't say that there

0:01:14.960,0:01:18.799
is a

0:01:16.159,0:01:19.759
a finite number of possibilities of the

0:01:18.799,0:01:21.040
output because the

0:01:19.759,0:01:23.439
the length of the output might be

0:01:21.040,0:01:25.840
variable uh but even if it's

0:01:23.439,0:01:27.759
if the length is has a maximum and the

0:01:25.840,0:01:31.119
number is in principle finite

0:01:27.759,0:01:33.280
because it's combinatorial um there's no

0:01:31.119,0:01:36.799
way to kind of enumerate all possible

0:01:33.280,0:01:38.560
different outputs and so to express the

0:01:36.799,0:01:40.320
the type constraint that the output has

0:01:38.560,0:01:42.640
to reflect that's that's

0:01:40.320,0:01:44.560
what's called structural prediction and

0:01:42.640,0:01:45.759
this uh

0:01:44.560,0:01:47.759
you know there's a lot of work on this

0:01:45.759,0:01:49.600
going back uh

0:01:47.759,0:01:51.200
uh to basically the early days of speech

0:01:49.600,0:01:52.320
recognition so this is not a recent

0:01:51.200,0:01:55.520
problem

0:01:52.320,0:01:57.920
um and in fact the the i'm going to

0:01:55.520,0:01:58.719
start by a little bit of history uh in

0:01:57.920,0:02:01.680
my mind the

0:01:58.719,0:02:02.719
the first model to do structural

0:02:01.680,0:02:04.079
prediction

0:02:02.719,0:02:07.520
combined with things like neural

0:02:04.079,0:02:10.000
networks uh trained discriminatively

0:02:07.520,0:02:11.200
was this speech recognition model for uh

0:02:10.000,0:02:14.640
four words

0:02:11.200,0:02:16.560
by uh javier de la cruz ii back in the

0:02:14.640,0:02:18.000
early 90s 1991

0:02:16.560,0:02:20.400
and there was kind of similar work about

0:02:18.000,0:02:22.319
the same time by joshua banjo and about

0:02:20.400,0:02:24.239
a year or two later by patrick hefner

0:02:22.319,0:02:25.680
so these are people who worked on

0:02:24.239,0:02:27.360
discriminative training

0:02:25.680,0:02:28.720
for systems are supposed to produce a

0:02:27.360,0:02:31.920
sequence of symbols

0:02:28.720,0:02:33.440
uh from uh uh you know a signal that

0:02:31.920,0:02:36.000
says speech

0:02:33.440,0:02:37.440
or handwriting and where once the the

0:02:36.000,0:02:40.800
first step basically

0:02:37.440,0:02:42.720
is a neural net uh here

0:02:40.800,0:02:44.480
uh in this this neural net i wrote td

0:02:42.720,0:02:46.160
and n this means time data neural net is

0:02:44.480,0:02:48.319
basically a temporal convolutional net

0:02:46.160,0:02:49.599
uh so this is the you know the first

0:02:48.319,0:02:53.200
model i i can

0:02:49.599,0:02:56.080
i can find of uh

0:02:53.200,0:02:57.760
structural prediction uh sort of

0:02:56.080,0:02:58.080
hybridized with with neural nets if you

0:02:57.760,0:03:00.400
want

0:02:58.080,0:03:02.640
so the problem that uh the idler

0:03:00.400,0:03:06.720
currently ubuntu are trying to solve was

0:03:02.640,0:03:09.360
recognizing words using a neural net

0:03:06.720,0:03:10.319
and to some extent the modern approaches

0:03:09.360,0:03:13.360
are kind of

0:03:10.319,0:03:15.680
similar to this but in in some ways

0:03:13.360,0:03:17.360
so the speech signal is represented as a

0:03:15.680,0:03:20.080
sequence of acoustic vectors

0:03:17.360,0:03:21.440
so those are uh you know you you you

0:03:20.080,0:03:23.280
slice the signal into

0:03:21.440,0:03:25.280
little chunks and then on one of the

0:03:23.280,0:03:25.599
chunks you do a for your transform which

0:03:25.280,0:03:27.920
uh

0:03:25.599,0:03:29.440
fredo has explained to you and you turn

0:03:27.920,0:03:30.799
it into basically a feature vector and

0:03:29.440,0:03:32.799
you have one of those vectors

0:03:30.799,0:03:33.920
it's typically 30 dimensions or so maybe

0:03:32.799,0:03:35.920
40.

0:03:33.920,0:03:37.280
and you want one of those vectors every

0:03:35.920,0:03:39.519
10 milliseconds so

0:03:37.280,0:03:40.959
so about 100 times per second so you

0:03:39.519,0:03:42.159
have a sequence of 40 dimensional

0:03:40.959,0:03:45.360
vectors

0:03:42.159,0:03:46.799
uh uh about 100 per second and you you

0:03:45.360,0:03:48.720
you run this through a convolutional net

0:03:46.799,0:03:49.920
a temporal convolutional net and at the

0:03:48.720,0:03:52.319
output of it

0:03:49.920,0:03:54.080
uh what you get is a sequence of feature

0:03:52.319,0:03:55.599
vectors you can think of it this way

0:03:54.080,0:03:57.840
uh in modern systems those feature

0:03:55.599,0:03:59.840
vectors are actually uh

0:03:57.840,0:04:02.400
kind of soft max factors that indicate a

0:03:59.840,0:04:06.080
category but in that case it wasn't

0:04:02.400,0:04:08.480
um and those can be at the same

0:04:06.080,0:04:09.760
rate or they can be slower so if the if

0:04:08.480,0:04:12.000
the neural net has

0:04:09.760,0:04:13.439
if the conversion that has temporal sep

0:04:12.000,0:04:14.959
sampling you're not going to get

0:04:13.439,0:04:17.440
10 of those feature vectors per second

0:04:14.959,0:04:19.199
you might get you know 25

0:04:17.440,0:04:20.479
um you know two and a half or something

0:04:19.199,0:04:22.160
right oh i'm sorry

0:04:20.479,0:04:23.680
at the input there is a hundred so if

0:04:22.160,0:04:24.160
you have a sub sampling by a factor of

0:04:23.680,0:04:25.840
four

0:04:24.160,0:04:28.320
you will get 25 feature vectors per

0:04:25.840,0:04:29.759
second not 100.

0:04:28.320,0:04:31.520
or something like that now here's the

0:04:29.759,0:04:34.880
problem the problem is you want to

0:04:31.520,0:04:38.639
recognize which word was just pronounced

0:04:34.880,0:04:41.680
and different people will pronounce

0:04:38.639,0:04:42.639
the same word at different speeds and so

0:04:41.680,0:04:44.639
what you need to do

0:04:42.639,0:04:46.400
is uh what's called dynamic time wiping

0:04:44.639,0:04:47.440
and i explained this uh already in the

0:04:46.400,0:04:49.360
in the past

0:04:47.440,0:04:51.199
so let's imagine that you've recorded

0:04:49.360,0:04:52.560
this person you don't want to do

0:04:51.199,0:04:55.040
uh speaker independent speech

0:04:52.560,0:04:58.240
recognition for now just speaker

0:04:55.040,0:05:01.919
specific so you've recorded that person

0:04:58.240,0:05:05.280
saying uh uh you know let's say the

0:05:01.919,0:05:08.320
the ten digits uh uh spoken digits

0:05:05.280,0:05:09.520
like zero one two three four five etc

0:05:08.320,0:05:11.520
because you're only interested in

0:05:09.520,0:05:13.680
recognizing uh

0:05:11.520,0:05:14.720
uh spoken digits isolated spoken digits

0:05:13.680,0:05:17.440
maybe this is

0:05:14.720,0:05:18.000
a system that is supposed to uh you know

0:05:17.440,0:05:20.880
dial

0:05:18.000,0:05:22.320
a number uh on your phone right so it

0:05:20.880,0:05:23.680
just needs to recognize sequences of

0:05:22.320,0:05:26.080
digits

0:05:23.680,0:05:28.160
or perhaps it's a very simple speech

0:05:26.080,0:05:31.360
recognition system that tries to spot

0:05:28.160,0:05:33.280
the the wake up word for uh you know

0:05:31.360,0:05:34.720
amazon alexa or something like this

0:05:33.280,0:05:37.039
so the only thing the system is supposed

0:05:34.720,0:05:39.600
to recognize is alexa or hey google

0:05:37.039,0:05:40.960
or you know something like that right a

0:05:39.600,0:05:44.160
wake up word

0:05:40.960,0:05:45.840
um so uh the system may have a bunch of

0:05:44.160,0:05:47.440
uh pre-recorded templates that

0:05:45.840,0:05:50.800
correspond to uh

0:05:47.440,0:05:54.240
sequences of future vectors uh that were

0:05:50.800,0:05:57.280
produced by uh someone

0:05:54.240,0:05:59.120
speaking each of the words um

0:05:57.280,0:06:01.520
and now the way you want to train the

0:05:59.120,0:06:03.919
system is that you would like to

0:06:01.520,0:06:06.080
train the neural net at the same time as

0:06:03.919,0:06:08.160
the template so that the overall system

0:06:06.080,0:06:09.919
recognizes the words uh as best as

0:06:08.160,0:06:11.440
possible this is a classification

0:06:09.919,0:06:12.800
problem

0:06:11.440,0:06:14.720
but there is a latent variable in it and

0:06:12.800,0:06:16.319
the latent variable is how

0:06:14.720,0:06:17.840
how are you going to time warp the

0:06:16.319,0:06:19.120
sequence of future vectors so that it

0:06:17.840,0:06:20.800
matches the length

0:06:19.120,0:06:22.000
of those templates and again i'm kind of

0:06:20.800,0:06:24.560
repeating myself because i talked a

0:06:22.000,0:06:26.240
little bit about this before

0:06:24.560,0:06:27.759
so you do this with damage time wiping

0:06:26.240,0:06:31.039
and what that consists of

0:06:27.759,0:06:32.400
is that you line up all the feature

0:06:31.039,0:06:34.479
vectors

0:06:32.400,0:06:36.000
along the bottom here of this so this is

0:06:34.479,0:06:38.160
think of this as a matrix

0:06:36.000,0:06:40.400
you line up all the future vectors uh at

0:06:38.160,0:06:42.800
the uh at the input so that you get the

0:06:40.400,0:06:44.319
the sequence of future vectors are here

0:06:42.800,0:06:45.840
and then you put the sequence of

0:06:44.319,0:06:47.360
template vectors so

0:06:45.840,0:06:50.000
uh future vectors coming out of the

0:06:47.360,0:06:53.199
template on this axis

0:06:50.000,0:06:54.880
and then each entry in the matrix

0:06:53.199,0:06:57.039
is an indication of the distance between

0:06:54.880,0:07:00.479
the feature vector

0:06:57.039,0:07:02.800
here and the feature vector there okay

0:07:00.479,0:07:04.639
so you get this probability matrix with

0:07:02.800,0:07:06.479
uh distances between feature vectors

0:07:04.639,0:07:09.360
essentially

0:07:06.479,0:07:10.880
um and the best uh the best way to map

0:07:09.360,0:07:13.039
the sequence of each vector into another

0:07:10.880,0:07:16.240
one to to see if they fit

0:07:13.039,0:07:17.360
is to basically view this matrix as kind

0:07:16.240,0:07:20.400
of a

0:07:17.360,0:07:21.840
a set of nodes in a graph and what you

0:07:20.400,0:07:24.160
want is go from the

0:07:21.840,0:07:26.319
lower left hand corner of that graph to

0:07:24.160,0:07:29.440
the upper right hand corner

0:07:26.319,0:07:31.599
by going through a path that

0:07:29.440,0:07:32.639
minimizes the sum of the distances along

0:07:31.599,0:07:35.120
the past

0:07:32.639,0:07:35.840
okay uh so obviously you're going to

0:07:35.120,0:07:38.319
have to go

0:07:35.840,0:07:39.520
horizontally you know more steps than

0:07:38.319,0:07:40.720
you go vertically so

0:07:39.520,0:07:42.080
on a few occasions you're going to

0:07:40.720,0:07:43.759
diagonally in a few occasions you're

0:07:42.080,0:07:44.879
going to go vertically up but on many

0:07:43.759,0:07:47.440
occasions you're probably going to go

0:07:44.879,0:07:49.680
horizontally

0:07:47.440,0:07:50.879
to the to the right and that would be

0:07:49.680,0:07:52.479
the situation where

0:07:50.879,0:07:54.000
you have multiple feature vectors here

0:07:52.479,0:07:54.560
that are essentially identical that

0:07:54.000,0:07:56.160
match

0:07:54.560,0:07:58.000
the single feature vector in the

0:07:56.160,0:08:00.479
template okay so for example

0:07:58.000,0:08:01.280
you pronounce the word seven very slowly

0:08:00.479,0:08:03.199
the uh

0:08:01.280,0:08:04.879
uh initially it's gonna be you know

0:08:03.199,0:08:06.160
repeated multiple times because you you

0:08:04.879,0:08:07.039
stick on it for like a quarter of a

0:08:06.160,0:08:09.919
second so

0:08:07.039,0:08:11.599
you're gonna have 25 instances of this

0:08:09.919,0:08:13.599
and all of this will be mapped to

0:08:11.599,0:08:14.720
maybe a single feature actor here that

0:08:13.599,0:08:18.160
corresponds to this

0:08:14.720,0:08:21.440
sound up um so

0:08:18.160,0:08:23.280
finding this path that best warps the

0:08:21.440,0:08:24.720
the the sequence into the template

0:08:23.280,0:08:26.160
sequence

0:08:24.720,0:08:27.759
is like minimizing with respect to a

0:08:26.160,0:08:29.520
related variable z okay

0:08:27.759,0:08:30.800
so it's like you have an energy function

0:08:29.520,0:08:31.360
and you're minimizing this energy

0:08:30.800,0:08:32.719
function

0:08:31.360,0:08:34.719
with respect to the relative variable

0:08:32.719,0:08:38.320
the latent variable is the path

0:08:34.719,0:08:40.479
in that graph um so now what you have is

0:08:38.320,0:08:41.839
the best warping that matches the

0:08:40.479,0:08:42.880
sequence of future vector to the first

0:08:41.839,0:08:44.640
template

0:08:42.880,0:08:46.240
now you keep doing it with doing the

0:08:44.640,0:08:46.880
with with doing this with all the

0:08:46.240,0:08:49.519
templates

0:08:46.880,0:08:50.720
okay so for every template every word

0:08:49.519,0:08:53.120
you know from zero

0:08:50.720,0:08:54.240
one two three to nine you have the best

0:08:53.120,0:08:56.480
way to warp

0:08:54.240,0:08:58.160
the the the feature vector to that

0:08:56.480,0:09:02.800
template

0:08:58.160,0:09:05.279
um and uh and now you can

0:09:02.800,0:09:06.000
if your system has been trained you you

0:09:05.279,0:09:08.080
pick

0:09:06.000,0:09:09.680
the the category of the word template

0:09:08.080,0:09:11.519
with a smaller distance

0:09:09.680,0:09:13.360
okay as simple as that that's for

0:09:11.519,0:09:14.640
classification now how about training so

0:09:13.360,0:09:16.399
for training

0:09:14.640,0:09:18.640
um this is a latent variable model

0:09:16.399,0:09:22.080
essentially and what you need to do

0:09:18.640,0:09:23.760
is you need to make the

0:09:22.080,0:09:25.360
energy for the correct answer as small

0:09:23.760,0:09:26.800
as possible

0:09:25.360,0:09:29.760
and make sure the energy for the

0:09:26.800,0:09:33.519
incorrect answers are larger

0:09:29.760,0:09:34.320
okay so um so let's imagine the correct

0:09:33.519,0:09:37.600
answer is

0:09:34.320,0:09:40.080
is oops sorry

0:09:37.600,0:09:41.839
is this word here the second last one

0:09:40.080,0:09:44.000
that correspond to zero one two three

0:09:41.839,0:09:45.600
the category three for example

0:09:44.000,0:09:48.480
okay so we know the correct answer is

0:09:45.600,0:09:52.080
three so what we need to do now is

0:09:48.480,0:09:54.320
uh basically change the word template

0:09:52.080,0:09:58.399
here a little bit so that it gets closer

0:09:54.320,0:09:59.680
to the the feature vector sequence

0:09:58.399,0:10:01.440
and then change the feature vector

0:09:59.680,0:10:02.160
sequence so that it gets closer to the

0:10:01.440,0:10:05.600
template

0:10:02.160,0:10:07.200
right you can think of this dtw uh

0:10:05.600,0:10:09.680
distance this dynamic time warping

0:10:07.200,0:10:11.279
distance as a kind of distance

0:10:09.680,0:10:12.640
uh which involves minimization with

0:10:11.279,0:10:15.600
respect to a path but in the end it's

0:10:12.640,0:10:17.200
some sort of distance or divergence

0:10:15.600,0:10:18.880
and what you need to do is and that's

0:10:17.200,0:10:20.480
basically your energy so what you need

0:10:18.880,0:10:22.399
to do is make that distance smaller

0:10:20.480,0:10:24.880
for the correct answer okay so the

0:10:22.399,0:10:26.959
energy of the correct answer goes down

0:10:24.880,0:10:28.640
uh and simultaneously you need to make

0:10:26.959,0:10:29.440
sure the energy of all the e correct

0:10:28.640,0:10:32.000
answers get

0:10:29.440,0:10:33.279
uh are larger okay so you might need to

0:10:32.000,0:10:34.959
push them away

0:10:33.279,0:10:36.800
so you might need to have an objective

0:10:34.959,0:10:38.399
function that is going to

0:10:36.800,0:10:40.560
uh take the templates for the wrong

0:10:38.399,0:10:43.839
words and somehow push them away

0:10:40.560,0:10:45.360
from the current sequence of features

0:10:43.839,0:10:47.279
okay so that's how you learn the

0:10:45.360,0:10:50.560
templates

0:10:47.279,0:10:53.040
um and then simultaneously uh you

0:10:50.560,0:10:55.040
you need to you you're gonna have kind

0:10:53.040,0:10:57.200
of a combination of gradients that are

0:10:55.040,0:10:57.920
gonna back propagate through this dtw

0:10:57.200,0:11:00.320
one

0:10:57.920,0:11:02.000
um one is going to try to make that a

0:11:00.320,0:11:04.959
sequence of feature vector

0:11:02.000,0:11:06.560
uh such that once you go the time wiping

0:11:04.959,0:11:09.600
it gets closer to the correct

0:11:06.560,0:11:13.519
word template but also uh

0:11:09.600,0:11:15.279
change it so that it gets away from the

0:11:13.519,0:11:16.640
the other templates the the templates

0:11:15.279,0:11:20.399
for the other categories

0:11:16.640,0:11:23.279
okay um so that is simply

0:11:20.399,0:11:24.720
back propagating through the uh you know

0:11:23.279,0:11:26.560
through the dynamic time marking and the

0:11:24.720,0:11:28.079
dynamic time marking really is a switch

0:11:26.560,0:11:30.720
it's a giant switch right

0:11:28.079,0:11:31.200
it basically tells you uh those values

0:11:30.720,0:11:34.399
here

0:11:31.200,0:11:36.240
on the along the path matter because uh

0:11:34.399,0:11:38.320
they are the ones that indicate whether

0:11:36.240,0:11:39.519
my input vector matches my template

0:11:38.320,0:11:41.200
vector okay

0:11:39.519,0:11:43.120
all the other ones that my pass is is

0:11:41.200,0:11:44.399
not taking are irrelevant they don't

0:11:43.120,0:11:46.480
matter

0:11:44.399,0:11:48.480
so the distance is just the sum of those

0:11:46.480,0:11:49.040
values so when i back propagate i just

0:11:48.480,0:11:52.000
get

0:11:49.040,0:11:53.440
uh for each vector here a gradient that

0:11:52.000,0:11:56.240
corresponds to

0:11:53.440,0:11:57.279
uh the gradient of uh you know basically

0:11:56.240,0:11:59.600
the

0:11:57.279,0:12:02.639
the distance uh to the corresponding

0:11:59.600,0:12:02.639
vector in the template

0:12:02.800,0:12:06.399
so for the correct template this is

0:12:05.519,0:12:08.000
going to cause

0:12:06.399,0:12:09.680
all those vectors to get closer to the

0:12:08.000,0:12:11.200
corresponding vector in the template

0:12:09.680,0:12:13.279
and all those vectors to move away from

0:12:11.200,0:12:14.800
the corresponding vector in the

0:12:13.279,0:12:16.560
bad templates that you decide to push

0:12:14.800,0:12:18.160
away and then you can just back prop and

0:12:16.560,0:12:20.639
get those gradients all the way through

0:12:18.160,0:12:21.279
now i'm kind of explaining the mechanics

0:12:20.639,0:12:23.519
of it

0:12:21.279,0:12:24.320
but you don't actually have to think

0:12:23.519,0:12:28.320
about it you know

0:12:24.320,0:12:28.320
in principle kind of um

0:12:28.720,0:12:33.680
conceptually it's just a

0:12:31.760,0:12:35.040
energy-based model with lithium variable

0:12:33.680,0:12:37.120
and you just compute the gradient of

0:12:35.040,0:12:39.600
your energy with respect to

0:12:37.120,0:12:40.639
uh you know to everything in your in

0:12:39.600,0:12:43.600
your network

0:12:40.639,0:12:44.480
uh for you know values related variables

0:12:43.600,0:12:47.600
that depend on

0:12:44.480,0:12:48.959
you know the position of the of uh of

0:12:47.600,0:12:50.639
this switch here you can think of this

0:12:48.959,0:12:54.079
switch as the one that tells you

0:12:50.639,0:12:55.600
which of the answers is correct

0:12:54.079,0:12:57.120
and so it's nothing more than an

0:12:55.600,0:13:00.240
energy-based model

0:12:57.120,0:13:02.480
okay now there's a question um so

0:13:00.240,0:13:03.600
why am i introducing this before talking

0:13:02.480,0:13:05.279
about structure prediction because this

0:13:03.600,0:13:06.399
is a very simple

0:13:05.279,0:13:08.320
form of structure prediction

0:13:06.399,0:13:10.000
particularly if now the problem is not

0:13:08.320,0:13:12.399
to recognize a single word

0:13:10.000,0:13:13.279
but it is to recognize a sequence of

0:13:12.399,0:13:16.000
words right so

0:13:13.279,0:13:17.760
a word is a sequence of sounds uh but a

0:13:16.000,0:13:19.440
sentence is a sequence of words

0:13:17.760,0:13:21.360
and so therefore also a sequence of

0:13:19.440,0:13:23.839
sounds right so i could

0:13:21.360,0:13:24.880
build uh a collection of possible

0:13:23.839,0:13:26.399
sequences

0:13:24.880,0:13:29.040
which are grammatically correct

0:13:26.399,0:13:31.519
sequences of words which correspond to

0:13:29.040,0:13:32.880
some grammatically correct sequences of

0:13:31.519,0:13:34.720
sounds

0:13:32.880,0:13:36.639
and then this kind of dynamic time

0:13:34.720,0:13:39.519
marking if you want will sort of find

0:13:36.639,0:13:39.920
among all the possible uh sequences of

0:13:39.519,0:13:42.880
uh

0:13:39.920,0:13:44.320
of symbols or or or sounds or words

0:13:42.880,0:13:45.199
we'll find the one that has the lowest

0:13:44.320,0:13:47.519
energy

0:13:45.199,0:13:50.320
okay that this future vector is closest

0:13:47.519,0:13:50.320
to somehow

0:13:52.160,0:13:56.560
okay so that's the general problem with

0:13:54.480,0:13:58.800
sequence labeling

0:13:56.560,0:13:58.800
and

0:14:01.120,0:14:08.079
um and it can be formulated uh

0:14:04.639,0:14:10.160
at a general level uh in this way um

0:14:08.079,0:14:11.279
now i'm i'm kind of uh i kind of set the

0:14:10.160,0:14:12.560
stage a little bit and now i'm going to

0:14:11.279,0:14:14.399
talk about something that you're not

0:14:12.560,0:14:18.000
going to see immediately is connected

0:14:14.399,0:14:20.240
but um it's gonna come up at the end

0:14:18.000,0:14:21.040
okay so let's say you have a learning

0:14:20.240,0:14:23.440
system

0:14:21.040,0:14:25.519
that is uh composed of an input x okay

0:14:23.440,0:14:27.680
it gets an input x

0:14:25.519,0:14:29.600
and it's an energy model in which the

0:14:27.680,0:14:31.279
energy is a sum of three terms in this

0:14:29.600,0:14:34.639
case so those those

0:14:31.279,0:14:35.360
uh blue squares here are basically

0:14:34.639,0:14:36.880
factors

0:14:35.360,0:14:38.639
in a factor graphs they're energy terms

0:14:36.880,0:14:40.079
additive energy terms in your energy

0:14:38.639,0:14:43.680
function

0:14:40.079,0:14:46.880
uh and your output is uh is a sequence

0:14:43.680,0:14:50.720
in this case a sequence of four symbols

0:14:46.880,0:14:52.880
and those symbols um do not all

0:14:50.720,0:14:54.639
uh contribute to all the terms in the

0:14:52.880,0:14:55.680
energy so basically your energy function

0:14:54.639,0:14:58.810
the first term

0:14:55.680,0:15:01.279
takes into account the first two output

0:14:58.810,0:15:03.120
[Music]

0:15:01.279,0:15:04.800
symbols or or variables in your in your

0:15:03.120,0:15:06.800
output in your sequence of output

0:15:04.800,0:15:08.720
the second one take the second two the

0:15:06.800,0:15:11.680
third one takes the

0:15:08.720,0:15:12.240
the third and fourth okay now imagine

0:15:11.680,0:15:15.760
that

0:15:12.240,0:15:17.120
um this were a sequence of words

0:15:15.760,0:15:18.800
and your system was supposed to do

0:15:17.120,0:15:21.120
something like speech recognition or

0:15:18.800,0:15:22.720
something so x is the speech signal

0:15:21.120,0:15:24.160
uh in the blue boxes you have neural

0:15:22.720,0:15:25.519
nets and various other things there

0:15:24.160,0:15:26.160
might be another neural net that looks

0:15:25.519,0:15:28.079
at x

0:15:26.160,0:15:29.600
x and then produces future vectors that

0:15:28.079,0:15:32.959
go into those boxes but

0:15:29.600,0:15:36.079
uh that's the detail for now um and

0:15:32.959,0:15:36.720
um what those blue boxes would would

0:15:36.079,0:15:38.959
have to

0:15:36.720,0:15:40.800
uh implement is basically grammatical

0:15:38.959,0:15:43.600
constraints so

0:15:40.800,0:15:45.440
uh you know in english certain words can

0:15:43.600,0:15:47.759
follow others but not

0:15:45.440,0:15:50.240
uh others right so you rarely have two

0:15:47.759,0:15:54.000
two verbs that follow each other

0:15:50.240,0:15:54.880
um and so you could implement this in

0:15:54.000,0:15:56.880
this uh

0:15:54.880,0:15:58.560
energy term that would make you pay a

0:15:56.880,0:16:00.800
price for

0:15:58.560,0:16:02.560
uh you know making a verb follow another

0:16:00.800,0:16:04.880
verb all right or

0:16:02.560,0:16:06.480
having uh i don't know you know two

0:16:04.880,0:16:07.600
prepositions you can have two objectives

0:16:06.480,0:16:09.120
that follow each other

0:16:07.600,0:16:10.720
you know things like that right so

0:16:09.120,0:16:12.399
basically those would you know implement

0:16:10.720,0:16:13.839
sort of basic grammatical rules

0:16:12.399,0:16:16.000
and you can think of this as kind of a

0:16:13.839,0:16:16.959
language model right so i know what word

0:16:16.000,0:16:19.199
came before

0:16:16.959,0:16:20.720
uh tell me what work and can come after

0:16:19.199,0:16:21.440
and i can train this on the corpus of

0:16:20.720,0:16:24.880
text

0:16:21.440,0:16:26.560
to learn this energy function so

0:16:24.880,0:16:28.480
you know it's a it would be a very basic

0:16:26.560,0:16:30.880
crude language model

0:16:28.480,0:16:32.160
so this type of model would implement a

0:16:30.880,0:16:33.440
very cool language model

0:16:32.160,0:16:35.440
by just you know taking the previous

0:16:33.440,0:16:36.079
word and then telling you what the the

0:16:35.440,0:16:37.920
next word

0:16:36.079,0:16:40.000
what next words are possible making you

0:16:37.920,0:16:40.320
pay a price for picking a word that is

0:16:40.000,0:16:44.240
not

0:16:40.320,0:16:47.600
uh not correct um

0:16:44.240,0:16:48.320
okay so um how you do inference this is

0:16:47.600,0:16:50.880
just a

0:16:48.320,0:16:52.160
basically an energy model here which in

0:16:50.880,0:16:53.759
this case doesn't actually have a

0:16:52.160,0:16:55.759
lithium variable but

0:16:53.759,0:16:57.440
basically i give you an x and you have

0:16:55.759,0:16:58.320
to find the sequence of y that minimizes

0:16:57.440,0:17:00.399
the energy

0:16:58.320,0:17:01.519
but in this case because the energy is

0:17:00.399,0:17:03.440
the sum of three terms

0:17:01.519,0:17:05.839
the efficient way to find the sequence

0:17:03.440,0:17:06.799
of y's that minimize the energy that may

0:17:05.839,0:17:08.319
not require

0:17:06.799,0:17:10.160
a completely exhaustive search or

0:17:08.319,0:17:11.760
gradient descent or something like this

0:17:10.160,0:17:13.520
okay i'm going to place myself in a

0:17:11.760,0:17:15.280
situation where why

0:17:13.520,0:17:16.799
the y's are actually discrete okay so

0:17:15.280,0:17:20.959
this thing's like words or

0:17:16.799,0:17:20.959
or sounds or categories of some kind

0:17:23.760,0:17:30.000
and so this applies to uh

0:17:28.000,0:17:31.360
you know this situation where the the

0:17:30.000,0:17:33.280
variables you need to infer are all

0:17:31.360,0:17:34.559
outputs

0:17:33.280,0:17:35.760
which means they're going to be visible

0:17:34.559,0:17:37.600
on the training set and you can train

0:17:35.760,0:17:40.480
your system to kind of infer them

0:17:37.600,0:17:41.679
correctly but it could also be another

0:17:40.480,0:17:43.440
situation where

0:17:41.679,0:17:46.000
uh you know some of the variables are

0:17:43.440,0:17:48.240
observed like x here on the left

0:17:46.000,0:17:49.840
and y is observed during training on the

0:17:48.240,0:17:50.960
right but then all the intermediate

0:17:49.840,0:17:52.400
variables are never observed there are

0:17:50.960,0:17:53.440
latent variables they need to you need

0:17:52.400,0:17:56.080
to minimize over

0:17:53.440,0:17:58.240
um as well but again here this factor

0:17:56.080,0:18:00.799
graph is factorized in the sense that

0:17:58.240,0:18:02.559
the energy is a sum of of different

0:18:00.799,0:18:04.640
terms that

0:18:02.559,0:18:06.960
only take subsets of the variables into

0:18:04.640,0:18:06.960
account

0:18:07.280,0:18:11.760
all right okay so let's say uh let's

0:18:09.919,0:18:15.600
take a very concrete example now

0:18:11.760,0:18:15.600
and let's say the uh

0:18:15.760,0:18:18.799
energy here in this case is a sum of

0:18:18.080,0:18:22.080
four terms

0:18:18.799,0:18:25.120
four energy terms okay

0:18:22.080,0:18:27.120
uh the first two uh depend on x

0:18:25.120,0:18:29.760
the observation the last one depend uh

0:18:27.120,0:18:30.880
the last two depend on y which is the

0:18:29.760,0:18:32.080
variable you need to predict which

0:18:30.880,0:18:32.880
you're given during training but not

0:18:32.080,0:18:34.720
during tests

0:18:32.880,0:18:36.000
and then two other nodes are our latent

0:18:34.720,0:18:39.200
variable nodes okay

0:18:36.000,0:18:40.640
and let's say uh x is some high

0:18:39.200,0:18:42.720
dimensional variable we don't care what

0:18:40.640,0:18:46.720
it is because we just observe it

0:18:42.720,0:18:49.919
and z1 is binary z2 is binary

0:18:46.720,0:18:51.520
y1 is binary and y2 is ternary so we can

0:18:49.919,0:18:54.559
take three values zero one two

0:18:51.520,0:18:56.160
okay now uh

0:18:54.559,0:18:58.480
if you count how many possible

0:18:56.160,0:19:00.080
configurations of z one z two y one y

0:18:58.480,0:19:03.120
two they are

0:19:00.080,0:19:06.320
there are basically uh

0:19:03.120,0:19:08.720
uh uh 24 right uh two

0:19:06.320,0:19:10.880
times two times two times three right

0:19:08.720,0:19:13.440
that's that's 24

0:19:10.880,0:19:15.360
uh different possible configurations of

0:19:13.440,0:19:18.320
values so if you wanted to do

0:19:15.360,0:19:20.240
exact inference uh you might have to try

0:19:18.320,0:19:22.080
all 24 of those configurations

0:19:20.240,0:19:23.679
and then compute the energy of all 24 of

0:19:22.080,0:19:25.039
those configurations and then pick the

0:19:23.679,0:19:29.280
one with the lowest energy

0:19:25.039,0:19:33.520
to do inference right and in fact those

0:19:29.280,0:19:36.559
those 24 configurations correspond to

0:19:33.520,0:19:39.039
uh 24 times three evaluations

0:19:36.559,0:19:40.240
of those energy terms right because we

0:19:39.039,0:19:42.080
have three energy terms

0:19:40.240,0:19:44.080
so we'll have to compute 96 different

0:19:42.080,0:19:45.919
energies to be able to do this

0:19:44.080,0:19:47.679
okay and this is a small example where

0:19:45.919,0:19:50.480
the sequence is short and the

0:19:47.679,0:19:52.080
the the variables are binary okay this

0:19:50.480,0:19:53.520
goes exponentially

0:19:52.080,0:19:56.000
with the number of with the length of

0:19:53.520,0:19:59.600
the sequence uh

0:19:56.000,0:20:03.840
and uh uh

0:19:59.600,0:20:07.600
uh sorry with the the number of uh

0:20:03.840,0:20:09.600
possible values of the of the

0:20:07.600,0:20:11.039
of the z's uh and the length of the

0:20:09.600,0:20:14.480
sequence so if you have

0:20:11.039,0:20:16.000
you know uh uh

0:20:14.480,0:20:17.600
you know n possibilities for each of the

0:20:16.000,0:20:19.440
variables and the length is l you know

0:20:17.600,0:20:21.760
it's n to the l right exponential in the

0:20:19.440,0:20:21.760
ninth

0:20:23.919,0:20:28.080
okay but the thing is there is a more

0:20:26.640,0:20:31.760
efficient way of figuring out

0:20:28.080,0:20:32.880
what is the configuration of lowest

0:20:31.760,0:20:34.159
energy

0:20:32.880,0:20:36.640
and it's due to the fact that you have

0:20:34.159,0:20:40.799
those kind of local this local structure

0:20:36.640,0:20:42.880
right so uh z1 can only take two values

0:20:40.799,0:20:43.840
okay and z2 can only also only take two

0:20:42.880,0:20:45.840
values

0:20:43.840,0:20:48.000
so this energy term here can only take

0:20:45.840,0:20:50.320
four values it's ever going to see

0:20:48.000,0:20:51.360
only four different values because it

0:20:50.320,0:20:54.080
can only see

0:20:51.360,0:20:54.960
zero zero zero one one zero one one

0:20:54.080,0:20:56.000
right

0:20:54.960,0:20:58.480
so you could imagine could be

0:20:56.000,0:21:00.159
pre-computing those four values

0:20:58.480,0:21:02.320
okay this guy is also going to see only

0:21:00.159,0:21:04.400
four values right so you can

0:21:02.320,0:21:05.600
you can because this is binary and that

0:21:04.400,0:21:07.679
binary that's binary

0:21:05.600,0:21:08.799
so you can pre-compute those four values

0:21:07.679,0:21:10.480
okay

0:21:08.799,0:21:12.320
so that's another four evaluation of an

0:21:10.480,0:21:12.960
objective function that we're up to

0:21:12.320,0:21:14.480
eight

0:21:12.960,0:21:16.240
and this guy has six different values

0:21:14.480,0:21:17.200
because this variable is binary this one

0:21:16.240,0:21:19.200
ternary so it's

0:21:17.200,0:21:20.720
two by two times three so now you have

0:21:19.200,0:21:22.880
six different configurations

0:21:20.720,0:21:25.280
so by pre-computing the four here the

0:21:22.880,0:21:30.480
four here and the six here

0:21:25.280,0:21:36.480
um uh you you have computed all the

0:21:30.480,0:21:36.480
possible uh configurations basically

0:21:39.600,0:21:43.200
and that's kind of represented here at

0:21:41.440,0:21:44.640
the bottom so

0:21:43.200,0:21:47.200
this is called a trellis and it's

0:21:44.640,0:21:48.240
basically a graph that has a source node

0:21:47.200,0:21:50.320
and a target node

0:21:48.240,0:21:52.080
and every path in a graph corresponds to

0:21:50.320,0:21:55.600
a particular

0:21:52.080,0:21:59.600
assignment of of the variables okay

0:21:55.600,0:22:03.840
so for example if i go through this path

0:21:59.600,0:22:07.280
okay it means y1 equals z1 equals 1

0:22:03.840,0:22:11.039
z2 equals 0 y1 equals 1

0:22:07.280,0:22:15.120
and y2 equals 2 let's say okay

0:22:11.039,0:22:17.919
and if i add up the the terms

0:22:15.120,0:22:19.280
on each arc i get the overall energy

0:22:17.919,0:22:23.600
each arc is

0:22:19.280,0:22:25.280
is uh basically annotated by the

0:22:23.600,0:22:26.880
energy term the value of the energy that

0:22:25.280,0:22:30.080
corresponds to this configuration

0:22:26.880,0:22:31.919
so for example this arc here is

0:22:30.080,0:22:34.000
this energy and that's the value of this

0:22:31.919,0:22:38.159
energy term for

0:22:34.000,0:22:38.159
y one equals one and y two equals two

0:22:38.840,0:22:45.039
okay uh

0:22:40.640,0:22:47.360
so each of these args is a value of

0:22:45.039,0:22:49.280
this energy term each of these rx is the

0:22:47.360,0:22:52.720
value of that energy term

0:22:49.280,0:22:55.679
etc and now the

0:22:52.720,0:22:57.120
finding the the best lowest energy

0:22:55.679,0:22:59.760
configuration

0:22:57.120,0:23:02.640
of z1 z2 y1 y2 simply consists in

0:22:59.760,0:23:06.000
finding the shortest path in this graph

0:23:02.640,0:23:09.039
okay and

0:23:06.000,0:23:12.000
to do this i only have to evaluate

0:23:09.039,0:23:13.440
uh four times four terms of energy here

0:23:12.000,0:23:13.760
four terms here and six terms here and

0:23:13.440,0:23:18.640
that's

0:23:13.760,0:23:21.120
it okay so that's uh

0:23:18.640,0:23:22.000
14 i don't know why i said 16 here or 16

0:23:21.120,0:23:26.640
because of the first two

0:23:22.000,0:23:26.640
here yes so 16 values total

0:23:28.000,0:23:35.360
but uh so that's a lot less than

0:23:31.840,0:23:37.840
96 okay and that's because

0:23:35.360,0:23:39.039
the energy is the sum of terms and you

0:23:37.840,0:23:40.000
can use those kind of efficient

0:23:39.039,0:23:43.279
algorithms

0:23:40.000,0:23:44.799
uh to do the inference okay so this is a

0:23:43.279,0:23:46.240
simple case where the output is a

0:23:44.799,0:23:47.679
sequence

0:23:46.240,0:23:49.440
and when the output is a sequence there

0:23:47.679,0:23:51.039
is a simple algorithm

0:23:49.440,0:23:53.279
and it's basically shoulder space in a

0:23:51.039,0:23:56.960
graph right in a in a trellis

0:23:53.279,0:23:58.720
so that's just dynamic programming uh um

0:23:56.960,0:24:00.000
and it's very simple it's efficient you

0:23:58.720,0:24:03.120
know it's uh

0:24:00.000,0:24:04.720
it's nice so to train a system like this

0:24:03.120,0:24:06.000
what you need to tell it is you need to

0:24:04.720,0:24:08.000
tell it here is the correct

0:24:06.000,0:24:10.400
configuration of y1 y2 i don't know what

0:24:08.000,0:24:12.799
z1z2 is because it's a latent variable

0:24:10.400,0:24:14.080
so find me the path that goes to the

0:24:12.799,0:24:17.679
correct

0:24:14.080,0:24:20.400
uh combination of y1 y2

0:24:17.679,0:24:21.679
okay so you know that let's say y1 equal

0:24:20.400,0:24:24.880
1 and y2 equal 2.

0:24:21.679,0:24:28.480
so you know that the correct path

0:24:24.880,0:24:31.600
has to include this link

0:24:28.480,0:24:32.159
right and so there's only a subset of

0:24:31.600,0:24:33.520
path

0:24:32.159,0:24:35.440
for the the previous ones that are

0:24:33.520,0:24:36.320
possible right you can't go to y one

0:24:35.440,0:24:39.679
equals zero because

0:24:36.320,0:24:41.919
that would be incorrect uh

0:24:39.679,0:24:43.679
so basically only this this guy survives

0:24:41.919,0:24:45.039
and then the other paths you can take

0:24:43.679,0:24:47.039
whatever you want as long as you get to

0:24:45.039,0:24:48.720
that point so you can just find the one

0:24:47.039,0:24:50.640
that minimizes the energy here

0:24:48.720,0:24:52.080
so minimizing energy with respect to z1

0:24:50.640,0:24:54.960
z2 so that

0:24:52.080,0:24:55.919
uh y1 and y2 take the right value okay

0:24:54.960,0:24:58.400
constraining where when

0:24:55.919,0:25:00.000
where you to take the the right value

0:24:58.400,0:25:03.360
and the way you train the system now is

0:25:00.000,0:25:04.799
that you migrate and descent you back

0:25:03.360,0:25:06.480
you back propagate the gradient of the

0:25:04.799,0:25:08.080
overall energy

0:25:06.480,0:25:10.400
okay for this particular y and this

0:25:08.080,0:25:11.600
particular x and the z that you obtain

0:25:10.400,0:25:14.640
by minimizing

0:25:11.600,0:25:17.039
you back propagate uh the

0:25:14.640,0:25:18.559
the the gradient of of this energy with

0:25:17.039,0:25:19.679
respect to the parameters of all those

0:25:18.559,0:25:22.080
energy terms

0:25:19.679,0:25:23.440
and you try to make that smaller right

0:25:22.080,0:25:23.840
you know you have the correct y the

0:25:23.440,0:25:26.880
correct

0:25:23.840,0:25:30.159
x and whatever z value uh

0:25:26.880,0:25:33.600
z mistake try to make that energy

0:25:30.159,0:25:36.000
lower by tweaking the parameters

0:25:33.600,0:25:38.000
at the same time uh you have to make

0:25:36.000,0:25:40.320
sure the energy of incorrect answers for

0:25:38.000,0:25:44.159
y1 and y2 that are incorrect

0:25:40.320,0:25:46.240
is higher right so you take other values

0:25:44.159,0:25:49.120
of y1 y2

0:25:46.240,0:25:50.320
uh including y1 equals zero and y2

0:25:49.120,0:25:52.320
equals whatever it wants

0:25:50.320,0:25:54.080
okay and for all of those other

0:25:52.320,0:25:55.600
configurations of y1y2

0:25:54.080,0:25:57.600
you want to make sure whatever energy

0:25:55.600,0:25:59.440
you you get by minimizing over z

0:25:57.600,0:26:00.799
is higher than whatever you got for the

0:25:59.440,0:26:02.799
correct one

0:26:00.799,0:26:04.159
okay so your loss function is going to

0:26:02.799,0:26:05.679
be something where

0:26:04.159,0:26:07.120
you take the energy of the correct

0:26:05.679,0:26:08.480
answer you try to make it lower and then

0:26:07.120,0:26:10.159
you take the energies of incorrect

0:26:08.480,0:26:13.919
answers and you try to make them

0:26:10.159,0:26:16.799
larger okay that's discriminative

0:26:13.919,0:26:17.919
uh training for structure prediction

0:26:16.799,0:26:19.360
structural prediction because the

0:26:17.919,0:26:22.720
structure here is

0:26:19.360,0:26:26.000
uh represented by this uh

0:26:22.720,0:26:28.320
you know sequence of of costs okay but

0:26:26.000,0:26:29.200
conceptually at a high level it's no

0:26:28.320,0:26:30.880
different from

0:26:29.200,0:26:32.320
everything we talked about before when

0:26:30.880,0:26:34.240
we have a latin variable

0:26:32.320,0:26:35.600
and when we train with a criterion that

0:26:34.240,0:26:37.679
says i want to make the energy of the

0:26:35.600,0:26:41.279
correct time so small and the energy of

0:26:37.679,0:26:43.520
all the other answers higher okay

0:26:41.279,0:26:44.799
any question at this point um i had a

0:26:43.520,0:26:48.320
question so

0:26:44.799,0:26:51.440
based on this diagram it seems like

0:26:48.320,0:26:54.320
this this

0:26:51.440,0:26:55.520
network only really takes discrete

0:26:54.320,0:26:59.679
values

0:26:55.520,0:27:02.400
um and my understanding was that

0:26:59.679,0:27:03.520
back propagation doesn't isn't really

0:27:02.400,0:27:06.960
effective if you

0:27:03.520,0:27:07.760
just have only working with discrete

0:27:06.960,0:27:09.279
values

0:27:07.760,0:27:11.520
so i'm wondering if i'm missing

0:27:09.279,0:27:15.200
something or if that's like

0:27:11.520,0:27:18.240
how how you connect those things

0:27:15.200,0:27:20.080
right okay so uh in this case z1 z2 y1

0:27:18.240,0:27:22.480
y2 are not variables that

0:27:20.080,0:27:23.840
that you learn okay they're labels

0:27:22.480,0:27:25.679
essentially

0:27:23.840,0:27:26.960
okay they're discrete so one way to are

0:27:25.679,0:27:29.440
discrete just like uh

0:27:26.960,0:27:31.360
you know the the class the category at

0:27:29.440,0:27:34.720
the output of the continent is discrete

0:27:31.360,0:27:36.640
except you have two of them but whatever

0:27:34.720,0:27:38.000
z1 z2 are basically of the same nature

0:27:36.640,0:27:39.120
the discrete variables

0:27:38.000,0:27:40.640
they're not things you're going to learn

0:27:39.120,0:27:42.159
by grading descent they're just latent

0:27:40.640,0:27:44.320
variable you you have to

0:27:42.159,0:27:46.240
to minimize over to do inference right

0:27:44.320,0:27:47.760
let's not talk about learning for now

0:27:46.240,0:27:49.279
once your system is trained right i give

0:27:47.760,0:27:51.840
you an x and by

0:27:49.279,0:27:53.840
energy minimization you find z1 z2 y1 y2

0:27:51.840,0:27:56.720
that minimizes the energy

0:27:53.840,0:27:58.640
okay and because you've trained the

0:27:56.720,0:28:00.080
correct y1y2

0:27:58.640,0:28:02.080
to have the lowest energy among all

0:28:00.080,0:28:04.320
possible configurations of y2

0:28:02.080,0:28:05.120
you're going to get the correct one okay

0:28:04.320,0:28:08.320
now

0:28:05.120,0:28:08.880
uh for training the the training takes

0:28:08.320,0:28:11.120
place

0:28:08.880,0:28:12.880
uh you know uh basically affects the

0:28:11.120,0:28:14.480
parameters of each of those

0:28:12.880,0:28:16.240
factors inside those factors there are

0:28:14.480,0:28:19.360
parameters you know

0:28:16.240,0:28:22.000
w a w b w c w d which i i didn't

0:28:19.360,0:28:22.960
represent here and the way you train the

0:28:22.000,0:28:25.760
system is you

0:28:22.960,0:28:27.520
you say you know at the gradient of the

0:28:25.760,0:28:29.440
energy of the correct answer

0:28:27.520,0:28:30.559
uh with respect to those parameters i'm

0:28:29.440,0:28:31.520
going to tweak the parameters so that

0:28:30.559,0:28:35.039
energy goes down

0:28:31.520,0:28:36.640
that's continuous uh differentiable

0:28:35.039,0:28:38.320
okay and then simultaneously i have the

0:28:36.640,0:28:39.760
energy of bad answers

0:28:38.320,0:28:42.000
i'm going to back propagate gradients

0:28:39.760,0:28:44.000
and according to my loss function i'm

0:28:42.000,0:28:46.080
going to push up the energy of those

0:28:44.000,0:28:47.840
so that my energy my loss function goes

0:28:46.080,0:28:51.039
down

0:28:47.840,0:28:54.559
okay my training objective goes down

0:28:51.039,0:28:57.039
not my energy right now

0:28:54.559,0:28:58.240
so so now the what i'm explaining it

0:28:57.039,0:29:00.480
down there with the trellis

0:28:58.240,0:29:01.679
is the fact that because those variables

0:29:00.480,0:29:04.240
are discrete

0:29:01.679,0:29:04.799
you can't use gradient descent to infer

0:29:04.240,0:29:09.679
them

0:29:04.799,0:29:13.039
okay and so you have to infer them by

0:29:09.679,0:29:15.520
combinatorial search essentially and

0:29:13.039,0:29:17.600
the first solution i i mentioned with

0:29:15.520,0:29:19.919
the 96 factor evaluations

0:29:17.600,0:29:22.080
uh basically is exhaustive search right

0:29:19.919,0:29:22.799
try every combination of z one z two y

0:29:22.080,0:29:24.960
one y two

0:29:22.799,0:29:26.480
and we and figure out which one has the

0:29:24.960,0:29:28.640
lowest energy

0:29:26.480,0:29:30.559
but uh but the whole point of this is

0:29:28.640,0:29:32.399
that this is wasteful

0:29:30.559,0:29:34.159
in the sense that because the energy

0:29:32.399,0:29:35.440
decomposes into terms that only take

0:29:34.159,0:29:38.320
subsets of variables

0:29:35.440,0:29:39.679
you can actually decompose this into uh

0:29:38.320,0:29:42.000
you can reduce this to finding the

0:29:39.679,0:29:45.440
shortest paths in a graph

0:29:42.000,0:29:48.320
where the transitions in this graph

0:29:45.440,0:29:49.600
are are annotated by the energies that

0:29:48.320,0:29:52.559
correspond to the

0:29:49.600,0:29:54.320
value of the variables of the two

0:29:52.559,0:29:57.520
corresponding nodes

0:29:54.320,0:29:58.640
okay now this is a slightly more general

0:29:57.520,0:30:02.399
form of what i told you

0:29:58.640,0:30:04.159
about earlier uh so this this model here

0:30:02.399,0:30:07.279
with the dynamic time marking

0:30:04.159,0:30:09.279
uh is very much the same right you you

0:30:07.279,0:30:11.600
you know the z1 z2 here are basically

0:30:09.279,0:30:14.960
the paths in the dynamic time marking

0:30:11.600,0:30:17.679
uh module the y

0:30:14.960,0:30:18.960
is which of the word template matches

0:30:17.679,0:30:21.360
okay

0:30:18.960,0:30:23.039
and the training consists in just you

0:30:21.360,0:30:23.520
know doing gradient descent to make the

0:30:23.039,0:30:24.960
energy

0:30:23.520,0:30:27.919
of the correct answer small and the

0:30:24.960,0:30:30.080
energy of the incorrect answer is larger

0:30:27.919,0:30:32.320
using some loss function which i lift i

0:30:30.080,0:30:33.760
live unspecified at the moment

0:30:32.320,0:30:35.919
professor when you say that you're

0:30:33.760,0:30:36.720
finding that the shortest um the

0:30:35.919,0:30:38.320
shortest path

0:30:36.720,0:30:41.039
you're saying that the distance between

0:30:38.320,0:30:45.279
nodes is the energy between the nodes

0:30:41.039,0:30:48.880
uh the the shortest path is the path

0:30:45.279,0:30:52.159
that has the smallest sum of terms

0:30:48.880,0:30:52.559
along along the along the edges right so

0:30:52.159,0:30:55.600
each

0:30:52.559,0:30:57.200
edge here is marked by an energy so for

0:30:55.600,0:30:57.760
example this edge here is marked by the

0:30:57.200,0:31:01.600
energy

0:30:57.760,0:31:05.279
of the term uh

0:31:01.600,0:31:09.279
b uh when z one equals zero

0:31:05.279,0:31:11.840
and z two equals one okay

0:31:09.279,0:31:13.440
okay so if i take this path i'm gonna

0:31:11.840,0:31:16.799
pay that energy

0:31:13.440,0:31:20.399
right and if i take this this

0:31:16.799,0:31:22.240
edge i'm going to pay that energy and so

0:31:20.399,0:31:23.919
you know finding the lowest energy

0:31:22.240,0:31:25.679
configuration of variables consisting

0:31:23.919,0:31:29.039
finding the path

0:31:25.679,0:31:32.000
with the the the smallest sum of

0:31:29.039,0:31:32.799
uh value along on the edges along that

0:31:32.000,0:31:34.240
path

0:31:32.799,0:31:36.480
okay so it's the shortest path in the

0:31:34.240,0:31:36.480
graph

0:31:36.720,0:31:42.480
okay is that clear

0:31:40.640,0:31:44.240
yeah that makes sense thank you and then

0:31:42.480,0:31:46.080
so the the zeros before the

0:31:44.240,0:31:48.320
the black node those are zeros just

0:31:46.080,0:31:49.519
because the the summation itself is zero

0:31:48.320,0:31:51.279
energy right

0:31:49.519,0:31:52.880
is that is that yeah that's right yeah

0:31:51.279,0:31:56.320
okay no i'm not counting

0:31:52.880,0:31:57.279
uh yeah like i don't care which of those

0:31:56.320,0:32:00.799
paths it is right

0:31:57.279,0:32:02.640
i don't have an energy term here for uh

0:32:00.799,0:32:04.000
like what's the value of y2 so if i had

0:32:02.640,0:32:07.200
an extra factor here

0:32:04.000,0:32:08.960
that only took y2 then

0:32:07.200,0:32:10.720
uh that factor would basically put an

0:32:08.960,0:32:15.600
energy here

0:32:10.720,0:32:17.840
on the you know we replace those zeros

0:32:15.600,0:32:17.840
right

0:32:21.039,0:32:24.320
okay there's a question here coming from

0:32:23.360,0:32:27.039
the students

0:32:24.320,0:32:28.799
so we are pushing down on the energy or

0:32:27.039,0:32:31.519
we are actually doing a minimization

0:32:28.799,0:32:33.600
for training and inference but then when

0:32:31.519,0:32:36.399
are we actually pushing up

0:32:33.600,0:32:37.679
just during training all right so let me

0:32:36.399,0:32:39.600
remind you how

0:32:37.679,0:32:42.000
uh training energy-based models work

0:32:39.600,0:32:45.120
right particularly contrasting methods

0:32:42.000,0:32:46.320
uh and if you have latent variables

0:32:45.120,0:32:48.799
right so you have your

0:32:46.320,0:32:49.760
you have your energy function e of xyz

0:32:48.799,0:32:51.120
uh sorry the

0:32:49.760,0:32:52.880
arguments are in the wrong order here it

0:32:51.120,0:32:54.240
doesn't matter so you have your energy

0:32:52.880,0:32:57.679
function e of xyz

0:32:54.240,0:33:00.000
i give you i give you an x so in

0:32:57.679,0:33:01.440
in training mode i give you an x and a y

0:33:00.000,0:33:03.120
i don't give you z ever

0:33:01.440,0:33:05.919
but i give you an x and a y here is a

0:33:03.120,0:33:07.360
training sample it's an x and a y

0:33:05.919,0:33:10.240
the first thing you do is you find a z

0:33:07.360,0:33:14.799
that minimizes the the energy e of x y z

0:33:10.240,0:33:17.039
okay and you call that f of x y

0:33:14.799,0:33:20.080
right but the way you compute it is just

0:33:17.039,0:33:23.679
min over z of e of x y z

0:33:20.080,0:33:25.440
now uh for

0:33:23.679,0:33:27.519
for the the correct y in your training

0:33:25.440,0:33:30.399
set you want that energy to be small

0:33:27.519,0:33:31.600
right and for your inference algorithm

0:33:30.399,0:33:34.399
to work

0:33:31.600,0:33:36.000
at test time at this time i don't give

0:33:34.399,0:33:37.600
you the y i just give you the x

0:33:36.000,0:33:39.600
and what you have to find is the y that

0:33:37.600,0:33:40.480
has the smallest energy so for this to

0:33:39.600,0:33:43.200
work

0:33:40.480,0:33:44.960
it has to be the case that the correct y

0:33:43.200,0:33:45.840
has the lowest energy among all possible

0:33:44.960,0:33:49.200
y's

0:33:45.840,0:33:50.080
right so what i need to do now during

0:33:49.200,0:33:52.960
training

0:33:50.080,0:33:54.720
is that i give you the correct y and

0:33:52.960,0:33:55.600
what you need to do is give a low energy

0:33:54.720,0:33:58.000
to the correct y

0:33:55.600,0:34:00.960
and give a higher energy to every other

0:33:58.000,0:34:04.159
possible configuration of y

0:34:00.960,0:34:07.679
right and

0:34:04.159,0:34:09.200
exactly how you do this or how

0:34:07.679,0:34:11.040
all those energies enter in your

0:34:09.200,0:34:12.960
objective function

0:34:11.040,0:34:14.480
uh depends on the objectives that you

0:34:12.960,0:34:14.960
choose we're going to come to this in a

0:34:14.480,0:34:17.200
minute

0:34:14.960,0:34:19.040
okay but almost certainly you're going

0:34:17.200,0:34:20.240
to have one term in your last function

0:34:19.040,0:34:21.599
that's going to say make the energy of

0:34:20.240,0:34:23.599
the correct answer

0:34:21.599,0:34:25.839
low and another term that that's going

0:34:23.599,0:34:27.280
to say make the energy of

0:34:25.839,0:34:28.879
all the other answers or some of them

0:34:27.280,0:34:31.760
high and we talked about this last time

0:34:28.879,0:34:34.240
actually three weeks ago okay but i'm

0:34:31.760,0:34:37.119
going to come back to it

0:34:34.240,0:34:38.560
right is that clear or do you need

0:34:37.119,0:34:42.639
another

0:34:38.560,0:34:44.879
clarification uh i don't see any reply

0:34:42.639,0:34:44.879
here

0:34:47.520,0:34:49.839
okay

0:34:51.200,0:34:55.119
all right um another one would be what

0:34:54.480,0:34:57.359
if the

0:34:55.119,0:34:59.200
factor graph is not possible do we have

0:34:57.359,0:34:59.920
to search for all possible combination

0:34:59.200,0:35:02.240
of ys

0:34:59.920,0:35:03.520
maybe i think this is the continuous

0:35:02.240,0:35:07.040
case i think

0:35:03.520,0:35:10.720
no not necessarily so uh i mean

0:35:07.040,0:35:13.280
this this idea of decomposing into uh

0:35:10.720,0:35:14.960
energies also gives you an advantage

0:35:13.280,0:35:17.280
even in the case of continuous variables

0:35:14.960,0:35:18.720
right because you can do independent

0:35:17.280,0:35:21.760
optimizations

0:35:18.720,0:35:23.599
like like the combination of uh values

0:35:21.760,0:35:27.119
of z1 and z2

0:35:23.599,0:35:28.640
uh only affects eb you know even if z1

0:35:27.119,0:35:30.560
and z2 are continuous

0:35:28.640,0:35:31.760
and so you can do kind of the a little

0:35:30.560,0:35:33.040
bit of the equivalent of

0:35:31.760,0:35:34.560
dynamic programming there it's a little

0:35:33.040,0:35:35.200
more complicated in the continuous case

0:35:34.560,0:35:38.400
but

0:35:35.200,0:35:39.839
uh but it could be uh uh possible yeah i

0:35:38.400,0:35:42.960
mean the the worst situation

0:35:39.839,0:35:45.520
is when uh all the z's and the y's

0:35:42.960,0:35:46.640
enter a giant factor and there is no way

0:35:45.520,0:35:48.960
of

0:35:46.640,0:35:50.160
factorizing it and then you know you

0:35:48.960,0:35:54.400
have to do exhaustive search

0:35:50.160,0:35:56.160
or some approximate uh search heuristics

0:35:54.400,0:35:57.599
okay inference algorithm that minimizes

0:35:56.160,0:35:59.359
the energy

0:35:57.599,0:36:00.880
yeah yeah that was actually the case the

0:35:59.359,0:36:02.960
student was referring it

0:36:00.880,0:36:06.800
and the other student is also satisfied

0:36:02.960,0:36:06.800
so you answer both questions

0:36:07.040,0:36:10.720
yeah don't hesitate to ask if you do

0:36:08.800,0:36:14.320
something that's not clear

0:36:10.720,0:36:17.040
okay so uh here is a an instance of this

0:36:14.320,0:36:17.680
uh and if you encounter this in uh the

0:36:17.040,0:36:18.960
literature

0:36:17.680,0:36:20.800
you'll know what it is it's called a

0:36:18.960,0:36:22.000
conditional random field okay so

0:36:20.800,0:36:25.040
conditional random field

0:36:22.000,0:36:27.119
is a very special type of such uh

0:36:25.040,0:36:29.599
structural prediction model

0:36:27.119,0:36:30.880
where uh you know the here you have y's

0:36:29.599,0:36:34.560
or these doesn't matter

0:36:30.880,0:36:36.640
here they're only wise but the

0:36:34.560,0:36:37.920
the way those factors are parameterized

0:36:36.640,0:36:41.040
is that there is a fixed

0:36:37.920,0:36:43.119
feature extractor let's call it f of x y

0:36:41.040,0:36:45.680
one y two in this case here

0:36:43.119,0:36:47.040
uh and then a weight vector that just

0:36:45.680,0:36:49.839
computes the dot product

0:36:47.040,0:36:51.200
of this future vector uh with with this

0:36:49.839,0:36:51.520
weight vector and that gives you a score

0:36:51.200,0:36:54.480
here

0:36:51.520,0:36:56.000
uh just an energy okay the overall

0:36:54.480,0:36:57.599
energy is just the sum of all those

0:36:56.000,0:36:58.720
terms so basically those are shadow

0:36:57.599,0:37:00.720
neural nets if you want

0:36:58.720,0:37:03.680
single layer neural nets with a feature

0:37:00.720,0:37:03.680
extractor at the input

0:37:04.160,0:37:07.839
and then we can think about like what

0:37:05.520,0:37:10.720
type of uh loss function are we going to

0:37:07.839,0:37:13.200
minimize to train something like this

0:37:10.720,0:37:13.760
so one possibility is to use the

0:37:13.200,0:37:16.000
negative

0:37:13.760,0:37:16.800
likelihood loss so basically you say i

0:37:16.000,0:37:18.320
want

0:37:16.800,0:37:20.880
the energy of the correct answer to be

0:37:18.320,0:37:23.760
low and i want the log

0:37:20.880,0:37:24.480
of the sum of the exponentials of all

0:37:23.760,0:37:26.480
the energies

0:37:24.480,0:37:27.760
of all the bad all the answers including

0:37:26.480,0:37:31.200
the good one

0:37:27.760,0:37:34.240
uh uh to be uh

0:37:31.200,0:37:37.200
to be large okay uh

0:37:34.240,0:37:37.599
in fact more more more correctly you

0:37:37.200,0:37:40.720
want

0:37:37.599,0:37:42.720
the minus log of the sum of

0:37:40.720,0:37:44.160
all configurations of your output of the

0:37:42.720,0:37:46.079
exponential minus

0:37:44.160,0:37:48.960
the energy of all those configurations

0:37:46.079,0:37:52.560
to be as small as possible

0:37:48.960,0:37:55.280
okay um so

0:37:52.560,0:37:56.880
um so basically you want the the

0:37:55.280,0:37:59.280
combination of energies or bad answers

0:37:56.880,0:38:02.480
to be as large as possible

0:37:59.280,0:38:03.200
okay and we've encountered that loss

0:38:02.480,0:38:04.640
function before

0:38:03.200,0:38:06.880
right i mean that's basically what's

0:38:04.640,0:38:07.680
using softmax softmax says that softmax

0:38:06.880,0:38:10.720
says

0:38:07.680,0:38:12.560
uh i want the the you know

0:38:10.720,0:38:14.079
negative log probability of the correct

0:38:12.560,0:38:15.359
answer to be as low as possible the

0:38:14.079,0:38:15.599
probability of the correct answer to be

0:38:15.359,0:38:18.400
as

0:38:15.599,0:38:19.920
large as possible that's like an energy

0:38:18.400,0:38:22.800
okay

0:38:19.920,0:38:24.400
but then simultaneously i compute the

0:38:22.800,0:38:25.839
the sum of the exponential the log of

0:38:24.400,0:38:27.040
the sum of the exponentials of all the

0:38:25.839,0:38:30.720
answers

0:38:27.040,0:38:32.400
okay and i want that to be uh

0:38:30.720,0:38:33.920
to be small i want all those energies to

0:38:32.400,0:38:35.280
be uh to be large

0:38:33.920,0:38:37.119
i want all those probabilities to be

0:38:35.280,0:38:39.280
small

0:38:37.119,0:38:41.040
okay soft max does that to you when you

0:38:39.280,0:38:42.960
or logs of max criterion

0:38:41.040,0:38:44.880
when you back propagate it it does

0:38:42.960,0:38:45.760
classification and it does exactly that

0:38:44.880,0:38:47.119
it pushes

0:38:45.760,0:38:48.560
down the energy of the correct answer it

0:38:47.119,0:38:49.280
pushes out the energies of all the other

0:38:48.560,0:38:51.200
answers

0:38:49.280,0:38:52.560
by computing the log of the sum of the

0:38:51.200,0:38:54.480
answers of exponential minus the

0:38:52.560,0:38:57.680
energies

0:38:54.480,0:39:00.000
um so here uh conditional random field

0:38:57.680,0:39:01.280
is basically an example of that but

0:39:00.000,0:39:03.200
you're not doing classification you're

0:39:01.280,0:39:06.160
doing kind of structured prediction

0:39:03.200,0:39:07.200
and uh in the positive case you have the

0:39:06.160,0:39:11.200
correct

0:39:07.200,0:39:13.440
uh configuration of y one y two y four

0:39:11.200,0:39:15.040
one one way two or three and four and

0:39:13.440,0:39:17.599
the incorrect ones are not

0:39:15.040,0:39:18.800
you know uh incorrect categories as in

0:39:17.599,0:39:20.079
uh

0:39:18.800,0:39:24.960
as in classification but there are

0:39:20.079,0:39:24.960
incorrect configurations of y1 y2 or 3y4

0:39:26.400,0:39:31.920
other than that it's just you know

0:39:30.320,0:39:34.480
backprop i mean it's not even backward

0:39:31.920,0:39:36.079
here because it's a shallow network

0:39:34.480,0:39:37.839
if you put a whole neural net in there

0:39:36.079,0:39:41.200
parametrized by w's that

0:39:37.839,0:39:42.880
would be perfectly kosher and uh

0:39:41.200,0:39:44.640
that would be kind of a deep conditional

0:39:42.880,0:39:45.920
random field if you want which happened

0:39:44.640,0:39:47.760
to actually exist before conditional

0:39:45.920,0:39:49.119
running fields

0:39:47.760,0:39:52.240
uh here's another idea you can use a

0:39:49.119,0:39:53.680
hinge loss so the hinge last says

0:39:52.240,0:39:56.160
i want the energy of the correct answer

0:39:53.680,0:39:58.400
to be low and then among all possible

0:39:56.160,0:40:00.640
configurations of incorrect answers

0:39:58.400,0:40:02.000
incorrect configurations of ys i'm going

0:40:00.640,0:40:02.880
to look for the one that has the lowest

0:40:02.000,0:40:05.599
energy

0:40:02.880,0:40:06.880
among all the wrong all the bad ones and

0:40:05.599,0:40:08.240
that one i'm going to push up

0:40:06.880,0:40:10.800
okay i don't need to push the other ones

0:40:08.240,0:40:13.520
because they are larger anyway

0:40:10.800,0:40:16.480
so i'm just going to you know figure out

0:40:13.520,0:40:19.119
which configuration of y1y2 by 3y4

0:40:16.480,0:40:20.319
is both incorrect but among all

0:40:19.119,0:40:22.319
incorrect configurations

0:40:20.319,0:40:23.359
has the lowest energy and then push that

0:40:22.319,0:40:25.440
up

0:40:23.359,0:40:26.640
okay and the way i push up and push down

0:40:25.440,0:40:28.800
is i'm going to put the difference of

0:40:26.640,0:40:32.160
those two energies in a hinge loss

0:40:28.800,0:40:33.760
so that the hinge will push the

0:40:32.160,0:40:35.040
energy of the correct answer to be low

0:40:33.760,0:40:36.079
and will push the energy of the

0:40:35.040,0:40:38.240
incorrect

0:40:36.079,0:40:39.599
most offending answer to be higher by at

0:40:38.240,0:40:42.880
least some margin

0:40:39.599,0:40:44.240
okay um so that's called the maximum

0:40:42.880,0:40:46.319
margin uh

0:40:44.240,0:40:48.000
mark of net if you regularize the

0:40:46.319,0:40:48.640
weights with least square and if you

0:40:48.000,0:40:50.720
have

0:40:48.640,0:40:52.560
this kind of linear parametrization of

0:40:50.720,0:40:55.920
the energy terms

0:40:52.560,0:40:58.240
you can also use the perception loss and

0:40:55.920,0:41:00.000
michael collins who's a well-known

0:40:58.240,0:41:02.400
professor at colombia in nlp

0:41:00.000,0:41:03.599
actually you know kind of made a success

0:41:02.400,0:41:04.400
you know built his career around this

0:41:03.599,0:41:05.680
idea of using

0:41:04.400,0:41:07.680
perception lots for structural

0:41:05.680,0:41:09.920
prediction so that perception loss only

0:41:07.680,0:41:12.160
works if you have linear parametrization

0:41:09.920,0:41:13.680
of the factors if you make them deep

0:41:12.160,0:41:14.400
neural nets you can't use the perception

0:41:13.680,0:41:16.240
loss anymore

0:41:14.400,0:41:17.440
it's because the margin is zero we

0:41:16.240,0:41:20.079
talked about this a little bit before

0:41:17.440,0:41:22.319
but i'll come back to it in a minute

0:41:20.079,0:41:22.319
um

0:41:24.480,0:41:29.280
right so uh so those ideas have been

0:41:27.760,0:41:30.079
around for a long time the probably the

0:41:29.280,0:41:31.359
first people to

0:41:30.079,0:41:34.720
to think about things like this where

0:41:31.359,0:41:36.160
people who worked on what's called

0:41:34.720,0:41:37.599
discriminative training for speech

0:41:36.160,0:41:39.440
recognition and that goes back to the

0:41:37.599,0:41:41.760
late 80s early 90s

0:41:39.440,0:41:42.800
so yo-yo and rabinera for example at

0:41:41.760,0:41:45.040
atnt

0:41:42.800,0:41:46.960
had something they called minimum

0:41:45.040,0:41:48.960
empirical error loss and this is

0:41:46.960,0:41:50.400
kind of a particular loss for a speech

0:41:48.960,0:41:51.599
recognition system

0:41:50.400,0:41:53.680
they didn't have neural nets they had

0:41:51.599,0:41:56.800
some other way of kind of

0:41:53.680,0:41:58.880
turning uh speech signals into you know

0:41:56.800,0:42:01.520
basically sound categories if you want

0:41:58.880,0:42:02.960
um but but they had this way of training

0:42:01.520,0:42:04.880
at the sequence level

0:42:02.960,0:42:06.240
uh by not telling the system you know

0:42:04.880,0:42:07.599
here is this sound at that location

0:42:06.240,0:42:10.400
that's on that location

0:42:07.599,0:42:10.880
we're just telling it uh here is uh an

0:42:10.400,0:42:13.119
input

0:42:10.880,0:42:14.400
uh sentence here is the transcription of

0:42:13.119,0:42:15.760
it in terms of words

0:42:14.400,0:42:17.760
you know figure it out by doing this

0:42:15.760,0:42:19.280
time warping um

0:42:17.760,0:42:21.520
you know in the context of hidden markov

0:42:19.280,0:42:23.200
models which is kind of very similar

0:42:21.520,0:42:26.640
to to the the damage mapping i was

0:42:23.200,0:42:29.280
talking you talking about earlier

0:42:26.640,0:42:30.800
then as i said in the early 90s people

0:42:29.280,0:42:33.520
started working on

0:42:30.800,0:42:34.240
using neural nets to kind of feed one of

0:42:33.520,0:42:37.359
those kind of

0:42:34.240,0:42:38.720
uh structural prediction system as i

0:42:37.359,0:42:40.560
said the first one i know about is by

0:42:38.720,0:42:41.599
xavier de la cruz number two for speech

0:42:40.560,0:42:43.760
recognition

0:42:41.599,0:42:45.599
but they had a 10 delay neural that

0:42:43.760,0:42:49.119
joshua benjo did his phd on this

0:42:45.599,0:42:50.800
um and and had some results around 1992

0:42:49.119,0:42:52.640
and patrick hafner the the year after

0:42:50.800,0:42:54.720
that um

0:42:52.640,0:42:58.240
uh leonbotul joshua benjamin patrick

0:42:54.720,0:43:01.520
hefner are the co-authors of uh

0:42:58.240,0:43:03.200
my paper from 1998 about handwriting

0:43:01.520,0:43:08.079
recognition because

0:43:03.200,0:43:08.079
i hired all three of them at a t

0:43:08.160,0:43:11.359
to work on this problem that figure

0:43:09.920,0:43:12.960
basically figured out

0:43:11.359,0:43:15.200
you know somewhere doing this in the phd

0:43:12.960,0:43:17.839
thesis and i i knew that was

0:43:15.200,0:43:18.720
the the trick uh that needed to be

0:43:17.839,0:43:20.480
worked on for

0:43:18.720,0:43:24.000
things like handwriting recognition

0:43:20.480,0:43:24.000
structural prediction with neural nets

0:43:24.839,0:43:30.079
um

0:43:26.640,0:43:30.079
right uh let's see

0:43:31.280,0:43:36.240
okay so um so here is a way um

0:43:34.880,0:43:38.079
and i only alluded to this really

0:43:36.240,0:43:39.200
quickly in an earlier lecture here's a

0:43:38.079,0:43:42.319
way to sort of

0:43:39.200,0:43:44.560
put this in the context of

0:43:42.319,0:43:45.760
deep learning so as i said before one

0:43:44.560,0:43:47.200
way to to

0:43:45.760,0:43:49.839
do this in the context of deep learning

0:43:47.200,0:43:51.520
is you make those factors

0:43:49.839,0:43:52.960
deep neural nets basically right they

0:43:51.520,0:43:54.240
just compute some energy

0:43:52.960,0:43:56.800
and they are parameterized by a bunch of

0:43:54.240,0:43:58.640
parameters and nothing changes

0:43:56.800,0:44:01.359
uh you you know we know how to do back

0:43:58.640,0:44:03.839
prop and we have pi torch

0:44:01.359,0:44:05.520
um but here is uh uh here's a slightly

0:44:03.839,0:44:08.240
different idea and this

0:44:05.520,0:44:10.960
which kind of draws on the same type of

0:44:08.240,0:44:14.160
of model

0:44:10.960,0:44:17.040
and this is when uh the

0:44:14.160,0:44:18.880
the structure is more complex than just

0:44:17.040,0:44:23.359
you know a bunch of fixed factors

0:44:18.880,0:44:26.400
from uh known um

0:44:23.359,0:44:28.160
with a known structure if you want

0:44:26.400,0:44:32.160
so the example i'm going to use here is

0:44:28.160,0:44:32.160
uh is handwriting recognition but

0:44:33.359,0:44:36.960
you know because there's a long history

0:44:34.880,0:44:39.839
of it and i have uh

0:44:36.960,0:44:40.640
uh you know drawings that are that are

0:44:39.839,0:44:42.400
prepared for this

0:44:40.640,0:44:43.920
that have been around for a long time

0:44:42.400,0:44:46.319
but okay so

0:44:43.920,0:44:47.440
um so here the problem we have is that

0:44:46.319,0:44:49.599
you know we have a sequence of

0:44:47.440,0:44:50.720
digits at the input and we don't know

0:44:49.599,0:44:54.640
how to

0:44:50.720,0:44:56.000
uh segment this digit into individual

0:44:54.640,0:44:57.440
this this sequence into individual

0:44:56.000,0:44:59.119
digits because we don't know what the

0:44:57.440,0:45:01.040
parts are for each of the digits

0:44:59.119,0:45:02.640
the four here is kind of broken into two

0:45:01.040,0:45:04.640
parts

0:45:02.640,0:45:06.319
and so what we can do is uh build a

0:45:04.640,0:45:09.920
graph in which each path

0:45:06.319,0:45:13.040
is a possible way of breaking up this

0:45:09.920,0:45:16.079
sequence of uh blobs into

0:45:13.040,0:45:17.760
into characters right so i can group you

0:45:16.079,0:45:19.680
know i can make each of the separate

0:45:17.760,0:45:21.839
pieces a separate character

0:45:19.680,0:45:23.359
so that's the path at the top i can

0:45:21.839,0:45:25.760
group the first two pieces

0:45:23.359,0:45:27.920
three four and the left part of the four

0:45:25.760,0:45:32.480
and then have the last two be separate

0:45:27.920,0:45:35.599
or i can have the first uh the first um

0:45:32.480,0:45:37.680
uh be by itself the

0:45:35.599,0:45:39.040
the following two be regrouped and then

0:45:37.680,0:45:41.599
the last one be by itself

0:45:39.040,0:45:42.480
right so what have i done here i've

0:45:41.599,0:45:46.560
basically

0:45:42.480,0:45:49.119
uh uh said okay

0:45:46.560,0:45:51.040
the way i do i did inference in the in

0:45:49.119,0:45:55.119
the context of soccer prediction

0:45:51.040,0:45:59.359
was by having uh uh energy terms

0:45:55.119,0:46:01.520
that tell me the the the

0:45:59.359,0:46:02.480
the cost of a particular combination of

0:46:01.520,0:46:05.280
variables

0:46:02.480,0:46:06.079
right so this this graph here represents

0:46:05.280,0:46:09.280
basically

0:46:06.079,0:46:12.560
is a explicit representation of that

0:46:09.280,0:46:14.960
energy model uh as long as i put on

0:46:12.560,0:46:16.880
those arcs here the energies that

0:46:14.960,0:46:18.480
are computed by those by those modules

0:46:16.880,0:46:20.880
for each each value

0:46:18.480,0:46:21.760
but what if i just manipulate this this

0:46:20.880,0:46:25.680
graph

0:46:21.760,0:46:29.359
so what if uh you know the state

0:46:25.680,0:46:31.200
that i manipulate in a neural net

0:46:29.359,0:46:33.040
is not a particular assignment or

0:46:31.200,0:46:34.640
variable together with

0:46:33.040,0:46:36.720
something to compute energies but it's

0:46:34.640,0:46:38.880
directly a graph like this

0:46:36.720,0:46:40.880
okay so a graph like this basically you

0:46:38.880,0:46:43.040
can think of as representing

0:46:40.880,0:46:44.319
a list of energies for every possible

0:46:43.040,0:46:47.359
configurations

0:46:44.319,0:46:49.839
of the of the variables of interest

0:46:47.359,0:46:51.359
okay it's a compact way of representing

0:46:49.839,0:46:52.960
uh a list of energies for all

0:46:51.359,0:46:56.800
configurations of the

0:46:52.960,0:47:02.160
sequences of symbols um

0:46:56.800,0:47:03.920
so what if i i build a neural net

0:47:02.160,0:47:05.599
so that this internal states of that

0:47:03.920,0:47:08.800
neural net are basically

0:47:05.599,0:47:11.119
those uh those graphs

0:47:08.800,0:47:13.359
and the graphs are just again i repeat a

0:47:11.119,0:47:14.560
compact way of representing a list of

0:47:13.359,0:47:16.319
energies for every possible

0:47:14.560,0:47:17.280
configurations of the variable of

0:47:16.319,0:47:21.359
interest

0:47:17.280,0:47:21.359
nothing more okay

0:47:22.640,0:47:27.200
so uh but i can use this for other

0:47:26.160,0:47:29.920
things and energies

0:47:27.200,0:47:31.920
so here a path in a graph corresponds to

0:47:29.920,0:47:35.200
a particular way of breaking up

0:47:31.920,0:47:37.520
this uh blobs of ink into

0:47:35.200,0:47:39.040
into characters and each path is a

0:47:37.520,0:47:42.240
particular way of grouping those

0:47:39.040,0:47:42.640
blobs into into characters i can run

0:47:42.240,0:47:45.119
those

0:47:42.640,0:47:46.960
uh images so now this graph is annotated

0:47:45.119,0:47:49.680
by images not by energies

0:47:46.960,0:47:51.200
okay uh i can run those images through a

0:47:49.680,0:47:53.599
comment the content is going to

0:47:51.200,0:47:54.720
tell me for each arcs in this graph is

0:47:53.599,0:47:56.319
going to tell me well

0:47:54.720,0:47:58.880
this is very likely to be a three and

0:47:56.319,0:48:00.160
here is the energy for that three

0:47:58.880,0:48:02.160
okay low energy if it's a good three

0:48:00.160,0:48:04.240
high energy if it's a bad three

0:48:02.160,0:48:06.240
um it could also tell me well this may

0:48:04.240,0:48:07.599
be a two but uh

0:48:06.240,0:48:09.839
with a higher energy or it could be a

0:48:07.599,0:48:11.760
zero with a higher energy okay

0:48:09.839,0:48:14.400
so it's going to build this graph which

0:48:11.760,0:48:14.400
you can uh

0:48:14.640,0:48:18.640
which you can call the uh interpretation

0:48:16.559,0:48:20.720
graph each path in this graph is a

0:48:18.640,0:48:23.760
possible labeling

0:48:20.720,0:48:27.359
of each path in this graph and the

0:48:23.760,0:48:30.319
labels you know indicate the categories

0:48:27.359,0:48:31.520
and the energies attached to the to the

0:48:30.319,0:48:33.680
arcs

0:48:31.520,0:48:34.880
are basically the energies of the you

0:48:33.680,0:48:36.400
know produced by

0:48:34.880,0:48:38.480
my comp net here for each of those

0:48:36.400,0:48:40.720
answers okay

0:48:38.480,0:48:42.960
so uh discount net is going to look at

0:48:40.720,0:48:44.640
this little piece of a four

0:48:42.960,0:48:46.319
it's going to tell me well that looks

0:48:44.640,0:48:47.760
kind of like a two with a low energy or

0:48:46.319,0:48:49.520
that may look like a piece of a four

0:48:47.760,0:48:52.800
with a higher energy

0:48:49.520,0:48:54.640
okay uh the guy that looks at this piece

0:48:52.800,0:48:56.000
who's you know it's somewhere here is

0:48:54.640,0:48:57.119
going to tell me well this is a four i'm

0:48:56.000,0:48:59.280
quite sure of it

0:48:57.119,0:49:00.319
uh with low energy and you know it's

0:48:59.280,0:49:01.680
going to tell me maybe it's something

0:49:00.319,0:49:03.839
else with higher energy

0:49:01.680,0:49:05.440
so each of those arcs here is going to

0:49:03.839,0:49:07.119
be replaced by 10 arcs i'm only

0:49:05.440,0:49:09.200
representing two here but

0:49:07.119,0:49:11.119
uh essentially 10 arcs corresponding to

0:49:09.200,0:49:13.119
the 10 possible categories

0:49:11.119,0:49:14.880
each of them with a different energy

0:49:13.119,0:49:16.800
that is just the output

0:49:14.880,0:49:19.760
of the corresponding output of the

0:49:16.800,0:49:21.359
continent that are applied here

0:49:19.760,0:49:23.359
now inference again is finding the

0:49:21.359,0:49:26.319
shortest path in this graph

0:49:23.359,0:49:27.839
so uh you know finding the path with the

0:49:26.319,0:49:30.079
minimum energy basically right

0:49:27.839,0:49:31.040
so finding the shortest path so it's

0:49:30.079,0:49:32.559
basically

0:49:31.040,0:49:34.079
you can think of it as a module that

0:49:32.559,0:49:35.680
selects uh

0:49:34.079,0:49:37.359
that you know selects the shortest path

0:49:35.680,0:49:37.839
in fact it's this one here i call the

0:49:37.359,0:49:39.920
future

0:49:37.839,0:49:41.280
transformer so the the word transformer

0:49:39.920,0:49:44.960
in the context of neural nets

0:49:41.280,0:49:45.599
was used in 1997 but it's been you know

0:49:44.960,0:49:48.960
kind of

0:49:45.599,0:49:48.960
recycled for something else now

0:49:50.640,0:49:56.839
right okay so here's a complete example

0:49:54.559,0:50:00.160
of how this might work

0:49:56.839,0:50:04.319
um so again we have

0:50:00.160,0:50:05.599
an input an input image

0:50:04.319,0:50:07.760
we run this through this kind of

0:50:05.599,0:50:10.160
segmenter that proposes

0:50:07.760,0:50:11.520
multiple alternative segmentations which

0:50:10.160,0:50:13.599
are ways to group those

0:50:11.520,0:50:14.800
blob of links together each fact in the

0:50:13.599,0:50:16.800
in this graph corresponds to one

0:50:14.800,0:50:19.119
particular way of grouping the

0:50:16.800,0:50:20.559
the blobs of ink we run each of those

0:50:19.119,0:50:22.880
through a neural net

0:50:20.559,0:50:24.240
uh identical copies of the same content

0:50:22.880,0:50:26.480
that just is trying to do character

0:50:24.240,0:50:29.839
recognition okay

0:50:26.480,0:50:31.359
each of those comp nets produces a list

0:50:29.839,0:50:33.119
of 10 scores so it tells

0:50:31.359,0:50:35.040
you know this guy tells me this is one

0:50:33.119,0:50:36.000
with energy 0.1 this is 4 with energy

0:50:35.040,0:50:39.119
2.4

0:50:36.000,0:50:41.200
etc this guy tells me well this piece is

0:50:39.119,0:50:45.119
four with energy point six or

0:50:41.200,0:50:47.280
nine with energy 1.2 or whatever etc etc

0:50:45.119,0:50:48.880
right uh this guy is going to give me

0:50:47.280,0:50:50.160
kind of relatively high energy for

0:50:48.880,0:50:51.200
everything because that doesn't look

0:50:50.160,0:50:54.480
good

0:50:51.200,0:50:57.040
same for this guy okay so now i get

0:50:54.480,0:50:58.960
a graph here and think of it as kind of

0:50:57.040,0:51:00.480
a weird form of tensor right it's a

0:50:58.960,0:51:03.520
sparse tensor really

0:51:00.480,0:51:06.000
okay it's something that says uh

0:51:03.520,0:51:08.000
it's a tensor that you know for each

0:51:06.000,0:51:10.640
possible configuration of this variable

0:51:08.000,0:51:11.680
tells me the the cost of that of that

0:51:10.640,0:51:13.119
variable

0:51:11.680,0:51:15.920
so it's not really it's more like a

0:51:13.119,0:51:18.319
distribution over tensors if you want

0:51:15.920,0:51:20.839
okay or log distribution it's not

0:51:18.319,0:51:22.480
normalized because we're talking about

0:51:20.839,0:51:25.599
energies

0:51:22.480,0:51:28.640
um okay then i take this

0:51:25.599,0:51:29.839
this graph and i uh

0:51:28.640,0:51:32.000
i want to compute the energy of the

0:51:29.839,0:51:34.160
correct answer because i'm you know i

0:51:32.000,0:51:35.839
might want to train the system right so

0:51:34.160,0:51:37.440
i'm telling you the correct answer is

0:51:35.839,0:51:39.520
three four

0:51:37.440,0:51:41.520
select within those paths the one that

0:51:39.520,0:51:43.200
actually says three four

0:51:41.520,0:51:44.720
okay and there's two of them there's

0:51:43.200,0:51:47.920
three four

0:51:44.720,0:51:50.880
with energy uh point one plus point six

0:51:47.920,0:51:52.480
and then there is three four with energy

0:51:50.880,0:51:54.720
3.4 plus 2.4

0:51:52.480,0:51:55.760
which is my trial right so i get those

0:51:54.720,0:51:57.839
two paths

0:51:55.760,0:51:59.040
and then among those two paths i i pick

0:51:57.839,0:52:02.079
the best one

0:51:59.040,0:52:04.640
uh three four okay so i told the system

0:52:02.079,0:52:06.880
here is the correct answer give me the

0:52:04.640,0:52:08.640
path that has the lowest energy but yet

0:52:06.880,0:52:11.599
gives me the correct answer

0:52:08.640,0:52:13.119
okay so finding that path is like

0:52:11.599,0:52:14.480
minimizing over a latent variable where

0:52:13.119,0:52:15.599
the latent variable is which path you

0:52:14.480,0:52:17.599
pick

0:52:15.599,0:52:18.880
right because actually it's an energy

0:52:17.599,0:52:20.400
model where the latent variable is a

0:52:18.880,0:52:23.680
path

0:52:20.400,0:52:25.680
a professor yes three or four in the

0:52:23.680,0:52:28.000
graph should be labeled before training

0:52:25.680,0:52:28.319
or that that latent variable will figure

0:52:28.000,0:52:31.760
out

0:52:28.319,0:52:33.520
for the system uh so here uh

0:52:31.760,0:52:35.200
i'm putting myself in a situation where

0:52:33.520,0:52:36.400
i'm gonna train the system supervised i

0:52:35.200,0:52:37.200
know the correct answer this is the

0:52:36.400,0:52:39.760
desired answer

0:52:37.200,0:52:40.720
think of this as a target okay so we

0:52:39.760,0:52:42.319
just know the

0:52:40.720,0:52:44.319
target but i don't know which part is

0:52:42.319,0:52:45.440
three and which part is for

0:52:44.319,0:52:47.200
well that's right so we know what the

0:52:45.440,0:52:48.000
target is we don't know which path has

0:52:47.200,0:52:50.640
the correct

0:52:48.000,0:52:51.280
uh is the correct segmentation right it

0:52:50.640,0:52:52.559
could be

0:52:51.280,0:52:54.960
it could be this path or it could be

0:52:52.559,0:52:54.960
that path

0:52:55.920,0:52:59.520
right and and here what we do is we just

0:52:58.079,0:53:00.640
pick the the one with the lowest energy

0:52:59.520,0:53:02.880
which happened to be the correct one

0:53:00.640,0:53:02.880
here

0:53:02.960,0:53:10.240
okay so is this recognition transformer

0:53:07.200,0:53:14.000
is each of these like um you know

0:53:10.240,0:53:17.920
n and box are each are those all shared

0:53:14.000,0:53:18.800
like okay yeah this is multiple copies

0:53:17.920,0:53:21.280
of the same neural net

0:53:18.800,0:53:23.839
right it's just a character recognition

0:53:21.280,0:53:23.839
in this case

0:53:24.000,0:53:28.000
um okay now you have we have the energy

0:53:26.559,0:53:32.800
of the correct answer

0:53:28.000,0:53:36.000
it's 0.7 it's the sum of 0.1 and 0.6

0:53:32.800,0:53:37.520
okay and what we need to do now is back

0:53:36.000,0:53:38.319
propagate gradient through this entire

0:53:37.520,0:53:40.000
structure

0:53:38.319,0:53:41.839
so that we can change the weights of

0:53:40.000,0:53:43.920
that neural net in such a way that this

0:53:41.839,0:53:45.920
energy goes down

0:53:43.920,0:53:47.040
okay and this looks daunting but it's

0:53:45.920,0:53:49.920
entirely possible

0:53:47.040,0:53:50.960
because this entire system here is built

0:53:49.920,0:53:53.680
out of

0:53:50.960,0:53:56.240
uh elements that we already know about

0:53:53.680,0:53:58.000
that's just a regular neural net

0:53:56.240,0:53:59.599
and those passed electrons and v2

0:53:58.000,0:54:00.400
transformers are basically switches that

0:53:59.599,0:54:02.880
pick

0:54:00.400,0:54:04.240
uh you know a particular edge or not

0:54:02.880,0:54:06.480
right so it's like a switch

0:54:04.240,0:54:07.920
it's like max pooling it's except it's

0:54:06.480,0:54:11.040
min pulling if you want

0:54:07.920,0:54:13.200
okay right so how do i propagate

0:54:11.040,0:54:15.119
well this point seven is just the sum of

0:54:13.200,0:54:17.920
this point one and this point six so

0:54:15.119,0:54:19.119
uh if i have uh if i compute the

0:54:17.920,0:54:21.359
gradient of this

0:54:19.119,0:54:22.240
with respect to this point one it's it's

0:54:21.359,0:54:25.440
just one

0:54:22.240,0:54:26.160
the gradient of of this output with

0:54:25.440,0:54:29.359
respect to

0:54:26.160,0:54:30.880
this value here 0.6 is also one okay

0:54:29.359,0:54:32.960
because that is just the sum of those

0:54:30.880,0:54:35.280
two things and just back propagating

0:54:32.960,0:54:37.839
one through a plot to a sum and that's

0:54:35.280,0:54:39.440
just that's just a y connection

0:54:37.839,0:54:40.799
okay now back propagating through the

0:54:39.440,0:54:42.799
viterbi transformer this guy just

0:54:40.799,0:54:44.240
selected one path among two

0:54:42.799,0:54:46.079
so what he's going to do is that it's

0:54:44.240,0:54:47.440
going to take those those gradients here

0:54:46.079,0:54:49.280
and just copy them

0:54:47.440,0:54:51.359
on the corresponding edge on the input

0:54:49.280,0:54:53.680
graph and then set the gradients for the

0:54:51.359,0:54:55.520
other path that was not selected to zero

0:54:53.680,0:54:57.119
it's exactly what's happening in you

0:54:55.520,0:54:59.200
know max putting or mean pulling

0:54:57.119,0:55:01.520
you're propagating through the switch at

0:54:59.200,0:55:05.440
that right position but not propagating

0:55:01.520,0:55:07.599
next to it so it's nothing fancy okay

0:55:05.440,0:55:09.359
um pass it actually is the same it's uh

0:55:07.599,0:55:10.160
it's just a system that selects the path

0:55:09.359,0:55:14.240
that could

0:55:10.160,0:55:17.440
uh produce the correct answer and so um

0:55:14.240,0:55:20.000
i'm i'm just gonna set you know

0:55:17.440,0:55:20.880
whatever uh you know through this i'm

0:55:20.000,0:55:23.599
gonna

0:55:20.880,0:55:24.000
propagate the plus one to the the arcs

0:55:23.599,0:55:26.319
uh

0:55:24.000,0:55:27.520
that appear here so this arc is that one

0:55:26.319,0:55:29.520
you see a zero here

0:55:27.520,0:55:31.119
but i'm coming back to this in a minute

0:55:29.520,0:55:33.040
it should be a one for now

0:55:31.119,0:55:35.119
and plus one here and that corresponds

0:55:33.040,0:55:36.400
to this plus one here

0:55:35.119,0:55:37.760
okay and then you can propagate those

0:55:36.400,0:55:38.720
gradients all the way to the neural net

0:55:37.760,0:55:41.040
and adjust the weights

0:55:38.720,0:55:42.880
so that this energy goes down okay so

0:55:41.040,0:55:44.960
that will take care of making the energy

0:55:42.880,0:55:47.280
of the correct answer small

0:55:44.960,0:55:48.960
okay by back propagating through this

0:55:47.280,0:55:49.599
thing now what's important about this is

0:55:48.960,0:55:52.319
that

0:55:49.599,0:55:53.200
this structure here is dynamic in the

0:55:52.319,0:55:56.480
sense that

0:55:53.200,0:55:58.319
if i give you a new input the number of

0:55:56.480,0:56:00.400
instances of this neural net will change

0:55:58.319,0:56:02.319
with the number of segments the graph

0:56:00.400,0:56:04.160
here will change uh

0:56:02.319,0:56:05.680
those graphs will change completely and

0:56:04.160,0:56:07.200
so i need to be able to back propagate

0:56:05.680,0:56:08.559
through this kind of dynamical structure

0:56:07.200,0:56:10.240
if you want

0:56:08.559,0:56:12.079
and this is you know situations where

0:56:10.240,0:56:13.440
things like pie torch are really

0:56:12.079,0:56:15.280
important because you want to be able to

0:56:13.440,0:56:17.359
handle those you know kind of

0:56:15.280,0:56:18.559
dynamical structures that change with

0:56:17.359,0:56:21.200
every sample

0:56:18.559,0:56:22.640
okay so this back propagation phase

0:56:21.200,0:56:24.319
takes care of

0:56:22.640,0:56:25.760
uh making the energy of the correct

0:56:24.319,0:56:27.040
answer small

0:56:25.760,0:56:29.680
now how do we make the energy of

0:56:27.040,0:56:32.720
incorrect answers large

0:56:29.680,0:56:34.480
well uh there's gonna be a second phase

0:56:32.720,0:56:35.839
where we're just gonna in this case

0:56:34.480,0:56:37.119
we're just gonna let the system pick

0:56:35.839,0:56:41.359
whatever answer it wants

0:56:37.119,0:56:43.599
okay um and

0:56:41.359,0:56:45.119
um this is kind of a simplified form of

0:56:43.599,0:56:46.240
uh discriminative training for

0:56:45.119,0:56:49.440
structural prediction

0:56:46.240,0:56:51.040
uh that used uses a form of perceptron

0:56:49.440,0:56:54.559
loss if you want

0:56:51.040,0:56:56.160
um okay so i'm the first few stages are

0:56:54.559,0:56:57.599
exactly identical to what i talked about

0:56:56.160,0:56:59.839
earlier

0:56:57.599,0:57:01.680
uh but here the viterbi transformer just

0:56:59.839,0:57:04.319
picks the best pass among all the paths

0:57:01.680,0:57:06.400
you don't constrain it to pick the

0:57:04.319,0:57:07.760
the uh the correct one you just

0:57:06.400,0:57:09.520
literally pick whatever it wants

0:57:07.760,0:57:11.280
okay so it's going to pick the best path

0:57:09.520,0:57:13.040
that it thinks that that has the lowest

0:57:11.280,0:57:15.680
energy that it thinks is the

0:57:13.040,0:57:17.200
you know gives the correct answer now

0:57:15.680,0:57:19.839
the energy you get out of this

0:57:17.200,0:57:22.240
necessarily is going to be smaller or

0:57:19.839,0:57:23.680
equal to the one you got previously

0:57:22.240,0:57:25.440
because this one is the smallest of all

0:57:23.680,0:57:26.960
the possible configurations the other

0:57:25.440,0:57:27.520
ones is not the smallest of all possible

0:57:26.960,0:57:29.680
ones it's

0:57:27.520,0:57:31.040
only the smallest of the correct ones

0:57:29.680,0:57:31.680
and so this guy necessarily is going to

0:57:31.040,0:57:33.760
get smaller

0:57:31.680,0:57:36.079
so we don't pick the one sorry i i lost

0:57:33.760,0:57:37.440
you do we pick the one do we take out

0:57:36.079,0:57:39.119
the one that are actually producing the

0:57:37.440,0:57:41.119
correct sequence or not

0:57:39.119,0:57:42.400
okay so you have two forms of it uh the

0:57:41.119,0:57:44.559
form i'm explaining here you're not

0:57:42.400,0:57:46.720
taking out the correct one

0:57:44.559,0:57:48.400
okay in fact in this particular example

0:57:46.720,0:57:50.400
it wouldn't make any difference but

0:57:48.400,0:57:52.319
uh if you want the system to work

0:57:50.400,0:57:52.720
properly what you what you should do

0:57:52.319,0:57:55.119
here

0:57:52.720,0:57:56.000
is have a pass selector that takes out

0:57:55.119,0:57:57.599
the correct answers

0:57:56.000,0:57:59.760
yeah yeah exactly yes yes yes that's

0:57:57.599,0:58:01.280
right so

0:57:59.760,0:58:02.960
you would want to tell the system give

0:58:01.280,0:58:06.000
me your best shot

0:58:02.960,0:58:08.400
a wrong answer okay the lowest energy

0:58:06.000,0:58:09.920
wrong answer exactly the white bar in

0:58:08.400,0:58:10.400
your paper right that would be the white

0:58:09.920,0:58:12.480
bar

0:58:10.400,0:58:13.839
yeah here i'm not doing this i'm just

0:58:12.480,0:58:14.799
asking you what's your best shot you

0:58:13.839,0:58:18.640
know i don't care if it's

0:58:14.799,0:58:21.359
correct or incorrect all right

0:58:18.640,0:58:21.839
uh we'll come back to this in a minute

0:58:21.359,0:58:23.359
i'm

0:58:21.839,0:58:24.640
putting this all together my loss

0:58:23.359,0:58:25.760
function is going to be the difference

0:58:24.640,0:58:28.640
between

0:58:25.760,0:58:30.079
the energy i get for the correct answer

0:58:28.640,0:58:32.000
minus the energy i get

0:58:30.079,0:58:33.599
for whatever answer the system wants to

0:58:32.000,0:58:35.839
produce

0:58:33.599,0:58:37.280
okay so i compute the difference between

0:58:35.839,0:58:38.720
those two and that's my loss function

0:58:37.280,0:58:40.400
now i can back propagate through this

0:58:38.720,0:58:42.000
entire thing i told you i was back

0:58:40.400,0:58:42.480
propagating just to make the energy here

0:58:42.000,0:58:43.920
small

0:58:42.480,0:58:45.599
i'm not actually doing this i'm

0:58:43.920,0:58:46.799
computing a loss function here which in

0:58:45.599,0:58:47.760
this case is just the difference between

0:58:46.799,0:58:50.160
this and that

0:58:47.760,0:58:51.520
and uh and i'm back propagating gradient

0:58:50.160,0:58:55.040
through this entire structure

0:58:51.520,0:58:58.240
right so whatever path

0:58:55.040,0:58:59.839
appears only on the left

0:58:58.240,0:59:01.839
we'll get a plus one so this guy gets a

0:58:59.839,0:59:02.960
plus one because this edge only appears

0:59:01.839,0:59:06.240
on this side

0:59:02.960,0:59:07.200
and so it gets a plus one the paths that

0:59:06.240,0:59:10.079
only appear

0:59:07.200,0:59:11.280
on the right side like this guy sorry

0:59:10.079,0:59:14.240
like this guy

0:59:11.280,0:59:15.760
get a minus one okay the gradient here

0:59:14.240,0:59:17.200
gets a minus one because

0:59:15.760,0:59:18.400
uh you have a minus sign here so the

0:59:17.200,0:59:21.520
gradients you know when they back

0:59:18.400,0:59:23.599
propagate they end up being minus ones

0:59:21.520,0:59:24.720
okay this guy also gets minus one this

0:59:23.599,0:59:27.599
guy here appear

0:59:24.720,0:59:29.040
appears on both sides and so the minus

0:59:27.599,0:59:30.640
one and the plus one cancel

0:59:29.040,0:59:33.200
and these guys you know get zero

0:59:30.640,0:59:34.400
gradient it's it's in the correct path

0:59:33.200,0:59:36.000
but it's also in the past that the

0:59:34.400,0:59:36.640
system produces so you shouldn't change

0:59:36.000,0:59:39.200
anything

0:59:36.640,0:59:41.839
it's it's okay right so the guys that

0:59:39.200,0:59:43.440
have a minus one are the incorrect paths

0:59:41.839,0:59:45.200
the other parts are in the incorrect

0:59:43.440,0:59:47.520
answer but not in the correct answer

0:59:45.200,0:59:49.520
the one that i have a plus one are the

0:59:47.520,0:59:53.440
the edges that are in the correct

0:59:49.520,0:59:55.760
uh answer but not in the incorrect one

0:59:53.440,0:59:57.200
okay the one has zero are in neither or

0:59:55.760,0:59:59.920
they are in both

0:59:57.200,1:00:01.520
right so now you get gradients here you

0:59:59.920,1:00:02.960
those gradients are gradients for all

1:00:01.520,1:00:04.559
the outputs of all those neural nets you

1:00:02.960,1:00:07.839
back propagate to the neural nets

1:00:04.559,1:00:10.240
and compute and update the weights okay

1:00:07.839,1:00:11.839
and what if you do this then the system

1:00:10.240,1:00:12.480
will eventually minimize its loss

1:00:11.839,1:00:13.680
function

1:00:12.480,1:00:15.200
which is the difference between the

1:00:13.680,1:00:16.480
energy of the correct answer and the

1:00:15.200,1:00:18.720
energy of

1:00:16.480,1:00:20.480
the best answer whatever it is that loss

1:00:18.720,1:00:23.599
function is the perceptron loss

1:00:20.480,1:00:25.280
and we talked about this before um in

1:00:23.599,1:00:28.960
fact let me go to this just now

1:00:25.280,1:00:30.400
okay right so the the loss function we

1:00:28.960,1:00:33.839
just talked about is the second

1:00:30.400,1:00:35.680
oops the second one in this list here

1:00:33.839,1:00:37.040
uh this is the energy of the correct

1:00:35.680,1:00:39.520
answer

1:00:37.040,1:00:40.960
minus the energy of whatever answer your

1:00:39.520,1:00:43.599
your system wants to produce

1:00:40.960,1:00:47.359
okay that's the perceptron loss or the

1:00:43.599,1:00:47.359
generalized perceptron loss if you want

1:00:48.079,1:00:51.839
and the bad news about this cross

1:00:50.720,1:00:53.680
function is that it doesn't have a

1:00:51.839,1:00:55.200
margin so it doesn't ensure that the

1:00:53.680,1:00:57.200
energy of the

1:00:55.200,1:00:58.480
incorrect answers is larger strictly

1:00:57.200,1:01:01.839
larger than the energy of the

1:00:58.480,1:01:03.680
good answer uh you know it might just

1:01:01.839,1:01:06.720
collapse it might just uh

1:01:03.680,1:01:09.839
make every energy zero or the same

1:01:06.720,1:01:10.559
okay so it's not a good loss function to

1:01:09.839,1:01:12.480
use

1:01:10.559,1:01:14.319
it just happens to work when your energy

1:01:12.480,1:01:17.680
is linearly parametrizing w

1:01:14.319,1:01:18.799
but in the general case it doesn't work

1:01:17.680,1:01:22.079
so you're much better off using

1:01:18.799,1:01:24.240
something like this a hinge

1:01:22.079,1:01:25.119
but in the case of a hinge what you need

1:01:24.240,1:01:27.920
to have here

1:01:25.119,1:01:30.559
is this y bar which is the energy of the

1:01:27.920,1:01:33.839
most offending incorrect answer

1:01:30.559,1:01:37.200
so basically uh in the second phase

1:01:33.839,1:01:39.599
as uh alfredo was uh pointing out

1:01:37.200,1:01:41.040
instead of picking the path with the

1:01:39.599,1:01:41.680
lowest energy the answer with the lowest

1:01:41.040,1:01:43.839
energy

1:01:41.680,1:01:45.839
you constrain the system to pick a wrong

1:01:43.839,1:01:47.920
answer and then among all of those pick

1:01:45.839,1:01:49.520
the one with the lowest energy

1:01:47.920,1:01:51.040
and then you take the difference between

1:01:49.520,1:01:51.520
those two energies so energy or correct

1:01:51.040,1:01:53.119
answer

1:01:51.520,1:01:55.359
energy of most offending incorrect

1:01:53.119,1:01:55.839
answer compute the difference between

1:01:55.359,1:01:58.880
them

1:01:55.839,1:01:59.680
and uh plug this into a hinge so that

1:01:58.880,1:02:02.000
you want

1:01:59.680,1:02:02.799
this energy to be lower than that energy

1:02:02.000,1:02:06.880
by at least

1:02:02.799,1:02:09.920
m okay uh

1:02:06.880,1:02:10.319
and this is the kind of objective here

1:02:09.920,1:02:14.400
that

1:02:10.319,1:02:16.480
uh on botu used um

1:02:14.400,1:02:18.240
so it's you know it looks very similar

1:02:16.480,1:02:20.640
um

1:02:18.240,1:02:22.640
uh this is uh someone something called

1:02:20.640,1:02:24.000
nce that people in um

1:02:22.640,1:02:25.839
speech recognition produce so it's

1:02:24.000,1:02:26.559
basically like this looks a bit like a

1:02:25.839,1:02:28.400
sigmoid

1:02:26.559,1:02:29.680
so basically it's a sigmoid function you

1:02:28.400,1:02:30.720
take the difference between the energy

1:02:29.680,1:02:32.799
of the correct cancer

1:02:30.720,1:02:33.839
and the energy of incorrect answers and

1:02:32.799,1:02:35.760
you plug them into

1:02:33.839,1:02:37.359
a sigmoid right it's one over one plus

1:02:35.760,1:02:38.799
exponential minus blah blah

1:02:37.359,1:02:40.480
and so it basically wants to make that

1:02:38.799,1:02:41.920
difference uh

1:02:40.480,1:02:44.799
you know kind of small but then it

1:02:41.920,1:02:46.839
doesn't care if it's if it's too small

1:02:44.799,1:02:48.559
and if it's too large it kind of gives

1:02:46.839,1:02:50.319
up

1:02:48.559,1:02:51.920
and then you have you know this is the

1:02:50.319,1:02:53.520
negative likelihood loss

1:02:51.920,1:02:55.119
so make the energy of the correct answer

1:02:53.520,1:02:57.839
small and then make

1:02:55.119,1:03:00.240
make the log of the sum over all answers

1:02:57.839,1:03:03.280
of e to the minus the energy of

1:03:00.240,1:03:06.799
those answers large

1:03:03.280,1:03:08.480
okay make the minus log large which

1:03:06.799,1:03:10.880
means make the logs small

1:03:08.480,1:03:12.559
which means makes those energies large

1:03:10.880,1:03:15.200
okay

1:03:12.559,1:03:16.000
and then the yoyo rabiner thing i was

1:03:15.200,1:03:18.000
telling you about is

1:03:16.000,1:03:20.319
another form of objective function that

1:03:18.000,1:03:22.079
sort of you know pushes them and push up

1:03:20.319,1:03:23.760
most of those are derived from sort of

1:03:22.079,1:03:24.640
privacy principles but many of them

1:03:23.760,1:03:27.680
aren't

1:03:24.640,1:03:29.119
okay all the ones at the top aren't

1:03:27.680,1:03:30.720
hey professor i had a question about the

1:03:29.119,1:03:32.400
margin of the losses

1:03:30.720,1:03:33.760
um i think in the previous lecture we

1:03:32.400,1:03:35.280
discussed how the negative

1:03:33.760,1:03:36.880
likelihood loss converges to the

1:03:35.280,1:03:39.280
perceptron loss when beta is

1:03:36.880,1:03:40.240
uh going towards infinity correct or

1:03:39.280,1:03:42.720
something like that but

1:03:40.240,1:03:43.280
how come the not the no loss has a

1:03:42.720,1:03:45.359
positive

1:03:43.280,1:03:47.200
margin and the perceptual loss doesn't

1:03:45.359,1:03:49.039
well just you know because the uh

1:03:47.200,1:03:51.200
because the the temperature is i mean

1:03:49.039,1:03:53.039
the the one over beta

1:03:51.200,1:03:54.799
because beta is not is not infinite

1:03:53.039,1:03:56.480
because one over beta is not zero

1:03:54.799,1:03:59.200
so yeah i mean if you take the limit of

1:03:56.480,1:04:02.400
this for beta goes to infinity

1:03:59.200,1:04:03.599
uh this one over beta log sum converges

1:04:02.400,1:04:06.799
to min over y

1:04:03.599,1:04:09.119
of uh energy of w

1:04:06.799,1:04:10.839
i x i and so that's exactly what the

1:04:09.119,1:04:13.359
perceptron does

1:04:10.839,1:04:15.280
okay so the perceptron is a

1:04:13.359,1:04:16.559
zero temperature limit or infinite beta

1:04:15.280,1:04:20.000
limit of

1:04:16.559,1:04:24.000
negative likelihood indeed

1:04:20.000,1:04:26.319
but the margin is essentially infinite

1:04:24.000,1:04:27.920
in this case whereas the margin here is

1:04:26.319,1:04:30.559
zero

1:04:27.920,1:04:32.079
okay so there's a bit of a discontinuity

1:04:30.559,1:04:34.559
here

1:04:32.079,1:04:36.720
uh admittedly though if you make beta

1:04:34.559,1:04:39.200
very large here numerically

1:04:36.720,1:04:40.000
uh the energies of anything but the

1:04:39.200,1:04:43.680
lowest energy

1:04:40.000,1:04:45.599
term uh i mean the the the role of the

1:04:43.680,1:04:48.240
importance of the terms

1:04:45.599,1:04:48.720
uh for various y's in this sum will kind

1:04:48.240,1:04:50.799
of

1:04:48.720,1:04:51.760
diminish and so numerically it may

1:04:50.799,1:04:54.000
actually

1:04:51.760,1:04:55.200
uh start behaving very much like the

1:04:54.000,1:04:58.319
perceptron

1:04:55.200,1:04:59.839
early on um

1:04:58.319,1:05:01.440
there's a problem with it also which i

1:04:59.839,1:05:05.039
mentioned before which is that

1:05:01.440,1:05:08.799
uh this uh this wants to make the energy

1:05:05.039,1:05:09.760
of incorrect answers infinite and

1:05:08.799,1:05:12.079
you know it's not going to make them

1:05:09.760,1:05:13.200
infinite because as they get larger and

1:05:12.079,1:05:17.119
larger they're

1:05:13.200,1:05:19.039
uh you know the the gradient of this sum

1:05:17.119,1:05:20.400
with respect to each of them gets very

1:05:19.039,1:05:22.400
small but

1:05:20.400,1:05:23.440
they're going to get pushed to infinity

1:05:22.400,1:05:24.880
and so

1:05:23.440,1:05:26.480
it's it's not necessarily a good thing

1:05:24.880,1:05:26.799
the hinge is better in a way because you

1:05:26.480,1:05:28.799
know

1:05:26.799,1:05:29.839
it just says well i just want it to be

1:05:28.799,1:05:32.640
larger

1:05:29.839,1:05:34.000
you know by by some value i don't care

1:05:32.640,1:05:36.240
how much

1:05:34.000,1:05:38.079
um i give you another form of the the

1:05:36.240,1:05:41.280
hinge loss in the past

1:05:38.079,1:05:44.240
uh where you have a a

1:05:41.280,1:05:45.599
sum over y so instead of just taking the

1:05:44.240,1:05:46.720
most of any incorrect answer in the

1:05:45.599,1:05:49.599
hinge

1:05:46.720,1:05:50.880
uh you take all answers and you sum over

1:05:49.599,1:05:52.400
all of them and for each of them you

1:05:50.880,1:05:53.200
have a different margin which depends on

1:05:52.400,1:05:56.240
the on the

1:05:53.200,1:05:57.680
y i and the y i bar

1:05:56.240,1:05:59.280
that's a more general form it might be

1:05:57.680,1:06:01.599
more expensive depending on how you

1:05:59.280,1:06:03.680
compute it

1:06:01.599,1:06:05.200
there are a bunch of questions here yeah

1:06:03.680,1:06:07.039
so first

1:06:05.200,1:06:09.280
there are a few students asking about

1:06:07.039,1:06:11.440
the segmenter do we learn the segmenter

1:06:09.280,1:06:13.839
is he also do we backprop there is a

1:06:11.440,1:06:15.839
latent variable something

1:06:13.839,1:06:18.160
uh in this particular case no it's just

1:06:15.839,1:06:21.280
uh it's just um

1:06:18.160,1:06:22.160
and handcrafted heuristics uh but you

1:06:21.280,1:06:24.400
could imagine

1:06:22.160,1:06:25.599
uh building a differentiable segmenter

1:06:24.400,1:06:26.400
and then back propagating all the way

1:06:25.599,1:06:27.680
through it yes

1:06:26.400,1:06:28.960
this was actually one of the original

1:06:27.680,1:06:32.000
plans when we built this thing in the

1:06:28.960,1:06:33.520
mid-90s we never got to it

1:06:32.000,1:06:35.599
but and the reason we never got to it is

1:06:33.520,1:06:37.599
because there is another approach to

1:06:35.599,1:06:38.960
character recognition which is the kind

1:06:37.599,1:06:39.920
of sliding window approach which i

1:06:38.960,1:06:41.119
explained right so

1:06:39.920,1:06:43.599
you just take the input you never

1:06:41.119,1:06:45.359
segment it you just apply the neural net

1:06:43.599,1:06:47.039
to every location on the input and you

1:06:45.359,1:06:48.480
record the output and then you do

1:06:47.039,1:06:51.280
structure prediction on top of that

1:06:48.480,1:06:53.280
so now you have to have some sort of

1:06:51.280,1:06:56.400
sequence models that tells you

1:06:53.280,1:06:59.200
uh you know if i observe uh

1:06:56.400,1:07:00.559
you know three three three blank blank

1:06:59.200,1:07:02.640
two

1:07:00.559,1:07:04.400
four four four four it's actually three

1:07:02.640,1:07:05.520
four the blank and the two basically are

1:07:04.400,1:07:08.880
spurious

1:07:05.520,1:07:11.760
right so you would have uh a grammar

1:07:08.880,1:07:12.000
uh that would indicate like what are you

1:07:11.760,1:07:13.680
know

1:07:12.000,1:07:15.200
correct combinations of characters on

1:07:13.680,1:07:17.200
the output

1:07:15.200,1:07:18.720
and you would do this to finding the

1:07:17.200,1:07:21.039
shortest paths in the graph

1:07:18.720,1:07:21.760
so the graph on the bottom is generated

1:07:21.039,1:07:24.160
by the

1:07:21.760,1:07:25.200
segmenter is it correct yeah the one

1:07:24.160,1:07:28.400
with the one hop

1:07:25.200,1:07:31.680
like two hops or two hops one up one hop

1:07:28.400,1:07:33.119
yep okay and there you go yeah you can

1:07:31.680,1:07:34.319
think of this as kind of a simple form

1:07:33.119,1:07:36.000
of graph neural net

1:07:34.319,1:07:37.680
or or kind of a specific form of graph

1:07:36.000,1:07:40.480
neural nets where

1:07:37.680,1:07:42.799
uh the this entire deep learning

1:07:40.480,1:07:45.839
architecture manipulates

1:07:42.799,1:07:48.640
graphs instead of tensors as its kind of

1:07:45.839,1:07:49.760
way of representing inputs okay or

1:07:48.640,1:07:51.200
states

1:07:49.760,1:07:53.280
so think of this as a multi-layer

1:07:51.200,1:07:54.880
architecture where the states or graphs

1:07:53.280,1:07:56.799
are annotated graphs

1:07:54.880,1:07:59.280
all right and then you can have modules

1:07:56.799,1:08:01.039
here that turns graph into other graphs

1:07:59.280,1:08:02.240
we used to call these graph transformers

1:08:01.039,1:08:03.920
in fact that's this is called graph

1:08:02.240,1:08:07.280
transformer network right

1:08:03.920,1:08:09.520
okay so this is from 1997 right this is

1:08:07.280,1:08:12.400
not recent

1:08:09.520,1:08:13.920
and uh in fact 1996. the first paper is

1:08:12.400,1:08:18.000
in 1997.

1:08:13.920,1:08:19.839
uh and uh uh and then those can be

1:08:18.000,1:08:21.600
you know as long as the way you compute

1:08:19.839,1:08:23.679
those scores

1:08:21.600,1:08:24.960
uh is with differentiable functions that

1:08:23.679,1:08:26.799
are parametrized you can back propagate

1:08:24.960,1:08:28.319
gradient through this entire thing

1:08:26.799,1:08:30.400
and i can demonstrate it how you do this

1:08:28.319,1:08:32.239
in this particular case

1:08:30.400,1:08:33.839
i see then there is another question

1:08:32.239,1:08:34.400
which i'm i may not be able to

1:08:33.839,1:08:37.040
understand

1:08:34.400,1:08:40.080
which is what are the dimensions of of

1:08:37.040,1:08:42.080
the interpretation graphs

1:08:40.080,1:08:43.839
i don't know what dimensions uh uh

1:08:42.080,1:08:46.719
dimensions so basically each

1:08:43.839,1:08:48.159
uh arc okay you can do it in two ways

1:08:46.719,1:08:51.600
the way i've represented it here

1:08:48.159,1:08:54.080
is that each arc has a label

1:08:51.600,1:08:56.000
uh three here for this particular edge

1:08:54.080,1:08:57.440
and an energy 3.4

1:08:56.000,1:08:59.839
and then the number in parenthesis is

1:08:57.440,1:09:03.040
the gradient that comes from the top

1:08:59.839,1:09:06.239
okay so here is a scalar but

1:09:03.040,1:09:08.000
here on the graph at the bottom the

1:09:06.239,1:09:12.000
annotation is an entire

1:09:08.000,1:09:16.080
tensor it's an image okay

1:09:12.000,1:09:19.520
so i don't specify what uh what you can

1:09:16.080,1:09:21.920
annotate the graphs with as long as

1:09:19.520,1:09:23.520
whatever it is that you annotate it with

1:09:21.920,1:09:25.120
if it's computed by some continuous

1:09:23.520,1:09:27.199
functions you you want to be able to

1:09:25.120,1:09:28.640
propagate gradient to it

1:09:27.199,1:09:30.560
now another way of representing this

1:09:28.640,1:09:33.359
graph is not by having

1:09:30.560,1:09:35.040
a separate arc here for each category

1:09:33.359,1:09:38.319
but by having a vector

1:09:35.040,1:09:38.319
and the vector just contains

1:09:38.560,1:09:43.120
the list of categories together with a

1:09:41.679,1:09:46.080
list of scores

1:09:43.120,1:09:46.719
okay so 0 to 9 and then the list of

1:09:46.080,1:09:48.319
energies

1:09:46.719,1:09:50.000
for each of the thing and that would be

1:09:48.319,1:09:51.120
just one arc but it would be annotated

1:09:50.000,1:09:55.040
by

1:09:51.120,1:09:55.040
this vector yeah i see i think okay okay

1:09:56.000,1:10:00.880
but because uh you know this guy the

1:09:58.880,1:10:02.960
viterbi and the past elector selects

1:10:00.880,1:10:04.960
individual paths it's clearer if you

1:10:02.960,1:10:08.320
kind of write it this way

1:10:04.960,1:10:10.320
how you implement it is up to you um

1:10:08.320,1:10:12.080
so those guys transform networks um

1:10:10.320,1:10:12.880
there are speech recognition systems

1:10:12.080,1:10:14.880
today

1:10:12.880,1:10:16.080
that uh so basically in a speech

1:10:14.880,1:10:19.199
recognition system

1:10:16.080,1:10:20.480
this whole way of inferring the correct

1:10:19.199,1:10:22.560
sequence

1:10:20.480,1:10:23.760
uh using for example a language model is

1:10:22.560,1:10:26.480
called a decoder

1:10:23.760,1:10:27.920
okay so a decoder at the output of a

1:10:26.480,1:10:28.480
neural net generally you have a sequence

1:10:27.920,1:10:31.040
of

1:10:28.480,1:10:33.120
vectors that indicate the the score the

1:10:31.040,1:10:35.679
energy the probability whatever you want

1:10:33.120,1:10:36.719
of individual sounds or phonemes or

1:10:35.679,1:10:37.920
sometimes words

1:10:36.719,1:10:39.600
and then you have to pass this to a

1:10:37.920,1:10:40.640
language model that tells you you know

1:10:39.600,1:10:42.239
this sequence

1:10:40.640,1:10:44.080
is possible that all the sequence isn't

1:10:42.239,1:10:46.640
and then it picks out the best

1:10:44.080,1:10:48.000
possible interpretation according to the

1:10:46.640,1:10:49.760
language model and according to the

1:10:48.000,1:10:53.120
scores produced by your system

1:10:49.760,1:10:53.920
that's called a decoder okay and the big

1:10:53.120,1:10:55.520
question is

1:10:53.920,1:10:57.120
how do you back proper get rid into the

1:10:55.520,1:10:58.640
decoder is only

1:10:57.120,1:11:00.640
a very small number of speech

1:10:58.640,1:11:01.760
recognition systems today that actually

1:11:00.640,1:11:03.840
do this

1:11:01.760,1:11:04.800
uh the latest one i think is by ronald

1:11:03.840,1:11:08.239
corbeau

1:11:04.800,1:11:08.239
the original author of torch

1:11:08.640,1:11:12.640
and here's how this works uh so let's

1:11:10.560,1:11:14.880
say you want to

1:11:12.640,1:11:16.239
so this is a a particular concept called

1:11:14.880,1:11:20.480
graph composition or

1:11:16.239,1:11:20.480
graph transducers and

1:11:21.679,1:11:24.800
which kind of explains how you can

1:11:23.360,1:11:25.840
combine graphs with each other for

1:11:24.800,1:11:28.960
example

1:11:25.840,1:11:32.080
together with a language model okay so

1:11:28.960,1:11:34.159
you can think of a language model

1:11:32.080,1:11:36.159
as as a graph you can represent it as a

1:11:34.159,1:11:36.800
graph as a neural net it doesn't matter

1:11:36.159,1:11:38.080
but

1:11:36.800,1:11:40.080
i'm going to represent i'm going to

1:11:38.080,1:11:41.840
represent it as a graph right so here

1:11:40.080,1:11:44.400
this is basically a lexicon that is

1:11:41.840,1:11:48.080
represented represented as a tree

1:11:44.400,1:11:48.719
tree t-r-i-e okay and it can represent

1:11:48.080,1:11:51.840
the words

1:11:48.719,1:11:55.040
barn but

1:11:51.840,1:11:58.400
cute cure

1:11:55.040,1:12:01.280
cap cat and card okay

1:11:58.400,1:12:02.239
so each terminal node is a word and each

1:12:01.280,1:12:03.679
path

1:12:02.239,1:12:05.199
and each terminal node corresponds to a

1:12:03.679,1:12:05.760
path and each path is is a word

1:12:05.199,1:12:09.199
basically

1:12:05.760,1:12:11.280
right the sequence of symbols is a word

1:12:09.199,1:12:13.120
now so let's imagine our entire lexicon

1:12:11.280,1:12:14.719
is is this

1:12:13.120,1:12:17.679
and we have a neural net or something

1:12:14.719,1:12:19.040
that produces uh a trellis of possible

1:12:17.679,1:12:19.920
interpretation that corresponds to this

1:12:19.040,1:12:21.360
graph so it says

1:12:19.920,1:12:23.199
the first character i can't tell you

1:12:21.360,1:12:24.719
exactly what it is but i think it's c

1:12:23.199,1:12:25.520
with energy point four it's always

1:12:24.719,1:12:28.320
energy one

1:12:25.520,1:12:30.239
or is d with energy one point eight okay

1:12:28.320,1:12:31.760
it's a character recognizer

1:12:30.239,1:12:33.440
and the second one says it's x with

1:12:31.760,1:12:35.120
energy point one or a with point two or

1:12:33.440,1:12:37.520
u with point eight and the last one

1:12:35.120,1:12:38.400
p with point two t with point eight so

1:12:37.520,1:12:42.560
we need to do now

1:12:38.400,1:12:46.239
is what is the best uh

1:12:42.560,1:12:48.239
interpretation uh of this the best path

1:12:46.239,1:12:51.440
in this that also happens to be present

1:12:48.239,1:12:52.719
in our lexicon okay and the operation

1:12:51.440,1:12:54.159
you need to do for this

1:12:52.719,1:12:57.120
is a concept that was invented by

1:12:54.159,1:12:58.320
fernando pereira who is head of the nlp

1:12:57.120,1:13:00.000
research group and more than that

1:12:58.320,1:13:01.840
actually at google research but that was

1:13:00.000,1:13:06.080
back when he was at at

1:13:01.840,1:13:08.080
their labs in the early 90s

1:13:06.080,1:13:09.920
and it was sort of implemented in an

1:13:08.080,1:13:11.440
open source library called the fsm

1:13:09.920,1:13:14.800
library which was implemented by mary

1:13:11.440,1:13:16.480
amore who is a professor at nyu

1:13:14.800,1:13:18.320
he did this while he was at a t and then

1:13:16.480,1:13:20.400
at google

1:13:18.320,1:13:22.719
and the the way you do this is this

1:13:20.400,1:13:23.360
composition operation so you you start

1:13:22.719,1:13:25.199
from

1:13:23.360,1:13:26.400
the initial node of both of those graphs

1:13:25.199,1:13:28.239
and you say

1:13:26.400,1:13:29.760
is there a path is there a path i can

1:13:28.239,1:13:33.760
take in this graph

1:13:29.760,1:13:36.800
that is legal here okay so here

1:13:33.760,1:13:41.760
i can have b or c and here i can have c

1:13:36.800,1:13:44.560
o or d only c is common between the two

1:13:41.760,1:13:46.400
okay so i'm going to combine those two

1:13:44.560,1:13:48.080
by saying in my output graph

1:13:46.400,1:13:50.480
i'm gonna have one of those transitions

1:13:48.080,1:13:53.600
which is the the only transition that is

1:13:50.480,1:13:56.560
common here and here okay so now

1:13:53.600,1:13:58.880
i'm in this node oops sorry i mean this

1:13:56.560,1:14:02.320
node here i mean that node there

1:13:58.880,1:14:04.719
here i can i can take x a or u

1:14:02.320,1:14:06.880
and here i can take u or a okay so i

1:14:04.719,1:14:09.199
have two possibilities u or a

1:14:06.880,1:14:11.120
a with point two u with point eight and

1:14:09.199,1:14:13.280
so i add those two here

1:14:11.120,1:14:14.719
okay so basically what i'm doing here is

1:14:13.280,1:14:16.400
uh whenever

1:14:14.719,1:14:18.560
i come at a node and i have to take a

1:14:16.400,1:14:20.239
transition i find you know which of the

1:14:18.560,1:14:22.640
nodes that can be in here

1:14:20.239,1:14:24.239
i look at the possible transitions and

1:14:22.640,1:14:25.120
if the transition exists if there is one

1:14:24.239,1:14:29.199
that matches

1:14:25.120,1:14:32.080
i create a outgoing uh i going arc

1:14:29.199,1:14:33.600
and i annotated by the energy of

1:14:32.080,1:14:35.520
whatever arc i had here

1:14:33.600,1:14:36.880
if i also had an energy in this arc i

1:14:35.520,1:14:38.719
could just add those two terms or

1:14:36.880,1:14:42.960
combine them in some way

1:14:38.719,1:14:46.239
okay so now i have two nodes here

1:14:42.960,1:14:48.000
um and uh

1:14:46.239,1:14:49.280
the last one can be p or t so i can

1:14:48.000,1:14:50.239
start from those two nodes and have

1:14:49.280,1:14:51.679
either p or t

1:14:50.239,1:14:53.520
and it can be in either in this node or

1:14:51.679,1:14:56.000
this node so i can go here with t

1:14:53.520,1:14:57.280
i can go here with p or here is t and i

1:14:56.000,1:14:59.920
end up with this

1:14:57.280,1:15:01.679
those three things so now i have my

1:14:59.920,1:15:05.120
interpretation is either cat

1:15:01.679,1:15:06.480
or cap or cut these are the three

1:15:05.120,1:15:08.640
interpretations that are

1:15:06.480,1:15:11.120
grammatically correct and at the same

1:15:08.640,1:15:13.040
time are present as a possibility

1:15:11.120,1:15:14.800
produced by neural net and now i just

1:15:13.040,1:15:18.320
have to find the shortest path there

1:15:14.800,1:15:20.640
and that's my answer okay

1:15:18.320,1:15:22.880
so that operation here is called graph

1:15:20.640,1:15:25.840
composition

1:15:22.880,1:15:26.880
and it basically allows you to basically

1:15:25.840,1:15:30.159
combine two graphs

1:15:26.880,1:15:31.520
essentially or combine two knowledge

1:15:30.159,1:15:32.880
bases that are

1:15:31.520,1:15:34.800
conceptually graphed but they could be

1:15:32.880,1:15:36.560
represented by neural nets so here i can

1:15:34.800,1:15:38.320
represent this thing this whole language

1:15:36.560,1:15:39.920
model by neural net uh

1:15:38.320,1:15:41.920
when i'm at a particular location it

1:15:39.920,1:15:42.960
means you know when i'm here it means i

1:15:41.920,1:15:45.760
observed

1:15:42.960,1:15:47.520
the sequence cu and then i can run cu in

1:15:45.760,1:15:48.960
my in my language model and ask my

1:15:47.520,1:15:49.600
neural net to predict so what's the next

1:15:48.960,1:15:50.960
letter

1:15:49.600,1:15:52.719
and my neural net would say well it's

1:15:50.960,1:15:55.600
either t or r

1:15:52.719,1:15:56.719
uh you know in this soft max output with

1:15:55.600,1:15:58.480
the 26 letters

1:15:56.719,1:15:59.760
it's going to tell me you know t and r

1:15:58.480,1:16:00.800
have five probabilities the other ones

1:15:59.760,1:16:02.800
are low probabilities

1:16:00.800,1:16:04.320
or if it produces energy is going to say

1:16:02.800,1:16:07.600
t and r have low energies

1:16:04.320,1:16:08.960
the other ones have higher energies

1:16:07.600,1:16:10.719
okay so it doesn't matter how you

1:16:08.960,1:16:13.840
actually represent this

1:16:10.719,1:16:15.760
if it's represented as a neural net then

1:16:13.840,1:16:16.960
uh implicitly then you can train this

1:16:15.760,1:16:17.440
neural net you can train the language

1:16:16.960,1:16:18.719
model

1:16:17.440,1:16:20.719
because you can back properly a gradient

1:16:18.719,1:16:22.800
to this entire thing okay

1:16:20.719,1:16:24.159
that so that would be sort of an example

1:16:22.800,1:16:26.400
of what people called

1:16:24.159,1:16:28.000
uh differentiable programming i mean

1:16:26.400,1:16:29.920
basically the way to implement this is a

1:16:28.000,1:16:31.199
really really complicated program

1:16:29.920,1:16:32.880
what you need to do is backdrop a

1:16:31.199,1:16:35.199
gradient through this entire program and

1:16:32.880,1:16:40.320
this program has loops and ifs and

1:16:35.199,1:16:40.320
recursions okay so not trivial

1:16:41.040,1:16:44.719
i'm not telling you how we actually

1:16:42.560,1:16:48.560
implemented this in 1994

1:16:44.719,1:16:50.400
1995 but but that's basically how

1:16:48.560,1:16:52.239
our check reading system back in those

1:16:50.400,1:16:54.800
days was uh was implemented

1:16:52.239,1:16:56.320
so the last function we used uh in the

1:16:54.800,1:16:56.880
end to train the system was actually the

1:16:56.320,1:17:00.400
negative

1:16:56.880,1:17:03.840
likelihood loss function so negative

1:17:00.400,1:17:06.239
likelihood says

1:17:03.840,1:17:07.440
you have an interpretation graph here

1:17:06.239,1:17:08.239
where each path is a possible

1:17:07.440,1:17:10.000
interpretation

1:17:08.239,1:17:11.199
and the sum of the energies along the

1:17:10.000,1:17:13.920
path is the energy of that

1:17:11.199,1:17:15.280
interpretation

1:17:13.920,1:17:16.560
you give it the correct answer you

1:17:15.280,1:17:17.840
select the path that have the correct

1:17:16.560,1:17:20.000
interpretation

1:17:17.840,1:17:21.520
okay same on the other side so here you

1:17:20.000,1:17:23.199
combine with the grammar so the grammar

1:17:21.520,1:17:25.440
restricts the

1:17:23.199,1:17:28.239
number the the the sequences to those

1:17:25.440,1:17:30.000
that are syntactically correct

1:17:28.239,1:17:31.520
okay so if it's an amount on the check

1:17:30.000,1:17:34.480
for example you know it's got

1:17:31.520,1:17:36.239
a decimal dot uh it might have you know

1:17:34.480,1:17:37.440
a dollar sign in front it might have

1:17:36.239,1:17:40.480
stars you know

1:17:37.440,1:17:41.440
um there's a grammar for it um which you

1:17:40.480,1:17:44.640
can build by hand

1:17:41.440,1:17:46.080
it's a finite state grammar uh you com

1:17:44.640,1:17:48.080
you compose those two graphs and you get

1:17:46.080,1:17:48.800
the set of paths in this graph that

1:17:48.080,1:17:50.400
actually

1:17:48.800,1:17:52.000
contain a grammatically correct

1:17:50.400,1:17:53.600
interpretation and now you don't do

1:17:52.000,1:17:54.640
viterbi you do forward okay what is

1:17:53.600,1:17:58.480
forward

1:17:54.640,1:18:00.239
so viterbi computes the path

1:17:58.480,1:18:02.239
in a graph that has the minimum energy

1:18:00.239,1:18:03.360
basically it minimizes with respect to

1:18:02.239,1:18:05.679
the latent variable where the latent

1:18:03.360,1:18:07.920
variable is a path in the graph

1:18:05.679,1:18:10.480
forward computes the log of the sum of

1:18:07.920,1:18:13.920
the exponentials of minus the energies

1:18:10.480,1:18:16.560
of all the paths okay

1:18:13.920,1:18:17.920
so basically it marginalizes over the

1:18:16.560,1:18:19.440
latent variable which is the path in the

1:18:17.920,1:18:21.600
graph

1:18:19.440,1:18:23.040
now it turns out that you can do this

1:18:21.600,1:18:24.960
very easily and

1:18:23.040,1:18:26.480
it's very cheap it doesn't cost more

1:18:24.960,1:18:28.719
than doing viterbi

1:18:26.480,1:18:29.679
uh and uh and you can back properly get

1:18:28.719,1:18:33.360
to it

1:18:29.679,1:18:34.000
um and i don't have a slide for this so

1:18:33.360,1:18:36.560
i'm gonna

1:18:34.000,1:18:36.560
switch to

1:18:38.719,1:18:42.159
drawing it okay so

1:18:48.800,1:18:57.840
all right

1:18:54.719,1:19:02.400
so you have one pass you have

1:18:57.840,1:19:02.400
another path and

1:19:02.719,1:19:06.719
maybe another path that skips over here

1:19:06.960,1:19:15.280
okay and each of those guys has

1:19:11.920,1:19:18.960
an energy right e1 e2

1:19:15.280,1:19:22.080
e3 e4 e5

1:19:18.960,1:19:24.640
e6 let's say okay

1:19:22.080,1:19:26.000
so if you do viterbi shortest paths in a

1:19:24.640,1:19:27.840
graph you're just going to find the path

1:19:26.000,1:19:30.320
that has the minimum energy

1:19:27.840,1:19:32.239
but what i'm going to talk about here is

1:19:30.320,1:19:33.840
computing

1:19:32.239,1:19:35.760
so think of the path as a latent

1:19:33.840,1:19:39.760
variable z and remember

1:19:35.760,1:19:41.360
to compute f of x y

1:19:39.760,1:19:43.280
you can do two things you can do mean

1:19:41.360,1:19:46.320
over z of e

1:19:43.280,1:19:48.960
of x y z and remember z is

1:19:46.320,1:19:50.880
the path or if you want to marginalize

1:19:48.960,1:19:55.440
you do minus 1 over beta

1:19:50.880,1:19:59.040
log sum over all z's

1:19:55.440,1:20:02.080
of e to the minus beta e of x y z

1:19:59.040,1:20:03.679
and that's marginalizing it's a discrete

1:20:02.080,1:20:05.440
sum of z is a discrete variable which is

1:20:03.679,1:20:06.480
the case here because it's a discrete

1:20:05.440,1:20:10.159
path

1:20:06.480,1:20:12.239
okay so this is f beta

1:20:10.159,1:20:13.920
x y and you can think of this as f

1:20:12.239,1:20:15.440
infinity right this is the limit for

1:20:13.920,1:20:17.920
beta goes to infinity of the one

1:20:15.440,1:20:17.920
at the bottom

1:20:21.920,1:20:25.760
or neural network to be trained in the

1:20:23.920,1:20:26.880
model

1:20:25.760,1:20:28.880
i didn't understand the question i'm

1:20:26.880,1:20:32.080
sorry can you repeat uh it's the

1:20:28.880,1:20:34.159
energy function here some simple some

1:20:32.080,1:20:38.080
simple function like loss function

1:20:34.159,1:20:39.679
or some some some some neural networks

1:20:38.080,1:20:41.920
to be trained in the model

1:20:39.679,1:20:44.480
it doesn't matter okay this is the this

1:20:41.920,1:20:47.360
is the energy that you use to measure

1:20:44.480,1:20:48.159
the score of an answer y okay the

1:20:47.360,1:20:51.600
observation

1:20:48.159,1:20:53.040
is x yeah the answer you are supposed to

1:20:51.600,1:20:57.040
predict in this case a

1:20:53.040,1:20:58.800
sequence of symbols uh is y

1:20:57.040,1:21:00.560
um and so for you know each of those

1:20:58.800,1:21:01.520
things here is annotated with a

1:21:00.560,1:21:05.520
particular y

1:21:01.520,1:21:07.760
okay so this could be uh you know

1:21:05.520,1:21:07.760
uh

1:21:09.280,1:21:12.880
you know each of those arcs is annotated

1:21:11.520,1:21:15.920
by a symbol

1:21:12.880,1:21:19.360
so let's say a

1:21:15.920,1:21:22.880
and this is b and this is b

1:21:19.360,1:21:26.800
and c and this is i don't know

1:21:22.880,1:21:28.560
x and this is g or something right

1:21:26.800,1:21:31.280
so here the possible interpretation for

1:21:28.560,1:21:32.560
y so y would be a string of symbols and

1:21:31.280,1:21:35.600
it can be either

1:21:32.560,1:21:39.920
a b or it can be

1:21:35.600,1:21:39.920
b c g or it can be

1:21:41.280,1:21:51.040
c cg okay this is c

1:21:46.560,1:21:52.320
okay those are the next x right ec

1:21:51.040,1:21:54.320
sorry i'm sorry you're right this is

1:21:52.320,1:21:58.320
this is a six it's x

1:21:54.320,1:22:00.159
thanks you're right sorry about that

1:21:58.320,1:22:01.440
okay those are the only three possible

1:22:00.159,1:22:03.120
interpretations

1:22:01.440,1:22:04.480
uh in that in that graph that can come

1:22:03.120,1:22:07.520
out of that graph

1:22:04.480,1:22:10.960
um and z

1:22:07.520,1:22:12.719
is which fast you're taking okay so

1:22:10.960,1:22:14.719
if z is the first path then the output

1:22:12.719,1:22:15.440
will be a b if z is the second part the

1:22:14.719,1:22:18.960
output will be

1:22:15.440,1:22:20.639
bcg et cetera okay okay thank you

1:22:18.960,1:22:22.840
right but this is not used for training

1:22:20.639,1:22:24.639
this is the energy function okay use for

1:22:22.840,1:22:27.840
inference

1:22:24.639,1:22:29.840
um okay so

1:22:27.840,1:22:31.520
this the the log of the sum of the

1:22:29.840,1:22:33.520
exponentials of the energies

1:22:31.520,1:22:34.880
uh for all the paths the sum of uh over

1:22:33.520,1:22:39.840
all the paths okay

1:22:34.880,1:22:39.840
so the sum here is over path

1:22:41.199,1:22:48.800
okay uh that's like marginalizing over z

1:22:46.639,1:22:51.440
and we saw that before right we

1:22:48.800,1:22:56.239
explained that before

1:22:51.440,1:23:00.480
um now how do i compute this um

1:22:56.239,1:23:01.840
now it turns out it's very simple

1:23:00.480,1:23:03.120
it's done using what's called a forward

1:23:01.840,1:23:04.719
algorithm i'm actually going to draw a

1:23:03.120,1:23:07.520
different

1:23:04.719,1:23:08.719
tree a different graph a graph that's

1:23:07.520,1:23:11.120
going to look more like the one i had

1:23:08.719,1:23:11.120
before

1:23:11.600,1:23:25.520
which was kind of like this right

1:23:20.560,1:23:25.520
okay so y is a sequence of uh

1:23:26.159,1:23:34.000
three symbols in this case and

1:23:29.440,1:23:38.880
uh each um

1:23:34.000,1:23:38.880
and the first symbol can be um

1:23:39.120,1:23:42.320
oh i'm sorry i'm using nodes here

1:23:40.560,1:23:45.280
instead of instead of arcs that's a

1:23:42.320,1:23:45.280
little confusing

1:23:45.920,1:23:49.840
let me correct that

1:23:52.400,1:23:55.840
okay so each path in this graph is a

1:23:54.239,1:23:59.360
possible interpretation

1:23:55.840,1:24:01.600
okay so for each each edge i'm taking

1:23:59.360,1:24:03.520
i'm emitting a symbol

1:24:01.600,1:24:04.960
and i don't have skipping connections so

1:24:03.520,1:24:08.400
here they all have

1:24:04.960,1:24:10.480
exactly four symbols coming out

1:24:08.400,1:24:12.719
because every path is of length four

1:24:10.480,1:24:12.719
okay

1:24:12.880,1:24:16.320
but how do i compute this sum this this

1:24:15.120,1:24:19.440
sum

1:24:16.320,1:24:21.040
uh basically i go at a node okay when

1:24:19.440,1:24:22.639
i'm at i don't know it here let's let's

1:24:21.040,1:24:25.440
take this node right right here i'm

1:24:22.639,1:24:25.440
gonna call it red

1:24:28.080,1:24:36.159
okay the the cost

1:24:31.360,1:24:39.520
from the input node

1:24:36.159,1:24:42.400
the the energy from the input node

1:24:39.520,1:24:46.080
to that node is the log of the sum of

1:24:42.400,1:24:46.080
the exponentials of the energies from

1:24:47.600,1:24:51.199
along all paths to go from the input

1:24:49.920,1:24:54.320
node to that node

1:24:51.199,1:24:57.440
okay so

1:24:54.320,1:24:58.719
and of course you know i have i have an

1:24:57.440,1:25:01.600
energy right here which is just the

1:24:58.719,1:25:03.040
energy of that branch

1:25:01.600,1:25:04.719
i have an energy here which is just the

1:25:03.040,1:25:08.239
energy of that branch

1:25:04.719,1:25:11.840
okay i have an f here um

1:25:08.239,1:25:14.639
and to compute the uh the f for this guy

1:25:11.840,1:25:16.400
i just compute the

1:25:14.639,1:25:19.040
log of the sum of the exponentials of

1:25:16.400,1:25:19.040
those two guys

1:25:21.120,1:25:26.639
okay right so let me

1:25:24.719,1:25:28.560
unwrap this okay i've got an energy here

1:25:26.639,1:25:31.600
y1 i've got energy here y2

1:25:28.560,1:25:32.639
i got one here y3 oh e3 sorry and one

1:25:31.600,1:25:36.639
here

1:25:32.639,1:25:36.639
uh this guy is e4

1:25:36.800,1:25:41.840
all right the f i'm gonna get here

1:25:43.120,1:25:46.960
okay so this is i'm going to call this

1:25:50.320,1:25:54.480
i'm going to call it anything so the

1:25:52.960,1:25:56.719
value i should have here

1:25:54.480,1:25:56.719
is

1:25:59.920,1:26:03.199
is e1 plus

1:26:06.840,1:26:09.840
e3

1:26:11.360,1:26:15.520
exponential of that minus beta that

1:26:15.679,1:26:25.520
plus exponential minus beta

1:26:22.239,1:26:25.520
e2 plus e4

1:26:26.080,1:26:32.320
and i take minus one over beta log of

1:26:30.080,1:26:32.320
this

1:26:34.840,1:26:39.679
okay

1:26:36.400,1:26:41.360
so how is the event calculated i mean

1:26:39.679,1:26:43.040
the smaller yield

1:26:41.360,1:26:44.800
lower list whatever comes out of your

1:26:43.040,1:26:46.159
energy right um

1:26:44.800,1:26:48.080
each of those graphs you know as i said

1:26:46.159,1:26:50.400
you you represent uh

1:26:48.080,1:26:52.239
possible interpretations as a graph each

1:26:50.400,1:26:54.320
node in the graph has an energy

1:26:52.239,1:26:56.560
and a complete energy of the function

1:26:54.320,1:27:00.639
which is an f of x y for a particular y

1:26:56.560,1:27:03.679
in a particular z following a path is uh

1:27:00.639,1:27:05.679
is is uh is e of x y z

1:27:03.679,1:27:08.000
and now what you want to compute is log

1:27:05.679,1:27:11.040
of sum of e to the minus

1:27:08.000,1:27:12.639
e of x y z uh which is the

1:27:11.040,1:27:14.880
marginalization over all the paths

1:27:12.639,1:27:16.480
so it's basically combining the cost of

1:27:14.880,1:27:20.080
all the paths

1:27:16.480,1:27:22.320
in kind of a soft minimum way right

1:27:20.080,1:27:23.120
but the algorithm is is super simple

1:27:22.320,1:27:26.239
because

1:27:23.120,1:27:27.840
uh you maintain a variable basically for

1:27:26.239,1:27:29.920
each node

1:27:27.840,1:27:31.600
for each node you compute a variable

1:27:29.920,1:27:33.120
alpha

1:27:31.600,1:27:35.520
for a particular node and it's going to

1:27:33.120,1:27:38.880
be equal to

1:27:35.520,1:27:42.400
minus log sum over

1:27:38.880,1:27:42.400
all the nodes that are

1:27:44.400,1:27:48.719
up from from so let's say it's not i up

1:27:47.520,1:27:52.080
from i

1:27:48.719,1:27:56.000
okay so all the parent nodes of of i

1:27:52.080,1:28:01.360
and then you do e to the minus beta the

1:27:56.000,1:28:04.800
alpha uh k

1:28:01.360,1:28:07.440
and and you add to it

1:28:04.800,1:28:07.440
uh e

1:28:08.000,1:28:10.800
of uh

1:28:11.440,1:28:15.280
k i which would be the energy of the

1:28:13.520,1:28:18.320
link coming from node k

1:28:15.280,1:28:18.320
to node i

1:28:19.360,1:28:29.840
okay that's called a forward algorithm

1:28:30.000,1:28:34.159
and if you've heard about this it's

1:28:32.080,1:28:38.159
actually a special case

1:28:34.159,1:28:38.159
of the so-called belief propagation

1:28:40.840,1:28:43.840
algorithm

1:28:43.920,1:28:49.120
so belief propagation is a general

1:28:46.239,1:28:51.810
algorithm for

1:28:49.120,1:28:53.040
graphical models and

1:28:51.810,1:28:54.880
[Music]

1:28:53.040,1:28:56.800
the fourth algorithm is a special case

1:28:54.880,1:28:57.600
when the your graph is basically a chain

1:28:56.800,1:29:00.480
graph

1:28:57.600,1:29:01.040
okay um but i'm not going to go into

1:29:00.480,1:29:03.679
this

1:29:01.040,1:29:05.040
you can take a course on video nets or

1:29:03.679,1:29:06.880
or graphical models

1:29:05.040,1:29:08.320
or probabilistic methods you take a

1:29:06.880,1:29:11.440
course you know with rajesh she will

1:29:08.320,1:29:14.800
explain that to you

1:29:11.440,1:29:15.280
this would take us too far but that that

1:29:14.800,1:29:19.520
would be

1:29:15.280,1:29:22.480
kind of the thing okay so now

1:29:19.520,1:29:23.760
this is just a feed forward neural net

1:29:22.480,1:29:25.520
where

1:29:23.760,1:29:27.440
basically where the function at each

1:29:25.520,1:29:29.600
node

1:29:27.440,1:29:31.199
is a log of sum of exponentials plus

1:29:29.600,1:29:34.320
addition

1:29:31.199,1:29:35.760
of some some term right this is a neural

1:29:34.320,1:29:39.040
net where

1:29:35.760,1:29:42.080
alpha i is the activation and of

1:29:39.040,1:29:46.400
of the the neurons if you want the nodes

1:29:42.080,1:29:50.639
and the weights are those uh

1:29:46.400,1:29:52.639
e of ki that link unit k to unit i

1:29:50.639,1:29:54.639
okay and the operations you do is log

1:29:52.639,1:29:56.719
some exponentials so instead of a neural

1:29:54.639,1:29:58.880
net in which you do

1:29:56.719,1:30:00.320
a product by a weight and then you sum

1:29:58.880,1:30:02.639
the products

1:30:00.320,1:30:04.480
here you add the weights and then you do

1:30:02.639,1:30:07.199
a log sum exponential

1:30:04.480,1:30:08.080
algebraically it's actually equivalent

1:30:07.199,1:30:11.679
this is like

1:30:08.080,1:30:11.679
weighted sum in the log domain

1:30:12.320,1:30:17.120
okay but the point is you can do this

1:30:15.440,1:30:18.800
forward prop this forward algorithm and

1:30:17.120,1:30:20.719
you can back propagate gradient so

1:30:18.800,1:30:23.440
whatever f you get at the end you know

1:30:20.719,1:30:27.520
by the time you run through this network

1:30:23.440,1:30:30.960
at the end here you basically get f

1:30:27.520,1:30:31.360
of x y the value at that node the alpha

1:30:30.960,1:30:34.560
here

1:30:31.360,1:30:37.920
is f of x y and

1:30:34.560,1:30:41.440
you've eliminated z by doing this log of

1:30:37.920,1:30:41.440
sum of exponentials over all paths

1:30:43.040,1:30:46.560
right now if you want to compute the the

1:30:45.600,1:30:48.880
gradient

1:30:46.560,1:30:49.760
of f of x y with respect to each of the

1:30:48.880,1:30:51.280
e k i

1:30:49.760,1:30:52.960
which themselves probably are outputs of

1:30:51.280,1:30:54.880
some neural net you can do that you can

1:30:52.960,1:30:57.040
back propagate to this network

1:30:54.880,1:30:58.639
okay it's a it's a neural net who's

1:30:57.040,1:31:00.560
again whose structure

1:30:58.639,1:31:03.120
is dynamic it you know it changes from

1:31:00.560,1:31:04.239
example to example

1:31:03.120,1:31:06.880
but you can you can clearly

1:31:04.239,1:31:08.719
backpropagate gradient to it

1:31:06.880,1:31:11.040
and that's basically what what we do

1:31:08.719,1:31:11.040
here

1:31:12.000,1:31:16.000
in this system we we run the forward

1:31:15.280,1:31:18.480
algorithm on this

1:31:16.000,1:31:20.560
graph and we get a score which is the

1:31:18.480,1:31:23.280
log of the sum of the exponentials of

1:31:20.560,1:31:23.920
minus the energies for all the paths

1:31:23.280,1:31:26.159
okay

1:31:23.920,1:31:27.760
we do the same here we get another score

1:31:26.159,1:31:29.360
i mean it's minus log of the sum of the

1:31:27.760,1:31:32.960
exponential of the energy of minus

1:31:29.360,1:31:34.639
energy okay

1:31:32.960,1:31:36.239
this guy necessarily is larger than this

1:31:34.639,1:31:38.000
one you compute the difference

1:31:36.239,1:31:39.840
and that's the negative likelihood loss

1:31:38.000,1:31:42.639
it's a difference between

1:31:39.840,1:31:44.480
the log sum x of energy over the

1:31:42.639,1:31:46.320
relative variable of the correct answer

1:31:44.480,1:31:49.040
and looks a mass over a latent variable

1:31:46.320,1:31:50.719
of every answer

1:31:49.040,1:31:52.880
although here these are grammatical

1:31:50.719,1:31:54.000
answers but it's the same

1:31:52.880,1:31:56.000
and then you just back propagate

1:31:54.000,1:31:56.800
gradients through this entire thing

1:31:56.000,1:31:59.679
and then it goes back properly

1:31:56.800,1:32:01.199
propagating gradient through this uh

1:31:59.679,1:32:02.960
graph here which you can think of as

1:32:01.199,1:32:04.639
some sort of weird neural net with

1:32:02.960,1:32:06.320
where the node operation is like some

1:32:04.639,1:32:07.679
exponentials

1:32:06.320,1:32:09.360
and you get gradients for each of the

1:32:07.679,1:32:11.280
e's and each of the e's

1:32:09.360,1:32:13.280
are the values that you get here which

1:32:11.280,1:32:14.159
are produced by the neural net and so

1:32:13.280,1:32:15.520
you get you get

1:32:14.159,1:32:18.159
gradients with respect to the parameters

1:32:15.520,1:32:18.159
when you're on it

1:32:22.400,1:32:26.239
okay so that's structural prediction for

1:32:24.840,1:32:27.600
you

1:32:26.239,1:32:29.920
there's a couple more topics i wanted to

1:32:27.600,1:32:32.000
talk about today and variational

1:32:29.920,1:32:34.159
methods in bayesian inference because we

1:32:32.000,1:32:35.760
talked about it in the context of ea

1:32:34.159,1:32:39.760
but without really explaining what it

1:32:35.760,1:32:42.719
was or at least i didn't maybe you did

1:32:39.760,1:32:44.080
alfredo but like the general form of

1:32:42.719,1:32:45.920
variational inference or i can talk

1:32:44.080,1:32:48.159
about the lagrangian formulation of

1:32:45.920,1:32:49.840
backprop i can actually do both because

1:32:48.159,1:32:50.960
it's kind of fast it will take more than

1:32:49.840,1:32:52.080
five minutes but you can leave whenever

1:32:50.960,1:32:54.320
you want

1:32:52.080,1:32:56.000
okay then let's go for both uh the long

1:32:54.320,1:32:58.719
orange thing is short so i'm going to do

1:32:56.000,1:32:58.719
that first okay

1:33:00.560,1:33:05.840
okay so you can formulate back prop as a

1:33:03.600,1:33:08.719
minimization under constraint

1:33:05.840,1:33:09.920
so you have an input variable x is going

1:33:08.719,1:33:13.440
through

1:33:09.920,1:33:14.800
a first functional module let's call it

1:33:13.440,1:33:17.920
f1

1:33:14.800,1:33:22.639
of x

1:33:17.920,1:33:27.520
w1 and it produces

1:33:22.639,1:33:31.440
uh we're gonna call it z1

1:33:27.520,1:33:33.520
actually let me call this f0 f0

1:33:31.440,1:33:35.600
okay and then the second one is going to

1:33:33.520,1:33:39.199
be f1

1:33:35.600,1:33:42.639
of z1 w1

1:33:39.199,1:33:46.320
and that produces z2

1:33:42.639,1:33:48.639
etc so and then at the end

1:33:46.320,1:33:50.000
we have the last module and it goes into

1:33:48.639,1:33:53.840
some sort of

1:33:50.000,1:33:55.440
uh energy term

1:33:53.840,1:33:56.639
okay with let's say desired output if we

1:33:55.440,1:33:59.679
do supervise running but it doesn't

1:33:56.639,1:33:59.679
matter it's just a cost

1:34:01.040,1:34:04.080
let's call this guy z

1:34:04.560,1:34:07.360
n okay

1:34:08.639,1:34:14.880
zn and y

1:34:11.840,1:34:18.239
um okay so

1:34:14.880,1:34:21.679
the the forward pass can be written as

1:34:18.239,1:34:21.679
z k

1:34:24.560,1:34:31.840
plus 1 is equal to f

1:34:28.320,1:34:35.920
k of

1:34:31.840,1:34:38.880
ck wk

1:34:35.920,1:34:40.880
okay that's just a forward pass and then

1:34:38.880,1:34:43.440
you have a cost function

1:34:40.880,1:34:44.159
c which you want to minimize which is z

1:34:43.440,1:34:47.920
of

1:34:44.159,1:34:49.600
c of zn

1:34:47.920,1:34:52.560
y okay that's just whatever cost

1:34:49.600,1:34:52.560
function you want to minimize

1:34:54.320,1:34:57.920
now you can write that you can write the

1:34:56.800,1:34:59.520
the entire

1:34:57.920,1:35:01.119
problem as a minimization of the

1:34:59.520,1:35:04.719
constraint so

1:35:01.119,1:35:06.880
and the and the statement is minimize

1:35:04.719,1:35:06.880
c

1:35:10.639,1:35:15.840
with the constraint such that

1:35:17.679,1:35:21.199
the above constraint is verified

1:35:21.679,1:35:26.560
okay and

1:35:25.199,1:35:28.159
when you have a minimization problem

1:35:26.560,1:35:31.600
under constraint the best thing to do is

1:35:28.159,1:35:33.760
to write a lagrangian right so you write

1:35:31.600,1:35:34.880
a lagrange function i'm not going to

1:35:33.760,1:35:37.119
tell you right away what is this

1:35:34.880,1:35:39.840
function of

1:35:37.119,1:35:41.360
and for a single training sample x y is

1:35:39.840,1:35:45.840
going to be the

1:35:41.360,1:35:48.080
the cost zn y

1:35:45.840,1:35:48.080
okay

1:35:48.960,1:35:52.239
well the other thing we might we might

1:35:50.320,1:35:53.440
say also is there is another constraint

1:35:52.239,1:35:57.360
which is that

1:35:53.440,1:36:00.480
z zero equals x

1:35:57.360,1:36:04.400
plus sum over layers

1:36:00.480,1:36:04.400
okay so we're going to have an index k

1:36:04.480,1:36:07.679
from 1 to n

1:36:07.920,1:36:14.639
and a lagrange

1:36:11.520,1:36:14.960
multiplier and a constraint which should

1:36:14.639,1:36:16.880
be

1:36:14.960,1:36:18.239
equal to 0 and that constraint is going

1:36:16.880,1:36:22.840
to be

1:36:18.239,1:36:26.560
zk plus 1 minus fk

1:36:22.840,1:36:31.440
of uh z k

1:36:26.560,1:36:33.679
w k and i need to

1:36:31.440,1:36:36.159
i'm gonna call this lambda k plus one

1:36:33.679,1:36:39.040
and this is gonna have to be

1:36:36.159,1:36:39.760
up to n minus one and probably starting

1:36:39.040,1:36:42.159
at zero

1:36:39.760,1:36:42.159
actually

1:36:43.679,1:36:47.600
okay so this is a lagrangian formulation

1:36:46.480,1:36:50.719
of

1:36:47.600,1:36:52.320
uh my back part problem where basically

1:36:50.719,1:36:54.239
you have an overall cost function

1:36:52.320,1:36:56.080
and i have a bunch of constraints so the

1:36:54.239,1:36:58.719
constraints are that

1:36:56.080,1:37:01.760
the input to layer k is the output of

1:36:58.719,1:37:01.760
layer k minus one

1:37:01.920,1:37:06.800
okay so this regression is a function of

1:37:05.920,1:37:10.960
x

1:37:06.800,1:37:15.760
y all the lambdas the lambda k's

1:37:10.960,1:37:20.639
all the z's and w

1:37:15.760,1:37:22.639
all the w's okay

1:37:20.639,1:37:24.320
so what i need to do now to do this uh

1:37:22.639,1:37:25.520
minimization under constraint is i need

1:37:24.320,1:37:29.199
to

1:37:25.520,1:37:33.920
do dl over d lambda k

1:37:29.199,1:37:34.719
equals zero okay and if i this condition

1:37:33.920,1:37:36.719
uh

1:37:34.719,1:37:38.000
the the gradient of l with respect to

1:37:36.719,1:37:41.040
lambda k

1:37:38.000,1:37:45.520
is just the it's just this

1:37:41.040,1:37:47.520
right i mean the k plus one i'm sorry

1:37:45.520,1:37:48.880
it's just this parenthesis okay so i

1:37:47.520,1:37:52.400
just get

1:37:48.880,1:37:56.320
zk plus 1 equal

1:37:52.400,1:37:59.280
fk of zk wk which is just the forward

1:37:56.320,1:37:59.280
propagation formula

1:38:00.400,1:38:08.400
if i do dl over d

1:38:04.239,1:38:08.400
z k equals zero

1:38:08.639,1:38:12.480
uh it's a little more complicated right

1:38:10.400,1:38:16.080
so i get the first term

1:38:12.480,1:38:17.920
which is lambda k because

1:38:16.080,1:38:20.080
i'm going to have a z k here and that z

1:38:17.920,1:38:23.119
k is going to be a factor of

1:38:20.080,1:38:26.080
this lambda lambda k here right so i

1:38:23.119,1:38:28.639
get uh i get i i guess i get longer k

1:38:26.080,1:38:28.639
transpose

1:38:29.440,1:38:32.639
and then i get minus and then for this

1:38:32.080,1:38:36.159
zk

1:38:32.639,1:38:39.199
i have a lambda k plus one here

1:38:36.159,1:38:41.679
times the jacobian function of this

1:38:39.199,1:38:42.560
with respect to z okay so it's going to

1:38:41.679,1:38:47.440
be

1:38:42.560,1:38:51.679
something like dfk of zkw

1:38:47.440,1:38:51.679
over zk

1:38:53.679,1:39:01.679
that's it times

1:38:58.639,1:39:01.679
ooh okay

1:39:04.719,1:39:09.520
times lambda k plus one transpose

1:39:10.239,1:39:13.360
and that should be equal to zero so i'm

1:39:12.639,1:39:15.040
going to

1:39:13.360,1:39:17.840
rearrange all that stuff and what i get

1:39:15.040,1:39:21.040
is uh

1:39:17.840,1:39:23.760
lambda k equals

1:39:21.040,1:39:23.760
dfk

1:39:24.840,1:39:32.400
zkw with respect to zk so that the

1:39:28.480,1:39:35.920
jacobian matrix uh of f transposed

1:39:32.400,1:39:38.000
times lambda k plus 1.

1:39:35.920,1:39:40.719
and funnily enough this is actually the

1:39:38.000,1:39:42.400
backpropagation formula

1:39:40.719,1:39:44.080
right this is the thing that gives you

1:39:42.400,1:39:46.880
the gradients at la at

1:39:44.080,1:39:50.080
level k given the gradients of level k

1:39:46.880,1:39:53.600
plus one you multiply by the jacobian of

1:39:50.080,1:39:55.040
of the of the box that you propagate it

1:39:53.600,1:39:58.239
through

1:39:55.040,1:39:59.840
okay so you don't have to think about it

1:39:58.239,1:40:02.239
you know you just kind of

1:39:59.840,1:40:03.840
write back prop as a constraint

1:40:02.239,1:40:05.920
optimization problem and backprop

1:40:03.840,1:40:07.360
naturally comes out of it

1:40:05.920,1:40:09.199
now the first people to figure this out

1:40:07.360,1:40:10.560
were people in control theory in fact

1:40:09.199,1:40:14.239
first people to figure this out

1:40:10.560,1:40:16.960
where people like lagrange or or

1:40:14.239,1:40:18.159
or euler or people like hamilton and

1:40:16.960,1:40:20.800
jacoby

1:40:18.159,1:40:23.360
that's the classical formulation of of

1:40:20.800,1:40:26.639
of mechanics if you want

1:40:23.360,1:40:29.360
um and in mechanics when you write uh

1:40:26.639,1:40:30.239
something like this you say uh where c

1:40:29.360,1:40:33.280
of zn

1:40:30.239,1:40:34.400
uh y would be uh uh the energy of the

1:40:33.280,1:40:35.679
system

1:40:34.400,1:40:37.600
like a potential energy is something

1:40:35.679,1:40:39.199
like this and then the other term was

1:40:37.600,1:40:41.040
basically implements the dynamic

1:40:39.199,1:40:43.280
constraints the fact that

1:40:41.040,1:40:44.719
you have a differential equation that

1:40:43.280,1:40:45.199
tells you that the state at time t plus

1:40:44.719,1:40:46.639
one

1:40:45.199,1:40:48.800
is a function of the state at time t

1:40:46.639,1:40:51.360
with some constraint right

1:40:48.800,1:40:52.880
so that's the dynamic constraint and

1:40:51.360,1:40:56.480
then if you do this

1:40:52.880,1:41:00.000
uh you you know you figure out that

1:40:56.480,1:41:02.080
uh the

1:41:00.000,1:41:05.760
if you have an energy for every you know

1:41:02.080,1:41:08.880
think of k now as a time step

1:41:05.760,1:41:11.840
and the forward propagation as

1:41:08.880,1:41:13.360
a differential equation that governs a

1:41:11.840,1:41:16.000
system

1:41:13.360,1:41:17.040
and then you could have a term here that

1:41:16.000,1:41:19.440
is not just

1:41:17.040,1:41:21.119
an energy term at the output but

1:41:19.440,1:41:23.360
basically an energy term that

1:41:21.119,1:41:23.360
uh

1:41:25.440,1:41:29.280
you can have one of those terms for

1:41:26.639,1:41:32.480
every time step right so the

1:41:29.280,1:41:33.440
lagonchan function would be summer time

1:41:32.480,1:41:36.880
steps

1:41:33.440,1:41:39.520
of c for that time step of

1:41:36.880,1:41:41.440
z k okay and there might be some

1:41:39.520,1:41:45.040
external variable

1:41:41.440,1:41:47.840
uh let's call it y k you know plus

1:41:45.040,1:41:47.840
those constraints

1:41:51.040,1:41:57.920
k plus one minus f k of

1:41:54.560,1:41:57.920
z k w k

1:41:59.280,1:42:02.880
okay and the the sun takes place over

1:42:02.000,1:42:06.400
all things

1:42:02.880,1:42:07.760
um when you look at uh you know lagoon

1:42:06.400,1:42:08.880
formulation of classical mechanics

1:42:07.760,1:42:10.880
that's basically the way they're

1:42:08.880,1:42:14.000
expressed

1:42:10.880,1:42:15.360
c is the energy and the second term are

1:42:14.000,1:42:16.639
the constraints now in classical

1:42:15.360,1:42:20.800
mechanics

1:42:16.639,1:42:22.880
uh the the lambda variable is actually

1:42:20.800,1:42:24.719
the momentum

1:42:22.880,1:42:26.320
so z is the position variables and

1:42:24.719,1:42:28.719
lambda becomes the momentum so the

1:42:26.320,1:42:33.520
second term becomes basically the

1:42:28.719,1:42:33.520
the kinetic energy

1:42:33.679,1:42:37.760
or the negative kinetic energy more more

1:42:35.920,1:42:41.119
specifically

1:42:37.760,1:42:43.520
um anyway this is just an apartheid okay

1:42:41.119,1:42:44.320
why am i telling you this it's because

1:42:43.520,1:42:46.560
conceptually

1:42:44.320,1:42:48.080
the you know the mathematics of this is

1:42:46.560,1:42:51.119
super simple if you know

1:42:48.080,1:42:53.600
lagrangian minimization of the

1:42:51.119,1:42:53.600
constraint

1:42:54.880,1:42:58.000
and this is something you can use also

1:42:57.520,1:43:01.280
in

1:42:58.000,1:43:04.480
a new class of model called neural ode

1:43:01.280,1:43:04.480
so neural

1:43:06.400,1:43:11.920
ordinary differential equation and this

1:43:10.480,1:43:14.159
is something else wanted me to talk

1:43:11.920,1:43:14.159
about

1:43:16.080,1:43:18.480
thank you

1:43:19.760,1:43:24.080
so neural od so this is a type of neural

1:43:22.960,1:43:26.080
net which is basically a recurrent

1:43:24.080,1:43:30.800
neural net where you say

1:43:26.080,1:43:34.400
my state at time t plus delta t

1:43:30.800,1:43:39.440
is equal to my state at time t plus

1:43:34.400,1:43:43.119
delta t times you know some function

1:43:39.440,1:43:46.159
uh which is a constant function of

1:43:43.119,1:43:49.040
uh zt

1:43:46.159,1:43:50.880
and a bunch of parameters which are

1:43:49.040,1:43:54.159
fixed okay they're not they don't value

1:43:50.880,1:43:55.280
with time i can write it this way i can

1:43:54.159,1:43:57.199
write it in a

1:43:55.280,1:43:58.719
differential equation form where i can

1:43:57.199,1:44:01.840
say dz

1:43:58.719,1:44:06.320
over dt at time t

1:44:01.840,1:44:08.719
is equal to f of zt

1:44:06.320,1:44:12.400
w okay so that's the differential

1:44:08.719,1:44:14.239
equation ordinary differential equation

1:44:12.400,1:44:15.440
uh in this case the first order well it

1:44:14.239,1:44:18.639
depends what's in z

1:44:15.440,1:44:18.639
but um if i

1:44:18.800,1:44:22.800
you know i could i could i can express

1:44:20.639,1:44:24.080
just about anything this way

1:44:22.800,1:44:26.560
uh and the question is how do you train

1:44:24.080,1:44:28.080
something like this and basically

1:44:26.560,1:44:31.199
uh if you write the lagrangian

1:44:28.080,1:44:34.239
formulation of this it's trivial

1:44:31.199,1:44:35.280
the uh so there are two ways

1:44:34.239,1:44:37.440
you might want to train something like

1:44:35.280,1:44:40.480
this you might want to train the system

1:44:37.440,1:44:43.520
to map one point

1:44:40.480,1:44:47.199
you know z at time zero

1:44:43.520,1:44:50.000
to a particular point zero times big t

1:44:47.199,1:44:51.360
after some trajectory you may not want

1:44:50.000,1:44:53.520
to constrain the trajectory you just

1:44:51.360,1:44:55.360
just want it to reach that point and you

1:44:53.520,1:44:57.360
don't care what it does afterwards

1:44:55.360,1:44:58.960
i just wanted to reach that point and so

1:44:57.360,1:44:59.840
you can have a cost function which is

1:44:58.960,1:45:02.400
basically

1:44:59.840,1:45:03.440
you know the distance of z to that

1:45:02.400,1:45:06.320
target point

1:45:03.440,1:45:06.800
uh zero big t you know i'm gonna call it

1:45:06.320,1:45:09.840
y

1:45:06.800,1:45:15.280
and then so the target would be

1:45:09.840,1:45:16.719
uh the target would be a point y

1:45:15.280,1:45:18.480
and then your cost function would be the

1:45:16.719,1:45:21.840
distance between

1:45:18.480,1:45:23.360
z t and y or something like that okay

1:45:21.840,1:45:24.800
another thing you might want to do is

1:45:23.360,1:45:25.679
you might want to train the system so

1:45:24.800,1:45:29.040
that

1:45:25.679,1:45:30.000
it has stable states at particular

1:45:29.040,1:45:33.280
points y

1:45:30.000,1:45:36.400
okay so that uh for a particular point y

1:45:33.280,1:45:40.400
that you decide from your training set

1:45:36.400,1:45:44.320
uh f of this particular y

1:45:40.400,1:45:47.040
w equals zero

1:45:44.320,1:45:47.679
which means you know that state is going

1:45:47.040,1:45:49.679
to be stable

1:45:47.679,1:45:50.719
right the the trajectory so you would

1:45:49.679,1:45:53.760
have a point

1:45:50.719,1:45:56.480
y in your space and then

1:45:53.760,1:45:57.679
you know you might start from some point

1:45:56.480,1:46:01.920
when you arrive at that point

1:45:57.679,1:46:01.920
the the dynamics stop stops

1:46:02.480,1:46:08.719
so if you formulate this in terms of uh

1:46:06.639,1:46:11.840
lagoon it becomes like super simple in

1:46:08.719,1:46:14.880
the sense that the gradients now

1:46:11.840,1:46:17.760
contrary to backpop through time so

1:46:14.880,1:46:20.480
if you were to unfold this network here

1:46:17.760,1:46:23.360
consider this a recurrent net

1:46:20.480,1:46:24.719
and you enfold it in time to compute the

1:46:23.360,1:46:27.760
gradient

1:46:24.719,1:46:28.719
of the end point with respect to the

1:46:27.760,1:46:30.400
parameters

1:46:28.719,1:46:32.159
you you cannot have to and with respect

1:46:30.400,1:46:33.440
to the initial for you have to back

1:46:32.159,1:46:34.000
propagate through time right you have to

1:46:33.440,1:46:36.000
kind of

1:46:34.000,1:46:38.639
remember the entire sequence and then do

1:46:36.000,1:46:41.440
backdrop two time

1:46:38.639,1:46:44.159
okay but if what you're interested in is

1:46:41.440,1:46:45.360
just learning a stable state like this

1:46:44.159,1:46:47.440
then you don't need to store the

1:46:45.360,1:46:50.800
trajectory you

1:46:47.440,1:46:52.239
uh you start from some point you convert

1:46:50.800,1:46:56.080
to

1:46:52.239,1:46:58.880
some other point and you want to make y

1:46:56.080,1:46:58.880
a stable state

1:47:01.040,1:47:08.080
what you just need to do is ensure that

1:47:04.000,1:47:11.280
this is true uh

1:47:08.080,1:47:13.119
and the way you can do this is uh

1:47:11.280,1:47:14.320
you know basically by minimizing your

1:47:13.119,1:47:15.920
cost which would be some

1:47:14.320,1:47:18.320
something like the norm the square norm

1:47:15.920,1:47:20.320
of f of y w

1:47:18.320,1:47:21.440
but the point of the the point is that

1:47:20.320,1:47:22.719
you don't need to remember the entire

1:47:21.440,1:47:25.440
trajectory

1:47:22.719,1:47:26.400
the gradient with respect to the weights

1:47:25.440,1:47:28.239
can be obtained by

1:47:26.400,1:47:32.080
running a very similar type of

1:47:28.239,1:47:32.080
differential equation backwards in time

1:47:34.480,1:47:37.840
and i'm sorry i'm not going to be able

1:47:35.760,1:47:39.360
to go into details of that i can refer

1:47:37.840,1:47:41.440
you to a paper

1:47:39.360,1:47:43.199
um so this is a neural ide paper which

1:47:41.440,1:47:48.560
doesn't really mention that but there is

1:47:43.199,1:47:48.560
an earlier paper of mine called a

1:47:48.840,1:47:51.840
theoretical

1:47:52.400,1:47:54.880
framework

1:47:57.360,1:48:02.159
for back propagation and basically it

1:47:59.440,1:48:02.159
explains this

1:48:02.960,1:48:06.719
uh lagrangian formulation as well as how

1:48:05.920,1:48:09.440
you apply it

1:48:06.719,1:48:09.760
for recurrent nets uh that might be sort

1:48:09.440,1:48:11.920
of

1:48:09.760,1:48:13.920
you know continuous and continuous in

1:48:11.920,1:48:16.480
time and that you want to train to

1:48:13.920,1:48:18.000
go to particular fixed points uh this is

1:48:16.480,1:48:21.280
paper from 1988.

1:48:18.000,1:48:22.239
it's not recent uh you'll find it on my

1:48:21.280,1:48:24.080
web page

1:48:22.239,1:48:26.159
down on the bottom of the publication

1:48:24.080,1:48:28.880
page but i don't want to go into the

1:48:26.159,1:48:28.880
details of this

1:48:31.679,1:48:34.719
and there is the bayesian stuff asian

1:48:34.400,1:48:37.760
stuff

1:48:34.719,1:48:39.440
yes people are still here i don't know

1:48:37.760,1:48:41.840
they are enjoying it stick around you

1:48:39.440,1:48:43.520
don't have to if you don't want to

1:48:41.840,1:48:45.199
uh it's not the basic stuff it's more

1:48:43.520,1:48:47.520
the virtual

1:48:45.199,1:48:49.040
oh sorry yeah you're right i'm not

1:48:47.520,1:48:51.280
confused

1:48:49.040,1:48:51.280
uh

1:48:52.400,1:48:56.560
so let's say i have some uh loss

1:48:55.280,1:48:57.920
function

1:48:56.560,1:48:59.360
okay and i'm going to talk about a loss

1:48:57.920,1:49:00.560
not not an energy but it's the same

1:48:59.360,1:49:03.679
thing

1:49:00.560,1:49:05.760
and my last function uh

1:49:03.679,1:49:07.040
is a marginalized loss function over a

1:49:05.760,1:49:10.239
latent variable right

1:49:07.040,1:49:10.639
so remember you know i i talked about

1:49:10.239,1:49:14.480
this

1:49:10.639,1:49:14.480
before if you have an energy function

1:49:16.560,1:49:22.400
f of x y let's say and

1:49:20.159,1:49:26.880
you want to derive it from a more

1:49:22.400,1:49:26.880
elementary energy function e of x y z

1:49:27.119,1:49:30.159
by doing the equivalent operation of

1:49:28.560,1:49:32.480
marginalizing over

1:49:30.159,1:49:34.080
z's yeah so the way you uh you

1:49:32.480,1:49:37.679
marginalize right is you

1:49:34.080,1:49:42.880
uh you do

1:49:37.679,1:49:46.400
minus beta e to the minus you sum over

1:49:42.880,1:49:47.119
all z's and you take minus one over beta

1:49:46.400,1:49:49.280
log

1:49:47.119,1:49:51.679
okay so this is uh the formula for

1:49:49.280,1:49:56.159
marginalizing a relative variable

1:49:51.679,1:49:57.280
um and that also applies to

1:49:56.159,1:49:58.719
loss functions you know whatever

1:49:57.280,1:49:59.679
function you want to marginalize over a

1:49:58.719,1:50:02.880
latent variable

1:49:59.679,1:50:03.920
that's what you compute so let's say you

1:50:02.880,1:50:07.119
have

1:50:03.920,1:50:08.159
a model with a latent variable and

1:50:07.119,1:50:10.000
you don't know what the value of the

1:50:08.159,1:50:12.159
latent variable is and you want to

1:50:10.000,1:50:13.760
compute what is my loss

1:50:12.159,1:50:15.280
uh which would be the log of the sum of

1:50:13.760,1:50:16.800
the exponentials of the loss over all

1:50:15.280,1:50:19.119
values of the latent variables so

1:50:16.800,1:50:21.199
right so i'm kind of marginalizing over

1:50:19.119,1:50:22.560
this certain variable let's say it's a

1:50:21.199,1:50:24.000
you know rational autoencoder or

1:50:22.560,1:50:24.639
something i have a latent variable in

1:50:24.000,1:50:27.679
the middle

1:50:24.639,1:50:27.679
and i want to compute the

1:50:28.000,1:50:32.639
minus 1 over beta log sum over all

1:50:30.960,1:50:34.719
values of my little variable of e to the

1:50:32.639,1:50:36.800
minus beta

1:50:34.719,1:50:38.000
l i'm using l but i could use just any

1:50:36.800,1:50:41.199
symbol here this is whatever

1:50:38.000,1:50:41.199
function you need to compute

1:50:42.800,1:50:46.480
but it's useful for things you want to

1:50:44.239,1:50:49.679
minimize like energies or

1:50:46.480,1:50:52.880
or or objectives okay so

1:50:49.679,1:50:55.679
here this loss function here is

1:50:52.880,1:50:58.480
no longer a function of z it's only a

1:50:55.679,1:50:58.480
function of x and y

1:51:00.080,1:51:10.719
i can rewrite this

1:51:07.280,1:51:10.719
as the following

1:51:11.360,1:51:21.280
q of z e to the minus beta

1:51:16.239,1:51:23.360
l of x y z

1:51:21.280,1:51:25.679
or eq right i've just multiplied and

1:51:23.360,1:51:28.800
divided by q of z

1:51:25.679,1:51:32.800
okay i've done nothing now

1:51:28.800,1:51:34.800
q of z here uh i assume is a

1:51:32.800,1:51:35.840
a probability distribution over z so

1:51:34.800,1:51:37.679
it's a

1:51:35.840,1:51:39.599
density function that integrates to 1

1:51:37.679,1:51:42.080
when i integrate over z

1:51:39.599,1:51:43.679
so you can interpret this integral as

1:51:42.080,1:51:46.960
the expected value

1:51:43.679,1:51:49.679
with respect to the distribution of

1:51:46.960,1:51:51.440
e to the minus beta l of x y z divided

1:51:49.679,1:51:54.480
by q of z

1:51:51.440,1:51:55.599
okay now

1:51:54.480,1:52:03.840
here's the trick there's something

1:51:55.599,1:52:03.840
called jensen's inequality

1:52:05.520,1:52:10.639
and jensen's inequality says something

1:52:07.360,1:52:17.199
very interesting it says

1:52:10.639,1:52:20.159
let's imagine i have a convex function

1:52:17.199,1:52:21.599
like say minus log okay i'm not drawing

1:52:20.159,1:52:26.239
minus log here very well but

1:52:21.599,1:52:31.280
it looks a bit like minus log now

1:52:26.239,1:52:36.480
if i uh if i take um

1:52:31.280,1:52:36.480
a bunch of values

1:52:36.639,1:52:41.440
over a range okay

1:52:41.520,1:52:45.360
and i compute the average of the value

1:52:44.639,1:52:49.040
of the function

1:52:45.360,1:52:49.040
minus log over that range

1:52:51.520,1:52:58.400
okay because the function is convex i'm

1:52:55.840,1:53:03.520
going to get a value that is smaller

1:52:58.400,1:53:05.199
than the function applied to the average

1:53:03.520,1:53:06.639
okay so my diagram is not that great

1:53:05.199,1:53:08.080
because the

1:53:06.639,1:53:10.719
the curvature is not high enough let me

1:53:08.080,1:53:10.719
draw it again

1:53:11.119,1:53:17.040
so here's a convex function i'm going to

1:53:14.080,1:53:19.679
vary a variable here over a range

1:53:17.040,1:53:20.239
okay and compute the average of that

1:53:19.679,1:53:22.480
function

1:53:20.239,1:53:23.280
over that range so it's going to you

1:53:22.480,1:53:26.480
know give me

1:53:23.280,1:53:29.280
some some value

1:53:26.480,1:53:31.599
you know probably around here and then

1:53:29.280,1:53:33.599
i'm going to take the

1:53:31.599,1:53:34.719
average of all those values in this

1:53:33.599,1:53:36.000
range the average of the range the

1:53:34.719,1:53:39.440
midpoint of that range

1:53:36.000,1:53:41.840
and pass it to the to the function

1:53:39.440,1:53:41.840
okay

1:53:49.040,1:53:52.480
and i get i get something below this so

1:53:51.199,1:53:54.159
i didn't draw this properly

1:53:52.480,1:53:56.320
so if i take the average of this plus

1:53:54.159,1:53:56.800
this you know this this this this and

1:53:56.320,1:53:58.080
this

1:53:56.800,1:54:00.239
i'm going to get something that's higher

1:53:58.080,1:54:01.840
than that because

1:54:00.239,1:54:03.760
because the function is not convex if

1:54:01.840,1:54:04.560
the is convex if the function was

1:54:03.760,1:54:07.119
straight

1:54:04.560,1:54:08.000
then the average after going to the

1:54:07.119,1:54:10.960
function would be the same

1:54:08.000,1:54:12.320
as before going to the function right if

1:54:10.960,1:54:14.080
i computed the average of all those

1:54:12.320,1:54:16.719
values

1:54:14.080,1:54:17.679
or the y values of those points it would

1:54:16.719,1:54:22.080
be at the same place

1:54:17.679,1:54:24.159
as the function applied to the average

1:54:22.080,1:54:26.320
okay so you can make the intercept

1:54:24.159,1:54:27.840
between the the convex function in a

1:54:26.320,1:54:28.880
line right that goes from those two

1:54:27.840,1:54:30.800
extrema

1:54:28.880,1:54:32.639
that's right yeah there you go so that

1:54:30.800,1:54:35.760
that the mean would be the

1:54:32.639,1:54:40.080
yeah that point yeah that's right

1:54:35.760,1:54:43.840
so the the the

1:54:40.080,1:54:45.360
the mean applied to the function values

1:54:43.840,1:54:47.599
would be something like this chord it

1:54:45.360,1:54:50.880
wouldn't be that but it would be kind of

1:54:47.599,1:54:52.400
close to that okay now it let's forget

1:54:50.880,1:54:53.840
about a function like

1:54:52.400,1:54:54.960
actually i should have explained this

1:54:53.840,1:54:56.159
inverse simple way which is just two

1:54:54.960,1:54:58.080
values

1:54:56.159,1:54:59.440
um let's say it's just the sum of two

1:54:58.080,1:55:01.199
terms

1:54:59.440,1:55:05.360
okay so i have a convex function i have

1:55:01.199,1:55:08.639
two values

1:55:05.360,1:55:09.840
uh the average of those two values after

1:55:08.639,1:55:13.119
i passed through the function

1:55:09.840,1:55:16.400
okay so basically let's say this is

1:55:13.119,1:55:18.880
my function is minus log

1:55:16.400,1:55:22.159
so the average of minus log of let's

1:55:18.880,1:55:22.159
call it x1 and x2

1:55:22.239,1:55:28.560
minus log of x1 plus minus log of x2

1:55:28.880,1:55:34.480
divided by 2 okay is

1:55:32.000,1:55:34.480
this point

1:55:35.360,1:55:40.480
okay and then

1:55:44.080,1:55:51.199
minus log of x1 plus

1:55:47.360,1:55:54.639
x2 divided by 2

1:55:51.199,1:55:56.400
is that point and that's below

1:55:54.639,1:55:58.320
okay and jensen's inequality basically

1:55:56.400,1:56:02.560
says if you have a convex function

1:55:58.320,1:56:04.800
like like minus log um

1:56:02.560,1:56:04.800
the

1:56:06.960,1:56:10.239
okay and then here i computed on average

1:56:08.880,1:56:11.360
but you know it's true for any

1:56:10.239,1:56:14.239
expectation

1:56:11.360,1:56:14.239
it says the

1:56:15.360,1:56:18.159
expectation

1:56:19.520,1:56:22.239
so basically

1:56:23.119,1:56:32.560
convex of expectation

1:56:28.239,1:56:32.560
over any distribution of some function h

1:56:34.560,1:56:38.239
well that would be z in that case

1:56:40.119,1:56:43.840
okay uh

1:56:43.920,1:56:48.320
i have to write this in the proper way

1:56:45.440,1:56:52.560
is less than or equal

1:56:48.320,1:56:57.840
to sum over z

1:56:52.560,1:56:57.840
q z of this convex function

1:56:58.960,1:57:02.000
applied to h and z

1:57:04.320,1:57:07.920
okay that's john's sense inequality

1:57:08.000,1:57:12.480
so this this works with uh minus log

1:57:10.400,1:57:16.159
which means i can write that

1:57:12.480,1:57:17.840
my objective function here is less than

1:57:16.159,1:57:20.000
uh minus one over beta which i'm going

1:57:17.840,1:57:20.000
to

1:57:20.400,1:57:25.840
actually put inside uh i take this back

1:57:28.400,1:57:35.760
it's going to be less than sum over z

1:57:32.239,1:57:35.760
of q of z

1:57:35.920,1:57:45.360
times minus 1 over beta log

1:57:40.719,1:57:45.360
e to the minus beta l of x y z

1:57:47.679,1:57:54.990
divided by q z

1:57:52.000,1:57:57.440
okay obviously the

1:57:54.990,1:57:59.199
[Music]

1:57:57.440,1:58:01.760
the one over beta the minus one over

1:57:59.199,1:58:04.880
beta log exponential minus beta cancel

1:58:01.760,1:58:09.199
okay so what i get is

1:58:04.880,1:58:09.199
sum summary z q of z

1:58:09.840,1:58:16.880
l of x y z

1:58:13.360,1:58:19.840
so that's just the expected value of l

1:58:16.880,1:58:22.960
averaged over the distribution q z and

1:58:19.840,1:58:22.960
then i get a second term

1:58:25.920,1:58:33.199
and the second term is the negative

1:58:29.840,1:58:35.119
log 1 over beta the negative log of

1:58:33.199,1:58:37.040
of q of z but there is you know q of z

1:58:35.119,1:58:38.000
is a denominator so i'm going to bring

1:58:37.040,1:58:39.520
it to the top

1:58:38.000,1:58:41.760
that's going to cancel the minus 1 over

1:58:39.520,1:58:42.560
beta and so i'm going to get something

1:58:41.760,1:58:45.920
like

1:58:42.560,1:58:50.560
plus 1 over beta

1:58:45.920,1:58:50.560
log q of z right

1:58:51.599,1:58:59.360
and i can write this again as sum over z

1:58:54.960,1:59:02.880
of q of z l

1:58:59.360,1:59:08.000
x y z

1:59:02.880,1:59:08.000
plus sum over z 1 over beta

1:59:11.119,1:59:19.760
q z plug q z

1:59:17.440,1:59:19.760
okay

1:59:20.960,1:59:23.599
this is the

1:59:24.639,1:59:27.040
average

1:59:27.760,1:59:32.800
loss energy whatever it is let's call it

1:59:30.480,1:59:35.520
energy

1:59:32.800,1:59:35.520
and this is the

1:59:36.800,1:59:42.560
uh this is one over this is minus one

1:59:39.520,1:59:42.560
over beta times the

1:59:42.840,1:59:45.840
entropy

1:59:46.320,1:59:48.639
of q

1:59:50.000,1:59:53.040
okay the entropy of a distribution is

1:59:52.159,1:59:56.480
minus

1:59:53.040,1:59:59.679
sum over the random variable of

1:59:56.480,2:00:01.520
distribution log distribution okay so

1:59:59.679,2:00:03.440
this is minus one over beta

2:00:01.520,2:00:04.560
e entropy so what does that mean what

2:00:03.440,2:00:06.800
that means is that

2:00:04.560,2:00:08.239
i have an upper bound on my the last

2:00:06.800,2:00:11.760
function that i want to minimize

2:00:08.239,2:00:13.040
l of x y okay for my energy that i want

2:00:11.760,2:00:14.159
to minimize whatever it is

2:00:13.040,2:00:16.719
whatever function it is i want to

2:00:14.159,2:00:18.400
minimize i have an upper bound on it now

2:00:16.719,2:00:20.639
and this upper bound is the sum of two

2:00:18.400,2:00:24.480
terms

2:00:20.639,2:00:27.040
one is the average of the energy i get

2:00:24.480,2:00:28.480
by basically sampling the latent

2:00:27.040,2:00:30.480
variable

2:00:28.480,2:00:32.000
okay so i have a system with a latent

2:00:30.480,2:00:33.520
variable i sample some

2:00:32.000,2:00:35.040
value of the latent variable according

2:00:33.520,2:00:36.880
to some distribution q

2:00:35.040,2:00:38.719
which of course i pick a queue from

2:00:36.880,2:00:41.119
which i can easily sample

2:00:38.719,2:00:42.960
okay i can choose q whatever i want

2:00:41.119,2:00:47.920
whatever i want right

2:00:42.960,2:00:49.840
um so i pick a queue gaussian whatever

2:00:47.920,2:00:51.920
and i i pick a z according to that

2:00:49.840,2:00:54.800
distribution and i compute the

2:00:51.920,2:00:57.119
expected value of the function i want to

2:00:54.800,2:00:58.960
minimize with respect to

2:00:57.119,2:01:00.400
uh to that q and i can do this by just

2:00:58.960,2:01:02.480
sampling

2:01:00.400,2:01:04.560
z from the the q distribution and then

2:01:02.480,2:01:08.000
computing the average of

2:01:04.560,2:01:10.159
the function l that i obtain as a result

2:01:08.000,2:01:11.920
okay so that's the first term and then

2:01:10.159,2:01:14.719
the second term

2:01:11.920,2:01:15.119
is the entropy of z so what i need to do

2:01:14.719,2:01:17.840
is

2:01:15.119,2:01:19.360
basically change my distribution z in

2:01:17.840,2:01:20.880
such a way that

2:01:19.360,2:01:22.400
the entropy is maximized so if it's a

2:01:20.880,2:01:23.440
gaussian for example it means i need to

2:01:22.400,2:01:25.360
make the variance

2:01:23.440,2:01:26.639
of z as large as possible but if i make

2:01:25.360,2:01:28.800
it too large then the

2:01:26.639,2:01:31.520
average energy term is going to blow up

2:01:28.800,2:01:31.920
so i need to optimize this overall this

2:01:31.520,2:01:34.400
this

2:01:31.920,2:01:35.280
this whole function and if i optimize

2:01:34.400,2:01:38.239
this whole function

2:01:35.280,2:01:39.760
with respect to q and with respect to

2:01:38.239,2:01:41.440
whatever parameter of l i want to

2:01:39.760,2:01:43.119
minimize

2:01:41.440,2:01:44.639
because l is an objective function with

2:01:43.119,2:01:47.360
respect to i don't know weights of a

2:01:44.639,2:01:49.520
neural net or something right

2:01:47.360,2:01:50.880
so i can simultaneously minimize with

2:01:49.520,2:01:52.800
respect to

2:01:50.880,2:01:54.960
those parameters w which i didn't write

2:01:52.800,2:01:57.119
here and with respect to the q

2:01:54.960,2:01:58.719
uh distribution and if the q

2:01:57.119,2:01:59.520
distribution is in a family that's wide

2:01:58.719,2:02:01.440
enough

2:01:59.520,2:02:02.719
uh then this upper bound will be fairly

2:02:01.440,2:02:05.440
close to the actual

2:02:02.719,2:02:08.159
loss that i want to minimize which is

2:02:05.440,2:02:10.239
the marginalized loss over the

2:02:08.159,2:02:12.320
over the latent variable but i never

2:02:10.239,2:02:14.080
need to actually compute explicitly the

2:02:12.320,2:02:15.679
marginalization of a latent variable so

2:02:14.080,2:02:16.719
this is a way of marginalizing a related

2:02:15.679,2:02:19.040
variable

2:02:16.719,2:02:20.480
without actually doing it okay by

2:02:19.040,2:02:21.119
marginalizing over a latent variable

2:02:20.480,2:02:23.440
that

2:02:21.119,2:02:24.800
you can sample from like a gaussian but

2:02:23.440,2:02:25.840
what you have to do is maximize its

2:02:24.800,2:02:26.880
entropy

2:02:25.840,2:02:29.360
and when you think about variational

2:02:26.880,2:02:31.840
autoencoders that's just what they do

2:02:29.360,2:02:34.639
okay they minimize the expected

2:02:31.840,2:02:36.320
reconstruction error which is l of x y z

2:02:34.639,2:02:37.920
with respect to the parameters by

2:02:36.320,2:02:40.080
sampling the latent variable z

2:02:37.920,2:02:42.480
according to a gaussian distribution

2:02:40.080,2:02:43.840
okay but at the same time there is

2:02:42.480,2:02:45.280
what's called the kl term which is the

2:02:43.840,2:02:46.480
second term but basically tries to make

2:02:45.280,2:02:49.280
that distribution

2:02:46.480,2:02:50.239
as high entropy as possible now this

2:02:49.280,2:02:52.960
formula

2:02:50.239,2:02:54.800
is exactly identical to a formula that

2:02:52.960,2:02:58.639
people use in uh

2:02:54.800,2:02:59.360
in statistical physics so physicists

2:02:58.639,2:03:02.159
have a very

2:02:59.360,2:03:02.159
famous formula

2:03:06.480,2:03:13.679
which is this it says

2:03:10.239,2:03:14.320
the free energy is equal to the average

2:03:13.679,2:03:17.599
energy

2:03:14.320,2:03:20.000
minus the temperature times the entropy

2:03:17.599,2:03:23.119
okay what they call the temperature is

2:03:20.000,2:03:23.119
what i call one over beta

2:03:24.080,2:03:29.440
okay and that's identical to this

2:03:27.840,2:03:31.840
formula because here this is the minus

2:03:29.440,2:03:31.840
entropy

2:03:34.080,2:03:37.760
okay this is the same formula so what

2:03:36.320,2:03:41.360
we're minimizing now is a

2:03:37.760,2:03:44.320
is a free energy and if q of z

2:03:41.360,2:03:45.440
is sufficiently powerful to actually be

2:03:44.320,2:03:49.199
the

2:03:45.440,2:03:49.199
actual distribution that it needs to be

2:03:49.599,2:03:53.360
then the inequality becomes an equality

2:03:52.880,2:03:56.480
but

2:03:53.360,2:04:00.000
um but that's the idea of variational

2:03:56.480,2:04:02.079
uh methods you basically use gensense

2:04:00.000,2:04:05.760
inequality to turn

2:04:02.079,2:04:08.800
the log of a of a of an average

2:04:05.760,2:04:11.199
into the average of the log okay and now

2:04:08.800,2:04:14.239
you get an upper bound

2:04:11.199,2:04:16.320
right so it's uh

2:04:14.239,2:04:18.000
this step right here when i turn the

2:04:16.320,2:04:20.719
equality

2:04:18.000,2:04:22.320
uh that was here into an inequality by

2:04:20.719,2:04:24.639
applying json's inequality

2:04:22.320,2:04:25.360
what i did is that i put the log inside

2:04:24.639,2:04:27.920
there was a log

2:04:25.360,2:04:29.440
outside and i put it inside so now it's

2:04:27.920,2:04:31.599
the expectation of a log instead of a

2:04:29.440,2:04:35.520
log of an expectation

2:04:31.599,2:04:36.719
okay and then

2:04:35.520,2:04:39.040
because this is a ratio it's a

2:04:36.719,2:04:40.960
difference of two logs and

2:04:39.040,2:04:42.320
because this is uh exponential of an

2:04:40.960,2:04:43.040
energy and i take the log and i divide

2:04:42.320,2:04:45.280
by beta

2:04:43.040,2:04:47.119
i get this kind of nice formula and now

2:04:45.280,2:04:50.079
this is called a variational free energy

2:04:47.119,2:04:51.040
okay and you get the expected value of

2:04:50.079,2:04:53.520
the energy

2:04:51.040,2:04:56.719
minus the inverse temperature times the

2:04:53.520,2:04:56.719
entropy of the distribution

2:04:57.040,2:05:01.040
now how you minimize this you know is

2:04:59.520,2:05:02.880
another story but

2:05:01.040,2:05:04.719
what that means now is that you can use

2:05:02.880,2:05:05.119
a surrogate distribution to sample from

2:05:04.719,2:05:06.880
your

2:05:05.119,2:05:08.560
to sample unit and variable form you

2:05:06.880,2:05:11.920
don't have to sample from the real

2:05:08.560,2:05:13.840
distribution when which uh

2:05:11.920,2:05:15.840
you know here the the real distribution

2:05:13.840,2:05:17.040
of z is really complicated i should have

2:05:15.840,2:05:21.840
written it

2:05:17.040,2:05:21.840
the real distribution of z p or z

2:05:21.920,2:05:26.159
is e to the minus theta this actually

2:05:25.040,2:05:28.320
would be a different beta it doesn't

2:05:26.159,2:05:32.239
have to be the same

2:05:28.320,2:05:35.440
e of x y z

2:05:32.239,2:05:36.639
divided by the integral over z of e to

2:05:35.440,2:05:39.760
the minus beta prime

2:05:36.639,2:05:39.760
of e of x y

2:05:40.719,2:05:49.280
z that's the real if you plug this p

2:05:44.960,2:05:52.560
into um

2:05:49.280,2:05:54.560
into here uh

2:05:52.560,2:05:56.239
the the equality here the inequality

2:05:54.560,2:05:59.840
here becomes an equality

2:05:56.239,2:06:02.079
okay you can show that uh

2:05:59.840,2:06:04.400
the the smallest value for this variable

2:06:02.079,2:06:07.040
is when q equals p

2:06:04.400,2:06:10.000
okay and then uh the two terms in the

2:06:07.040,2:06:10.000
inequality are equal

2:06:10.239,2:06:13.280
okay so that's kind of the sort of

2:06:11.760,2:06:17.840
energy view if you want of

2:06:13.280,2:06:17.840
variational inference

2:06:17.920,2:06:21.040
if you need to compute the log of a sum

2:06:19.840,2:06:25.199
of exponentials

2:06:21.040,2:06:25.199
replace it by uh

2:06:26.000,2:06:31.599
the average of

2:06:29.119,2:06:33.119
your function plus a entropy term and

2:06:31.599,2:06:34.960
that will give you an upper bound

2:06:33.119,2:06:36.400
you minimize the upper bound and because

2:06:34.960,2:06:37.679
you push down on you you push down on

2:06:36.400,2:06:38.560
the upper bound you also push down on

2:06:37.679,2:06:40.400
the function you actually want to

2:06:38.560,2:06:43.679
minimize

2:06:40.400,2:06:45.280
beautiful it's like you know the bare

2:06:43.679,2:06:48.480
bones kind of

2:06:45.280,2:06:49.360
simplest formulation of variational

2:06:48.480,2:06:54.079
inference

2:06:49.360,2:06:57.199
okay in terms of energy

2:06:54.079,2:06:59.360
uh i mean you you can replace l by

2:06:57.199,2:07:01.040
p and with some normalized stuff right

2:06:59.360,2:07:01.760
but it doesn't it makes it more

2:07:01.040,2:07:02.800
complicated

2:07:01.760,2:07:04.719
i mean it doesn't make any difference

2:07:02.800,2:07:07.119
really but it makes it harder to

2:07:04.719,2:07:07.119
interpret

2:07:08.480,2:07:15.520
okay uh i think we're done

2:07:13.119,2:07:17.199
this is a lot of people stuck around for

2:07:15.520,2:07:19.360
this sort of extracurricular

2:07:17.199,2:07:21.199
session of more than half an hour yeah

2:07:19.360,2:07:23.119
40 people

2:07:21.199,2:07:25.040
uh it was a pleasure teaching this class

2:07:23.119,2:07:27.280
particularly given the circumstances

2:07:25.040,2:07:28.239
alright see you tomorrow guys and stay

2:07:27.280,2:07:32.079
safe all right

2:07:28.239,2:07:32.079
take care bye


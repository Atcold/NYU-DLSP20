

#  Week 14 Practicum
 
## Overfitting, underfitting, and right-fitting. 

We hear about underfitting and overfitting many times. Let's look at an example (Figure 1 below):
If we want to fit a set of points like below:
* Underfitting would be fitting a parabola with a straight line. 
* Overfitting (when your model complexity is greater than the data complexity ), the model will not select a parabola. 
Since there is som enoise in the datapoints, the model that perfectly goes through the points would be much more complex than a parabola. 
* Right-fitting is when the model complexity matches the data complexity, which in this case is a parabola. 


### Why do we want overparameterization in Deep learning? 

* For pverparameterzation, you can start with differ initialized values, all of the models will converge into different positions. 
* Overparameteriszation helps with optimization (makes it easier). 
* Also helps with debugging your model, since your model _should_ overfit to a single batch if it is learning. It should be able to 
memorize the labels. 

## Regularizers

Definition of Regularization:
 
-Regularization adds prior knowledge to a model ; a prior distribution is specified for the parameters. 

-A regularization is a  restriction fo set of possible learnable functions (so the behavior of the models are not too extreme). 

Weak regularization will not curb the overparameterization of an overfitting model, whereas high regularization has the same 
effect as reducing the complexity of a model. See Figure 1 for an example. 

<center>
<img src="{{site.baseurl}}/images/week14/14-3/figure1.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>Figure 1:</b> An example of various model complexities and amount of regularizations on the same datapoints  
</center>




<center>
<img src="{{site.baseurl}}/images/week14/14-2/GT.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
Figure 2: Network Architecture
</center>

In the figure above, incorrect paths have -1.

Professor LeCun starts by going over perceptron loss, which is used in the example of Graph Transformer Model in the figure above. The goal is to make energy of wrong answers large, and correct ones small.

In terms of implementation, you would represent the arcs in the visualization with a vector. Instead and f a separate arc for each catoregy, but a vector that contains the list of categories with the list of scores for the categories.

Q: How is the segmentor implemented in the model above?

A: The segment is handcrafted heuristics. They use a handcrafted segment although there is a way to make it be trainable end-to-end. This handcrafted approach got superseded by the sliding window approach to character recognition.


## Overview of Losses

<center>
<img src="{{site.baseurl}}/images/week14/14-2/Table_loss.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
Figure 2: Various losses
</center>

The perceptron loss, which is the second in the table above, does not have a margin, and thus the loss has a risk of collapsing

* Hinge loss is taking the energy of the MOST offending answer, and the correct answer, and computing their difference. Intuitively, with a margin m, the hinge will only have loss of 0 when the correct energy is lower than the most offending energy by _at least_ m.
* MCE loss is used in speech recognition, and looks similar to a sigmoid.
* NLL loss aims to make the energy of the correct answer small, and make the log component of the equation large.

Q: How may hinge be better than NLL loss?

A: Hinge is better than nLL in a way, because ht hinge says â€œI WANT TO MAKE IT LARGER by some value, not to try to push it to infinity).

### DEFINITION:

A decoder inputs a sequence of vectors that indicate the scores or energy of individual sounds or images, and picks out the best possible output.

Q: What are some examples of problems that can use decoders? 
A: Language modelling, machine translation, and sequence tagging. 

## Graph Composition


<center>
<img src="{{site.baseurl}}/images/week14/14-2/language_models.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
Figure 2: Graph Composition Example
</center>

You can think of a language model as a graph.

After sampling from the decoder, which creates a tree of predictions in which the first layer are the top characters (with the largest probabiilties from the model), the second layer is the top character s(or tokens) that the model thinks is mot probable given the firt tokens, and so on.

We are also given a gold standard trie of the lexicon. This process compares the paths outputted from the neural network with the ground truth to create an interpretation graph.


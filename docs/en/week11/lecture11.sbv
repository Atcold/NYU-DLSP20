0:00:00.030,0:00:04.080
all right so we're going to talk about two or three topics today and the first

0:00:04.080,0:00:08.790
one is going to be kind of a review of some of the functions that exist inside

0:00:08.790,0:00:16.139
torch and kind of when and how to use them so the first the first set of

0:00:16.139,0:00:22.410
topics is about activation functions and there is a whole bunch of them defined

0:00:22.410,0:00:27.000
in in tight torch and they basically come from you know various papers that

0:00:27.000,0:00:30.300
people have written where they claim that this or that particular objective

0:00:30.300,0:00:35.790
function or activation function works better for their problem so of course

0:00:35.790,0:00:40.140
everybody knows the value that's very standard one but there's lots of

0:00:40.140,0:00:46.500
variations of values these values where the the the bottom part is not constant

0:00:46.500,0:00:50.340
and set to zero they can be allowed to change either only with the positive

0:00:50.340,0:00:55.379
slope or force to be to have a negative slope or sometimes being random in the

0:00:55.379,0:01:00.870
case of the randomized VQ value so they have you know a nice named likely key

0:01:00.870,0:01:08.369
value pair you revalue random value etc so the key value is one where you allow

0:01:08.369,0:01:16.350
the bottom part to have negative so and that kind of prevents the issue that

0:01:16.350,0:01:20.250
sometimes pops up that you know when radio is off it doesn't get any gradient

0:01:20.250,0:01:26.939
so here here you get a chance for that system that function to actually

0:01:26.939,0:01:30.119
propagate gradient and perhaps do something useful can go all the way to

0:01:30.119,0:01:35.570
kind of compete full ratification of the signal kind of like an absolute value

0:01:35.570,0:01:41.280
value it's well yeah the previous the previous activation was

0:01:41.280,0:01:46.530
is user using the discriminatory in again such that we always have gradients

0:01:46.530,0:01:51.329
going backwards for the generator and also these activation was necessary in

0:01:51.329,0:01:55.409
order to train the very skinny network I show at the beginning of the class

0:01:55.409,0:01:59.700
because again having like a very very skinny network it was basically

0:01:59.700,0:02:04.829
impossible to get gradients flowing back because we were like ending up in one of

0:02:04.829,0:02:09.179
the quadrants without you know where everything was zero out and then nothing

0:02:09.179,0:02:12.800
would have been actually trained if you wouldn't have

0:02:12.800,0:02:17.390
use you know this activation function that allows me to get some kind of

0:02:17.390,0:02:21.350
gradients even if we are in the regions where we are trying to suppress the

0:02:21.350,0:02:32.180
output so yeah right so a prelude is it's similar except now the the slope

0:02:32.180,0:02:38.930
and they get your side can be just about anything and okay what's what's

0:02:38.930,0:02:42.620
interesting about those all those functions that we just saw is that they

0:02:42.620,0:02:47.090
are scale invariant in the sense that they you know you can multiply the

0:02:47.090,0:02:52.700
signal by two and the output will not be changed

0:02:52.700,0:02:55.280
yeah I mean it would be multiplied by two but otherwise unchanged so they are

0:02:55.280,0:02:59.840
equivalent to scale there's no sort of intrinsic scale in those in those

0:02:59.840,0:03:05.260
functions right because there's only one non-linearity and it's a sharp one so

0:03:05.260,0:03:09.530
now we're getting into functions where the scale matters so the amplitude of

0:03:09.530,0:03:14.840
the incoming signal will affect the type of non the type of non-linearity that

0:03:14.840,0:03:19.280
you're going to get and one of those is the the soft press so soft rice it's

0:03:19.280,0:03:24.110
sort of a differentiable version of radio if you want it's it's kind of the

0:03:24.110,0:03:26.480
soft version of positive part and it's usually

0:03:26.480,0:03:29.780
permit rised as you can see at the top here

0:03:29.780,0:03:34.670
whatever Bay dialog 1 plus X financial beta X so it's it's kind of like the log

0:03:34.670,0:03:41.420
some exponential that we've been using a lot for service purpose except here one

0:03:41.420,0:03:45.440
of the terms in the sum is equal to 1 which is kind of like exponential zero

0:03:45.440,0:03:52.760
if you want so that looks like kind of a function that so synthetic ly is the

0:03:52.760,0:03:56.390
identity function for large policy values and asymptotically zero for

0:03:56.390,0:04:00.560
negative values so the approximate sir value it has a scale parameter though

0:04:00.560,0:04:05.690
this this beta parameter the larger beta the more the function will look like a

0:04:05.690,0:04:10.400
value so the cake will be kind of the corner will be kind of sharper if petah

0:04:10.400,0:04:17.380
goes to infinity but that that function has a scale

0:04:18.260,0:04:23.710
now you can prioritize those functions in in various ways and this is sort of a

0:04:23.710,0:04:29.360
another example of kind of a soft version of value if you

0:04:29.360,0:04:35.150
want where we're here you use value as a basis and then you add a small constant

0:04:35.150,0:04:40.880
to it that kind of makes it smooth you know I can't tell you that

0:04:40.880,0:04:44.840
any of those has any particular advantage over the others it really

0:04:44.840,0:04:48.500
depends on the only problem but they they all have kind of similar properties

0:04:48.500,0:04:56.660
if you want this also you can make sort of continuously closer to value that's

0:04:56.660,0:05:04.820
yet ok so when when difference here in this case is that this guy actually goes

0:05:04.820,0:05:10.070
negative right so unlike the value that has its minimum at zero this it's

0:05:10.070,0:05:17.900
horizontal asymptote at zero this guy goes below zero and that may or may or

0:05:17.900,0:05:21.650
may not be advantageous depending on the application you have sometimes it's

0:05:21.650,0:05:26.450
advantageous because it allows the system to basically make the average of

0:05:26.450,0:05:32.510
the output zero which is advantageous for certain types of for grading design

0:05:32.510,0:05:36.200
convergence the weights that are connected to units like this will see

0:05:36.200,0:05:40.010
both both positive and negative values which will then converge faster than if

0:05:40.010,0:05:46.310
you only see positive values so it's been the same here and it's just kind of

0:05:46.310,0:05:50.360
a differently you know a different permutation of kind of the same thing if

0:05:50.360,0:05:56.960
you want with different properties so of course there's tons of variations of

0:05:56.960,0:06:03.050
this you know with various parameters with different properties and you know

0:06:03.050,0:06:09.820
some of them that have particular properties that can relate them to

0:06:09.820,0:06:14.990
Gaussian distributions for example this is not the cumulative distribution of a

0:06:14.990,0:06:20.510
Gaussian but okay so those those were things that have one kinks in them and

0:06:20.510,0:06:25.310
if the kink is sharp there's no scale if the King has some scale in it there is

0:06:25.310,0:06:29.240
some scale but it's still sort of a single kick non-linearity now we're

0:06:29.240,0:06:33.080
getting into nonlinearities I have two kings okay so this one is basically a

0:06:33.080,0:06:37.400
saturating value I know sure white saturates at six you

0:06:37.400,0:06:41.670
know why not but why not parameterize this a little

0:06:41.670,0:06:46.230
better so here's a smooth function that you're familiar with because it's used

0:06:46.230,0:06:51.720
in in recurrent Nets in gated recurrent Nets and then STM

0:06:51.720,0:06:56.730
in soft max you know basically this is a a two-way soft max you can think of it

0:06:56.730,0:07:03.800
this way and this is just a function that goes kind of smoothly between 0 & 1

0:07:03.800,0:07:08.780
it's sometimes called a Fermi Dirac function as well because it derives from

0:07:08.780,0:07:15.240
some work in physics it's equal physics and then there is the hyperbola tangent

0:07:15.240,0:07:19.830
that we also talked about it's basically identical to the sigmoid except it's

0:07:19.830,0:07:24.360
centered so it goes between minus 1 and plus 1 and it's a little you know it's

0:07:24.360,0:07:28.710
twice the amplitude and the gain is a little different but it plays the same

0:07:28.710,0:07:31.080
role the advantage of hyperbolic tangent is

0:07:31.080,0:07:36.990
that the output is you can expect the output to not have zero mean but get

0:07:36.990,0:07:43.500
close to having zero mean and again that's advantageous for the weights that

0:07:43.500,0:07:46.890
follow because they see positive and negative values and they tend to

0:07:46.890,0:07:52.310
converge faster that's the case so I used to be a big fan of those

0:07:52.310,0:07:57.390
unfortunately if you stack a lot of sigmoids in many layers you know in a

0:07:57.390,0:08:05.160
neural net you you you can tend to not learn very efficiently you have to be

0:08:05.160,0:08:09.030
very careful about normalization if you want the system to to converge if you

0:08:09.030,0:08:13.800
have many layers so in that sense the the single kick functions are better for

0:08:13.800,0:08:21.660
deeper networks so it's signed this is basically a bit like the sigmoid except

0:08:21.660,0:08:25.560
that it's it doesn't get to the asymptotes as fast so it doesn't get

0:08:25.560,0:08:31.169
stuck towards the asymptotes as quickly so one problem with hyperbolic tangent

0:08:31.169,0:08:37.440
and and the sigmoid is that we get close to the asymptotes the the gradient goes

0:08:37.440,0:08:43.979
to zero fairly quickly and so if the weights of a unit become too large they

0:08:43.979,0:08:48.690
kind of saturate this unit and the gradients get very small and then the

0:08:48.690,0:08:54.029
the unit doesn't run very very quickly anymore it's a problem that

0:08:54.029,0:09:00.779
exists both sorry both in sigmoids and hyperbole tension and so such sign is a

0:09:00.779,0:09:05.790
function of that was proposed by your venue and so these collaborators and it

0:09:05.790,0:09:12.230
kind of saturates slower and so it doesn't have the that same problem I

0:09:12.230,0:09:17.819
mean it has the problem also but not to the same extent okay and this is kind of

0:09:17.819,0:09:23.819
the opposite heart tangent hard 10h I don't know if it deserves that name but

0:09:23.819,0:09:29.720
it's basically just a ramp okay and that works surprisingly well

0:09:29.720,0:09:35.310
particularly if your weights are somehow kept within a kind of small value so the

0:09:35.310,0:09:42.569
the units don't saturate too much it's surprising how well it works and you

0:09:42.569,0:09:46.579
know people use this and service context but that's sort of you know non-standard

0:09:49.170,0:09:53.850
so hard threshold is very rarely used because you can't really propagate

0:09:53.850,0:09:58.680
gradient to it okay and this is really what kept people from inventing back

0:09:58.680,0:10:03.120
prop in the 60s and 70s which is that they were using binary neurons and so

0:10:03.120,0:10:09.930
they didn't think of the whole idea of gradients because of that okay those

0:10:09.930,0:10:16.439
other functions are rarely used in the context of neural nets or at least for

0:10:16.439,0:10:20.730
kind of activation function in a traditional neuron that they used mostly

0:10:20.730,0:10:26.220
for sometimes for things like sparse coding so one step in sparse coding

0:10:26.220,0:10:32.279
consists in to compute the value of the latent variable consistent shrinking all

0:10:32.279,0:10:35.879
the values in the latent variable is the agent vector by some value and you do

0:10:35.879,0:10:40.019
this with a shrink function a shrinkage function this is kind of a soft version

0:10:40.019,0:10:44.160
of shrinkage function the hard version is here I mean it's called soft shrink

0:10:44.160,0:10:50.699
soft shrink but it actually has corners in it the reason it's called soft shrink

0:10:50.699,0:10:54.029
is because there is a hard shrink that looks different did that show you in a

0:10:54.029,0:10:59.819
minute so this basically just changes variable

0:10:59.819,0:11:05.120
by a constant toward zero right and if he goes below zero it connect its

0:11:05.120,0:11:16.910
client at zero if it if it's brought too long and so if the this is basically

0:11:16.910,0:11:20.600
just the identity function to which you subtract hyperbola tangent to make it

0:11:20.600,0:11:25.430
look like a shrink shrink basically design if we if we try to get the

0:11:25.430,0:11:29.060
whatever value close to zero and they actually are forced to zero basically

0:11:29.060,0:11:33.860
right right so small values are forced to zero others are shrunk toward zero

0:11:33.860,0:11:37.850
but you know it's a large enough they're not going to get to zero so again that's

0:11:37.850,0:11:44.390
used mostly as you know you can think of it as a step of gradient for an l1

0:11:44.390,0:11:48.290
criterion okay so if you have a variable you have

0:11:48.290,0:11:52.940
an l1 cost function on it and you take a step in the negative gradient of the one

0:11:52.940,0:11:58.580
so every one cost is an absolute value this will cause the variable to kind of

0:11:58.580,0:12:02.570
go towards zero by a constant which is the slope of that at one criterion and

0:12:02.570,0:12:07.730
to kind of stay at zero you know coming from either side it doesn't kind of

0:12:07.730,0:12:12.290
overshoot if you want and so that that's an on-ear function you use and that's

0:12:12.290,0:12:17.300
one of the steps in the East algorithm that is used for inference in in sparse

0:12:17.300,0:12:23.990
coding but again it's it's really used in sort of regular neural nets unless

0:12:23.990,0:12:34.490
your encoder is kind of used as kind of a estimation of sparse coding this is

0:12:34.490,0:12:40.010
the hard shrink so hard shrink basically clients every value smaller than lambda

0:12:40.010,0:12:48.500
to zero okay so if a value is smaller than lambda or larger than - from there

0:12:48.500,0:12:51.560
sort of between minus lambda lambda when lambda is some constant is just set it

0:12:51.560,0:12:57.590
to zero again it's used for things like you know certain types of sparse coding

0:12:57.590,0:13:04.250
but rarely as an activation function in the night so log sigmoid is mostly used

0:13:04.250,0:13:08.930
in cost functions not really as an activation function either but it's a

0:13:08.930,0:13:15.320
useful function to have if you want to plug this into into a loss function and

0:13:15.320,0:13:19.300
we'll see we'll see that ten minute so something we've seen this is the same

0:13:21.470,0:13:27.589
as softmax except you have minus signs so this is sort of more so that so those

0:13:27.589,0:13:31.490
are multi-dimensional nonlinearities all right you you have a vector in and you

0:13:31.490,0:13:35.930
get a vector out which is at same size as the input vector and we know about

0:13:35.930,0:13:40.790
softmax is you know exponential X I divided by sum over J exponential X J

0:13:40.790,0:13:45.800
this is such min where you put a minus sign in front of the X so you view the

0:13:45.800,0:13:51.790
X's if you want as energies instead of scores as penalties instead of scores

0:13:51.790,0:13:58.160
it's a good way of turning a bunch of numbers to something that looks a bit

0:13:58.160,0:14:02.569
like a probability distribution which means numbers between 0 and 1 that's 1

0:14:02.569,0:14:10.190
to 1 and that's the softmax which we all know so locks off max again it's not

0:14:10.190,0:14:14.959
very much used as a non-linearity within the neural net but it's used a lot at

0:14:14.959,0:14:18.920
the output as kind of one piece of a loss function it will see this in a

0:14:18.920,0:14:26.839
minute ok so those questions yes we have a question so for pre low I'm not sure I

0:14:26.839,0:14:32.000
understand number one why we want the same value for all channels and number

0:14:32.000,0:14:37.250
two how learning a it would actually be advantages you could have a different a

0:14:37.250,0:14:41.480
for different channels so different units can have a different a it could be

0:14:41.480,0:14:48.019
you could use it this as a as a parameter very every unit or not you

0:14:48.019,0:14:51.860
could be shared that's gonna up to you it could be shared at the level or a

0:14:51.860,0:14:55.069
feature map it accomplished on that or it could be share all future maps or it

0:14:55.069,0:14:59.240
could be individual to every unit if you really want to preserve the coalitional

0:14:59.240,0:15:02.899
nature of a commercial net you probably want to have the same a for every unit

0:15:02.899,0:15:05.420
in the future that but you think you can have different aids for different

0:15:05.420,0:15:11.480
feature Maps okay well it's the second question why learning actually a

0:15:11.480,0:15:16.459
specific value would be advantages like why are we learning a you can learn it

0:15:16.459,0:15:21.949
or not you can fix it the the reason for fixing it would be you know not

0:15:21.949,0:15:27.750
necessarily to kind of have sort of more powerful non-linearity but to kind of

0:15:27.750,0:15:32.190
ensure that the non-linearity gives you a non 0 gradient even if it's even if

0:15:32.190,0:15:41.430
it's in the negative region so you know runnable not nor lib not renewable so to

0:15:41.430,0:15:46.200
make it learnable allows the system to basically turn a non-linearity into

0:15:46.200,0:15:50.970
either the linear mapping which of course is not particularly interesting

0:15:50.970,0:15:57.330
but why not value or something like a full rectification okay where a would be

0:15:57.330,0:16:03.720
minus 1 in the in the negative part which you know can be it can be

0:16:03.720,0:16:06.150
interesting for certain types of application so for example if you have

0:16:06.150,0:16:10.440
accomplish on that that has an edge detector an edge detector as a priority

0:16:10.440,0:16:14.550
right it's it's got press coefficients on one side minus coefficients on the

0:16:14.550,0:16:19.800
other side and so it's gonna react so if you have an edge in an image that goes

0:16:19.800,0:16:25.140
from say dr. bright the you know the composition will react positively to

0:16:25.140,0:16:31.110
this one but if you have another edge from from you know in the opposite

0:16:31.110,0:16:36.510
direction then the the react the the filter will react negatively now if you

0:16:36.510,0:16:41.370
want your filter to react to an edge regardless of its priority you rectify

0:16:41.370,0:16:46.320
it okay so that would be kind of just absolute value now you could of course

0:16:46.320,0:16:50.610
bake this in you know how to use a prelude you can just use the absolute

0:16:50.610,0:16:55.170
value probably a better idea is to use a square actually so you take the square

0:16:55.170,0:16:59.730
of square non-linearity it's not implemented as kind of a neural net

0:16:59.730,0:17:02.430
non-linearity but you know in the functional form of by torture just right

0:17:02.430,0:17:06.420
square and that's it hope answered the question

0:17:06.420,0:17:14.180
any other question on this topic I have a question it seems to me like these

0:17:14.180,0:17:21.230
nonlinearities are trying to basically make a linear function nonlinear and the

0:17:21.230,0:17:27.360
tweak in the in the lines denote like the change in that function so they can

0:17:27.360,0:17:37.440
we think of this as if we want to model of curve in the line should we have

0:17:37.440,0:17:42.990
learnable on both like before before the 0 and

0:17:42.990,0:17:48.330
after the 0 on the x-axis like well so yeah I mean there is diminishing return

0:17:48.330,0:17:52.560
so the question is you know how complex do you want your non-linearity to be so

0:17:52.560,0:17:58.200
you could imagine of course primate rising an entire nonlinear function you

0:17:58.200,0:18:02.520
know with sprite parameters or busy curves or something like this right or I

0:18:02.520,0:18:07.890
don't know chebyshev polynomials you know I mean you can probably try any any

0:18:07.890,0:18:12.180
mapping you want right you can imagine those those parameters could be part of

0:18:12.180,0:18:17.730
the learning process however you know what is the advantage of doing this

0:18:17.730,0:18:23.220
versus just you know having more units in your in your system and relying on

0:18:23.220,0:18:28.080
the fact that multiple units will be added in the end to approximate the

0:18:28.080,0:18:33.600
function you want generally it really depends on what like if you want to to

0:18:33.600,0:18:37.260
do regression in a fairly low dimensional space so perhaps you want

0:18:37.260,0:18:43.080
some parameterize nonlinearities that might help you might have like you might

0:18:43.080,0:18:48.390
want to have a collection of different nonlinearities with maybe things like

0:18:48.390,0:18:53.250
like gbj polynomials if you want to do good approximation approximations but

0:18:53.250,0:18:56.370
for like you know high dimensional tasks like image recognition or things like

0:18:56.370,0:19:01.620
this you just want a non-linearity and it works better if the non-linearity is

0:19:01.620,0:19:05.700
monotonic otherwise it creates all kinds of issues because you could have two

0:19:05.700,0:19:09.510
points that will produce the same output and so it's you land videos for the

0:19:09.510,0:19:14.910
system to learn the right function there so you want it it's it's much better if

0:19:14.910,0:19:18.810
the function is monotonic and almost all the functions here are monotonic except

0:19:18.810,0:19:23.940
if you have a negative a here in the in the prelude case there's big advantage

0:19:23.940,0:19:30.480
to having monotonic functions but in principle you could probably try as you

0:19:30.480,0:19:34.140
know any function you want people are played with this you know they're not

0:19:34.140,0:19:39.140
very popular because mostly they don't seem to be bringing a huge advantage in

0:19:39.140,0:19:47.580
in the kind of applications that people use a large neural nets for other

0:19:47.580,0:19:52.560
questions another question is going to be keen commercial smooth yeah

0:19:52.560,0:19:57.690
I should never have any application with the choice so funny

0:19:57.690,0:20:04.710
yeah teas made of bacon packed the only thing I'm aware of is using a single

0:20:04.710,0:20:12.000
function silica double King for DPL networks healthily in better well so

0:20:12.000,0:20:16.020
here's a party with double kick double kick has a built-in scale in it which

0:20:16.020,0:20:21.300
means if you're it's a waste of the incoming layer are multiplied by two or

0:20:21.300,0:20:25.590
the signal amplitude is multiplied by 2 the result on the output would be

0:20:25.590,0:20:29.940
completely different right yeah because you will be you know the signal would be

0:20:29.940,0:20:35.250
more in the non-linearity so so you'll get completely different behavior of

0:20:35.250,0:20:39.390
your layer whereas if you have a function with only one King if you

0:20:39.390,0:20:43.110
multiply the input by two with the output it's also multiplied by two in a

0:20:43.110,0:20:49.380
modulo bias but the signal device is fine so what I mean to ask you something

0:20:49.380,0:20:53.670
of a situation with a choice of activation function made a big

0:20:53.670,0:20:58.410
difference in the performance of the model except for the in networks using

0:20:58.410,0:21:07.890
today move instead of sigmoid there is no sort of general answer to this like

0:21:07.890,0:21:13.620
if you're going to use attention you have to use softmax I mean you have no

0:21:13.620,0:21:17.460
choice right I mean it's not like you have to use shaft softmax but you want

0:21:17.460,0:21:21.300
to have something where you get coefficients right to to kind of focus

0:21:21.300,0:21:26.460
the attention of the system on or to kind of spread the attention of the

0:21:26.460,0:21:31.470
system and not allow it to cheat which is to pay attention to multiple things

0:21:31.470,0:21:36.180
at one time you have to have some sort of normalization of the of the

0:21:36.180,0:21:40.980
coefficients that come out of the attention system right so so normally in

0:21:40.980,0:21:44.480
most attention systems like in transformers and stuff the the

0:21:44.480,0:21:48.750
coefficients are passed through softmax so you get a bunch of coefficients that

0:21:48.750,0:21:54.630
are between 0 & 1 & 7 to 1 and so that causes the system to have to pay

0:21:54.630,0:21:59.370
attention to you know a small number of things right you can only concentrate

0:21:59.370,0:22:06.009
the coefficients on a small number of items and it has to spread it right

0:22:06.009,0:22:11.779
there are other ways to do normalization you you can do and in fact is something

0:22:11.779,0:22:18.049
that's wrong with softmax normalization for for for transformers or for

0:22:18.049,0:22:22.159
attention which is that if you want a coefficient coming out with softmax to

0:22:22.159,0:22:26.840
be close to zero you need the input to be close to minus

0:22:26.840,0:22:32.419
infinity okay or to be considerably smaller than the largest one right when

0:22:32.419,0:22:38.389
you go into the softmax when I put the largest input is going to cause the

0:22:38.389,0:22:42.200
corresponding output to be the to be large if you want that I'd put to be

0:22:42.200,0:22:46.129
close to one and all the other ones to be close to zero you basically want this

0:22:46.129,0:22:53.379
input to be extremely large and all the other ones to be large and negative okay

0:22:53.379,0:23:02.149
now that you know that that can be a problem when the what you are computing

0:23:02.149,0:23:08.090
the input are our dot products because the result is that you know the easiest

0:23:08.090,0:23:14.690
way for a system to to produce a small dot product is to have two vectors that

0:23:14.690,0:23:18.590
are orthogonal to each other which gives the dot product is zero if you insist

0:23:18.590,0:23:20.480
that the dot product should be very very small

0:23:20.480,0:23:26.090
then either you have to make the so you have to make the two vectors basically

0:23:26.090,0:23:31.580
point in opposite directions and you have to make them very long and that's

0:23:31.580,0:23:39.769
not so great and so using softmax for attention basically limits the the the

0:23:39.769,0:23:43.639
contrast that you're gonna have between two coefficients which is not

0:23:43.639,0:23:50.600
necessarily a good thing so you know same thing for ASTM gates

0:23:50.600,0:23:57.499
gated recurrent Nets etc you you need Sigma it's there because you need

0:23:57.499,0:24:01.039
coefficients there are between 0 and 1 you know that either

0:24:01.039,0:24:06.590
we set the memory cell or make it a fast-food so that it keeps it so it's

0:24:06.590,0:24:12.200
previous memory or kind of right the the new input in it so there it's nice to

0:24:12.200,0:24:14.770
have an output that varies continuously

0:24:14.770,0:24:21.010
between zero and one there you have no choice so I mean I don't think you can

0:24:21.010,0:24:26.350
say just you know in generic term you know this this non-linearity is better

0:24:26.350,0:24:29.110
than this other one there are certain cases where it learns better though

0:24:29.110,0:24:33.970
certain cases where it relieves you from having to initialize properly the

0:24:33.970,0:24:37.690
certain cases where it works better if you have lots of layers like you know

0:24:37.690,0:24:40.900
single click functions work better if you have lots of layers better than

0:24:40.900,0:24:48.130
Sigma large functions there's no kind of logical answer I had a question just

0:24:48.130,0:24:54.190
regarding the general differences between a nonlinear activation that has

0:24:54.190,0:25:00.130
kinks versus a smooth nonlinear activation yeah is there sort of any

0:25:00.130,0:25:06.340
general reason or rule to why we would prefer to have kinks in the function or

0:25:06.340,0:25:11.620
not it's a matter of scale invariant of scale equivariance so if the kick is

0:25:11.620,0:25:15.610
hard again you multiply the input by 2 the output is multiplied by 2 but

0:25:15.610,0:25:22.660
otherwise unchanged okay if you have a smooth transition if you multiply the

0:25:22.660,0:25:31.390
input by let's say 100 the output now will look like you had a hard kick okay

0:25:31.390,0:25:35.860
because the the smooth part now it's become shrunk by a factor of 100 if you

0:25:35.860,0:25:42.370
divide the input by 100 now the the cake becomes a very very smooth sort of

0:25:42.370,0:25:47.710
convex functions okay so it changes the behavior but by changing the scale the

0:25:47.710,0:25:52.810
input you change the behavior of the of the unit and that might be a problem

0:25:52.810,0:25:58.360
sometimes because when you when you train a multi-layer neural net and you

0:25:58.360,0:26:04.680
have two layers that are one after the other you don't have a good control for

0:26:04.680,0:26:09.370
like how big the weights of this layer are relative to that other way so

0:26:09.370,0:26:13.030
imagine you have a two layer Network where you don't have a non-linearity in

0:26:13.030,0:26:19.510
the middle so the system is completely linear right if the network has arrived

0:26:19.510,0:26:24.100
at the solution you can multiply the incoming the the first layer weight

0:26:24.100,0:26:27.520
matrix by two divide the second weight matrix by two

0:26:27.520,0:26:31.300
and overall the network will have exactly the same output okay you won't

0:26:31.300,0:26:36.370
have changed anything and what that means is that when you do training there

0:26:36.370,0:26:41.350
is nothing that forces the system to have a particular scale for the weight

0:26:41.350,0:26:45.190
matrices all right so now if you put a non-linearity in the

0:26:45.190,0:26:49.050
middle and you still don't have it because train for the system to kind of

0:26:49.050,0:26:53.820
you know have scales for the first leg of weight versus the second leg away

0:26:53.820,0:26:58.990
that you know you'd better have a non-linearity that doesn't care about

0:26:58.990,0:27:04.950
scale okay so if you have non-linearity that does care about scales about scale

0:27:04.950,0:27:10.030
then your network doesn't have a choice of what size weight matrix it can use in

0:27:10.030,0:27:14.170
the first layer because that will completely change the behavior and you

0:27:14.170,0:27:17.740
may want to have large weights for some other reason which will saturate the

0:27:17.740,0:27:23.560
non-linearity and then kind of create you know vanishing gradient issues so I

0:27:23.560,0:27:29.350
it's not entirely clear you know what why is it that you know did networks

0:27:29.350,0:27:34.000
work better with single King functions but it's probably due to that scale

0:27:34.000,0:27:38.140
invariance property let's get a quick variance property now there would be

0:27:38.140,0:27:42.190
other ways of fixing this fixing this problem which would be to basically set

0:27:42.190,0:27:45.340
a heart scale on the weights of every layer so you couldn't like normalize the

0:27:45.340,0:27:50.860
weights of the layers so that the the the variance that of things that go into

0:27:50.860,0:27:53.560
unit you know it's always constant in fact that's a little bit with batch

0:27:53.560,0:27:56.830
normalization does so the various normalization schemes they do that to

0:27:56.830,0:28:02.530
some extent you know they could mean at zero and and the variance is constant so

0:28:02.530,0:28:07.570
so now the variance of the amplitude of the output doesn't depend on the size of

0:28:07.570,0:28:16.020
the weights because it's not wise so you know that is partially why things like

0:28:16.020,0:28:21.670
like like that norm and good norm and things like this help is because they

0:28:21.670,0:28:32.200
can fix the scale a little bit but then if you fix the scale then with something

0:28:32.200,0:28:37.420
like that norm you don't the system now doesn't have any way of choosing which

0:28:37.420,0:28:41.320
part of the non-linearity it's going to use in the to cake function system

0:28:41.320,0:28:45.820
okay so things like group normalization or batch normalization are incompatible

0:28:45.820,0:28:51.159
with kin sigmoids if you want share a sigmoid you don't want normalization

0:28:51.159,0:28:57.899
just before it see that provides some really good intuition thank you okay I

0:28:57.899,0:29:03.549
have one more question I noticed in a soft malfunction some people use the

0:29:03.549,0:29:07.840
temperature coefficient so in what cases would we want to use their temperature

0:29:07.840,0:29:12.580
and why would we use it well to so to some extent the temperature is redundant

0:29:12.580,0:29:18.059
with incoming weights so if you have weighted sounds coming into your softmax

0:29:18.059,0:29:22.750
you know having a data parameter in your softmax equal to two instead of one it's

0:29:22.750,0:29:26.730
the same as just making your waste twice as big it has exactly the same effect

0:29:26.730,0:29:32.320
okay so that beta parameter is redundant with the size of the weights but again

0:29:32.320,0:29:36.009
if you were or the size of the weighted sum the variance of the weighted stones

0:29:36.009,0:29:39.700
if you want but again if you have a batch normalization in there then the

0:29:39.700,0:29:45.190
temperature parameter matters because now the input variances are fixed so so

0:29:45.190,0:29:51.460
now the the temperature matters temperature basically new controls how

0:29:51.460,0:29:58.389
hard the the you know the the distribution on the output will be so

0:29:58.389,0:30:02.470
with a very very large beta you basically will have one of the outputs

0:30:02.470,0:30:05.379
equal to one and all the other ones really close to zero I mean very close

0:30:05.379,0:30:09.850
to one and very close to zero where beta is small then this software in the limit

0:30:09.850,0:30:13.029
of beta equal to zero it's more like an average actually that you get like soft

0:30:13.029,0:30:19.779
max behaves a little bit like an average so you know beta goes infinity at it so

0:30:19.779,0:30:24.159
behaves a bit like AG Max and there goes to zero it behaves of it like reckon

0:30:24.159,0:30:32.440
average so so if you have some sort of normalization before the softmax then

0:30:32.440,0:30:36.419
tuning is parameter allows you to control this could have hardness and

0:30:36.419,0:30:42.850
what people do sometimes in certain scenarios is that they start with a

0:30:42.850,0:30:49.000
relatively low beta so that the the numbers that are produced are kind of

0:30:49.000,0:30:53.740
soft so you get gradients everywhere you know it's kind of well-behaved in terms

0:30:53.740,0:30:57.179
of gradient and then as running proceeds if you want

0:30:57.179,0:31:01.139
kind of harder decisions in your attention mechanism or whatever you

0:31:01.139,0:31:05.580
increase data and so that makes the system kind of make harder decisions it

0:31:05.580,0:31:08.789
doesn't run as well anymore but it's you know presumably after a few iterations

0:31:08.789,0:31:13.379
it's kind of in the right ballpark so you can sort of sharpen the the

0:31:13.379,0:31:18.509
decisions there but can be increasing data it's useful for example in a

0:31:18.509,0:31:24.509
mixture of mixture of experts and you know self attention systems are kind of

0:31:24.509,0:31:31.169
you can think of as sort of a form of a weird form of mixture of experts so a

0:31:31.169,0:31:35.070
mixture of experts you know you have multiple sub networks and their outputs

0:31:35.070,0:31:38.549
are can linearly combine with coefficients that are the output of the

0:31:38.549,0:31:45.450
softmax itself could you know controlled by another neural net so if you want

0:31:45.450,0:31:48.539
kind of a soft mixture you have a low beta and as you increase their to

0:31:48.539,0:31:52.739
infinity basically you're going to select one of the experts and ignore all

0:31:52.739,0:31:57.090
the other ones there might be useful for example if you want to train a mixture

0:31:57.090,0:32:00.960
of experts or an attention mechanism but in the end you want to save computation

0:32:00.960,0:32:05.159
by just determining which expert do I need to compute and just not computing

0:32:05.159,0:32:08.460
the other ones so in that case you want those coefficients to be basically

0:32:08.460,0:32:13.350
either one or zero and and you can train the system progressively to do this by

0:32:13.350,0:32:19.649
increasing increasing data this is called the physicists have a name for

0:32:19.649,0:32:22.859
this because it uses kind of tricks or various other things that's called

0:32:22.859,0:32:30.480
annealing it has the same meaning as so annealing comes from metalwork right you

0:32:30.480,0:32:37.289
you're making a steel or something and you make a sword or something right and

0:32:37.289,0:32:43.919
you heat it up and then you you you you you cool it and depending on whether you

0:32:43.919,0:32:49.649
cool it quickly or slowly you'll you change the crystal no crystalline

0:32:49.649,0:32:54.389
structure of the of the metal so this idea of annealing of progressively

0:32:54.389,0:32:58.320
lowering the temperature correspond to this increasing this beta beta is like

0:32:58.320,0:33:04.570
an inverse temperature it's akin to an inverse temperature any other question

0:33:04.570,0:33:18.850
I think we are good all right okay so next topic is lost functions so PI torch

0:33:18.850,0:33:25.420
has a whole bunch of loss functions as you might have seen and of course there

0:33:25.420,0:33:32.110
are things are simple ones like like mean square error so I don't need to

0:33:32.110,0:33:35.020
explain to you what it is you know compute the square of the error between

0:33:35.020,0:33:41.830
the desired output Y and actually I put X and if it's already bet with n samples

0:33:41.830,0:33:49.240
then you have you know n losses one for each of the samples in the batch and you

0:33:49.240,0:33:52.660
can you can tell this last function to either keep that vector or to kind of

0:33:52.660,0:34:02.590
reduce it by academia or some okay pretty simple mmm here's a different

0:34:02.590,0:34:06.010
loss that's the everyone knows so this is basically the absolute value of the

0:34:06.010,0:34:10.780
difference between between the desired output and the actual output and you

0:34:10.780,0:34:15.970
want to use this to do what's called robust regression so if you want small

0:34:15.970,0:34:20.350
errors to count a lot and large errors to count but you know not as much as if

0:34:20.350,0:34:23.620
you use the square perhaps because you have noise in your

0:34:23.620,0:34:27.820
data so you know that you have a bunch of data points you're trying to kind of

0:34:27.820,0:34:33.370
train and know on that to something to kind of you know fit a curve or you know

0:34:33.370,0:34:36.850
do regression but you know that you have a few outliers so you have a few points

0:34:36.850,0:34:40.000
there are you know very far away from what they should be just because you

0:34:40.000,0:34:44.500
know the system as noise or something or the data was collected with with some

0:34:44.500,0:34:47.740
noise so you want the system to be robust to that noise you don't want the

0:34:47.740,0:34:53.800
cost function to increase too quickly as the points are far away from you know

0:34:53.800,0:35:01.180
the kind of the general curve so at one loss will be more robust now the problem

0:35:01.180,0:35:05.890
with that one loss is that it's not differentiable at the bottom and so you

0:35:05.890,0:35:11.110
know you have to kind of be careful when you get to the bottom of how you how you

0:35:11.110,0:35:16.390
do the the gradient that's basically done with this soft shrink essentially

0:35:16.390,0:35:26.710
that's that's the everyone knows now to correct for that

0:35:26.710,0:35:33.759
people have come up with various ways of kind of making the l1 notes for bus for

0:35:33.759,0:35:38.529
large losses but then still smooth at the bottom kind of behaving like squad

0:35:38.529,0:35:43.259
error at the bottom so an example of this is is this particular function

0:35:43.259,0:35:48.309
smooth say one loss it's basically a one far away and it's sort of l2 nearby and

0:35:48.309,0:35:53.619
that presents sometimes that's called a Google loss some people call this also

0:35:53.619,0:36:02.470
elastic Network because it's an old paper from the 1980s or 1990s that kind

0:36:02.470,0:36:08.589
of proposed this this kind of objective function for different purpose so that's

0:36:08.589,0:36:16.390
useful that was advertised by was gaerste confess our CNN paper for and

0:36:16.390,0:36:20.140
it's used quite a bit in Cuba division for survivors purposes again it's for

0:36:20.140,0:36:29.680
protecting against outliers sharper jesus also sharper results now when we

0:36:29.680,0:36:42.880
do like image prediction sharper than using the MSE not particularly I mean

0:36:42.880,0:36:49.119
it's it's just like the MSE for small errors okay so that doesn't make any

0:36:49.119,0:36:54.609
difference but it it doesn't or maybe I misunderstood what your point

0:36:54.609,0:36:59.769
was sorry I was trying to compare the l1 versus the in the l2 that the l2 gives

0:36:59.769,0:37:04.359
us like usually blurry blurry predictions whenever we try to do

0:37:04.359,0:37:09.400
prediction by using like the l2 minimizing the l2 weather whereas like

0:37:09.400,0:37:14.259
people are minimizing the l1 in order to have like sharper overall predictions

0:37:14.259,0:37:19.869
okay so if you take if you take a bunch of points okay if you take a bunch of

0:37:19.869,0:37:26.200
y-values okay and you ask the question what value so you take a bunch of points

0:37:26.200,0:37:30.710
on on why okay yeah you ask the question what value of y minimizes

0:37:30.710,0:37:35.710
the squirrel Ellis the answer is that it's the average of all the whites okay

0:37:35.710,0:37:40.460
okay so if you so if for a single X you have a whole bunch of wise which means

0:37:40.460,0:37:47.090
you have noise in your data your system will want to produce the average of all

0:37:47.090,0:37:52.340
the ways that you're observing okay and if the Y you're observing is not a

0:37:52.340,0:37:57.200
single value but is I don't know an image the average of a bunch of images

0:37:57.200,0:38:00.730
is a blurry image okay that's why you get those very effects

0:38:00.730,0:38:09.860
now with air one the value of y that minimizes the l1 norm the l1 distance so

0:38:09.860,0:38:13.610
basically the sum of the absolute values of the differences between the value

0:38:13.610,0:38:17.830
you're considering and all the points all the white points that's the median

0:38:17.830,0:38:26.330
okay so it's it's a given point all right uh-huh and I see media and of

0:38:26.330,0:38:34.280
course it's not blurry it's just an image although it's kind of difficult to

0:38:34.280,0:38:42.650
define in multiple dimensions but so one problem with this loss is that it has a

0:38:42.650,0:38:48.440
scale right so here the transition here is at point five but why should it be at

0:38:48.440,0:38:52.430
point five you know it could be it depends what the scale of your of your

0:38:52.430,0:39:01.700
errors are okay negative exactly who lost this is really not the negative of

0:39:01.700,0:39:06.190
likelihood loss I'm not sure why it's called this way by torch but basically

0:39:06.190,0:39:11.810
here imagine that you have an X vector coming out okay and you your loss

0:39:11.810,0:39:17.740
function is there is one correct X okay so imagine each X correspond to a score

0:39:17.740,0:39:22.550
for lies a multi-class classification right so you have a desired class which

0:39:22.550,0:39:27.680
is one particular index in that vector okay now what you want is you want to

0:39:27.680,0:39:32.390
make that score as large as possible okay if those scores are likelihoods

0:39:32.390,0:39:38.600
then this is minimum negative log likelihood if those scores are log

0:39:38.600,0:39:42.380
likelihoods and this is maximum likelihood or minimum negative log

0:39:42.380,0:39:45.690
likelihood okay but there is nothing in this module

0:39:45.690,0:39:49.380
that actually specifies that the elds have to be log likelihoods so this is

0:39:49.380,0:39:55.020
just you know make my desired component as large as possible that's it

0:39:55.020,0:40:02.070
if you put negative signs in front so now you you can interpret the axis as

0:40:02.070,0:40:06.630
energies as opposed to scores okay they're not called six-course they're

0:40:06.630,0:40:13.440
like they're like penalties if you want but it's the same so the formula here

0:40:13.440,0:40:20.160
says you know just pick the X that happens to be the correct one for one

0:40:20.160,0:40:25.920
sample in the batch and make that score as large as possible now this particular

0:40:25.920,0:40:32.630
one allows you to give a different way to different to different categories

0:40:32.630,0:40:38.280
which is W those w's it's a it's a weight vector that gives a way to each

0:40:38.280,0:40:43.140
of the each of the categories it's useful in a lot of cases particularly if

0:40:43.140,0:40:51.390
you have widely different frequencies for the for the categories you might

0:40:51.390,0:40:56.990
want to increase the weight of samples for which you have a small number of

0:40:56.990,0:41:02.510
examples I mean for categories for which you have a small number of samples

0:41:02.510,0:41:08.580
however I'm actually not a big fan of this of this I think it's a much better

0:41:08.580,0:41:16.590
idea to just increase the frequency of the of the samples from the classes for

0:41:16.590,0:41:20.280
the class I have you know that appears rarely so that you equalize the

0:41:20.280,0:41:26.840
frequency the frequencies of the classes when you train it's much better because

0:41:26.840,0:41:33.180
it exploits stochastic gradient in a better way okay so so the bottom line of

0:41:33.180,0:41:40.260
that is if you have let me actually draw a picture of this so let's say you have

0:41:40.260,0:41:45.000
a problem where you have tons of samples for category one and then the small

0:41:45.000,0:41:48.810
number of samples for category two in a tiny number of samples for category

0:41:48.810,0:41:55.500
three you could so let's say you know here you have I don't know a thousand

0:41:55.500,0:41:59.670
samples and here you have 500 samples and you have I don't know

0:41:59.670,0:42:06.119
200 samples right so you could do is using this this kind of weight function

0:42:06.119,0:42:12.869
you could give this a weight of of 1 and this guy weight of 2 and this guy weight

0:42:12.869,0:42:17.099
of 5 and then you can equalize the weights if you want it's really better

0:42:17.099,0:42:19.650
to make sure that the way it's normalized to 1 that would be probably a

0:42:19.650,0:42:26.549
better idea but what I recommend is not that what I recommend is when you pick

0:42:26.549,0:42:36.660
your samples you basically pick one sample from class 1 and then one sample

0:42:36.660,0:42:40.529
from class 2 and super from class 3 and then you know you keep doing this during

0:42:40.529,0:42:46.950
your training session and when you get to the end of class 3 you go back to the

0:42:46.950,0:42:52.200
beginning ok so you keep going here but here you go back to the first sample

0:42:52.200,0:42:58.380
keep going here go back and now you have the second sample ok and now you get to

0:42:58.380,0:43:05.490
the end of class to go back to the start ok so the next sample is going to be

0:43:05.490,0:43:13.380
here here and here and then the next one here here and here here here and then

0:43:13.380,0:43:19.380
this guy wraps around again etc right so you basically have equal probability

0:43:19.380,0:43:23.519
equal frequencies for all the categories but just going through those kind of

0:43:23.519,0:43:29.789
circular buffers more often for categories for which you have fewer

0:43:29.789,0:43:37.470
samples ok once you should absolutely never do is equalize the frequencies by

0:43:37.470,0:43:42.869
by just not using all the samples in categories that are frequent I mean

0:43:42.869,0:43:46.680
that's horrible you should never let any data on the floor it's never any reason

0:43:46.680,0:43:52.680
to leave that on the floor ok now here's a problem with this the problem with

0:43:52.680,0:43:56.609
this is that after you've trained your your neuron that to do this you know one

0:43:56.609,0:44:02.430
that does not know about the relative likelihood the relative frequencies of

0:44:02.430,0:44:07.230
the samples and so let's say this is a system that those medical diagnoses it

0:44:07.230,0:44:14.260
doesn't know that the common cold is a way more frequent than you know lung

0:44:14.260,0:44:22.869
cancer or something right so what you need to do in the end is do a pass a few

0:44:22.869,0:44:28.150
passes perhaps where you can fine tune your system so that with the actual

0:44:28.150,0:44:31.750
frequencies of the categories and the effect of this is going to be for the

0:44:31.750,0:44:38.490
system to adapt the biases at the output layer so that the likelihood of you know

0:44:38.490,0:44:43.390
diagnosis corresponds to the the frequency of it right it's gonna favor

0:44:43.390,0:44:47.200
things that are more frequent the reason why I don't want to do this during the

0:44:47.200,0:44:53.560
entire training is because if you train a multi-layer net the the the system

0:44:53.560,0:44:57.940
basically never develops the right features for rare cases and they have

0:44:57.940,0:45:05.560
spoken about this already in the class in in past weeks to kind of recycle the

0:45:05.560,0:45:10.660
example of medical school you you don't spend when you go to medical school you

0:45:10.660,0:45:16.089
don't spend time studying the food that is proportional to the frequency of the

0:45:16.089,0:45:21.099
food with respect to very rare diseases for example right you spend basically

0:45:21.099,0:45:24.820
the same time studying all the diseases in fact you spend more time studying

0:45:24.820,0:45:30.070
complicated one which usually tend to be rarer and that's because you need to

0:45:30.070,0:45:33.640
develop the features for it okay and then you need to kind of correct for the

0:45:33.640,0:45:39.160
fact that you know those rare diseases are rare so you don't do that that you

0:45:39.160,0:45:46.900
know you don't suspect the diagnosis for rare diseases very often because you

0:45:46.900,0:45:58.300
know it's rare okay so that's all for for weights question tributo so you'll

0:45:58.300,0:46:05.260
be using this a lot of course and cross country pillows is a kind of merging of

0:46:05.260,0:46:10.900
two things merging of lot softmax function and negative low likelihood

0:46:10.900,0:46:16.150
loss okay and so and the reason why you want to have this is for numerical

0:46:16.150,0:46:18.510
reasons so the locks off max is you know

0:46:23.150,0:46:26.660
basically the softmax followed by log right so you first compute the softmax

0:46:26.660,0:46:31.369
then you do the log if you do softmax and then log and you back propagate

0:46:31.369,0:46:36.680
through this you might have gradients in the middle between the log and the

0:46:36.680,0:46:44.780
softmax that end up being infinite so for example if if the the maximum value

0:46:44.780,0:46:48.160
of one of the stars max is close to one and some of the other ones are close to

0:46:50.990,0:46:55.430
zero you take the log you get something that's close to minus infinity you back

0:46:55.430,0:46:59.090
propagate through the log you get something that's close to infinity okay

0:46:59.090,0:47:07.040
because the the slope of vlog goes to zero is very very close to infinity but

0:47:07.040,0:47:10.849
now you multiply this by a soft max that is saturated so it's multiplied by

0:47:10.849,0:47:13.730
something that's very close to zero so in the end you get a reasonable number

0:47:13.730,0:47:19.280
but because the intermediate numbers are close to infinity or 0 you multiply plus

0:47:19.280,0:47:22.940
something that's close to plus infinity by something that's close to 0 you get

0:47:22.940,0:47:26.750
numerical issues so you don't want to separate log and soft max you want to do

0:47:26.750,0:47:30.680
lots of Max in one in one go it simplifies the formula it makes the

0:47:30.680,0:47:37.660
whole thing much more stable numerically and for similar reasons you also want to

0:47:37.660,0:47:42.740
merge lots of Max and get you really good loss so basically if you have locks

0:47:42.740,0:47:45.589
off Max and you get you've log-likelihood loss it says I got a

0:47:45.589,0:47:48.890
bunch of weighted sums and a percent through soft max I'm going to take the

0:47:48.890,0:47:54.950
log of those and then I want to make the the output of the log short max for the

0:47:54.950,0:47:59.420
correct class as large as possible okay that's what the negative load actually

0:47:59.420,0:48:02.869
lost does it wants to make the score of the correct class as large as possible

0:48:02.869,0:48:08.750
we saw that just a minute ago right when you back propagate through the blocks

0:48:08.750,0:48:12.230
off max as a consequence it's going to make the score of all the other classes

0:48:12.230,0:48:19.580
as small as possible right because of the normalization and

0:48:19.580,0:48:26.109
so that you know that's why sometimes the the whole idea of sort of building a

0:48:26.109,0:48:30.440
network by you know modules sometimes there is an advantage instead

0:48:30.440,0:48:44.219
of merging in modules into single wine by end right so so the cross

0:48:44.219,0:48:50.269
entropy does in fact this explains a little bit you know those numerical

0:48:50.269,0:48:58.410
simplifications so the loss you know takes an X Factor and a category a

0:48:58.410,0:49:04.049
desired category a class okay and computes the negative log of the softmax

0:49:04.049,0:49:11.279
applied to the vector of scores but the one that's on the numerator numerator

0:49:11.279,0:49:18.029
here is the the X of the index of the correct class okay so that's that's your

0:49:18.029,0:49:22.440
loss the negative log of exponential the score of the correct class divided by

0:49:22.440,0:49:27.930
the sum of the Exponential's all the scores okay you can think of the X's as

0:49:27.930,0:49:38.759
negative energies okay it's completely equivalent now when you do the math and

0:49:38.759,0:49:42.329
you simplify your the log and the Exponential's gonna simplify and so you

0:49:42.329,0:49:45.450
just get the score of the correct class the negative score of the correct class

0:49:45.450,0:49:51.059
okay so to make that small you make the score large and then press the log of

0:49:51.059,0:49:54.509
the sum of the Exponential's of the scores of all the other class to make

0:49:54.509,0:50:01.529
that small you make all the edges small negative as far as fat you know as

0:50:01.529,0:50:06.029
negative as possible okay so this will make the score of the correct class

0:50:06.029,0:50:13.769
large like this core of everything else small again like in the aisle you can

0:50:13.769,0:50:18.989
you can have a weight per category also there is a physical interpretation right

0:50:18.989,0:50:25.499
of the cross entropy right okay so why is it called course entropy because it

0:50:25.499,0:50:27.660
is the cross entropy between two distributions

0:50:27.660,0:50:31.049
it's the KL divergence really between two distributions

0:50:31.049,0:50:37.079
it doesn't appear clearly here in this formula but think of the softmax applied

0:50:37.079,0:50:41.339
to the X vector as a distribution okay so take the X Factor's this course

0:50:41.339,0:50:46.890
rather than to a softmax you get a bunch of numbers between 0 & 1 that's 1 2

0:50:46.890,0:50:52.020
and now you have a desired distribution and the desired distribution the target

0:50:52.020,0:50:56.700
distribution if you want is one in which all the wrong categories had zero and

0:50:56.700,0:51:01.410
the correct category has one okay now compute the KL divergence between those

0:51:01.410,0:51:11.040
two distributions okay so it's the sum over indices of the correct probability

0:51:11.040,0:51:19.280
okay which is zero for except for one term times the ratio between the log of

0:51:19.280,0:51:24.750
the the probability that the system produces and the correct probability

0:51:24.750,0:51:31.440
which is one okay so all of those terms you know reduce to kind of a single term

0:51:31.440,0:51:36.630
which is just the one for which the correct probability term is one okay so

0:51:36.630,0:51:41.010
we end up with this with this term it's just a negative log of the softmax

0:51:41.010,0:51:46.230
output for the correct class okay we can use this as a cross entropy between the

0:51:46.230,0:51:51.180
distribution produced by the system and the one hot vector corresponding to the

0:51:51.180,0:51:55.970
desired distribution if you want okay so now there is a there would be another

0:51:55.970,0:52:01.140
can a more sophisticated version of this which would be the actual KL divergence

0:52:01.140,0:52:04.620
between the distribution produced by the system and a distribution that you

0:52:04.620,0:52:07.950
propose whatever it is a target distribution which now is not binary

0:52:07.950,0:52:11.820
it's not the one hard vector anymore but it's just a vector of numbers and that's

0:52:11.820,0:52:21.960
called the KL divergence in fact it's we'll see it in a minute so Kerala

0:52:21.960,0:52:25.620
vergence is a kind of you know it's not a distance because it's not symmetric

0:52:25.620,0:52:31.980
but it's a sort of a divergence between between distributions discrete

0:52:31.980,0:52:40.400
distributions okay so this one is a bit of a kind of a extension if you want of

0:52:40.400,0:52:48.110
lakhs of Max and it's a version of it that is applicable for very very large

0:52:48.110,0:52:53.580
categorization so if you have many many many categories what you might want to

0:52:53.580,0:52:58.690
do is kind of cut some corners you don't want to compute the giant softmax over

0:52:58.690,0:53:04.390
million categories or maybe even more so there you can sort of basically ignore

0:53:04.390,0:53:10.390
the ones that are small and you know kind of use tricks to kind of you know

0:53:10.390,0:53:16.270
improve the speed of the of the computation and this is what this does

0:53:16.270,0:53:19.300
I'm not going to go into the details exactly what it does because actually I

0:53:19.300,0:53:22.839
don't know the details but but it's basically an efficient approximation

0:53:22.839,0:53:34.089
softmax for a very very large number of categories so this is a special case of

0:53:34.089,0:53:40.000
course entropy when you only have two categories and in that case it kind of

0:53:40.000,0:53:45.310
reduces to something simple so this does not include stuff max this is just a

0:53:45.310,0:53:50.589
quest entropy when you have two categories and as I as I said before the

0:53:50.589,0:54:02.859
the the cross entropy loss is the sum of our categories of the probability I mean

0:54:02.859,0:54:06.849
some of our indices or some of our categories of the probability for the

0:54:06.849,0:54:12.750
target the target probability for that category times the ratio between the log

0:54:12.750,0:54:21.550
of the probability for of produced by the system divided by the probability of

0:54:21.550,0:54:27.220
the target category and if you work it out for two categories necessarily one

0:54:27.220,0:54:33.130
score is one minus the other one if you have two exclusive categories and and it

0:54:33.130,0:54:39.609
comes down to this okay now this presupposes that x and y

0:54:39.609,0:54:44.829
is x and y are kind of probabilities they have to be between strictly between

0:54:44.829,0:54:47.589
0 & 1 I mean not strictly but well kind of

0:54:47.589,0:54:55.540
strictly because otherwise the logs gonna blow up here is the KL divergence

0:54:55.540,0:55:01.650
process I was to tell you about earlier so here it's the

0:55:01.650,0:55:10.019
I mean it it's it's real turn here in a funny form but it's basically the here

0:55:10.019,0:55:13.980
again it's it sort of assumes this is another one I was telling you about

0:55:13.980,0:55:19.410
earlier actually this one is also a simplified one when you have a a one hot

0:55:19.410,0:55:28.799
distribution for the target so why is it's a category but it has the

0:55:28.799,0:55:32.069
disadvantage of not being merged with something like softmax or log softmax so

0:55:32.069,0:55:42.420
it may reach I mean it may have kind of numerical issues again it assumes x and

0:55:42.420,0:55:46.970
y are you know distributions this is values personal loss okay so

0:55:54.089,0:56:00.509
this version of binary choice entropy here takes scores that haven't gone

0:56:00.509,0:56:06.980
through sigmoid so this one does not assume that X the x's are between 0 & 1

0:56:06.980,0:56:12.210
it just takes you know values whatever they are and it it passes them to a

0:56:12.210,0:56:17.430
sigmoid to make sure there are between 0 & 1 strictly ok and so that is more

0:56:17.430,0:56:23.369
likely to be a numerically stable it's a bit the same ideas kind of emerging

0:56:23.369,0:56:33.480
locks off max and you get your lucky hood very yeah same thing here that's

0:56:33.480,0:56:37.950
what I was talking okay margin losses so this is sort of a important category of

0:56:37.950,0:56:50.579
losses those losses basically say if I have in this case two inputs the last

0:56:50.579,0:56:56.579
function here says I want one input to be larger than the other one by at least

0:56:56.579,0:57:02.160
two margin okay so imagine the two inputs or scores for two categories you

0:57:02.160,0:57:04.680
want the score for the correct category to be larger than the score for the

0:57:04.680,0:57:08.579
incorrect category but it hits some margin that you passed through the

0:57:08.579,0:57:14.730
system and that's the the formula you see down there so it's basically a him

0:57:14.730,0:57:19.260
okay and it takes the difference between the two scores and so why is a binary

0:57:19.260,0:57:22.760
variable this plus one or minus one and it controls whether you want X to be

0:57:22.760,0:57:28.430
larger than x1 to be larger than x2 or whether you want x2 to be larger than x1

0:57:28.430,0:57:32.910
okay we basically give you two scores and you tell it which one you want to be

0:57:32.910,0:57:39.150
the larger score and then the cost function says you know if this one is

0:57:39.150,0:57:43.380
larger than that one by at least a margin then the cost is zero if it's if

0:57:43.380,0:57:47.609
if it's smaller than the margin or if it's in the other direction and the cost

0:57:47.609,0:58:00.150
increases linearly okay so that's called a hingeless okay so that's very useful

0:58:00.150,0:58:14.160
for a number of different things we've we've seen an example of this in so yeah

0:58:14.160,0:58:19.940
for example so this is sort of a margin ranking class so you have two values but

0:58:19.940,0:58:24.390
there are sort of is a simplified version of it I mean there's a simpler

0:58:24.390,0:58:29.430
version of it which I don't have here for some reason we only have an X okay

0:58:29.430,0:58:35.660
so basically the loss is max of 0 and minus X times the margin and it just

0:58:39.630,0:58:46.020
wants to make to make X smaller than the margin right and so this is sort of a

0:58:46.020,0:58:51.000
special case where you have a ranking between two scores of two categories so

0:58:51.000,0:58:56.280
here is how you would use this for classification you would basically run

0:58:56.280,0:59:01.770
your classifier you would get scores ok idea so before you do any non-linearity

0:59:01.770,0:59:07.020
weighted sums and then you know the correct category so you say I want this

0:59:07.020,0:59:13.050
correct category to have a high score and then what you do is you take another

0:59:13.050,0:59:18.480
category that has the most offending score so either another category so a

0:59:18.480,0:59:23.070
category that is incorrect that has a higher score than the correct one or

0:59:23.070,0:59:27.850
that has a lower score but the lowest score is too close ok so you take the

0:59:27.850,0:59:34.240
the category that whose core is the closest to the to the correct one or who

0:59:34.240,0:59:40.030
score is higher than the correct one and you feed those two scores to the last

0:59:40.030,0:59:43.510
function like this so basically it's going to push up the score in the

0:59:43.510,0:59:46.690
correct category push down the score the incorrect category until the difference

0:59:46.690,0:59:52.630
is that eastern margin equal to the margin okay that's you know a perfectly

0:59:52.630,0:59:57.160
good way of training something in the context of an energy-based model for

0:59:57.160,1:00:02.310
example that's the sort of things you might want to do you might want to say X

1:00:02.310,1:00:08.230
1/4 minus x1 is the energy I mean X you know minus X 1 will be the energy of the

1:00:08.230,1:00:13.540
correct answer and minus X 2 would be the energy of the incorrect answer like

1:00:13.540,1:00:19.240
an a contrastive term an incorrect answer and you want to push down the

1:00:19.240,1:00:22.060
energy of the correct answer which are the energy of incorrect answer so that

1:00:22.060,1:00:27.870
the difference is at least some margin okay can use this kind of laws for that

1:00:27.870,1:00:31.630
the to play Clause is gonna be refinement on this so this is used a lot

1:00:31.630,1:00:37.420
for metric learning for the kind of Sammys nets that each n was each and

1:00:37.420,1:00:46.290
mister I was talking about last week and and there the idea is let's say I have a

1:00:47.010,1:00:53.620
distance so let's say I have three samples I have one sample and another

1:00:53.620,1:00:57.040
sample is very similar to it I've run them through to commercial Nets I get

1:00:57.040,1:01:01.210
two vectors I compute the distance between those two vectors D of a IP I

1:01:01.210,1:01:05.620
for example okay I want to make this distance as small as possible because

1:01:05.620,1:01:10.540
that's the core example and then I take two samples that I know are semantically

1:01:10.540,1:01:15.430
different ok the image of a cat and whatever table and I want to make the

1:01:15.430,1:01:18.670
vector as far away from each other so I compute the distance I want to make this

1:01:18.670,1:01:27.100
distance large alright now I can insist that the first is done B zero and it can

1:01:27.100,1:01:29.590
insist that the second distance be larger than the margin that would be

1:01:29.590,1:01:35.260
kind of a margin loss type type thing but what I can do is one of those

1:01:35.260,1:01:39.700
triplet margin notes where I say the only thing I care about is that the

1:01:39.700,1:01:43.500
distance that I get for the good is smaller than the distance that I get

1:01:43.500,1:01:47.580
for the bad pit I don't care if the distance is small I just wanted to be

1:01:47.580,1:01:54.450
smaller than the distance for the bad pair okay and that's what those ranking

1:01:54.450,1:02:02.670
goes to a bunch of those were I mean one of the first I think that was proposed

1:02:02.670,1:02:08.820
was by Chisholm Weston and Sammy bengio back when Jaden Weston was today Google

1:02:08.820,1:02:15.869
and they used this to train kind of an image search system for Google so back

1:02:15.869,1:02:19.350
then I'm not sure is true anymore but back then you would type a query on

1:02:19.350,1:02:24.780
Google Google would encode that query into a vector then we compare this to a

1:02:24.780,1:02:30.440
whole bunch of vectors describing images that I've been previously indexed and

1:02:30.440,1:02:35.040
and then would kind of retrieve the images whose vector were close to the

1:02:35.040,1:02:40.260
one that that you had and the way you train those those the networks that

1:02:40.260,1:02:45.180
compute those vectors in that case back then it was in your networks actually as

1:02:45.180,1:02:50.900
you train them with those trip pillows okay so you said good hits for my search

1:02:50.900,1:02:55.020
should should have a distance between the vectors that is smaller than any bad

1:02:55.020,1:02:58.500
hit and I don't care if the distance is small I just wanted to be smaller than

1:02:58.500,1:03:10.140
for that it's any question that's kind of a graphical explanation of this where

1:03:10.140,1:03:17.310
P is a positive sample so it's you know similar to a so a is the sample you

1:03:17.310,1:03:21.240
considered P is kind of a positive sample and N is a negative sample

1:03:21.240,1:03:27.869
contrast example you want to push anyway and bring P closer and as soon as P is

1:03:27.869,1:03:38.220
closer than n by some margin you you stop pushing and pulling your soft

1:03:38.220,1:03:41.790
versions of this and in fact you can think of nce the the kind of loss

1:03:41.790,1:03:46.920
function that hn was talking about as kind of a soft version of that where you

1:03:46.920,1:03:51.450
basically you have a bunch of positives and bunch of negatives or you have one

1:03:51.450,1:03:57.140
positive and bunch of negatives and you run them through softmax and you say

1:03:57.140,1:04:03.820
I want this the you know e to the minus distance for the correct one to be

1:04:03.820,1:04:11.360
smaller than you know e to the minus the other one so it kind of you pushes the

1:04:11.360,1:04:15.170
positive closer to you and pushes the other ones further to you but now with

1:04:15.170,1:04:19.730
some sort of stuff maxi that sort of exponential decay as opposed to sort of

1:04:19.730,1:04:27.440
a hard margin so in PI torch you have things that allow you to have multi

1:04:27.440,1:04:33.290
labels so this allows you to basically give multiple correct outputs so instead

1:04:33.290,1:04:38.750
of you know this is a ranking Mouse but it's serving sitting that there is only

1:04:38.750,1:04:42.680
one correct category and you know you you you want a high score for the

1:04:42.680,1:04:46.430
correct category and that's core for everything else here you can have a

1:04:46.430,1:04:50.360
number of categories for which you want high scores and then all the other ones

1:04:50.360,1:04:54.200
will get pushed away all right we'll get the scores will be

1:04:54.200,1:04:59.810
pushed down so here it's a it's a hinge loss but you do a sum of those this

1:04:59.810,1:05:07.520
hinge loss overall categories and and for each category if the category is a

1:05:07.520,1:05:12.170
desired one you push it up if it's a non desire one you push it down which is

1:05:12.170,1:05:19.340
what this video formula says and of course you have the soft version of this

1:05:19.340,1:05:26.000
which I'm not going to go into the details of and the multi margin version

1:05:26.000,1:05:35.060
of it so this pushing and pulling for metric learning for embedding for Sammis

1:05:35.060,1:05:40.400
nets that I would say telling you about it's actually kind of all implemented if

1:05:40.400,1:05:46.700
you want in one of those engine vending laws so engine 30 loss is a loss for

1:05:46.700,1:05:51.920
Siamese nets that kind of pushes things are semantically similar to you and push

1:05:51.920,1:05:56.810
away things that are not okay so the Y variable indicates whether the pair you

1:05:56.810,1:06:00.590
are or whether the score you are giving to the system is one that should be

1:06:00.590,1:06:05.780
pushed up a window should be pushed down and it it chooses the hinge loss that

1:06:05.780,1:06:12.230
makes the score positive if Y is plus one and it makes a

1:06:12.230,1:06:19.240
score negative by some margin Delta if if Y is minus one

1:06:25.480,1:06:29.890
very often when you are doing Siamese Nets the way you compute the

1:06:29.890,1:06:32.950
similarity between two vectors is not through Euclidean distance between

1:06:32.950,1:06:39.160
cosine distance so the one minus the cosine between the of the angle between

1:06:39.160,1:06:42.790
the two vectors this is basically a normalized Euclidean distance if you

1:06:42.790,1:06:47.950
want you can think of it this way the advantage of this is that whenever you

1:06:47.950,1:06:52.210
can push the distance whenever your two vectors and you want to make the

1:06:52.210,1:06:55.690
distance as large as possible there's a very easy way for the system to get away

1:06:55.690,1:07:00.310
with it by making the the two vectors very large very long you know not

1:07:00.310,1:07:04.390
pointing in the same direction and make them very very long so now the distance

1:07:04.390,1:07:08.109
would be large but of course that's not what you want you don't want the system

1:07:08.109,1:07:11.950
to just make the the vectors bigger you wanted to actually rotate the vector in

1:07:11.950,1:07:15.820
the right direction so you you normalize the vectors and then computer normalize

1:07:15.820,1:07:19.390
Euclidean distance and that's basically what this does and what this does is

1:07:19.390,1:07:24.369
that it for positive cases it tries to make the vectors as aligned with each

1:07:24.369,1:07:31.720
other as possible and for negative pairs it tries to make the cosine smaller than

1:07:31.720,1:07:35.890
the particular margin the margin in that case which should probably be something

1:07:35.890,1:07:42.910
that kind of is close to zero so you want the the cosine you know in a high

1:07:42.910,1:07:47.500
dimensional space there's a lot of space near the equator of the sphere of the

1:07:47.500,1:07:50.950
high dimensions here okay so all your points now are normalized so this here

1:07:50.950,1:07:55.540
what you want is samples that are symmetrically similar to you should be

1:07:55.540,1:07:59.589
close to you the samples that are dissimilar should be orthogonal you

1:07:59.589,1:08:03.640
don't want them to be Oppo opposed because there is only one point in the

1:08:03.640,1:08:09.330
South Pole whereas on the equator is a very very high large space the entire

1:08:09.330,1:08:15.250
sphere minus one dimension basically okay so you can make the margin just you

1:08:15.250,1:08:18.969
know some small so small positive value and and then you get the entire equator

1:08:18.969,1:08:22.480
essentially of this here which contains you almost the entire volume of the

1:08:22.480,1:08:25.350
sphere and I dimension since you see loss this is a little more

1:08:30.969,1:08:37.269
complicated because that's a task that is basically uses structure prediction

1:08:37.269,1:08:42.940
what's called structure prediction so this is I sort of briefly talked about

1:08:42.940,1:08:48.909
it very quickly a few weeks ago was something very similar to this so this

1:08:48.909,1:08:56.949
is it also is applicable when you your output is a sequence of vectors and

1:08:56.949,1:09:05.230
scores where the vectors correspond to scores of categories okay and so you

1:09:05.230,1:09:09.639
have so your system computes a vector of such score so imagine for example speech

1:09:09.639,1:09:15.069
recognition system speech recognition system every 10 milliseconds gives you a

1:09:15.069,1:09:20.099
vector of probabilities for what the sound being pronounced right now is and

1:09:20.099,1:09:23.589
the number of categories usually is quite large on the order of a few

1:09:23.589,1:09:28.119
thousand okay so give you basically a softmax vector of a size you know

1:09:28.119,1:09:34.619
typically three thousand let's say one of those every 10 milliseconds all right

1:09:34.619,1:09:38.409
and what you like you know you have a desired output and the desired output is

1:09:38.409,1:09:42.609
what word was being pronounced and a word that's being pronounced that

1:09:42.609,1:09:48.519
corresponds to kind of a particular sequence of sounds if you want that you

1:09:48.519,1:09:53.380
might you might know so what you need now is a course that basically is row if

1:09:53.380,1:10:00.820
that sequence looks like like that sequence but what you might allow is for

1:10:00.820,1:10:10.090
the input sequence to repeat some of the sounds if you want right so so for

1:10:10.090,1:10:16.630
example you know my my cost to make the target might be the word seven they say

1:10:16.630,1:10:20.440
and it's pronounced really quickly 7 so you basically have you know a very small

1:10:20.440,1:10:25.059
number of samples of each sound in the sequence but then perhaps the the person

1:10:25.059,1:10:28.750
who is pronouncing the the word now that user as a training sample pronounced it

1:10:28.750,1:10:35.559
very slowly like seven right so now the the first the first takes you know

1:10:35.559,1:10:40.030
several several frames of 10 milliseconds that should all be mapped

1:10:40.030,1:10:43.050
to the the same instance of the earth in the indian foot

1:10:47.560,1:10:55.960
and i do that picture before but i gonna do it again right so the you have let's

1:10:55.960,1:11:06.730
see you have a sequence of scores coming out of soft max's let's say it's

1:11:06.730,1:11:12.640
actually better if there are energies but firstly TC they need to be and then

1:11:12.640,1:11:25.530
you have the target sequence and i think of this as some sort of matrix and each

1:11:25.530,1:11:30.850
entry in that matrix basically measures the distance between the two vectors

1:11:30.850,1:11:37.570
that are here okay so when i'm treating the matrix indicates how this vector

1:11:37.570,1:11:41.290
looks like that vector for example with the course entropy or something like

1:11:41.290,1:11:49.770
that okay or is quite error it doesn't matter what the last function is so now

1:11:49.800,1:12:05.290
if this is the word seven pronounced slowly okay and this has perhaps only

1:12:05.290,1:12:15.190
one instance of each sound you want all of those you know you would want all of

1:12:15.190,1:12:27.250
those vectors corresponding to the e to be mapped to that vector here okay so

1:12:27.250,1:12:33.010
you want to compute that cost of you know confusing that those all of those I

1:12:33.010,1:12:35.470
mean map matching those ease to that to that

1:12:35.470,1:12:39.100
e now of course here the system could use the correct answer so you don't have

1:12:39.100,1:12:43.420
much of a problem but if the target is seven but the word that was pronounced

1:12:43.420,1:12:51.780
here or the output that was produced by the system does not correspond to seven

1:12:51.780,1:12:55.210
that's that's that's when you run into into trouble so

1:12:55.210,1:13:01.420
here what you do is you find the best mapping from the input sequence to the

1:13:01.420,1:13:06.190
output sequence okay so the s gets mapped to the s the e to the e the V to

1:13:06.190,1:13:12.460
the V the east to the e and the end to the N so you get this kind of path if

1:13:12.460,1:13:15.880
you want that think of this as a path in the graph

1:13:15.880,1:13:20.350
and the way you determine this is basically by using a dynamic programming

1:13:20.350,1:13:23.890
algorithm the short path algorithm that figures out how do I get from here to

1:13:23.890,1:13:30.480
here you know path that minimizes the sum of the distance distances between

1:13:30.480,1:13:35.890
the the all the vectors of the distances between the vectors of you know all the

1:13:35.890,1:13:41.890
points are going through ok so there's a optimization respect to a latent

1:13:41.890,1:13:45.670
variable if you want okay and CGC basically decide for you right so you

1:13:45.670,1:13:50.230
give it two sequences and it computes the distance between them and you know

1:13:50.230,1:13:58.960
kind of the best kind of mapping between the two by allowing basically to to map

1:13:58.960,1:14:03.550
multiple input vectors to kind of a single one on the output it cannot

1:14:03.550,1:14:09.489
expand it it can only kind of reduce if you want and then that's done in a way

1:14:09.489,1:14:13.989
that you can back propagate gradient to it we'll come back to this two more

1:14:13.989,1:14:21.820
things like this at the end if you can oops so this is what this the target is

1:14:21.820,1:14:26.260
assumed to be many-to-one the alignment of the input to the target is assumed to

1:14:26.260,1:14:29.020
be miniature when which leave is the length of the target sequence such that

1:14:29.020,1:14:33.010
it must be smaller than the length of the input that's for the reason I just

1:14:33.010,1:14:37.930
explained okay so it's basically differentiable time-warping you could

1:14:37.930,1:14:43.420
think of it this way or sort of a module that does that any time marking or

1:14:43.420,1:14:48.400
dynamic programming and it's still differentiable the idea for this goes

1:14:48.400,1:14:54.340
back in the early 90s in the Lobo two species actually that's very old very a

1:14:54.340,1:15:00.550
good paper or resource to learn more about that dynamic programming algorithm

1:15:00.550,1:15:05.980
there actually that's kind of what I'm gonna talk about next I may not have

1:15:05.980,1:15:12.329
time to go through it but I'll try to okay but basically the last

1:15:12.329,1:15:17.909
part of the energy based model tutorial okay

1:15:17.909,1:15:24.239
so the initial base model tutorial the 2006 paper that we give you a reference

1:15:24.239,1:15:31.110
a link to a tutorial on energy based models the this the second part is all

1:15:31.110,1:15:38.219
about this kind of stuff essentially okay so it's more energy based models

1:15:38.219,1:15:42.510
but now in getting more of a supervised context if you want

1:15:42.510,1:15:52.050
so preliminary so before I get to this I want to come back to the sort of more

1:15:52.050,1:16:01.949
general formulation of energy based models and the idea that so if you want

1:16:01.949,1:16:06.300
to kind of define energy based models in the proper way these are the conditional

1:16:06.300,1:16:13.079
versions you have a a training set a bunch of pairs x y y I for I equals 1 to

1:16:13.079,1:16:18.869
P you have a loss function also the last functional L of ENS so it takes the

1:16:18.869,1:16:23.820
energy function computed by the system okay and the training set and it gives

1:16:23.820,1:16:28.139
you a scalar value now you can you can think of this as a functional functional

1:16:28.139,1:16:32.219
is a function of a function ok but in fact because the energy function itself

1:16:32.219,1:16:36.510
is parametrized by parameter W you can turn this functional into a loss

1:16:36.510,1:16:40.469
function which is not just a function of W another function of the energy

1:16:40.469,1:16:46.739
function okay and of course the the set of energy functions is called epsilon

1:16:46.739,1:16:53.909
here it's family tries by the parameter W which is taken within the set so

1:16:53.909,1:16:57.630
training consistent of course minimizing the the loss function or with respect to

1:16:57.630,1:17:02.400
W and finding the W that minimizes it and and so one question you might ask

1:17:02.400,1:17:05.429
yourself you know I went to a whole bunch of objective function loss

1:17:05.429,1:17:08.880
functions here and the question is if you are in an energy based framework

1:17:08.880,1:17:14.670
what loss functions are good ones and what all functions are bad ones how do

1:17:14.670,1:17:18.929
you characterize a loss function that actually will do something useful for

1:17:18.929,1:17:23.480
you ok so here is a general formulation of the

1:17:23.480,1:17:29.300
last function it's it's an average over training samples so here I'm kind of

1:17:29.300,1:17:34.070
assuming that it's invariant under permutation of the samples so an average

1:17:34.070,1:17:38.510
is as good as any other aggregation go-getting function so it's the average

1:17:38.510,1:17:44.150
of our training samples of a person for loss function capital L and it takes the

1:17:44.150,1:17:50.000
desired answer Y which could be just a category or it could be a whole image or

1:17:50.000,1:17:56.180
whatever and it takes the energy function where X the X variable X I is

1:17:56.180,1:18:03.200
is equal to X I the ice training sample the Y variable is undetermined okay so e

1:18:03.200,1:18:10.370
of W Y and X I is basically the entire shape of the energy function for values

1:18:10.370,1:18:15.200
of Y over values of Y for a given X okay X equal to X I and you can have a

1:18:15.200,1:18:22.790
regularizer if you want okay so here this is a loss functional again again of

1:18:22.790,1:18:26.090
course we have to design this loss function all so that it makes the energy

1:18:26.090,1:18:31.520
of correct answers small and the energy of incorrect answers large in some ways

1:18:31.520,1:18:38.060
right ok now we're going to go through a bunch of different types of loss

1:18:38.060,1:18:45.050
functions so one thing we could do is say my loss function is just going to be

1:18:45.050,1:18:50.180
the energy of the correct answer so I'm gonna place myself in the context of

1:18:50.180,1:18:53.960
energy based model my system produces scores I interpret those scores as

1:18:53.960,1:19:01.540
energies so high is bad good is good that means low is good as opposed to

1:19:01.540,1:19:13.460
positive scores and what I'm just going to do is define my energy functional as

1:19:13.460,1:19:16.820
unfortunately the energy function of the function of Y as simply the energy that

1:19:16.820,1:19:24.950
my model gives to the correct answer okay so basically I give it an X and I

1:19:24.950,1:19:27.890
give it the correct answer Y and as a system what energy do you give to that

1:19:27.890,1:19:33.110
pair and then I try to make that energy as small as possible okay so you have

1:19:33.110,1:19:36.700
this landscape of energy is here now we honor I showed you this

1:19:36.700,1:19:40.900
slide in the context of unsupervised super Ronnie here I'm showing to you in

1:19:40.900,1:19:44.710
the context of supervised Ronnie so imagine that one of the variables is X

1:19:44.710,1:19:49.180
and the other variable is y okay and the blue beads are training samples and you

1:19:49.180,1:19:54.940
want to make the energy of the blue beads as far as possible so you're

1:19:54.940,1:19:58.330
pulling down on the blue beads but you're not doing anything else and so as

1:19:58.330,1:20:01.510
a result depending on the architecture or your network if your network is not

1:20:01.510,1:20:07.300
designed properly or if it's designing a bit in no particular way it could very

1:20:07.300,1:20:10.570
well be that the energy function is going to become flat everywhere okay

1:20:10.570,1:20:13.630
you're just trying to make the energy of the correct answer small are you not

1:20:13.630,1:20:16.930
telling the system the energy of everything else should be higher and so

1:20:16.930,1:20:24.670
the system might just collapse all right so energy loss is not good in

1:20:24.670,1:20:29.100
that sense but there are certain situations where it's applicable because

1:20:29.100,1:20:35.500
if the shape of the energy function is such that it cannot make the you can

1:20:35.500,1:20:40.450
only make the energy of a single answer small all the other ones being larger

1:20:40.450,1:20:45.370
then you need to have a quadratic term okay and we've seen this in the context

1:20:45.370,1:20:49.720
of supervised learning people are completely lost about the loss

1:20:49.720,1:20:57.760
functional right okay so this is a function Al and it's a function of

1:20:57.760,1:21:03.400
another function e okay so it's called a functional because it's

1:21:03.400,1:21:07.360
a function of a function right it's not a function of a point it's a function of

1:21:07.360,1:21:15.730
a function now if that second function is permit rised by a parameter W then

1:21:15.730,1:21:18.430
you can say that the last function is actually a function of that parameter W

1:21:18.430,1:21:23.110
now it becomes a regular function okay that's what I had can you can you write

1:21:23.110,1:21:27.100
it down it's basically written here okay you can

1:21:27.100,1:21:33.520
either write the functional as if I can find it as L of e and s so that's a

1:21:33.520,1:21:39.700
functional because it's a function of e which itself is a function okay but e

1:21:39.700,1:21:45.610
itself is a function of W and so if I write the last function directly as a

1:21:45.610,1:21:49.290
function of W now it's just a regular function

1:21:50.190,1:21:59.920
okay yeah I mean I asked the question that was asked in the chat I yeah I was

1:21:59.920,1:22:17.530
kind of you I know before that you know for okay we've seen the negative log

1:22:17.530,1:22:25.380
likelihood loss before I talked about this so this is a loss function that

1:22:25.380,1:22:31.210
tries to make the energy of the correct answer so look at the rectangle in red

1:22:31.210,1:22:35.740
tries to make the energy of the correct answer as low as possible and then you

1:22:35.740,1:22:40.090
have the second term whatever beta log sum over all why's of e to the minus

1:22:40.090,1:22:49.330
beta e of WI X and this one is trying to make the energy of all Y's for this

1:22:49.330,1:22:53.320
given X as large as possible okay because the best way to make this term

1:22:53.320,1:22:58.150
small is to make those energies large because they enter in there as a

1:22:58.150,1:23:06.340
negative of negative its financial okay so this has this kind of pushing down on

1:23:06.340,1:23:13.360
the correct answer pushing up on incorrect answer behavior and we've seen

1:23:13.360,1:23:20.110
before we just talked about margin loss and and other types of losses here is

1:23:20.110,1:23:22.900
something that's called a perceptron loss because it's basically very similar

1:23:22.900,1:23:28.900
to I mean it's exactly the same as the loss that was used for the perceptron 60

1:23:28.900,1:23:34.480
years ago over 60 years ago so this one says I want to make the energy of the

1:23:34.480,1:23:43.410
correct answer small and the same time I want to make the energy of the smallest

1:23:43.410,1:23:49.660
the smallest energy for all answers as large as possible okay so pick the Y

1:23:49.660,1:23:54.280
that has the smallest energy in your system make that as large as you can the

1:23:54.280,1:23:57.760
same time picks the correct energy make that as small as you can now there is a

1:23:57.760,1:24:00.690
point at which the answer with the correct energy is going

1:24:02.360,1:24:07.370
to equal to the correct answer and so that difference can never be negative

1:24:07.370,1:24:13.970
okay because the first term is necessarily one term in that minimum and

1:24:13.970,1:24:22.010
so the difference is at best zero and for every other cases is in spot this

1:24:22.010,1:24:27.250
trick is positive it's only zero when the system gives you the correct answer

1:24:27.250,1:24:33.740
okay but this objective function does not prevent the system from giving the

1:24:33.740,1:24:38.510
very same energy to every answer okay so in that sense it's a bad energy it's a

1:24:38.510,1:24:43.670
bad loss function so bad a loss function because it it says I want the energy of

1:24:43.670,1:24:47.720
the correct answer to be small I want the energy of all the other answers to

1:24:47.720,1:24:51.170
be large but I don't insist that there is any difference between them so the

1:24:51.170,1:24:59.000
system can choose to make every answer the same energy and that's a class okay

1:24:59.000,1:25:02.510
so the spectrum loss is not good it's actually only good for linear systems

1:25:02.510,1:25:11.150
but it's not good for as a objective function for nonlinear systems so here

1:25:11.150,1:25:16.670
is a way to design an objective function that will always be good and you you

1:25:16.670,1:25:19.790
take the energy of the correct answer and you take the energy of the most

1:25:19.790,1:25:24.260
offending incorrect answer which means the value of y that is incorrect but at

1:25:24.260,1:25:30.920
the same time is the lowest energy of all the incorrect answers okay and your

1:25:30.920,1:25:35.600
system will work if that difference is negative in other words if the energy of

1:25:35.600,1:25:39.410
the correct answer is smaller than the energy of the most of any correct answer

1:25:39.410,1:25:45.710
but at least some quantity some margin okay so as long as your objective

1:25:45.710,1:25:49.550
function will you design it ensures that the energy of the correct answer is

1:25:49.550,1:25:52.580
smaller than the energy of the most of any correct answer by at least a margin

1:25:52.580,1:26:02.510
nonzero margin then you'll find your loss function is good okay so things

1:26:02.510,1:26:07.580
like kyndra's are good the heat loss basically says and we talked about this

1:26:07.580,1:26:11.210
just just before I want the energy of the correct answer to be smaller than

1:26:11.210,1:26:12.889
the energy of the most offending incorrect

1:26:12.889,1:26:18.679
which is denoted why I bar here but at least M okay this is what this house

1:26:18.679,1:26:22.610
function does it's a hinge loss and it wants to push down the energy of this

1:26:22.610,1:26:31.340
guy below the energy of that guy by at least this margin so this has a margin M

1:26:31.340,1:26:36.230
and this will you know if you train a system with this loss a well and he can

1:26:36.230,1:26:42.260
run the task he will run the task and probably produce the good answers the

1:26:42.260,1:26:47.210
heat loss the soft hinge loss which is in the context of Nigeria's models is

1:26:47.210,1:26:51.830
expressed this way basically instead of feeling the difference between the

1:26:51.830,1:26:55.670

0:00:00.020,0:00:03.451
All right, so we're going to talk about two or three topics today,

0:00:03.476,0:00:09.432
and the first one is going to be kind of a review of some of the functions that exist in Pytorch.

0:00:09.773,0:00:12.330
And kind of when and how to use them.

0:00:13.488,0:00:17.955
So, the first set of topics is about activation functions,

0:00:18.931,0:00:23.724
and there is a whole bunch of them defined in Pytorch,

0:00:23.749,0:00:28.015
and they basically come from, you know, various papers that people have written

0:00:28.046,0:00:33.991
where they claim that this or that particular objective function or activation function works better for their problem.

0:00:34.332,0:00:37.207
So, of course, everybody knows the ReLU.

0:00:37.279,0:00:41.382
That's very standard one, but there's lots of variations of ReLUs.

0:00:41.453,0:00:47.293
These ReLUs where the bottom part is not constant and set to zero.

0:00:47.293,0:00:52.507
but it can be allowed to change either only with the positive slope or force to be to have a negative slope,

0:00:52.729,0:00:57.361
or sometimes being random in the case of the randomized Leaky ReLU.

0:00:58.090,0:01:05.100
So, they have, you know, nice names like Leaky ReLU, PReLU, RReLU, random ReLU, etc.

0:01:06.394,0:01:12.083
So, Leaky ReLU is one where you allow the bottom part to have slight negative slope.

0:01:12.956,0:01:18.000
and that kind of prevents the issue that sometimes pops up that you know,

0:01:18.025,0:01:20.326
when ReLU is off, it doesn't get any gradient.

0:01:20.326,0:01:27.950
So, here you get a chance for that system that function to actually propagate gradient,

0:01:28.133,0:01:32.839
and perhaps do something useful can go all the way to kind of compete for ratification of the signal,

0:01:33.164,0:01:35.280
kind of like an absolute value if you want.

0:01:35.900,0:01:38.431
PReLU is one ....  yeah, go ahead.

0:01:39.090,0:01:44.375
The previous activation was is user using the discriminator in a GAN,

0:01:44.400,0:01:48.396
such that we always have gradients going backward for the generator,

0:01:48.833,0:01:53.650
and also this activation was necessary in order to train the very skinny network.

0:01:53.675,0:01:58.780
I show at the beginning of the class because again having like a very very skinny network,

0:01:58.939,0:02:02.900
it was basically impossible to get gradients flowing back,

0:02:02.900,0:02:08.689
because we were like ending up in one of the quadrants without, you know, where everything was zero out,

0:02:08.689,0:02:15.649
and then nothing would have been actually trained if you wouldn't have use, you know, this activation function

0:02:15.649,0:02:17.943
that allows me to get some kind of gradients

0:02:17.943,0:02:21.993
even if we are in the regions where we are trying to suppress the output.

0:02:22.242,0:02:23.900
So, yeah, right.

0:02:27.329,0:02:34.852
So, PReLU is fairly similar except that now the slope and the negative side can be just about anything.

0:02:37.031,0:02:43.395
And okay, what's interesting about those all those functions that we just saw is that they are scale-invariant

0:02:43.395,0:02:52.642
in the sense that they, you know, you can multiply the signal by two and the output will not be changed.

0:02:52.736,0:02:56.736
Yeah, I mean it would be multiplied, by two but otherwise unchanged, so they are equivalent to scale.

0:02:57.226,0:03:00.769
There's no sort of intrinsic scale in those functions, right?

0:03:01.290,0:03:04.081
Because there's only one non-linearity, and it's a sharp one.

0:03:05.004,0:03:05.760
So

0:03:06.007,0:03:08.497
Now we're getting into functions where the scale matters.

0:03:08.497,0:03:15.975
So, the amplitude of the incoming signal will affect the type of non-linearity that you're going to get,

0:03:16.212,0:03:18.073
and one of those is the Softplus.

0:03:18.073,0:03:22.621
So, Softplus is sort of a differentiable version of ReLU if you want.

0:03:23.075,0:03:25.599
It's kind of the soft version of positive part,

0:03:25.599,0:03:29.859
and it's usually parameterized as you can see at the top here.

0:03:29.931,0:03:32.776
1/β * log( 1+ exp(β*X) )

0:03:32.920,0:03:41.242
So it's kind of like the log sum exponential that we've been using a lot for supervised purpose, except here.

0:03:41.242,0:03:46.166
One of the terms in the sum is equal to 1 which is kind of like exponential zero if you want.

0:03:46.950,0:03:54.800
So, that looks like kind of a function that sort of asymptotically is the identity function for large positive values,

0:03:54.800,0:03:57.113
and asymptotically zero for negative values.

0:03:57.308,0:03:58.527
So, approximates the ReLU.

0:03:59.061,0:04:02.150
It has a scale parameter though, this β parameter,

0:04:02.519,0:04:06.210
The larger β, the more the function will look like a ReLU.

0:04:06.235,0:04:11.292
So the kink, the corner, will be kind of sharper if β goes to infinity.

0:04:12.764,0:04:15.570
But that function has a scale.

0:04:19.053,0:04:22.357
Now, you can parameterize those functions in various ways,

0:04:22.382,0:04:29.732
and this is sort of a another example of kind of a soft version of ReLU if you want

0:04:30.692,0:04:36.502
where here you use ReLU as a basis, something that you add a small constant to it, that kind of makes it smooth.

0:04:38.656,0:04:43.360
You know, I can't tell you that any of those has any particular advantage over the others.

0:04:44.377,0:04:49.159
It really depends on the problem, but they all have kind of similar properties if you want.

0:04:49.937,0:04:54.521
This also you can make sort of continuously closer to ReLU.

0:04:56.148,0:04:58.160
That's yet

0:04:58.272,0:05:03.100
Okay, so when difference here, in this case, is that

0:05:03.418,0:05:05.564
this guy actually goes negative. Right?

0:05:05.564,0:05:13.799
So, unlike the ReLU that has its minimum at zero, it's horizontal asymptote at zero, this guy goes below zero.

0:05:15.444,0:05:20.417
And that may or may not be advantageous depending on the application you have.

0:05:20.800,0:05:27.365
Sometimes it's advantageous because it allows the system to basically makes the average of the output zero

0:05:27.870,0:05:33.244
which is advantageous for certain types of gradient descent convergence.

0:05:33.309,0:05:38.271
The weights that are connected to units like this will see both positive and negative values

0:05:38.458,0:05:41.584
which will then converge faster than if they only see positive values.

0:05:43.201,0:05:47.991
So, it's been the same here, and it's just kind of a differently,

0:05:48.016,0:05:50.600
you know, a different parametrization of kind of the same thing if you want

0:05:53.733,0:05:55.010
with different properties.

0:05:55.090,0:05:59.977
So, of course, there's tons of variations of this, you know with various parameters with different properties,

0:06:01.915,0:06:11.275
and, you know, some of them that have particular properties that can relate them to gaussian distributions,

0:06:11.300,0:06:15.638
for example, this is not the cumulative distribution of a gaussian, but ...

0:06:16.969,0:06:20.543
Okay, so those were things that have one kinks in them,

0:06:20.846,0:06:22.873
and if the kink is sharp, there's no scale.

0:06:22.981,0:06:28.879
If the kink has some scale in it, there is some scale, but it's still sort of a single kink non-linearity.

0:06:29.008,0:06:31.115
Now, we're getting into nonlinearities that have two kinks.

0:06:31.440,0:06:34.283
Okay, so this one is basically a saturating ReLU.

0:06:34.529,0:06:38.324
I don't show why saturates at 6, you know, why not?

0:06:39.307,0:06:42.134
But, you know, why not parameterize this a little better,

0:06:42.159,0:06:45.619
so, here's a smooth function that you're familiar with

0:06:45.644,0:06:53.442
because it's used in Recurrent Nets, in Gated Recurrent Nets, in LSTM, in Softmax.

0:06:53.467,0:06:57.356
You know, basically, this is a two-way Softmax. You can think of it this way.

0:06:59.521,0:07:02.868
and this is just a function that goes kind of smoothly between 0 & 1.

0:07:04.167,0:07:11.445
It's sometimes called a Fermi-Dirac function as well because it derives from some work in physics, in statistical physics.

0:07:13.148,0:07:16.651
And then there is the hyperbolic tangent that we also talked about.

0:07:16.676,0:07:20.524
It's basically identical to the Sigmoid except it's centered.

0:07:20.549,0:07:22.995
So, it goes between -1 and +1 and it's a little

0:07:23.435,0:07:29.200
you know, it's twice the amplitude and the gain is a little different, but it plays the same role.

0:07:29.416,0:07:38.274
The advantage of hyperbolic tangent is that the output is you can expect the output to not have zero mean, but be close to having zero mean,

0:07:39.342,0:07:44.060
and again, that's advantageous for the weights that follow

0:07:44.085,0:07:48.262
because they see positive and negative values, and they tend to converge faster, that's the case.

0:07:50.029,0:07:52.021
So, I used to be a big fan of those.

0:07:52.800,0:07:57.871
Unfortunately, if you stack a lot of Sigmoids in many layers in a neural net,

0:07:58.304,0:08:03.398
you can attend to not learn very efficiently.

0:08:03.600,0:08:09.727
You have to be very careful about normalization if you want the system to converge if you have many layers.

0:08:10.059,0:08:15.196
So, in that sense, the single kink functions are better for deeper networks.

0:08:16.394,0:08:24.547
So, it's Softsign, this is basically a bit like the Sigmoid except that it doesn't get to the asymptotes as fast,

0:08:24.951,0:08:29.135
so, it doesn't get stuck towards the asymptote as quickly.

0:08:29.160,0:08:35.693
So, one problem with hyperbolic tangent and the Sigmoid is that we get close to the asymptote,

0:08:35.996,0:08:39.173
the gradient goes to zero fairly quickly.

0:08:39.334,0:08:43.541
And so, if the weights of a unit become too large,

0:08:43.663,0:08:48.166
they kind of saturate this unit, and the gradients get very small,

0:08:48.166,0:08:51.397
and then the unit doesn't learn very quickly anymore.

0:08:53.083,0:08:58.241
It's a problem that exists both, sorry, both in Sigmoids and hyperbolic tangent.

0:08:58.869,0:09:05.139
And so, SoftSign is a function of that was proposed by Yoshua Bengio and some his collaborators,

0:09:05.528,0:09:11.000
and it kind of saturates slower, and so it doesn't have that same problem.

0:09:12.730,0:09:15.250
I mean it has the problem also, but not to the same extent.

0:09:16.520,0:09:21.061
Okay, and this is kind of the opposite hard tangent, Hardtanh,

0:09:22.149,0:09:26.339
I don't know if it deserves that name, but it's basically just a ramp, okay?

0:09:27.583,0:09:34.800
And that works surprisingly well, particularly, if your weights are somehow kept within a kind of small value.

0:09:34.800,0:09:38.500
So, the units don't saturate too much.

0:09:39.895,0:09:44.707
It's surprisingly how well it works, and you know, people use this in supervised context.

0:09:47.500,0:09:49.275
But that's sort of, you know, non-standard.

0:09:49.342,0:09:54.473
so hard threshold is very rarely used because you can't really propagate gradient to it.

0:09:54.704,0:10:00.353
Okay? and this is really what kept people from inventing backprop in the 60s and 70s

0:10:00.353,0:10:07.431
which is that they were using binary neurons, and so they didn't think of the whole idea of gradients because of that.

0:10:09.257,0:10:14.091
Okay, those other functions are rarely used in the context of neural nets,

0:10:15.087,0:10:19.659
or at least for kind of activation function in a traditional neural net.

0:10:19.904,0:10:24.200
They used mostly for sometimes for things like sparse coding.

0:10:24.614,0:10:30.260
So, one step in sparse coding consists in to compute the value of the latent variable,

0:10:30.563,0:10:35.500
consist in shrinking all the values in the latent variable in the agent vector by some value,

0:10:35.500,0:10:38.675
and you do this with a shrinkage function.

0:10:38.766,0:10:42.417
This is kind of a soft version of shrinkage function, the hard version is here.

0:10:42.980,0:10:47.930
I mean, it's called Soft Shrink, but it actually has corners in it.

0:10:49.178,0:10:54.517
The reason it's called soft shrink is because there is a hard shrink that looks different. We show it you in a minute.

0:10:55.469,0:11:02.468
So, this basically just changes a variable by a constant toward zero. Right?

0:11:02.468,0:11:10.975
and if it goes below zero, it climbs at zero, if it's brought too long.

0:11:14.712,0:11:21.527
And this is basically just the identity function to which you subtract hyperbolic tangent to make it look like a shrink.

0:11:23.070,0:11:29.582
Basically, if we try to get the whatever value close to zero, they actually are forced to zero basically, right?

0:11:30.159,0:11:33.971
Right. So, small values are forced to zero, others are shrunk toward zero,

0:11:33.971,0:11:37.099
but you know, since they are large enough, they're not going to get to zero.

0:11:37.318,0:11:45.108
So, again that's used mostly as, you know, you can think of it as a step of gradient for an L1 criterion.

0:11:45.429,0:11:45.845
Okay?

0:11:46.075,0:11:49.800
So, if you have a variable, you have an L1 cost function on it,

0:11:49.824,0:11:55.225
and you take a step in the negative gradient of the L1, so L1 cost is an absolute value.

0:11:55.643,0:12:02.506
This will cause the variable to kind of go towards zero by a constant which is the slope of that L1 criterion,

0:12:03.032,0:12:08.985
and to kind of stay at zero, you know coming from either side, it doesn't kind of overshoot if you want

0:12:09.296,0:12:11.881
and so that's the non-linear function you will use,

0:12:11.910,0:12:17.600
and that's one of the steps in the EAST algorithm that is used for inference in sparse coding.

0:12:19.321,0:12:23.000
But again, it's rarely used in sort of regular neural nets,

0:12:23.445,0:12:32.478
unless your encoder is kind of used as kind of a estimation of sparse coding.

0:12:33.930,0:12:42.000
This is the hard shrink, so hard shrink basically climbs every value smaller than λ to zero, Okay?

0:12:42.025,0:12:52.000
So, if a value is smaller than λ or larger than -λ, then sort of between -λ and λ, and λ is some constant is just set it to zero.

0:12:53.070,0:13:00.188
Again, it's used for things like, you know, certain types of sparse coding, but rarely as an activation function in the neural net.

0:13:02.742,0:13:07.731
So, LogSigmoid is mostly used in cost functions not really as an activation function either,

0:13:07.861,0:13:17.268
but it's a useful function to have if you want to plug this into a loss function, and we'll see that in a minute.

0:13:19.447,0:13:25.767
So, Softmin we've seen this is the same as Softmax except you have minus signs, so this is sort of more ...

0:13:26.611,0:13:30.100
So, those are multi-dimensional non-linearities. All right?

0:13:30.110,0:13:35.000
You have a vector in and you get a vector out which is at same size as the input vector,

0:13:35.000,0:13:41.096
and we know about Softmax is you know exponential Xi divided by sum over j exponential Xj,

0:13:41.433,0:13:45.169
this is Softmin where you put a minus sign in front of the X.

0:13:45.170,0:13:50.600
So, you view the X as if you want as energies instead of scores, as penalties instead of scores.

0:13:52.261,0:14:03.100
It's a good way of turning a bunch of numbers to something that looks a bit like a probability distribution, which means numbers between 0 and 1 that sum to 1,

0:14:04.290,0:14:06.290
and that's the Softmax which we all know.

0:14:08.399,0:14:13.466
So, Log Softmax again, it's not very much used as a non-linearity within the neural net,

0:14:13.491,0:14:19.180
but it's used a lot at the output as kind of one piece of a loss function, it will see this in a minute.

0:14:20.430,0:14:22.400
Okay. So, those ...

0:14:22.400,0:14:24.500
Questions. Yes. We have a question.

0:14:24.852,0:14:31.603
So, for PReLU, I'm not sure I understand number one, why we want the same value for all channels?

0:14:31.628,0:14:35.362
and number two, how learning "a" would actually be advantageous?

0:14:35.940,0:14:38.744
You could have a different "a" for different channels.

0:14:39.126,0:14:48.671
So, different units can have a different "a" it could be used this as a parameter of every unit or not, it could be shared.

0:14:48.880,0:14:50.338
That's gonna up to you.

0:14:50.457,0:14:53.082
It could be shared at the level of a feature map in the convolutional net,

0:14:53.107,0:14:56.652
or it could be shareed on all future maps, or it could be individual to every unit.

0:14:56.955,0:15:00.471
If you really want to preserve the convolutional nature of a convolutional net,

0:15:00.471,0:15:03.591
you probably want to have the same "a" for every unit in the future map,

0:15:03.656,0:15:05.869
but you couldn't have different a's for different feature maps.

0:15:07.259,0:15:08.911
Okay. What was the second question?

0:15:09.409,0:15:15.198
Why learning actually specific value would be advantageous like why are we learning "a"?

0:15:15.509,0:15:17.870
You can learn it or not, you can fix it.

0:15:17.979,0:15:26.800
The reason for fixing it would be, you know, not necessarily to kind of have sort of more powerful non-linearity,

0:15:26.800,0:15:33.673
but to kind of ensure that the non-linearity gives you a non-zero gradient, even if it's in the negative region.

0:15:35.700,0:15:39.173
So, you know, learnable not learnable.

0:15:39.613,0:15:48.666
So, to make it learnable allows the system to basically turn a non-linearity into either the linear mapping,

0:15:48.691,0:15:51.529
which of course is not particularly interesting, but why not,

0:15:52.561,0:15:55.590
ReLU or something like a full rectification,

0:15:56.026,0:16:00.570
Okay? where "a" would be -1 in the negative part,

0:16:01.660,0:16:05.346
which, you know, can be interesting for certain types of application.

0:16:05.346,0:16:11.008
So, for example, if you have a convolutional net that has an edge-detector, an edge-detector as a polarity, right?

0:16:11.423,0:16:15.057
It's got plus coefficients on one side minus coefficients on the other side.

0:16:15.150,0:16:21.082
And so, it's gonna react. So, if you have an edge in an image that goes from say Dark to Bright,

0:16:22.040,0:16:25.800
the, you know, the convolution will react positively to this one,

0:16:26.037,0:16:32.100
but if you have another edge from, you know, in the opposite direction,

0:16:32.100,0:16:35.704
then the filter will react negatively.

0:16:36.137,0:16:41.731
Now, if you want your filter to react to an edge regardless of its polarity, you rectify it.

0:16:41.926,0:16:45.263
Okay? so that would be kind of just absolute value.

0:16:45.645,0:16:51.008
Now, you could, of course, ??? ??? ??? in have to use a PReLU, you can just use the absolute value.

0:16:51.795,0:16:54.076
Probably a better idea is to use a square actually.

0:16:54.369,0:16:57.082
So, you take the square of square non-linearity.

0:16:57.183,0:17:00.229
It's not implemented as kind of a neural net non-linearity,

0:17:00.229,0:17:03.249
but, you know, in the functional form of Pytorch you just write square, and that's it.

0:17:05.440,0:17:07.948
Hope answered the question. Any other question on this topic?

0:17:09.653,0:17:19.929
I have a question, it seems to me like these non-linearities are trying to basically make a linear function non-linear,

0:17:19.929,0:17:26.764
and the tweak in the lines denote like the change in that function.

0:17:26.764,0:17:36.053
So, can we think of this as if we want to model of curve in the line,

0:17:36.673,0:17:45.105
should we have learnable parameters on both, like before the 0 and after the 0 on the x-axis?

0:17:45.314,0:17:48.546
Well. So, yeah. I mean there is diminishing return.

0:17:48.546,0:17:52.163
So, the question is, you know, how complex do you want your non-linearity to be?

0:17:52.365,0:17:58.000
So, you could imagine of course parameterizing an entire non-linear function,

0:17:58.041,0:18:01.631
you know with spline parameters or Bayesian curves or something like this right.

0:18:01.977,0:18:05.519
Or, I don't know, Chebyshev polynomials,

0:18:05.519,0:18:09.025
you know, I mean, you can parameterize any mapping you want, right?

0:18:09.335,0:18:13.246
You can imagine those parameters could be part of the learning process.

0:18:14.963,0:18:22.500
However, you know, what is the advantage of doing this versus just you know having more units in your system?

0:18:22.500,0:18:28.800
and relying on the fact that multiple units will be added in the end to approximate the function you want?

0:18:29.405,0:18:31.996
Generally, it really depends on what like,

0:18:32.021,0:18:40.018
if you want to do regression in a fairly low dimensional space, so perhaps you want some parameterize non-linearities, that might help.

0:18:41.736,0:18:49.677
You might want to have a collection of different non-linearities with maybe things like Chebyshev polynomials,

0:18:49.677,0:18:52.674
if you want to do a good approximations.

0:18:53.067,0:18:56.763
But for like, you know, high-dimensional tasks like image recognition or things like this,

0:18:57.131,0:19:02.177
you just want a non-linearity, and it works better if the non-linearity is monotonic.

0:19:02.950,0:19:07.783
Otherwise, it creates all kinds of issues because you could have two points that will produce the same output,

0:19:07.808,0:19:12.600
and so it will ambiguous for the system to learn the right function there.

0:19:13.350,0:19:18.083
So you want it, it's much better if the function is monotonic, and almost all the functions here are monotonic,

0:19:18.681,0:19:21.671
except if you have a negative "a" here in the  PReLU case.

0:19:23.217,0:19:25.223
So it's a big advantage to having monotonic functions.

0:19:28.313,0:19:32.000
But in principle, you could parameterize, you know, any function you want.

0:19:32.200,0:19:34.929
People are played with this, you know. They're not very popular,

0:19:34.929,0:19:45.487
because mostly they don't seem to be bringing a huge  advantage in the kind of applications that people use a large neural nets for.

0:19:47.305,0:19:48.293
Other questions?

0:19:48.700,0:19:51.077
Another question is going to be kink converge smooth?

0:19:51.500,0:19:52.100
Yeah.

0:19:53.140,0:20:00.548
Do we have any application with the choice of non-linearities made of ??? ???,

0:20:00.829,0:20:10.303
the only thing I'm aware of is using a single ??? function ??? double kink for deep neural networks ???? in better?

0:20:11.674,0:20:15.822
Well, so here's a party with double kink, double kink has a built-in scale in it,

0:20:15.822,0:20:24.187
which means if the weights of the incoming layer are multiplied by 2, or the signal amplitude is multiplied by 2,

0:20:24.352,0:20:27.094
the result on the output would be completely different, right?

0:20:27.556,0:20:31.211
because you will be, you know, the signal would be more in the non-linearity.

0:20:31.236,0:20:35.907
So, you'll get completely different behavior of your layer,

0:20:36.736,0:20:42.121
whereas if you have a function with only one kink, if you multiply the input by 2 with the output get also multiplied by 2,

0:20:42.676,0:20:46.035
in a module of bias, but the signal of the bias is fine.

0:20:46.836,0:20:55.947
What I meant to ask is taken of a situation with a choice of activation function made a big difference in the performance of the model?

0:20:56.230,0:21:00.177
except for the deep networks using ReLU instead of Sigmoid?

0:21:04.004,0:21:06.868
There is no sort of general answer to this.

0:21:07.525,0:21:10.685
Like if you're going to use attention,

0:21:11.881,0:21:14.399
you have to use Softmax. I mean you have no choice, right?

0:21:15.077,0:21:17.148
I mean, it's not like you have to use Softmax,

0:21:17.148,0:21:23.622
but you want to have something where you get coefficients, right, to kind of focus the attention of the system on,

0:21:24.685,0:21:27.023
or to kind of spread the attention of the system,

0:21:28.264,0:21:31.817
and not allow it to cheat which is to pay attention to multiple things at one time,

0:21:31.842,0:21:38.906
you have to have some sort of normalization of the coefficients that come out of the attention system, right?

0:21:38.931,0:21:43.707
So, normally in most attention systems like in transformers and stuff,

0:21:44.219,0:21:47.511
the coefficients are passed through a Softmax,

0:21:47.511,0:21:51.100
so you get a bunch of coefficients that are between 0 & 1, and sum to 1.

0:21:51.475,0:21:58.348
And so that causes the system to have to pay attention to, you know, a small number of things,

0:21:58.348,0:22:02.167
so it can only concentrate the coefficients on a small number of items.

0:22:02.860,0:22:04.425
And it has to spread it, right?

0:22:06.832,0:22:10.259
The other ways to do normalization you can do...

0:22:11.082,0:22:18.668
and in fact, there is something that's wrong with Softmax normalization for transformers or for attention,

0:22:19.064,0:22:23.542
which is that if you want a coefficient coming out with Softmax to be close to zero,

0:22:24.378,0:22:27.409
you need the input to be close to minus infinity (-∞)

0:22:28.275,0:22:32.262
Okay? or to be a considerably smaller than the largest one, right?

0:22:32.262,0:22:33.975
When you go into the Softmax,

0:22:34.000,0:22:40.224
one output, the largest input is going to cause the corresponding output to be large.

0:22:40.433,0:22:44.432
If you want that output to be close to 1, and all the other ones to be close to 0,

0:22:45.153,0:22:47.551
you basically want this input to be extremely large,

0:22:47.575,0:22:53.164
and all the other ones to be large and negative. Okay?

0:22:54.188,0:23:04.558
Now, you know, that can be a problem when the what you are computing at the input are dot products

0:23:05.330,0:23:13.987
because the result is that, you know, the easiest way for a system to produce a small dot product is

0:23:13.987,0:23:17.542
to have two vectors that are orthogonal to each other which gives the dot product to zero.

0:23:18.039,0:23:20.729
If you insist that the dot product should be very very small,

0:23:21.119,0:23:27.629
then either you have to make the, so you have to make the two vectors basically point in opposite directions,

0:23:27.653,0:23:29.011
and you have to make them very long.

0:23:30.500,0:23:32.352
And that's not so great.

0:23:32.735,0:23:41.301
And so, using softmax for attention basically limits the contrast that you're gonna have

0:23:41.909,0:23:44.888
between the coefficients which is not necessarily a good thing.

0:23:45.978,0:23:54.289
So, you know, same thing for LSTM, Gated Recurrent Nets, etc,

0:23:55.458,0:23:59.276
you need Sigmoids there because you need coefficients that are between 0 and 1,

0:23:59.490,0:24:05.450
you know, that either we set the memory cell or make it a ????

0:24:05.450,0:24:10.253
So that, it keeps it, so it's previous memory or kind of write the new input in it.

0:24:11.320,0:24:15.952
So there, it's nice to have an output that varies continuously between 0 and 1.

0:24:17.280,0:24:18.249
Then you have no choice.

0:24:18.743,0:24:23.238
So I mean, I don't think you can say just, you know, in generic term,

0:24:24.335,0:24:27.158
you know, this non-linearity is better than the other one.

0:24:27.245,0:24:28.784
There are certain cases where it learns better,

0:24:29.020,0:24:32.890
there are certain cases where it relieves you from having to initialize properly,

0:24:33.621,0:24:37.362
there is certain cases where it works better if you have lots of layers

0:24:37.387,0:24:42.208
like you know, single kink functions work better if you have lots of layers better than Sigmoid like functions.

0:24:42.770,0:24:45.541
There's no kind of, there is no single answer, basically.

0:24:46.804,0:24:58.175
I have a question just regarding the general differences between a non-linear activation that has kinks versus a smooth non-linear activation.

0:24:58.200,0:24:58.820
Yeah

0:24:58.957,0:25:07.100
Is there sort of any general reason or rule to why we would prefer to have kinks in the function or not?

0:25:07.317,0:25:12.217
It's a matter of scale-invariant of scale equivalence. So if the kink is hard,

0:25:12.678,0:25:17.500
again, you multiply the input by 2, the output is multiplied by 2, but otherwise unchanged, okay?

0:25:18.075,0:25:25.733
If you have a smooth transition, if you multiply the input by let's say 100,

0:25:27.370,0:25:30.287
the output now will look like you had a hard kink.

0:25:31.064,0:25:35.186
okay, because the smooth part now is become shrunk by a factor of 100.

0:25:35.662,0:25:43.220
if you divide the input by 100, now, the kink becomes a very very smooth sort of convex functions.

0:25:43.790,0:25:50.610
Okay, so it changes the behavior by changing the scale the input you change the behavior of the unit,

0:25:51.317,0:25:53.445
and that might be a problem sometimes,

0:25:53.470,0:25:57.975
because when you train a multi-layer neural net,

0:25:57.999,0:26:00.967
and you have two layers that are one after the other,

0:26:02.850,0:26:09.023
you don't have a good control for like how big the weights of this layer are relative to that other weights.

0:26:09.129,0:26:13.351
So imagine you have a two layer network where you don't have a non-linearity in the middle,

0:26:13.351,0:26:15.564
so the system is completely linear, right?

0:26:17.524,0:26:20.324
If the network has arrived at a solution,

0:26:21.298,0:26:27.662
you can multiply the incoming the first layer weight matrix by 2 divide the second weight matrix by 2,

0:26:27.872,0:26:30.420
and overall the network will have exactly the same output.

0:26:30.716,0:26:32.400
Okay? you won't have changed anything.

0:26:33.736,0:26:36.235
And what that means is that when you do training,

0:26:36.259,0:26:41.993
there is nothing that forces the system to have a particular scale for the weight matrices.

0:26:42.459,0:26:42.849
All right?

0:26:43.447,0:26:45.657
So, now, if you put a non-linearity in the middle,

0:26:46.480,0:26:49.149
and you still don't have it because train for the system to kind of,

0:26:49.550,0:26:53.483
you know, have scales for the first layer weight versus the second layer weight

0:26:54.652,0:26:59.482
that, you know, you'd better have a non-linearity that doesn't care about scale.

0:27:00.369,0:27:00.889
Okay?

0:27:01.379,0:27:05.112
So, if you have non-linearity that does care about scale,

0:27:05.689,0:27:10.689
then your network doesn't have a choice of what size weight matrix it can use in the first layer,

0:27:11.013,0:27:12.730
because that will completely change the behavior,

0:27:13.935,0:27:18.275
and it may want to have large weights for some other reason which will saturate the non-linearity

0:27:18.300,0:27:22.383
and then kind of create, you know, vanishing gradient issues.

0:27:23.386,0:27:25.312
So, it's not entirely clear.

0:27:26.473,0:27:31.131
You know, what? Why is it that? you know, did networks work better with single kink functions,

0:27:31.156,0:27:36.110
but it's probably due to that scale-invariant property or scale equivalence property.

0:27:37.076,0:27:40.100
Now, there would be other ways of fixing this problem,

0:27:40.100,0:27:44.206
which would be to basically set a hard scale on the weights of every layer.

0:27:44.231,0:27:46.138
so you couldn't like normalize the weights of the layers.

0:27:46.163,0:27:52.205
so that, the variance that of things that's going to unit, you know, it's always constant.

0:27:52.230,0:27:54.348
In fact, that's a little bit what batch normalization does,

0:27:54.348,0:27:57.449
or the various normalization schemes, they do that to some extent,

0:27:57.781,0:28:02.733
you know, they put the mean at zero, and the variance is constant.

0:28:02.913,0:28:08.864
So, now, the variance of the amplitude of the output doesn't depend on the size of the weights because it's normalized.

0:28:10.321,0:28:20.500
So, you know that is partially why things like batch norm and group norm and things like this help

0:28:20.524,0:28:25.853
is because they can fix the scale a little bit,

0:28:27.443,0:28:33.000
but then, if you fix the scale, then with something like batch norm,

0:28:33.200,0:28:40.813
the system now doesn't have any way of choosing which part of the non-linearity it's going to use in the two kink function system.

0:28:41.320,0:28:48.189
Okay, so things like group normalization or batch normalization are incompatible with kind of Sigmoids if you want.

0:28:49.200,0:28:51.819
??? a Sigmoid, you don't want normalization just before it.

0:28:53.495,0:28:55.950
See, that provides some really good intuition. Thank you.

0:28:56.318,0:28:58.399
Okay, any other question others?

0:28:58.504,0:29:05.143
I have one more question. I noticed in a Softmax function some people use the temperature coefficient.

0:29:05.168,0:29:09.159
So, in what cases would we want to use their temperature? And why would we use it?

0:29:10.168,0:29:13.738
Well, so to some extent the temperature is redundant with incoming weights.

0:29:13.738,0:29:17.004
So, if you have weighted sum's coming into your Softmax,

0:29:18.916,0:29:22.610
you know, having β parameter in your Softmax equal to 2 instead of 1

0:29:22.610,0:29:26.732
is the same as just making your weights twice as big, it has exactly the same effect.

0:29:27.439,0:29:31.961
Okay, so that β parameter is redundant with the size of the weights.

0:29:31.986,0:29:36.459
But again, if you are or the size of the weighted sum the variance of the weighted sum if you want

0:29:36.743,0:29:41.154
but again, if you have a batch normalization in there, then the temperature parameter matters,

0:29:41.306,0:29:43.665
because now the input variances are fixed.

0:29:43.655,0:29:48.185
So now, the temperature matters.

0:29:49.159,0:29:57.800
Temperature basically controls how hard, you know, the distribution on the output will be.

0:29:58.223,0:30:03.194
So, with a very very large β, you basically will have one of the outputs equal to 1,

0:30:03.218,0:30:04.815
and all the other ones very close to 0.

0:30:04.815,0:30:08.819
I mean very close to 1 and very close to 0 where β is small than this softer,

0:30:09.424,0:30:14.980
in the limited β equal to 0, it's more like an average actually that you get like Softmax behaves a little bit like an average.

0:30:16.516,0:30:21.400
So, you know, β goes to infinity, it so behaves a bit like argMax,

0:30:21.400,0:30:24.521
and β goes to 0, it behaves a bit like an average.

0:30:25.300,0:30:31.923
So if you have some sort of normalization before the Softmax,

0:30:31.923,0:30:35.671
then tuning this parameter allows you to control this kind of hardness.

0:30:36.519,0:30:44.186
And what people do sometimes in certain scenarios is that they start with a relatively low β,

0:30:44.994,0:30:49.504
So that, the numbers that are produced are kind of soft.

0:30:49.742,0:30:54.663
So, you get gradients everywhere, you know, it's kind of well-behaved in terms of gradient descent.

0:30:55.038,0:31:01.908
And then as learning proceeds, if you want kind of harder decisions in your attention mechanism or whatever, you increase β.

0:31:02.622,0:31:05.183
And so that makes the system kind of make harder decisions.

0:31:05.523,0:31:10.405
It doesn't learn as well anymore, but it's you know, presumably after a few iterations, it's kind of in the right ballpark.

0:31:10.405,0:31:16.275
So, you can sort of sharpen the decisions there by can be increasing β.

0:31:16.840,0:31:20.500
It's useful for example in a mixture of experts,

0:31:20.976,0:31:30.669
and you know, self attention systems are kind of you can think of as a weird form of mixture of experts.

0:31:30.980,0:31:34.125
So, a mixture of experts, you know, you have multiple sub networks,

0:31:34.342,0:31:37.793
and their outputs are can linearly combine with coefficients that are

0:31:37.817,0:31:43.481
the output of the Softmax itself could, you know, controlled by another neural net.

0:31:43.853,0:31:47.190
So, if you want kind of a soft mixture, you have a low β,

0:31:47.190,0:31:53.359
and as you increase β to infinity, basically, you're going to select one of the experts and ignore all the other ones.

0:31:53.880,0:31:58.753
That might be useful, for example, if you want to train a mixture of experts or an attention mechanism,

0:31:58.753,0:32:05.852
but in the end, you want to safe computation by just determining which expert do I need to compute and just not computing the other ones.

0:32:06.235,0:32:09.540
So, in that case you want those coefficients to be basically either 1 or 0,

0:32:10.044,0:32:16.749
and you can train the system progressively to do this by increasing β.

0:32:17.370,0:32:20.018
The physicists have a name for this,

0:32:20.018,0:32:23.539
because it uses kind of tricks or various other things, that's called annealing.

0:32:24.072,0:32:30.373
It has the same meaning as, so annealing comes from metalwork, right?

0:32:30.398,0:32:37.189
You're making a steel or something, and you make a sword or something, right?

0:32:37.261,0:32:42.364
And you heat it up, and then you cool it.

0:32:42.523,0:32:45.464
And depending on whether you cool it quickly or slowly,

0:32:46.871,0:32:51.202
you change the crystal instructure of the metal,

0:32:51.227,0:32:57.785
so this idea of annealing of progressively lowering the temperature correspond to this increasing this β.

0:32:57.785,0:33:01.737
β is like an inverse temperature, it's akin to an inverse temperature.

0:33:02.764,0:33:03.687
Any other question?

0:33:06.200,0:33:07.194
I think we are good.

0:33:07.403,0:33:13.000
All right. Okay. So next topic is Loss Functions.

0:33:17.990,0:33:22.973
So, Pytorch has a whole bunch of loss functions as you might have seen.

0:33:24.350,0:33:29.518
And of course there are things are simple ones like mean square error.

0:33:30.073,0:33:39.320
So, I don't need to explain to you what it is, you know compute the square of the error between the desired output Y, and actually input X,

0:33:39.685,0:33:46.233
and if it's already be batched with n samples, then you have you know n losses,

0:33:46.233,0:33:47.976
one for each of the samples in the batch .

0:33:48.679,0:33:55.383
and you can tell this loss function to either keep that vector or to kind of reduce it by computing mean or sum.

0:33:56.011,0:33:57.137
Okay, pretty simple.

0:34:02.171,0:34:03.775
Here's a different loss, that's the L1 loss.

0:34:03.775,0:34:09.261
So, this is basically the absolute value of the difference between the desired output and the actual output,

0:34:09.292,0:34:14.576
and you want to use this to do what's called "robust regression".

0:34:14.576,0:34:20.955
So, if you want small errors to count a lot and large errors to count but you know not as much as if you use the square,

0:34:22.256,0:34:26.773
perhaps because you have noise in your data, so you know that you have a bunch of data points.

0:34:26.773,0:34:34.141
You're trying to kind of train a neural net or something to kind of, you know, fit a curve or, you know, do regression,

0:34:34.740,0:34:36.225
but you know, that you have a few outliers,

0:34:36.225,0:34:39.136
so you have a few points there are, you know, very far away from what they should be

0:34:39.237,0:34:44.800
just because you know the system has noise or something, or the data was collected with some noise.

0:34:44.968,0:34:47.017
So, you want the system to be robust to that noise,

0:34:47.168,0:34:56.654
you don't want the cost function to increase too quickly as the points are far away from, you know, the kind of the general curve.

0:34:57.534,0:35:00.500
So, L1 loss would be more robust.

0:35:00.709,0:35:04.300
Now, the problem with the L1 loss is that it's not differentiable at the bottom,

0:35:05.051,0:35:12.439
and so, you know, you have to kind of be careful when you get to the bottom of how you do the gradient

0:35:13.636,0:35:20.960
that's basically done with the soft shrink essentially that's the gradient of L1 loss.

0:35:25.029,0:35:35.259
Now, to correct for that people have come up with various ways of kind of making the L1 loss for bias for large losses,

0:35:35.259,0:35:39.302
but then still smooth at the bottom kind of behaving like squared error at the bottom.

0:35:39.858,0:35:45.000
So, an example of this is particular function, Smooth L1 Loss.

0:35:45.174,0:35:48.542
It's basically L1 far away and it's sort of L2 nearby,

0:35:48.566,0:35:51.551
and that presents sometimes that's called a Huber loss.

0:35:52.511,0:35:56.731
Some people call this also elastic Network because this ...

0:35:58.460,0:36:05.976
an old paper from the 1980s or 1990s that kind of proposed this kind of objective function for different purpose.

0:36:07.871,0:36:15.428
So, that's useful that was advertised by Ross Girshick at Fast R-CNN paper for ...

0:36:15.996,0:36:19.445
and it's used quite a bit in computer vision for supervised purposes.

0:36:19.445,0:36:22.037
Again, it's for protecting against outliers.

0:36:25.453,0:36:31.585
This is also sharper results, now when we do like image prediction?

0:36:34.279,0:36:35.726
sharper than?

0:36:36.152,0:36:37.349
using the MSE.

0:36:41.260,0:36:46.318
Not particularly, I mean it's just like the MSE for small errors, okay?

0:36:47.097,0:36:49.562
So, that doesn't make any difference,

0:36:49.586,0:36:55.389
but it doesn't, or maybe I misunderstood what your point was.

0:36:55.414,0:37:03.433
Sorry, I was trying to compare the L1 versus the L2 that L2 gives us like usually blurry predictions

0:37:03.433,0:37:08.355
whenever we try it to do prediction by using like the minimizing the L2,

0:37:08.380,0:37:14.640
whereas like people are minimizing the L1 in order to have like sharper overall predictions.

0:37:14.763,0:37:19.260
Okay, so if you take a bunch of points, okay?

0:37:19.285,0:37:24.377
if you take a bunch of Y values, okay, and you ask the question, what value ...

0:37:24.960,0:37:27.071
So, you take a bunch of points on Y, Okay?

0:37:27.095,0:37:31.665
Yeah, you ask the question what value of Y minimizes the square loss?

0:37:32.076,0:37:34.789
The answer is the average of all the Y's. Okay?

0:37:36.427,0:37:41.725
Okay, so if you, so if for a single X you have a whole bunch of Y which means you have noise in your data,

0:37:43.948,0:37:48.140
your system will want to produce the average of all the Y's that you're observing.

0:37:49.700,0:37:55.017
Okay, and if the Y you're observing is not a single value, but is I don't know an image,

0:37:56.063,0:37:58.991
the average of a bunch of images is a blurry image, okay?

0:37:59.113,0:38:00.924
that's why you get those blurry effects.

0:38:01.588,0:38:09.696
Now, with L1, the value of Y that minimizes the L1 norm, the L1 distance,

0:38:09.696,0:38:15.961
so, basically the sum of the absolute values of the differences between the value you're considering and all the points, all the Y's points,

0:38:16.693,0:38:17.617
that's the median.

0:38:18.504,0:38:21.628
Okay, so it's a given point.

0:38:23.309,0:38:23.795
All right?

0:38:24.213,0:38:25.721
uh-huh, I see.

0:38:25.722,0:38:32.543
Median of course is not blurry, it's just an image.

0:38:32.824,0:38:36.247
Although it's kind of difficult to define in multiple dimensions, but ...

0:38:40.592,0:38:43.500
So, one problem with this loss is that it has a scale, right?

0:38:43.860,0:38:48.949
So, here the transition here is at point 5, but why should it be at point 5?

0:38:48.973,0:38:53.997
You know, it could be it depends what the scale of your errors are.

0:38:57.459,0:38:58.082
Okay.

0:38:58.984,0:39:02.442
Negative log likelihood loss, this is really not the negative log likelihood loss.

0:39:02.466,0:39:04.214
 I'm not sure why it's called this way in Pytorch,

0:39:04.496,0:39:09.721
but basically here imagine that you have an X vector coming out.

0:39:10.781,0:39:14.310
Okay, and your loss function is there is one correct X.

0:39:14.555,0:39:20.963
So imagine each X correspond to a score for multi-class classification.

0:39:21.309,0:39:25.610
So, you have a desired class, which is one particular index in that vector.

0:39:26.108,0:39:29.167
Okay? Now, what you want is you want to make that score as large as possible.

0:39:29.911,0:39:32.700
Okay? If those scores are likelihoods,

0:39:32.965,0:39:39.461
then this is minimum negative log likelihood, if those scores are log likelihoods.

0:39:40.046,0:39:42.917
And this is maximum likelihood or minimum negative log likelihood.

0:39:43.430,0:39:48.759
Okay, but there is nothing in this module that actually specifies that the L has to be log likelihoods.

0:39:48.968,0:39:53.699
So, this is just you know, make my desired component as large as possible. That's it.

0:39:55.896,0:40:04.272
If you put negative signs in front, so now you can interpret the X's as energies as opposed to scores.

0:40:04.337,0:40:12.447
Okay, they're not positive scores. They're like penalties if you want, but it's the same.

0:40:12.533,0:40:21.261
So, the formula here says, you know, just pick the X that happens to be the correct one for one sample in the batch,

0:40:21.939,0:40:24.277
and make that score as large as possible.

0:40:24.704,0:40:32.700
Now, this particular one allows you to give a different weight to different categories,

0:40:33.389,0:40:40.291
which is those W's, it's a weight vector that gives a weight to each of the categories.

0:40:40.349,0:40:51.203
It's useful in a lot of cases particularly if you have widely different frequencies for the categories,

0:40:51.203,0:40:58.178
you might want to increase the weight of samples for which you have a small number of examples.

0:40:59.238,0:41:01.771
I mean for categories for which you have a small number of samples.

0:41:03.010,0:41:07.075
However, I'm actually not a big fan of this.

0:41:07.255,0:41:18.995
I think it's a much better idea to just increase the frequency of the samples from the classes, you know, that appears rarely.

0:41:19.211,0:41:23.302
So that, you equalize the frequency, the frequencies of the classes when you train.

0:41:25.999,0:41:30.862
It's much better because it exploits stochastic gradient in a better way. Okay?

0:41:31.223,0:41:35.388
So, bottom line of that is if you have ...

0:41:35.412,0:41:37.412
let me actually draw a picture of this.

0:41:37.738,0:41:43.446
So, let's say you have a problem where you have tons of samples for category 1,

0:41:44.254,0:41:46.984
and then the small number of samples for category 2,

0:41:47.114,0:41:49.235
and a tiny number of samples for category 3.

0:41:50.875,0:41:52.873
You could, so let's say

0:41:53.234,0:41:56.228
you know, here you have, I don't know, 1000 samples,

0:41:56.252,0:41:58.252
and here you have 500 samples,

0:41:58.496,0:42:01.400
and here you have, I don't know, 200 samples, right?

0:42:01.990,0:42:06.371
So, you could do is using this kind of weight function.

0:42:06.500,0:42:13.800
You could give this a weight of 1, and this guy weight of 2, and this guy weight of 5.

0:42:13.932,0:42:15.707
And then you can equalize the weights if you want.

0:42:16.240,0:42:20.200
It's really better to make sure that the weights normalized to 1 that would be probably a better idea.

0:42:20.434,0:42:23.720
But what I recommend is not that.

0:42:23.745,0:42:27.749
What I recommend is when you pick your samples,

0:42:32.619,0:42:38.484
you basically pick one sample from class 1, and then one sample from class 2, and one sample from class 3.

0:42:38.592,0:42:42.892
and then you know you keep doing this during your training session,

0:42:43.556,0:42:47.458
and when you get to the end of class 3 you go back to the beginning.

0:42:48.136,0:42:48.831
Okay?

0:42:49.286,0:42:52.345
So, you keep going here, but here you go back to the first sample.

0:42:52.958,0:42:56.196
Keep going here, go back, and now you have the second sample.

0:42:57.188,0:43:03.000
Okay? And now you get to the end of class 2, go back to the start.

0:43:03.702,0:43:06.126
Okay? So, the next sample is going to be here

0:43:06.782,0:43:08.638
here and here.

0:43:08.991,0:43:11.281
And then the next one here here and here.

0:43:11.758,0:43:16.365
Here here, and then this guy wraps around again, etc.

0:43:16.390,0:43:21.903
Right? So, you basically have equal probability equal frequencies, for all the categories

0:43:22.321,0:43:30.416
by just going through those kind of circular buffers, more often for categories for which you have fewer samples.

0:43:31.000,0:43:31.731
Okay?

0:43:32.522,0:43:42.038
One thing you should absolutely never do is equalize the frequencies by just not using all the samples in categories that are frequent,

0:43:42.608,0:43:45.835
I mean that's horrible. You should never let any data on the floor.

0:43:45.859,0:43:48.310
There's never any reason to leave that on the floor. Okay?

0:43:50.893,0:43:52.235
Now here is a problem with this.

0:43:52.235,0:43:55.725
The problem with this is that after you've trained your neural net that to do this,

0:43:56.290,0:44:03.231
the neural net that does not know about the relative likelihood, the relative frequencies of the samples.

0:44:03.917,0:44:07.006
And so let's say this is a system that does medical diagnosis.

0:44:07.129,0:44:15.679
It doesn't know that the common cold is very very more frequent than, you know, lung cancer or something, right?

0:44:16.768,0:44:26.005
So, what you need to do in the end is do a path a few pathes perhaps where you can fine tune your system,

0:44:26.005,0:44:29.915
so that with the actual frequencies of the categories,

0:44:30.175,0:44:34.374
and the effect of this is going to be for the system to adapt the biases at the output layer.

0:44:34.511,0:44:42.219
So that the likelihood of, you know, a diagnosis corresponds to the frequency of it, right?

0:44:42.219,0:44:44.691
It's gonna favor things that are more frequent.

0:44:45.254,0:44:51.000
The reason why I don't want to do this during the entire training is because if you train a multi-layer net,

0:44:51.531,0:44:57.303
the system basically never develops the right features for rare cases,

0:44:57.483,0:45:02.545
and I may have spoken about this already in the class in past weeks,

0:45:04.412,0:45:07.125
to kind of recycle the example of medical school,

0:45:07.745,0:45:10.693
you don't spend when you go to medical school,

0:45:10.693,0:45:19.505
you don't spend time studying the flue that is proportional to the frequency of the flue with respect to very rare diseases, for example, right?

0:45:20.150,0:45:23.187
You spend basically the same time studying all the diseases.

0:45:23.187,0:45:27.375
In fact, you spend more time studying complicated one, which usually tends to be rarer.

0:45:28.262,0:45:31.619
and that's because you need to develop the features for it, okay?

0:45:32.016,0:45:37.024
And then you need to kind of correct for the fact that you know, those rare diseases are rare.

0:45:37.435,0:45:47.728
So, you don't do that, you know, you don't suspect the diagnosis for rare diseases very often because, you know, it's rare.

0:45:51.429,0:45:51.992
Okay.

0:45:54.156,0:45:55.967
So that's all for weights.

0:45:57.273,0:45:59.978
Cross entropy loss, so you'll be using this a lot of course.

0:46:00.844,0:46:06.552
And cross entropy loss is a kind of merging of two things:

0:46:07.115,0:46:11.400
merging of log Softmax function, and negative log likelihood loss, okay?

0:46:11.500,0:46:16.694
And the reason why you want to have this is for numerical reasons.

0:46:19.775,0:46:24.899
So, the log Softmax is, you know, basically the Softmax followed by log, right?

0:46:24.899,0:46:27.734
So, you first compute the softmax then you do the log.

0:46:28.102,0:46:31.829
If you do Softmax, and then log and you backpropagate through this,

0:46:31.966,0:46:38.651
you might have gradients in the middle between the log and the Softmax that end up being infinite.

0:46:40.952,0:46:46.350
So, for example, if the maximum value of one of the Softmax is close to 1,

0:46:49.592,0:46:51.293
and sum of the other ones are close to 0,

0:46:51.293,0:46:54.610
you take the log, you get something that's close to minus infinity.

0:46:55.204,0:46:58.205
You backpropagate through the log, you get something that's close to infinity.

0:46:58.804,0:47:06.169
Okay? because the slope of log close to zero is very very close to infinity.

0:47:06.898,0:47:10.040
But now you multiply this by a Softmax that is saturated.

0:47:10.040,0:47:12.019
So, it's multiplied by something that's very close to zero.

0:47:12.120,0:47:13.874
So, in the end you get a reasonable number.

0:47:14.530,0:47:18.137
But because the intermediate numbers are close to infinity or 0,

0:47:18.310,0:47:22.502
you multiply something that's close to plus infinity by something that's close to 0,

0:47:22.668,0:47:23.765
you get numerical issues.

0:47:24.103,0:47:28.550
So, you don't want to separate log and Softmax, you want to do log SoftMax in one go.

0:47:28.654,0:47:33.175
It simplifies the formula, it makes the whole thing much more stable numerically.

0:47:34.985,0:47:40.974
And for similar reasons, you also want to merge log Softmax and negative log likelihood loss.

0:47:40.999,0:47:44.390
So, basically if you have log Softmax and negative log likelihood loss,

0:47:44.390,0:47:46.638
it says I'm gonna a bunch of weighted sums,

0:47:46.763,0:47:50.275
I'm gonna pass them through Softmax, I'm gonna take the log of those,

0:47:50.434,0:47:56.747
and then I want to make the output of the log Softmax for the correct class as large as possible.

0:47:57.450,0:48:00.172
Okay, that's what the negative log likelihood loss does.

0:48:00.317,0:48:02.914
It wants to make the score of the correct class as large as possible.

0:48:03.571,0:48:05.200
We saw that just a minute ago, right?

0:48:06.631,0:48:10.461
when you backpropagate through the log Softmax, as a consequence,

0:48:10.461,0:48:13.712
it's gonna make the score of all the other classes as small as possible.

0:48:14.289,0:48:16.165
Right? Because of the normalization.

0:48:19.899,0:48:27.847
And so that, you know, that's why sometimes the whole idea of sort of building a network by

0:48:28.064,0:48:33.364
you know, modules sometimes there is an advantage instead of merging in modules into single one by end.

0:48:41.050,0:48:41.519
Right.

0:48:41.544,0:48:51.386
So, the cross entropy loss, in fact, this explains a little bit, you know, those numerical simplifications.

0:48:51.386,0:49:00.484
So, the loss, you know, takes an X vector and a category, a desired category, a class, Okay?

0:49:01.072,0:49:05.773
and computes the negative log of the Softmax applied to the vector of scores,

0:49:06.574,0:49:16.164
but the one that's on the numerator here is the X of the index of the correct class

0:49:16.315,0:49:18.238
Okay, so that's your loss.

0:49:18.238,0:49:24.309
The negative log of exponential the score of the correct class divided by the sum of the exponential's of all the scores.

0:49:25.081,0:49:25.687
Okay?

0:49:26.380,0:49:28.919
You can think of the X's as negative energies.

0:49:29.586,0:49:30.293
Okay?

0:49:30.683,0:49:32.133
It's completely equivalent.

0:49:34.348,0:49:41.954
Now, when you do the math and you simplify, the log and the exponential's gonna simplify,

0:49:41.954,0:49:45.753
and so you just get the score of the correct class, the negative score of the correct class.

0:49:45.840,0:49:46.294
Okay?

0:49:46.778,0:49:49.000
So, to make that small, you make the score large,

0:49:49.861,0:49:55.252
and then plus the log of the sum of the exponential's of the scores of all the other class to make that small.

0:49:55.642,0:50:02.692
You make all the Xj's  small, negative as far as, you know, as negative as possible.

0:50:03.594,0:50:07.822
Okay, so this will make the score of the correct class large, make the score of everything else small.

0:50:11.431,0:50:15.601
Again, like in the ???? you can re-weight per category.

0:50:17.275,0:50:20.084
Also, there is a physical interpretation, right? of the cross entropy.

0:50:21.758,0:50:24.858
Right. Okay, so why is it called cross entropy?

0:50:24.900,0:50:27.816
Because it is the cross entropy between two distributions.

0:50:27.902,0:50:30.392
It's the KL divergence really between two distributions.

0:50:31.019,0:50:33.629
It doesn't appear clearly here in this formula.

0:50:34.040,0:50:39.389
But think of the Softmax applied to the X vector as a distribution, Okay?

0:50:39.389,0:50:45.452
So, take the X vectors the scores rather than to a Softmax you get a bunch of numbers between 0 & 1 that sum to 1.

0:50:47.189,0:50:50.724
And now you have a desired distribution, and the desired distribution,

0:50:51.130,0:50:57.939
the target distribution if you want is Y in which all the wrong categories had 0 and the correct category is 1.

0:50:58.444,0:51:02.580
Okay? Now, compute the KL divergence between those two distributions.

0:51:03.142,0:51:12.200
Okay, so it's the sum over indices of the correct probability. Okay?

0:51:12.700,0:51:15.220
Which is 0 for except for one term.

0:51:16.600,0:51:26.782
Times the ratio between the log of the probability that the system produces and the correct probability, which is one. Okay?

0:51:26.955,0:51:35.213
So, all of those terms, you know, reduce to kind of a single term which is just the one for which the correct probability term is 1.

0:51:36.101,0:51:38.302
Okay? So we end up with this term.

0:51:38.453,0:51:42.566
It's just a negative log of the Softmax output for the correct class.

0:51:43.435,0:51:43.839
Okay?

0:51:43.976,0:51:47.908
We can use this as a cross entropy between the distribution produced by the system,

0:51:48.161,0:51:53.675
and the one-hot vector corresponding to the desired distribution if you want. Okay?

0:51:53.700,0:51:58.622
So now there is a, there be another kind of more sophisticated version of this,

0:51:59.005,0:52:05.569
which would be the actual KL divergence between the distribution produced by the system and a distribution that you propose whatever it is,

0:52:05.838,0:52:07.784
a target distribution which now is not binary,

0:52:07.784,0:52:11.519
it's not the one-hot vector anymore, but it's just a vector of numbers,

0:52:11.519,0:52:15.117
and that's called the KL divergence loss, in fact. It's we'll see it in a minute.

0:52:21.057,0:52:25.674
So, KL divergence is a kind of, you know, it's not a distance because it's not symmetric,

0:52:25.698,0:52:32.746
but it's a sort of a divergence between discrete distributions.

0:52:32.870,0:52:42.320
Okay. So this one is a bit of, kind of a extension if you want of log Softmax,

0:52:43.953,0:52:49.544
and it's a version of it that is applicable for very very large categorization,

0:52:49.544,0:52:55.893
so if you have many many many categories, what you might want to do is kind of cut some corners;

0:52:55.893,0:53:01.595
you don't want to compute the giant Softmax over million categories or maybe even more.

0:53:02.633,0:53:05.311
So there you can sort of basically ignore the ones that are small,

0:53:05.311,0:53:14.947
and you know, kind of use tricks to kind of, you know, improve the speed of the computation,

0:53:14.947,0:53:16.416
and this is what this does.

0:53:16.520,0:53:20.087
I'm not going to go into the details exactly what it does, because actually I don't know the details,

0:53:20.112,0:53:25.119
but it's basically an efficient approximation Softmax for a very very large number of categories.

0:53:31.190,0:53:38.079
So this is a special case of cross entropy when you only have two categories,

0:53:38.550,0:53:41.599
and in that case it kind of reduces to something simple.

0:53:42.589,0:53:47.779
So this does not include Softmax, this is just a cross entropy when you have two categories,

0:53:48.270,0:54:01.239
and as I said before, the cross entropy loss is the sum over categories of the probability,

0:54:02.290,0:54:09.229
I mean, sum over indices or sum over categories of the probability for the target probability for that category,

0:54:10.427,0:54:23.389
times the ratio between the log of the probability of produced by the system divided by the probability of the target category.

0:54:23.472,0:54:30.705
and if you work it out for two categories, necessarily one score is one minus the other one if you have two exclusive categories,

0:54:31.019,0:54:34.775
and it comes down to this. Okay?

0:54:35.042,0:54:45.516
Now, this supposes that X and Y are kind of probabilities that have to be strictly between 0 & 1,

0:54:45.730,0:54:46.920
 I mean not strictly, but ...

0:54:47.141,0:54:49.637
well, kind of strictly because otherwise the logs gonna blow up.

0:54:54.470,0:54:57.680
Here is the KL divergence loss, I was tell you about earlier.

0:54:58.609,0:55:00.609
So here it's the ...

0:55:01.650,0:55:07.350
I mean it's return here in a funny form, but it's basically the ...,

0:55:09.754,0:55:11.519
Here again, it's sort of assumes ...

0:55:12.612,0:55:25.589
This is another one I was telling you about earlier actually this one is also a simplified one when you have a one-hot distribution for the target, so Y is a category.

0:55:28.171,0:55:31.954
But it has the disadvantage of not being merged with something like Softmax or log Softmax,

0:55:31.954,0:55:38.913
so it may reach, it may have kind of numerical issues.

0:55:40.698,0:55:45.185
Again it assumes X and Y are you know distributions.

0:55:48.749,0:55:52.398
This is barely used Poisson loss.

0:55:53.206,0:56:01.396
Okay, so this version of binary cross entropy here, takes scores that haven't gone through Sigmoid.

0:56:01.396,0:56:06.759
So this one does not assume that the X's are between 0 & 1,

0:56:07.599,0:56:10.108
it just takes, you know, values whatever they are.

0:56:10.108,0:56:15.024
And it passes them through a Sigmoid to make sure there are between 0 & 1 strictly.

0:56:15.251,0:56:15.886
Okay?

0:56:16.290,0:56:18.919
And so that is more likely to be a numerically stable.

0:56:21.099,0:56:25.813
It's a bit the same idea as kind of emerging log Softmax and negative log likelihood.

0:56:30.479,0:56:34.096
Very, yeah, same thing here. That's what I was talking.

0:56:34.096,0:56:38.572
Okay, margin losses. So this is sort of a important category of losses.

0:56:39.289,0:56:45.679
Those losses basically say if I have, in this case, two inputs,

0:56:50.143,0:56:57.205
the loss function here says I want one input to be larger than the other one by at least a margin.

0:56:58.049,0:57:01.458
Okay? So imagine the two inputs are scores for two categories.

0:57:01.981,0:57:09.069
You want the score for the correct category to be larger than the score for the incorrect category by at least some margin that you passed through the system.

0:57:11.424,0:57:15.010
And that's the formula you see down there, so it's basically a Hinge.

0:57:15.130,0:57:17.762
Okay? And it takes the difference between the two scores,

0:57:17.798,0:57:20.615
and so Y is a binary variable is +1 or -1,

0:57:20.615,0:57:25.899
and it controls whether you want X1 to be larger than X2,

0:57:26.563,0:57:28.777
or whether you want X2 to be larger than X1.

0:57:29.194,0:57:29.914
Okay?

0:57:29.915,0:57:31.495
We basically give it two scores,

0:57:31.519,0:57:34.576
and you tell it which one you want to be the larger score.

0:57:36.074,0:57:42.621
And then the cost function says, you know, if this one is larger than that one by at least a margin, then the cost is zero.

0:57:42.722,0:57:48.639
If it's smaller than the margin, or if it's in the other direction, and the cost increases linearly.

0:57:48.947,0:57:50.700
Okay? So that's called a Hinge loss.

0:57:51.069,0:57:51.502
Okay?

0:57:57.389,0:58:01.755
So that's very useful for a number of different things.

0:58:01.885,0:58:10.400
We've seen an example of this in .... so....

0:58:13.460,0:58:19.670
Yeah, for example, so this is sort of a margin ranking loss, so you have two values,

0:58:19.725,0:58:23.099
but there are sort of, this is a simplified version of it,

0:58:23.099,0:58:27.171
I mean there's a simpler version of it, which I don't have here for some reason.

0:58:27.760,0:58:29.475
We only have an X, okay?

0:58:29.475,0:58:32.373
So basically the loss is max of 0,

0:58:33.123,0:58:38.320
and -X times the margin,

0:58:38.770,0:58:43.039
and it just wants to make X smaller than the margin.

0:58:43.657,0:58:44.313
Right?

0:58:45.042,0:58:50.337
And so this is sort of a special case where you have a ranking between two scores of two categories.

0:58:50.500,0:58:53.369
So here is how you would use this for classification.

0:58:53.612,0:58:59.015
You would basically run your classifier; you would get scores.

0:58:59.116,0:58:59.678
Okay?

0:58:59.750,0:59:05.410
So before you do it in non-linearity weighted sums, and then you know the correct category,

0:59:05.824,0:59:09.790
so you say I want this correct category to have a high score,

0:59:11.208,0:59:16.670
and then what you do is you take another category that has the most offending score.

0:59:16.700,0:59:23.131
So either another category, so a category that is incorrect that has a higher score than the correct one,

0:59:23.621,0:59:26.240
or that has a lower score, but the lower score is too close.

0:59:26.300,0:59:36.856
 Okay? So you take the category that who score is the closest to the correct one, or who score is higher than the correct one,

0:59:37.773,0:59:40.979
and you feed those two scores to the loss function like this,

0:59:41.556,0:59:45.799
so basically it's going to push up the score in the correct category, push down the score the incorrect category

0:59:45.824,0:59:48.440
until the difference is at least equal to the margin.

0:59:49.293,0:59:50.065
Okay?

0:59:51.494,0:59:57.095
That's you know a perfectly good way of training something in the context of an energy-based model,

0:59:57.095,0:59:59.296
for example, that's the sort of things you might want to do

0:59:59.469,1:00:05.299
you might want to say X1 or -X1 is the energy,

1:00:05.566,1:00:08.996
I mean X, you know, -X1 would be the energy of the correct answer,

1:00:08.996,1:00:12.772
and -X2 would be the energy of the incorrect answer.

1:00:13.040,1:00:17.430
like an a contrastive term an incorrect answer.

1:00:17.995,1:00:23.749
and you want to push down the energy of the correct answer, push up the energy of incorrect answer, so that the difference is at least sum margin.

1:00:24.416,1:00:24.863
Okay?

1:00:25.782,1:00:27.319
We can use this kind of loss for that.

1:00:28.480,1:00:30.819
The Triplet loss is gonna be refinement on this.

1:00:30.819,1:00:39.583
So this is used a lot for metric learning for the kind of Siamese nets that Ishan Misra was talking about last week.

1:00:40.369,1:00:48.060
And there the idea is, let's say I have a distance.

1:00:48.060,1:00:49.558
So let's say I have three samples;

1:00:50.063,1:00:54.823
I have one sample, and another sample is very similar to it.

1:00:54.823,1:00:57.812
I've run them to two convolutional nets, I get two vectors.

1:00:57.978,1:01:02.049
I compute the distance between those two vectors: d(ai, pi) for example.

1:01:02.073,1:01:02.561
okay?

1:01:03.403,1:01:06.729
I want to make this distance as small as possible, because that's the correct sample.

1:01:07.559,1:01:11.017
And then I take two samples that I know are semantically different.

1:01:11.305,1:01:14.098
Okay? The image of a cat and whatever, table.

1:01:14.350,1:01:16.962
and I want to make the vector as far away from each other.

1:01:17.085,1:01:19.721
So I compute the distance, and I want to make this distance large.

1:01:20.067,1:01:20.810
All right?

1:01:21.772,1:01:26.538
Now, I can insist that the first distance be 0,

1:01:26.877,1:01:32.535
and it can insist that the second distance be larger than the margin that would be kind of a margin loss type thing.

1:01:33.682,1:01:37.398
But what I can do is one of those triplet margin loss where I say

1:01:37.679,1:01:44.536
the only thing I care about is that the distance that I get for the good pair is smaller than the distance that I get for the bad pair.

1:01:45.149,1:01:46.735
I don't care if the distance is small,

1:01:46.843,1:01:49.960
I just wanted to be smaller than the distance for the bad pair.

1:01:50.580,1:01:51.150
Okay?

1:01:53.174,1:01:55.179
And that's what those ranking loss do.

1:01:56.442,1:01:58.037
A bunch of those were..

1:02:00.110,1:02:09.060
I mean one of the first I think that was proposed was by Jason Weston and Samy Bengio back when Jason Weston was to the Google.

1:02:09.405,1:02:14.446
And they used this to train kind of an image search system for Google,

1:02:15.139,1:02:17.267
so back then I'm not sure is true anymore,

1:02:17.267,1:02:19.916
but back then you would type a query on Google,

1:02:19.940,1:02:23.114
Google would encode that query into a vector,

1:02:23.530,1:02:29.069
then we compare this to a whole bunch of vectors describing images that I've been previously indexed,

1:02:29.981,1:02:37.029
and then would kind of retrieve the images whose vector were close to the one that you had,

1:02:37.210,1:02:46.799
and the way you train the networks that compute those vectors, in that case back then it was linear networks actually, is you train them with those triplet loss.

1:02:47.000,1:02:55.430
Okay, so you said good hits for my search should have a distance between the vectors that is smaller than any bad hit.

1:02:55.530,1:02:59.569
and I don't care if the distance is small, I just wanted to be smaller than for bad hits.

1:03:01.577,1:03:02.356
Any question?

1:03:07.226,1:03:12.800
That's kind of a graphical explanation of this where p is a positive sample,

1:03:12.904,1:03:15.083
so it's you know similar to "a".

1:03:15.170,1:03:22.249
So "a" is the sample you considered, p is kind of a positive sample, and n is a negative sample or contrastive sample.

1:03:22.689,1:03:25.617
You want to push n away and bring p closer,

1:03:25.754,1:03:31.072
and as soon as p is closer than n by some margin, you stop pushing and pulling.

1:03:37.680,1:03:43.410
Yeah, soft versions of this, and in fact, you can think of NCE, the kind of loss function that Ishan was talking about,

1:03:43.851,1:03:50.980
as kind of a soft version of that where you basically have a bunch of positives and bunch of negatives,

1:03:50.980,1:03:52.980
or you have one positive and bunch of negatives,

1:03:53.650,1:03:56.137
and you run them through a Softmax,

1:03:56.649,1:04:07.983
and you say I want this the, you know, e to the minus distance for the correct one to be smaller than, you know, e to the minus the other one,

1:04:07.983,1:04:14.672
so it kind of you pushes the positive closer to you and pushes the other ones further to you,

1:04:14.672,1:04:20.293
but now with some sort of Softmax-ie sort of exponential decay as opposed to sort of a hard margin

1:04:24.876,1:04:27.786
So in Pytorch you have things that allow you to have multi labels,

1:04:27.786,1:04:32.793
so this allows you to basically give multiple correct outputs.

1:04:32.793,1:04:39.886
This is a ranking loss but instead of insisting that there is only one correct category,

1:04:39.911,1:04:44.888
and you know, you want a high score for the correct category, and bad score for everything else.

1:04:45.150,1:04:48.756
Here you can have a number of categories for which you want high scores,

1:04:49.463,1:04:51.563
and then all the other ones will get pushed away.

1:04:52.241,1:04:54.646
All right, they'll get, the scores will be pushed down.

1:04:55.680,1:05:01.529
So here it's a hinge loss, but you do a sum of those this hinge loss over all categories.

1:05:02.330,1:05:08.786
and for each category, if the category is a desired one you push it up,

1:05:08.810,1:05:13.683
if it's a non-desired one you push it down, which is what this formula says.

1:05:17.325,1:05:21.553
Of course you have the soft version of this which I'm not going to go into the details of,

1:05:23.605,1:05:26.337
and the multi margin version of it.

1:05:31.110,1:05:37.012
So, this pushing and pulling for metric learning for embedding for Siamese nets that I was telling you about.

1:05:38.250,1:05:43.572
It's actually kind of all implemented if you want in one of those Hinge Embedding Loss.

1:05:43.572,1:05:51.577
So Hinge Embedding Loss is a loss for Siamese nets that kind of pushes things are semantically similar to you,

1:05:51.577,1:05:54.530
and push away things that are not. Okay?

1:05:54.530,1:06:01.987
So, the Y variable indicates whether the pair you are or whether the score you are giving to the system is one that should be pushed up, or one should be pushed down.

1:06:02.449,1:06:11.433
and it chooses the hinge loss that makes the score positive if Y is +1,

1:06:11.610,1:06:17.470
and it makes a score negative by some margin Δ if Y is -1.

1:06:25.880,1:06:28.804
Very often when you are doing Siamese nets,

1:06:28.828,1:06:34.262
the way you compute the similarity between two vectors is not through Euclidean distance, but through cosine distance.

1:06:34.880,1:06:40.000
So the one minus the cosine of the angle between the two vectors,

1:06:40.231,1:06:44.512
this is basically a normalized Euclidean distance, if you want, you can think of it this way.

1:06:45.792,1:06:49.575
The advantage of this is that whenever you can push the distance,

1:06:50.289,1:06:53.609
whenever you have two vectors, and you want to make the distance as large as possible,

1:06:53.918,1:06:59.046
there's a very easy way for the system to get away with it by making the two vectors very large, very long,

1:06:59.313,1:07:03.483
you know, not pointing in the same direction and make them very very long,

1:07:03.507,1:07:05.004
so now the distance would be large.

1:07:06.050,1:07:07.180
But of course, that's not what you want.

1:07:07.180,1:07:09.581
You don't want the system to just make the vectors bigger;

1:07:09.605,1:07:12.503
you wanted to actually rotate the vector in the right direction.

1:07:12.765,1:07:17.909
So, you normalized the vectors, and then computed the normalized Euclidean distance, and that's basically what this does.

1:07:18.508,1:07:25.299
And what this does is that for positive cases, it tries to make the vectors as aligned with each other as possible,

1:07:25.881,1:07:32.781
and for negative pairs, it tries to make the cosine smaller than the particular margin.

1:07:33.322,1:07:38.850
The margin in that case should probably be something that kind of is close to zero.

1:07:39.650,1:07:41.650
So you want the cosine

1:07:41.930,1:07:48.403
in a high-dimensional space there's a lot of space near the equator of the sphere of the high-dimension sphere,

1:07:48.626,1:07:50.926
Okay, so all your points now are normalized on the sphere.

1:07:51.517,1:07:56.272
And what you want is samples that are semantically similar to you should be close to you,

1:07:57.059,1:07:59.485
but samples that are dissimilar should be orthogonal.

1:07:59.485,1:08:01.335
You don't want them to be opposed,

1:08:01.725,1:08:04.156
because there is only one point in the south pole,

1:08:04.582,1:08:11.654
whereas on the equator is a very very high large space, the entire sphere minus one dimension basically.

1:08:12.099,1:08:12.618
Okay?

1:08:13.737,1:08:20.539
So you can make the margin just, you know, so small positive value, and then you get the entire equator essentially of the sphere,

1:08:20.635,1:08:23.639
which contains the almost the entire volume of the sphere in high-dimension.

1:08:29.230,1:08:31.447
CTC Loss, this is a little more complicated,

1:08:31.447,1:08:38.577
because that's a loss that is basically uses structure prediction what's called structure prediction.

1:08:39.380,1:08:47.710
so this is... I sort of briefly talked about it very quickly a few weeks ago on something very similar to this.

1:08:48.219,1:09:03.019
So this is a loss is applicable when your output is a sequence of vectors of scores where the vectors correspond to scores of categories, okay?

1:09:04.549,1:09:07.955
And so you have ..., so your system computes a vector of such score.

1:09:07.955,1:09:10.663
So imagine, for example, speech recognition system.

1:09:10.929,1:09:20.018
Speech recognition system, every 10 milliseconds, gives you a vector of probabilities for what the sound being pronounced right now is,

1:09:20.799,1:09:24.169
and the number of categories usually is quite large on the order of a few thousand.

1:09:24.573,1:09:33.069
Okay? So give you basically a Softmax vector of a size, you know, typically 3000 let's say, one of those every 10 milliseconds.

1:09:33.741,1:09:34.216
All right?

1:09:35.421,1:09:41.039
And what you like, you know, you have a desired output, and the desired output is what word was being pronounced,

1:09:41.039,1:09:49.618
and a word that's being pronounced that corresponds to kind of a particular sequence of sounds if you want that you might you might know.

1:09:49.724,1:09:56.558
So what you need now is a cost that basically is low if that sequence looks like that sequence,

1:09:57.142,1:10:06.108
but what you might allow is for the input sequence to repeat some of the sounds, if you want.

1:10:06.415,1:10:06.899
Right?

1:10:06.900,1:10:16.704
So, for example, you know my cost to make the ..., the target might be the word seven let's say,

1:10:16.728,1:10:18.433
and it's pronounced really quickly, seven.

1:10:18.433,1:10:22.923
So you basically have, you know, a very small number of samples of each sound in the sequence.

1:10:23.565,1:10:30.479
But then perhaps the person who is pronouncing the word now that uses as a training sample pronounced it very slowly like seeevveeenn.

1:10:30.789,1:10:47.749
Right? So now the first "a" takes, you know, several frames of 10 milliseconds that should all be mapped to the the same instance of the "a" in the output.

1:10:48.180,1:10:51.259
And I do that picture before, but I gonna do it again.

1:10:51.726,1:10:53.660
Right? So you have ...

1:10:55.760,1:10:56.460
Let's see

1:11:00.330,1:11:06.287
You have a sequence of scores coming out of Softmaxes, let's say.

1:11:06.436,1:11:10.685
It's actually better if they are energies, but for CTC they need to be ...

1:11:12.260,1:11:15.520
and then you have the target sequence,

1:11:18.420,1:11:23.204
and think of this as some sort of matrix,

1:11:23.229,1:11:33.094
and each entry in that matrix basically measures the distance between the two vectors that are here.

1:11:33.296,1:11:33.866
Okay?

1:11:34.420,1:11:39.033
So one entry in the matrix indicates how this vector looks like that vector,

1:11:39.610,1:11:41.604
for example, with the cross entropy or something like that.

1:11:42.297,1:11:42.809
Okay?

1:11:43.105,1:11:45.998
Or squared error. It doesn't matter what the loss function is.

1:11:47.120,1:12:01.420
So now if this is the word seven pronounced slowly.

1:12:02.019,1:12:02.842
Okay?

1:12:03.499,1:12:08.540
and this has perhaps only one instance of each sound

1:12:09.620,1:12:11.620
You want all of those

1:12:12.350,1:12:24.550
you know, you would want all of those vectors corresponding to the "e" to be mapped to that "e" vector here.

1:12:25.617,1:12:26.574
Okay?

1:12:27.099,1:12:29.790
So you want to compute that cost of, you know,

1:12:30.740,1:12:35.919
confusing that those all of those, I mean, map matching those e's to that "e".

1:12:36.040,1:12:38.511
Now, of course here the system produce the correct answer,

1:12:38.691,1:12:39.862
so you don't have much of a problem,

1:12:40.331,1:12:42.195
but if the target is seven,

1:12:42.195,1:12:51.015
but the word that was pronounced here, or the output that was produced by the system does not correspond to seven,

1:12:52.631,1:12:54.824
that's when you run into trouble.

1:12:54.860,1:13:02.203
So here what you do is you find the best mapping from the input sequence to the output sequence.

1:13:02.228,1:13:06.865
Okay, so the "s" gets mapped to the "s", the "e" to the "e", the "v" to the "v",

1:13:06.890,1:13:10.413
the e's to the "e", and the "n" to the "n".

1:13:10.817,1:13:14.659
So you get this kind of path if you want that think of this as a path in the graph.

1:13:15.680,1:13:24.590
And the way determine this is basically by using a dynamic programming algorithm the shorted path algorithm that figures out how do I get from here to here

1:13:25.690,1:13:36.800
in the path that minimizes the sum of the distances between the all the vectors... all the distances between the vectors of, you know, all the points are going through.

1:13:38.131,1:13:38.831
Okay?

1:13:39.560,1:13:43.259
So there's a optimization respect to a latent variable if you want. Okay?

1:13:43.783,1:13:45.460
and CTC basically does that for you, right?

1:13:45.460,1:13:48.590
So you give it two sequences, and it computes the distance between them,

1:13:48.590,1:14:03.010
and you know, kind of the best kind of mapping between the two by allowing basically to map multiple input vectors to kind of a single one on the output,

1:14:03.010,1:14:06.529
it cannot expand it, it can only kind of reduce if you want.

1:14:07.920,1:14:10.939
And then that's done in a way that you can back propagate gradient to it.

1:14:11.870,1:14:15.359
We'll come back to this to more things like this at the end if you can.

1:14:15.883,1:14:16.283
oops

1:14:19.170,1:14:23.170
So this is what this the target is assumed to be many-to-one,

1:14:24.030,1:14:26.620
the alignment of the input to the target is assumed to be many-to-one,

1:14:26.620,1:14:31.896
which means the length of the target sequence such that it must be smaller than the length of the input.

1:14:31.920,1:14:33.520
That's for the reason I just explained.

1:14:35.210,1:14:38.567
Okay, so it's basically differentiable time-warping, you could think of it this way.

1:14:40.050,1:14:46.930
Or sort of a module that does dynamic time-warping or dynamic programming, and it's still differentiable.

1:14:47.220,1:14:54.050
The idea for this goes back in the early 90s in Léon Bottu PhD thesis, actually. That's very old.

1:14:53.330,1:15:00.965
Is there a good paper or resource to learn more about that dynamic programming algorithm there?

1:15:01.434,1:15:03.944
Actually, that's kind of what I'm gonna talk about next.

1:15:05.445,1:15:09.269
I may not have time to go through it, but I'll try to.

1:15:09.760,1:15:16.010
okay, but basically the last part of the energy based model tutorial,

1:15:17.375,1:15:17.959
Okay,

1:15:18.060,1:15:25.327
so the initial base model tutorial the 2006 paper that, we give you a reference a link to,

1:15:26.027,1:15:35.049
a tutorial on energy based models, the second part is all about this kind of stuff essentially.

1:15:36.422,1:15:41.198
Okay, so it's more energy based models, but now I'm gonna more of a supervised context if you want.

1:15:43.144,1:15:43.949
So

1:15:45.400,1:15:47.827
Preliminary, so before I get to this,

1:15:48.387,1:15:53.733
I want to come back to the sort of more general formulation of energy based models,

1:15:54.495,1:16:01.370
and the idea that ...

1:16:01.395,1:16:07.349
so if you want to kind of define energy based models in the proper way, these are the conditional versions,

1:16:08.280,1:16:13.402
you have a training set, a bunch of pairs (Xi, Yi), for i = 1 to P.

1:16:14.312,1:16:17.969
You have a loss functional. So, the loss functional L of (E, S).

1:16:18.199,1:16:20.851
So it takes the energy function computed by the system.

1:16:21.190,1:16:21.774
Okay?

1:16:21.998,1:16:25.040
and the training set, and it gives you a scalar value.

1:16:25.080,1:16:29.740
Now, you can think of this as a functional, so functional is a function of a function. Okay?

1:16:30.370,1:16:34.359
But in fact, because the energy function itself is parametrized by parameter W,

1:16:34.696,1:16:41.379
you can turn this loss functional into a loss function, which is not just a function of W another function of the energy function. Okay?

1:16:43.870,1:16:51.249
And of course the set of energy functions is called ε here is parametrized by the parameter W which is taken within the set.

1:16:53.709,1:16:59.569
So training consist in, of course minimizing the loss functional with respect to W and finding the W that minimizes it,

1:17:00.841,1:17:03.363
and so one question, you might ask yourself, you know,

1:17:03.363,1:17:06.434
I went to a whole bunch of objective function, loss functions here,

1:17:06.460,1:17:10.999
and the question is if you are in an energy based framework, what loss functions are good ones?

1:17:11.760,1:17:13.460
and what loss functions are bad ones?

1:17:14.510,1:17:19.180
how do you characterize a loss function that actually will do something useful for you?

1:17:19.504,1:17:20.389
Okay?

1:17:21.478,1:17:27.160
So here is a general formulation of the loss function, it's an average over training samples,

1:17:28.495,1:17:32.889
so here I'm kind of assuming that it's invariant under permutation of the samples.

1:17:32.890,1:17:37.799
So an average is as good as any other aggregation aggregating functions.

1:17:37.840,1:17:43.020
So it's the average over training samples of a per-sample loss function, capital L,

1:17:43.100,1:17:50.469
and it takes the desired answer Y which could be just a category, or it could be a whole image or whatever.

1:17:50.850,1:17:58.950
And it takes the energy function where the X variable Xi is equal to Xi, the i is training sample.

1:18:00.197,1:18:02.590
And the Y variable is undetermined. Okay?

1:18:02.640,1:18:12.730
So E(W,y,Xi) is basically the entire shape of the energy function for all values of Y, over values of Y, for a given X.

1:18:12.950,1:18:14.727
Okay? X equal to Xi.

1:18:14.751,1:18:16.751
And you can have a regularizer, if you want.

1:18:18.460,1:18:21.010
Okay? So here this is a loss functional again.

1:18:22.100,1:18:25.193
Again, of course we have to design this loss functional,

1:18:25.193,1:18:27.297
so that it makes the energy of correct answer small,

1:18:27.321,1:18:32.059
and the energy of incorrect answers large in some ways, right?

1:18:34.890,1:18:38.745
Okay. Now, we're going to go through a bunch of different types of loss functions.

1:18:39.978,1:18:47.110
So one thing we could do is say my loss function is just going to be the energy of the correct answer.

1:18:47.217,1:18:51.210
So I'm gonna place myself in the context of energy based model,

1:18:51.520,1:18:54.700
my system produces scores, I interpret those scores as energies,

1:18:54.859,1:19:03.164
so high is bad good is good, I mean low is good, as opposed to positive scores.

1:19:05.120,1:19:12.489
and what I'm just going to do is define my energy functional,

1:19:13.260,1:19:18.402
as a function of the energy function of the function of Y as simply the energy that my model gives to the correct answer.

1:19:18.684,1:19:19.204
Okay?

1:19:20.192,1:19:28.590
So basically I give it an X, and I give it the correct answer Y, and ask the system what energy do you give to that pair,

1:19:28.700,1:19:30.994
and then I try to make that energy as small as possible.

1:19:31.281,1:19:31.801
Okay?

1:19:32.140,1:19:35.020
So you have this landscape of energies here.

1:19:35.045,1:19:39.813
Now, I showed you this slide in the context of unsupervised self-supervised learning,

1:19:39.838,1:19:42.211
here I'm showing to you in the context of supervised learning.

1:19:42.417,1:19:46.175
So imagine that one of the variables is X, and the other variable is Y.

1:19:46.261,1:19:46.694
Okay?

1:19:47.163,1:19:51.415
And the blue beads are training samples, and you want to make the energy of the blue beads as small as possible.

1:19:54.633,1:19:57.297
So you're pulling down on the blue beads, but you're not doing anything else.

1:19:57.758,1:20:00.499
and so as a result depending on the architecture or your network,

1:20:00.700,1:20:05.230
if your network is not designed properly, or if it's designing in no particular way,

1:20:06.658,1:20:09.820
it could very well be that the energy function is going to become flat everywhere.

1:20:10.277,1:20:13.062
Okay? you're just trying to make the energy of the correct answer small,

1:20:13.278,1:20:16.027
and you not telling the system the energy of everything else should be higher,

1:20:16.705,1:20:20.611
and so the system might just collapse. All right?

1:20:22.910,1:20:25.362
So energy loss is not good in that sense,

1:20:25.874,1:20:27.960
but there are certain situations where it's applicable,

1:20:27.960,1:20:34.658
because if the shape of the energy function is such that, it cannot make the

1:20:34.820,1:20:40.644
it can only make the energy of a single answer small, the all the other ones being larger.

1:20:40.798,1:20:43.466
They don't need to have a contrastive term.

1:20:44.036,1:20:46.583
Okay? and we've seen this in the context of self-supervised learning.

1:20:47.456,1:20:50.860
People are completely lost about the loss functional.

1:20:52.635,1:20:54.143
Right, okay.

1:20:55.376,1:20:59.883
So this is a functional L, and it's a function of another function E, okay?

1:21:02.002,1:21:04.389
So it's called a functional, because it's a function of a function.

1:21:05.564,1:21:07.825
Right? It's not a function of a point, it's a function of a function.

1:21:07.850,1:21:14.730
Now, if that second function is parameterized by a parameter W,

1:21:15.230,1:21:20.991
then you can say that the loss function is actually a function of that parameter W, now, it becomes a regular function.

1:21:21.016,1:21:22.254
Okay, that's what I had in the ...

1:21:22.301,1:21:23.682
Can you can you write it down?

1:21:24.373,1:21:26.027
It's basically written here.

1:21:26.547,1:21:36.306
Okay, you can either write the functional as L(E,S), so that's a functional because it's a function of E which itself is a function.

1:21:37.374,1:21:38.053
Okay?

1:21:39.261,1:21:42.832
But E itself is a function of W,

1:21:43.086,1:21:47.562
and so, if I write the loss function directly as a function of W, now, it's just a regular function.

1:21:50.789,1:21:51.395
Okay?

1:21:55.895,1:21:59.220
Yeah, I mean I asked the question that was asked in the chat.

1:21:59.245,1:22:03.189
Yeah, I understand, I know.

1:22:11.816,1:22:13.450
Before that, you know for

1:22:15.483,1:22:21.739
Okay, we've seen the negative log likelihood loss before, I talked about this.

1:22:21.739,1:22:28.524
So this is a loss function that tries to make the energy of the correct answer ...

1:22:28.524,1:22:31.262
so look at the rectangle in red ...

1:22:31.374,1:22:33.916
tries to make the energy of the correct answer as low as possible.

1:22:35.440,1:22:42.696
And then you have the second term: 1/β log sum over all Y's of e to the -βE(W,y,Xi)

1:22:44.684,1:22:51.150
and this one is trying to make the energy of all Y's for this given X as large as possible.

1:22:51.628,1:22:55.852
Okay? because the best way to make this term small is to make those energies large,

1:22:55.884,1:23:00.580
because they enter in there as a negative exponential.

1:23:02.870,1:23:09.647
Okay, so this has this kind of pushing down on the correct answer, pushing up on incorrect answer behavior.

1:23:12.352,1:23:18.222
And we've seen before. We just talked about margin loss and other types of losses.

1:23:19.520,1:23:21.543
Here is something that's called a perceptron loss,

1:23:21.543,1:23:30.836
because it's basically very similar to, I mean, it's exactly the same as the loss that was used for the perceptron over 60 years ago.

1:23:30.915,1:23:36.851
So this one says I want to make the energy of the correct answer small,

1:23:39.761,1:23:48.439
and the same time I want to make the energy of the the smallest energy for all answers as large as possible, okay?

1:23:48.439,1:23:53.653
So pick the Y that has the smallest energy in your system, make that as large as you can,

1:23:54.002,1:23:57.069
and the same time pick the correct energy, make that as small as you can.

1:23:57.153,1:24:05.060
Now, there is a point at which the answer with the correct energy is going to equal to the correct answer,

1:24:05.929,1:24:08.736
so that difference can never be negative. Okay?

1:24:09.037,1:24:13.886
Because the first term is necessarily one term in that minimum,

1:24:14.243,1:24:23.297
and so the difference is at best zero, and for every other cases is positive.

1:24:24.060,1:24:26.786
It's only zero when the system gives you the correct answer.

1:24:27.825,1:24:28.582
Okay?

1:24:29.087,1:24:35.523
But this objective function does not prevent the system from giving the very same energy to every answer.

1:24:36.114,1:24:36.716
Okay?

1:24:36.819,1:24:39.318
So in that sense, it's a bad energy, it's a bad loss function.

1:24:40.039,1:24:44.997
It's a bad loss function because it says I want the energy of the correct answer to be small,

1:24:45.203,1:24:48.262
I want the energy of all the other answers to be large,

1:24:48.716,1:24:50.732
but I don't insist that there is any difference between them.

1:24:50.732,1:24:53.722
So the system can choose to make every answer the same energy.

1:24:55.290,1:24:56.731
And that's a collapse.

1:24:58.643,1:25:08.493
Okay, so perceptron loss is not good, it's actually only good for linear systems, but it's not good as a objective function for non-linear systems.

1:25:10.398,1:25:14.390
So here is a way to design an objective function that will always be good,

1:25:15.333,1:25:21.475
and you take the energy of the correct answer, and you take the energy of the most offending incorrect answer which means

1:25:22.317,1:25:28.499
the value of Y that is incorrect, but at the same time is the lowest energy of all the incorrect answers.

1:25:28.874,1:25:29.321
Okay?

1:25:30.526,1:25:34.216
And your system will work if that difference is negative.

1:25:34.264,1:25:42.118
In other words, if the energy of the correct answer is smaller than the energy of the most offending incorrect answer by at least some quantity, some margin.

1:25:43.316,1:25:48.396
Okay? So as long as your objective function will you design it ensures that

1:25:48.396,1:25:53.955
the energy of the correct answer is smaller than the energy of the most offending incorrect answer by at least some margin, nonzero margin,

1:25:54.489,1:25:59.068
then you'll fine, your loss function is good.

1:26:00.840,1:26:04.078
Okay, so things like hinge loss are good.

1:26:04.149,1:26:08.660
The hinge loss basically says, and we talked about this just before,

1:26:09.244,1:26:12.977
I want the energy of the correct answer to be smaller than the energy of the most offending incorrect answer,

1:26:12.977,1:26:16.919
which is denoted Yi-bar here, by at least m.

1:26:17.543,1:26:20.150
Okay? This is what this loss function does. It's a hinge loss,

1:26:20.816,1:26:27.090
and it wants to push down the energy of this guy below the energy of that guy by at least this margin.

1:26:29.642,1:26:31.569
So this has a margin m,

1:26:31.569,1:26:36.995
and this will, you know, if you train a system with this loss, it will and it can run the task.

1:26:36.995,1:26:41.020
you will run the task, and probably produce the good answers.

1:26:42.147,1:26:48.389
The hinge loss, the soft hinge loss, which is in the context of energy based models is expressed this way,

1:26:48.719,1:26:56.613
basically instead of feeding the difference between the energies of the correct answer and the most offending incorrect one in to a hinge,

1:26:56.613,1:26:59.751
 it feeds it to a soft hinge.

1:26:59.898,1:27:03.152
Okay, which we talked about 30 minutes ago

1:27:04.908,1:27:08.343
And there, this was also has a margin, the margin of

1:27:08.518,1:27:12.337
How to pick m? the question would be how to pick m?

1:27:14.508,1:27:20.372
It's arbitrary. You can set m to 1, you can set m to 0.1.

1:27:20.372,1:27:25.372
 I mean, it's kind of arbitrary because it will just determine the size of the weights of your last layer.

1:27:25.411,1:27:26.895
That's all it does. Okay?

1:27:30.383,1:27:31.587
So it's basically up to you.

1:27:34.619,1:27:37.857
Yeah, so the soft hinge loss has an infinite margin.

1:27:37.881,1:27:40.388
it wants the difference between those two energies to be infinite,

1:27:40.595,1:27:43.689
but the slope decreases exponentially,

1:27:43.689,1:27:51.141
so it's never going to get there because, you know, the gradients get very small as the difference increases.

1:27:55.745,1:28:01.705
Here's another example of a margin loss, the square loss, the square-square loss. Okay?

1:28:01.705,1:28:09.462
So this is a loss that tries to make the energy of the correct answer squared as small as possible,

1:28:09.487,1:28:15.949
and then it has a square hinge to push away to push up the energy of the most offending incorrect answers.

1:28:17.399,1:28:24.895
Okay, and again that works, and this is very similar to the kind of loss that people use in Siamese nets and stuff like that you've heard about.

1:28:25.583,1:28:28.837
There's a whole menagerie of such losses, which I'm not going to go through.

1:28:29.742,1:28:34.384
There's actually a whole table here, which is also in this paper the tutorial energy based models,

1:28:34.607,1:28:41.065
and what's indicated on the right side is whether they have a margin or not.

1:28:41.534,1:28:44.699
So the energy loss does not have a margin, it doesn't push up anything,

1:28:44.724,1:28:48.653
so no margin, it doesn't work always,

1:28:48.708,1:28:52.795
you have to design the machine, so that this loss may work for that system.

1:28:53.192,1:28:56.652
The perceptron loss does not work in general,

1:28:56.652,1:29:01.833
it only works if you have a linear parameterization of your energy as a function of the parameters,

1:29:01.858,1:29:04.596
but that's a special case, and that's the case for the perceptron,

1:29:06.329,1:29:10.405
and then some of them have a finite margin like the hinge loss,

1:29:10.480,1:29:19.280
and some of them have an infinite margin like the log, the soft hinge, if you want.

1:29:21.473,1:29:30.547
Whole bunch of those losses some of those were used were invented in the context of discriminative learning for speech recognition systems,

1:29:30.928,1:29:37.999
but not ... they were invented before people in machine learning actually got interested in this.

1:29:38.730,1:29:41.903
The question would be like how you find the Y-bar?

1:29:41.928,1:29:46.808
So if you have like a discrete code, we can find simply like, you know, the minimum value.

1:29:46.808,1:29:49.245
But otherwise, are we running gradient descent?

1:29:53.940,1:29:56.884
Right, so if Y is continuous,

1:29:57.503,1:30:03.263
then there is no kind of clear definition for what is the most offending incorrect answer, okay?

1:30:03.485,1:30:12.154
You will have to define some sort of distance around the correct answer, above which you consider an answer to be incorrect.

1:30:12.979,1:30:18.653
Okay? So for example you are in a continuous energy landscape, there is one training sample here.

1:30:19.010,1:30:22.271
You want to make that the energy of that training sample small easy enough,

1:30:22.809,1:30:27.110
compute the energy through your neural net, push it down, back propagate, update the weight.

1:30:27.135,1:30:28.808
So that the energy goes down, easy enough.

1:30:28.872,1:30:34.714
Now, the incorrect answer if you take an answer that just kind of ε outside of that,

1:30:36.735,1:30:37.522
and you push up,

1:30:37.547,1:30:45.027
you know, your energy surface might be a little stiff because it's computed by a parameterized neural net, so that may not be possible.

1:30:45.058,1:30:52.583
So you probably want to have a incorrect answer that's, you know, quite a bit outside that you're gonna, you know, push up.

1:30:54.545,1:30:59.839
And so that's how you define, you know, the whole question is how you define the contrastive sample that you gonna push up.

1:31:00.480,1:31:12.171
And the a lot of those objective functions here, those loss functions use a single, you know, Y-bar, negative sample.

1:31:12.386,1:31:19.116
But there is no single correct way of picking this Y-bar,

1:31:19.116,1:31:25.814
you can imagine, you know, particularly in the kind of, in the sort of continuous case, or

1:31:25.850,1:31:33.559
in the case where Y is either very very large or continuous and high-dimensional.

1:31:33.929,1:31:38.296
There's no simple way to pick Y-bar.

1:31:38.376,1:31:45.291
You know a lot of discussions we've had about contrastive methods that Ishan talked about for Siamese nets and that we talked about before,

1:31:45.690,1:31:51.149
where basically, how do you pick a Y-bar in the self-supervised case, in the self-supervised you don't have X.

1:31:51.691,1:31:52.045
Right?

1:31:53.715,1:31:57.106
And, you know, there is many ways you can pick it up,

1:32:00.785,1:32:03.049
it's only obvious how to pick it up in kind of small cases.

1:32:03.761,1:32:06.524
I just want to point out the formula here at the bottom,

1:32:07.022,1:32:19.592
so this is a kind of you can think of this as sort of, a general form of, sort of hinge type, contrastive losses where

1:32:20.586,1:32:24.114
you have an H function here, think of it as a hinge of some type.

1:32:24.547,1:32:25.106
Okay?

1:32:25.651,1:32:30.878
and instead of that hinge, you have the energy of the correct answer,

1:32:30.878,1:32:33.362
so that's the energy of (W,Yi,Xi),

1:32:33.362,1:32:36.746
so this is your training sample, that's the energy of the system gives to the training sample.

1:32:37.500,1:32:43.158
The second term is the energy of some/sum other answer, y, okay?

1:32:44.945,1:32:47.646
for the same X training sample,

1:32:47.693,1:32:53.756
and then there is a margin, but that margin C is actually a function of Yi and y,

1:32:53.756,1:32:56.822
and you might imagine the margin is actually also a function of X and Xi.

1:32:58.099,1:33:03.582
So basically you determine a margin as a function of the distance between the Y's.

1:33:03.891,1:33:04.504
Okay?

1:33:06.655,1:33:08.976
and you feed that to let's say a hinge.

1:33:09.670,1:33:10.586
Now, the thing is

1:33:10.611,1:33:13.066
this loss function is summed over all Y's,

1:33:13.090,1:33:16.191
here is a discrete song, because Y is discrete, but you could imagine an integral.

1:33:16.371,1:33:16.789
Okay?

1:33:17.470,1:33:23.313
So this kind of loss says, you know, I have an energy for my correct answer,

1:33:23.574,1:33:26.419
I have energies for every other answer in my space,

1:33:27.352,1:33:29.603
and I want to push up the energy of all other answers,

1:33:30.210,1:33:39.855
but the amount by which I want to make them higher the margin depends on the distance between Y and Y-bar,

1:33:40.993,1:33:46.882
or in this case between Yi which is this, and y which is the otherwise.

1:33:47.725,1:33:53.967
Okay, so you could imagine that this margin will come, you know, will come smaller and smaller as the two y is gonna get closer to each other,

1:33:53.967,1:34:00.597
in this case, you don't push up too much for things are too close than you push up in proportion to the distance of the Y,

1:34:00.622,1:34:02.788
you know, whatever distance you think is appropriate.

1:34:04.658,1:34:08.647
This is of course a more difficult loss function to optimize.

1:34:10.799,1:34:17.326
I'm outta time, so I might talk about the structure prediction issue that I said I was going to talk about at the later time.

1:34:18.720,1:34:20.119
Any more question?

1:34:25.578,1:34:31.657
The contrastive method for the self-supervised learning papers,

1:34:31.847,1:34:40.152
they are usually ??? take the random images as the negative examples.

1:34:40.177,1:34:45.240
Do you have any idea have been used these functions anyone's tried experimented with these?

1:34:45.642,1:34:47.470
Either use what kind of function?

1:34:47.570,1:34:51.255
These loss functions that you explain to us now.

1:34:52.042,1:34:59.456
So most of them use the basically the negative log likelihood loss here, which in this panel is called NLL/MMI.

1:34:59.780,1:35:05.176
Okay, so NCE that you heard about from Ishan, that's what they used, right?

1:35:05.383,1:35:08.430
They're trying to make the distance between the samples as small as possible,

1:35:08.430,1:35:14.206
and then the contrastive term is, you know, is basically your log softmax of the distances.

1:35:14.486,1:35:19.678
So when you compute the log softmax, you think of a distance as an energy,

1:35:19.678,1:35:21.934
and then you compute the log softmax of those energies,

1:35:22.822,1:35:28.014
you get this formula here in the second last line called NLL/MMI.

1:35:28.497,1:35:32.806
MMI means maximum, ???? approximated by taking random images?

1:35:35.030,1:35:36.038
The random what?

1:35:36.760,1:35:42.101
Making random images as negative example, so that will use for approximate the integral?

1:35:42.706,1:35:46.965
Well, so basically you can't compute this integral over all Y, so

1:35:47.632,1:35:49.846
or this a sum if Y is discrete,

1:35:50.382,1:35:58.671
and so you basically approximate the sum by, you know, a few terms that you pick randomly.

1:35:58.747,1:35:59.123
Right?

1:35:59.248,1:36:00.351
Yeah. That's called Monte Carlo.

1:36:00.444,1:36:07.483
I mean basically if you want to do this properly, you have to pick those samples according to the rule of Monte Carlo sampling,

1:36:07.900,1:36:11.770
 but it doesn't matter. I mean that's why hard-negative mining is hard.

1:36:12.338,1:36:19.018
Okay, that's why what makes the difference between MoCo, PIRL, SimCLR, etc is how you pick those negative samples.

1:36:19.709,1:36:31.097
That's why I said there is no kind of in cases where Y space is high-dimensional, there is no, you know, predefined way of picking negative samples essentially.

1:36:31.328,1:36:33.169
It's only in classification that this easy.

1:36:33.756,1:36:39.940
Have other people experiment about other losses?

1:36:41.340,1:36:49.981
Yeah, I mean there are a lot of people are using the square-square or the sort of hinge, you know, with the difference of energies,

1:36:50.702,1:36:54.562
so some of the systems are being used by at least at some point,

1:36:55.385,1:37:01.149
the system that deep face which is the face recognition system that used by Facebook to tag people.

1:37:02.678,1:37:11.974
It used the convolutional net trained in supervised mode with a certain number of categories basically images from, I don't know, million people or something,

1:37:12.747,1:37:17.855
but then there is a fine-tuning phase that use metric learning basically Siamese nets

1:37:17.885,1:37:23.461
where you show two photos of the same person, and you say those are the same person,

1:37:23.612,1:37:25.746
and then two photos of different people, and you push them apart,

1:37:26.144,1:37:29.223
and that used, they tried different objective functions,

1:37:29.233,1:37:34.375
but I think they were using the square-square loss at some point, or maybe the squared exponential ,

1:37:34.771,1:37:38.536
and, now, entirely sure what they're using now, but, you know, it's one of those.

1:37:41.322,1:37:44.790
Professor, what topics will you covered in the next lecture?

1:37:47.229,1:37:50.760
Okay, so we're gonna have two guest lectures,

1:37:50.816,1:37:58.052
so next is ‪Mike Lewis, ‪Mike Lewis‬ is a research scientist at Facebook AI research in Seattle,

1:37:58.878,1:38:03.687
and he is a specialist of natural language processing and translation,

1:38:04.056,1:38:14.529
so it's gonna, you know, tell you all the interesting tidbits about sequence to sequence, about transformers, about NLP, and about translation.

1:38:14.961,1:38:15.452
Okay?

1:38:17.693,1:38:23.986
And you know, he knows a lot, you know, much better the details about this than I do, so he's the right person to talk about this.

1:38:24.686,1:38:27.804
We're gonna have another guest lecture is going to be Xavier Bresson.

1:38:27.829,1:38:32.641
He is, you know, one of the world specialist of graph neural nets.

1:38:35.151,1:38:38.833
So this is the whole idea of, you know, how do you apply neural nets,

1:38:39.000,1:38:44.318
you know, you can think of an image as a function on a regular grid, okay?

1:38:44.357,1:38:49.881
every pixel is a location on a regular grid, you can think of an image as a function on that grid, okay?

1:38:50.119,1:38:54.989
So grid is a graph of a particular type, and the image is just a function on the graph.

1:38:55.965,1:39:02.832
You can think of, I don't know, a video as, you know, a regular 3D-grid where you have space and time,

1:39:03.340,1:39:11.028
and you know, most natural signals you can think of them as functions on kind of regular graphs, okay?

1:39:11.514,1:39:18.679
What about the case where the function you're interested in is not on the euclidean graph if you want?

1:39:19.031,1:39:26.031
So let's imagine, for example, you take a photo with a panoramic camera, okay? 360 camera, right?

1:39:26.072,1:39:30.047
So it's a camera that basically takes a spherical image, okay?

1:39:30.047,1:39:35.151
So now your pixels live on the sphere, how do you compute the convolution on the sphere?

1:39:36.500,1:39:37.063
Okay?

1:39:38.390,1:39:42.080
So you want to run your convolutional net on this image that now lives on the sphere,

1:39:43.598,1:39:48.586
You can't use the standard ways of computing convolutions.

1:39:50.163,1:39:52.832
So you have to figure out how to compute convolutions on the sphere.

1:39:53.251,1:39:53.640
Right?

1:39:53.641,1:39:55.078
So that's an example.

1:39:55.582,1:40:01.939
Now, here's something a little more complicated, imagine now that you have a 3D-scanner,

1:40:02.547,1:40:07.720
and you're capturing, I don't know, a dancer, you know, someone kind of in front of a 3D-scanner.

1:40:10.197,1:40:13.506
and that person has a particular pose, let's say like this. Okay?

1:40:13.531,1:40:20.671
And then you take another 3D-picture, 3D-data from another person.

1:40:20.696,1:40:24.164
And that other person is, you know, in another pose.

1:40:24.189,1:40:28.325
That person has a different body shape, she's in a different body pose.

1:40:29.674,1:40:33.896
And now what you want is you want to be able to map one on to the other,

1:40:33.896,1:40:39.875
you want to be able to say like, you know, what is the hand in the first person? where is the hand in the second person?

1:40:40.019,1:40:49.144
so what you have to do now is basically have a neural net that takes into account a 3D-mesh that represents the geometry of a hand,

1:40:51.807,1:40:53.978
and train it to tell you it's a hand.

1:40:54.346,1:40:56.389
So that when you apply it to the hand, it tells you it's a hand,

1:40:56.414,1:41:00.620
when you apply to the other parts of the body, it tells you it's something else,

1:41:00.692,1:41:03.245
but the data you have is not an image, it's a 3D-mesh.

1:41:03.569,1:41:04.110
Okay?

1:41:04.184,1:41:08.612
The mesh may have different resolutions, the triangles make here occur at different places,

1:41:09.014,1:41:15.813
so how you define your convolutions on the domain like this that is independent of the resolution of the mesh,

1:41:16.037,1:41:18.497
and only kind of depends on the shape,

1:41:18.764,1:41:25.021
so that you can classify a hand regardless of the orientation, the size, the conformation,

1:41:25.390,1:41:28.729
and the body shape of the person, you know, things like that, right?

1:41:29.246,1:41:34.602
So, here's another example, that's perhaps more interesting.

1:41:35.332,1:41:40.887
You want to do, you want to train something like a Siamese net,

1:41:41.882,1:41:44.735
but you want to train this Siamese net to tell you

1:41:44.966,1:41:49.854
whether one molecule is going to stick to another molecule, right?

1:41:49.854,1:41:56.470
So you give two molecules to your neural net, and the neural net produces two vectors,

1:41:56.605,1:42:03.625
if those two molecules stick together, it gives you two vectors whose distance is small.

1:42:04.745,1:42:05.451
Okay?

1:42:06.056,1:42:08.372
And if they don't stick together, then the distance is large.

1:42:08.427,1:42:12.117
Okay, so you can think of the distance as kind of the negative free energy of the binding,

1:42:12.204,1:42:18.760
the binding energy of the two, you know, the two molecules.

1:42:19.735,1:42:20.346
Right?

1:42:20.901,1:42:25.241
Or the, you know, the the free energy, you know, minus a constant if you want.

1:42:25.449,1:42:26.592
So ...

1:42:30.242,1:42:31.845
So you would train this as a Siamese net,

1:42:31.870,1:42:39.463
but then the problem is how you represent a molecule to a network knowing that it's the same network you're going to apply to this molecule on that molecule,

1:42:39.488,1:42:43.088
and the two molecules don't have the same shape, they don't have the same length,

1:42:43.113,1:42:45.103
they don't have the same number of atoms, they don't have the same ...

1:42:45.128,1:42:47.849
Like how do you represent a molecule?

1:42:47.874,1:42:49.929
The best way to represent a molecule is as a graph.

1:42:50.310,1:42:53.339
It's basically a graph whose structure changes with the molecule,

1:42:53.920,1:43:00.856
and this graph is annotated by the identity of the atoms at each site

1:43:00.912,1:43:04.450
maybe by the location in 3D-space for their relative location,

1:43:04.475,1:43:09.020
maybe by the angle of the bonds between two successive atoms,

1:43:10.456,1:43:12.908
or the binding energy of that particular bond or things like this.

1:43:12.933,1:43:13.456
So ...

1:43:14.678,1:43:19.158
so the best way to represent a molecule is by representing as a graph basically,

1:43:19.770,1:43:24.019
and there's another example, perhaps more relevant to something like Facebook.

1:43:25.356,1:43:26.189
Let's say ...

1:43:29.712,1:43:33.631
Let's say I want to kind of infer, or let's say Amazon or something.

1:43:33.996,1:43:36.774
I want to infer what type of ...

1:43:37.925,1:43:39.583
Let's say I'm Amazon, right?

1:43:39.608,1:43:43.814
and I have a customer, and that customer has bought a whole bunch of different things,

1:43:43.893,1:43:47.161
and that customer has commented of whole bunch of different things.

1:43:47.249,1:43:53.985
I could think of kind of encoding this as a vector, but it would be a vector of variable size,

1:43:54.010,1:43:57.740
because you know people buy different numbers of things and stuff like that,

1:43:57.740,1:44:00.461
so we need to, so find a way to aggregate that data,

1:44:00.486,1:44:03.456
so that everybody can be represented by the same fixed size vector,

1:44:03.719,1:44:15.004
but what if instead, I represent the person, and all the things that that person has bought, and all the, you know, reviews that person are made, and etc as a graph essentially.

1:44:15.321,1:44:22.542
and then I represent what I feed to the neural net is the graph with values on the nodes and perhaps the arcs.

1:44:23.610,1:44:28.933
if I have a way of representing a graph, so that it can connect a neural net independently of the shape of the graph,

1:44:29.227,1:44:30.687
then I can do this kind of application.

1:44:31.916,1:44:36.089
And so this is what graph neural nets are about. It's a very very hot topic at the moment.

1:44:36.297,1:44:40.422
It's extremely promising for a lot of applications particularly in biomedicine,

1:44:40.447,1:44:47.139
you know, in chemistry, in material science, but also in social science for the social network analysis,

1:44:47.560,1:44:53.163
and you know, all kinds of applications, computer graphics, you know, all kind of the stuff.

1:44:53.188,1:44:54.902
So it's really cool.

1:44:54.926,1:45:00.362
Xavier is really one of the experts on this topic, so I'm really happy that he accepted to give us a talk.

1:45:00.387,1:45:03.466
So it's not gonna be easy for him because he is in Singapore.

1:45:03.491,1:45:06.006
So he's gonna be fine the morning for him.

1:45:06.292,1:45:06.712
That's right.

1:45:06.737,1:45:11.506
Well, I'm giving a lecture in a couple days in Hong Kong, so same thing for me.

1:45:12.540,1:45:17.420
I see. So, actually, he's from Nanyang Technological University

1:45:17.445,1:45:19.992
NTU, right?

1:45:20.079,1:45:21.397
NTU, Yeah.

1:45:21.468,1:45:25.418
Yeah. I was confused, correct.

1:45:26.505,1:45:27.703
All right? so that was it.

1:45:27.727,1:45:29.727
Sorry, there is one more questions.

1:45:29.973,1:45:32.838
Yeah, that was really interesting professor.

1:45:33.121,1:45:37.221
I have one more question. I was reading this term called normalizing flows.

1:45:37.295,1:45:37.866
Yeah

1:45:37.891,1:45:39.581
and I don't understand what they are?

1:45:39.606,1:45:44.099
Could you just give some intuition into why people are excited about it?

1:45:45.432,1:45:46.083
Right.

1:45:46.108,1:45:47.781
So normalizing flows

1:45:48.655,1:45:51.766
So it's not a technic that have a lot of experience with, but, you know, I've read the papers.

1:45:52.410,1:45:53.820
it's

1:45:54.081,1:46:02.038
so it's proposed by Danilo Rezende and Shakir Mohamed at DeepMind

1:46:02.063,1:46:05.380
 a while ago, and a while back, maybe five years ago or so.

1:46:06.134,1:46:11.881
and it's basically a sort of a density estimation method, so it's a little bit like Gans.

1:46:11.906,1:46:14.465
It has a bit of the same spirit as Gans,

1:46:15.901,1:46:21.028
and it gets inspiration from ICA (independent component analysis).

1:46:21.687,1:46:25.409
Although it's not kind of explicit in the original paper, but here's the basic idea.

1:46:25.434,1:46:35.799
Basic idea is you want to train a neural net to transform a known distribution from which you can sample, into a distribution that happens to be the distribution of your data.

1:46:36.385,1:46:36.890
Okay?

1:46:37.258,1:46:42.814
So let's imagine that you have a latent variable z that you sample from a gaussian distribution,

1:46:43.772,1:46:48.382
and you run it through a function or uniform distribution over a domain, Okay?

1:46:48.407,1:46:50.684
You run it through a function implemented by neural net,

1:46:51.396,1:46:56.874
and you want to train this neural net that, so that the distribution you get at the output is the one you want that corresponds to your data.

1:46:57.746,1:46:58.317
Okay?

1:46:59.976,1:47:00.714
and

1:47:02.836,1:47:04.099
So the

1:47:07.080,1:47:11.837
Let me give you a very simple example.

1:47:11.862,1:47:19.429
So let's say have a variable z, and have observed variable y,

1:47:20.308,1:47:27.371
and I sample my variable z with the uniform distribution, okay? between say 0 and 1.

1:47:28.625,1:47:29.236
Okay?

1:47:30.744,1:47:37.743
and what I want on the output is, I don't know, say gaussian.

1:47:38.041,1:47:42.920
It's kind of stupid to want to gaussian, but it's say I want the gaussian, because I could sample from a gaussian easily.

1:47:44.246,1:47:51.183
So what I need to do is kind of transform this uniform distribution into a gaussian by a mapping

1:47:51.208,1:47:52.946
 and the mapping is going to be a function like ...

1:47:57.566,1:48:01.876
It's gonna be if function ...

1:48:01.900,1:48:03.400
Okay? zero is here.

1:48:07.923,1:48:09.400
Kind of like this if you want.

1:48:10.613,1:48:11.311
Okay

1:48:13.613,1:48:18.866
And this is the inverse of the integral of the gaussian distribution.

1:48:20.314,1:48:21.050
Okay?

1:48:21.479,1:48:25.469
So if I take the derivative of this function, okay.

1:48:25.494,1:48:27.820
So, now, let me kind of draw this.

1:48:32.539,1:48:36.904
It's a little difficult to see, but if I map.

1:48:38.594,1:48:46.096
Okay, the derivative of this function here will indicate how much I stretch a little piece here into a piece here, right?

1:48:46.612,1:48:49.580
So the larger the derivative, the more I stretch, right?

1:48:49.612,1:48:54.540
If the slope here is 1, then this piece of the distribution here is not going to stretch,

1:48:54.540,1:48:58.923
it's going to be kind of passed unchanged, okay?

1:49:00.173,1:49:04.627
And the larger the slope, the more stretch the distribution,

1:49:05.008,1:49:06.706
I stretch a little piece here,

1:49:06.849,1:49:15.898
and therefore, I kind of distribute all the samples that fall into this, you know, location here, I stretched them over a large region.

1:49:16.684,1:49:17.125
Right?

1:49:18.184,1:49:24.549
And so what I need to do is design this function in such a way that stretches my input distribution,

1:49:24.890,1:49:29.409
so that, that distribution get transformed into the output distribution I want.

1:49:30.291,1:49:31.108
All right?

1:49:32.389,1:49:33.862
So there is a formula that says

1:49:33.887,1:49:41.478
so in multi-dimension is a little more complicated than this, but it says that the distribution you're going to get on y

1:49:41.478,1:49:45.599
is going to be equal to the distribution that you started with in z

1:49:45.993,1:49:54.119
multiplied by the inverse of the determinant of the Jacobian of this f function, so this is f.

1:50:03.537,1:50:05.151
minus 1

1:50:05.684,1:50:10.050
So it's actually the original formula is this one,

1:50:12.775,1:50:14.032
but those two things are equal.

1:50:14.826,1:50:15.602
Okay?

1:50:15.954,1:50:18.948
So if you take, so this is for a multi-dimensional vector function, right?

1:50:18.973,1:50:22.479
So it has a Jacobian to map z to y,

1:50:23.387,1:50:31.774
and so if you take the determinant of the inverse Jacobian of that function,

1:50:33.245,1:50:44.676
which is a scalar value indicates by how much the distribution get stretched, or compressed in that case at q.

1:50:45.200,1:50:50.323
So in that case here it's the compression ratio, it's the inverse of the derivative, so it's the compression.

1:50:51.443,1:51:00.040
Right? And so the more you compress here, the more the probability will be high,

1:51:00.067,1:51:07.553
more p(y) will be large, the density p(y) for this y will be large for a given q.

1:51:07.578,1:51:12.552
So this is for y equals f(z).

1:51:15.607,1:51:16.321
Okay?

1:51:18.314,1:51:28.875
So the big question of normalizing flow methods is how you do this, right?

1:51:28.875,1:51:38.145
given a number of samples of p(y), and given that you sample your distribution q, you have your distribution q you sample from it.

1:51:39.510,1:51:43.204
You know, how do you kind of minimize an objective function that

1:51:44.517,1:51:46.180
knowing that you know,

1:51:46.205,1:51:54.574
the p you get at the output is equal to the q you put at the input multiplied by this inverse determinant of the Jacobian of the f function.

1:51:54.840,1:51:57.930
What you have to find is the f function, so basically you have to differentiate.

1:51:58.102,1:52:07.839
So basically it compute the distance between those, you know, divergence, KL divergence for example, between p(y) and the thing on the right side of the equal sign.

1:52:09.085,1:52:11.792
And you have to differentiate this with respect to the parameters of f.

1:52:12.476,1:52:18.161
So you have to basically propagate through the inverse gradient of the Jacobian of f, right?

1:52:19.078,1:52:19.757
It's not easy.

1:52:20.780,1:52:31.398
Very often what people do is that they write f as a succession a very simple f's that only modify the distribution just a little bit,

1:52:33.624,1:52:40.815
So f very often is, you know, something like the identity plus some deviation, a bit like ResNet if you want,

1:52:41.382,1:52:46.916
and then you stack lots and lots of layers of that, and the problem becomes simpler,

1:52:47.017,1:52:53.644
because when those functions do a little bit of modification,

1:52:53.644,1:53:01.466
then the a lot of those kind of issues can become simple the determinant here kind of simplifies.

1:53:01.930,1:53:06.599
Okay, that's a very sort of abstractable description of normalizing flow.

1:53:08.786,1:53:21.429
There is interesting papers about this in recent years on, even recent months, on sort of using this for like particle physics and stuff like that.

1:53:21.429,1:53:24.086
Kyle Cranmer at NYU is actually a kind of a specialist of that.

1:53:26.075,1:53:27.998
Thank you so much professor.

1:53:29.448,1:53:31.097
All right, any other question?

1:53:32.587,1:53:33.606
Okay, that was it.

1:53:33.879,1:53:36.128
Great, thank you very much everyone.

1:53:36.730,1:53:37.962
Yeah, see you tomorrow guys.

1:53:38.908,1:53:39.322
All right.

1:53:39.347,1:53:39.701
Bye Bye.

1:53:40.359,1:53:41.001
Take care.
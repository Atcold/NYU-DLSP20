0:00:00.030,0:00:04.080
all right so we're going to talk about

0:00:01.260,0:00:07.350
two or three topics today and the first

0:00:04.080,0:00:08.790
one is going to be kind of a review of

0:00:07.350,0:00:12.090
some of the functions that exist inside

0:00:08.790,0:00:16.139
torch and kind of when and how to use

0:00:12.090,0:00:20.330
them so the first the first set of

0:00:16.139,0:00:22.410
topics is about activation functions and

0:00:20.330,0:00:24.300
there is a whole bunch of them defined

0:00:22.410,0:00:27.000
in in tight torch and they basically

0:00:24.300,0:00:28.560
come from you know various papers that

0:00:27.000,0:00:30.300
people have written where they claim

0:00:28.560,0:00:32.189
that this or that particular objective

0:00:30.300,0:00:35.790
function or activation function works

0:00:32.189,0:00:38.070
better for their problem so of course

0:00:35.790,0:00:40.140
everybody knows the value that's very

0:00:38.070,0:00:42.300
standard one but there's lots of

0:00:40.140,0:00:46.500
variations of values these values where

0:00:42.300,0:00:47.969
the the the bottom part is not constant

0:00:46.500,0:00:50.340
and set to zero they can be allowed to

0:00:47.969,0:00:52.050
change either only with the positive

0:00:50.340,0:00:55.379
slope or force to be to have a negative

0:00:52.050,0:00:58.710
slope or sometimes being random in the

0:00:55.379,0:01:00.870
case of the randomized VQ value so they

0:00:58.710,0:01:05.630
have you know a nice named likely key

0:01:00.870,0:01:08.369
value pair you revalue random value etc

0:01:05.630,0:01:13.170
so the key value is one where you allow

0:01:08.369,0:01:16.350
the bottom part to have negative so and

0:01:13.170,0:01:18.420
that kind of prevents the issue that

0:01:16.350,0:01:20.250
sometimes pops up that you know when

0:01:18.420,0:01:23.090
radio is off it doesn't get any gradient

0:01:20.250,0:01:26.939
so here here you get a chance for that

0:01:23.090,0:01:28.799
system that function to actually

0:01:26.939,0:01:30.119
propagate gradient and perhaps do

0:01:28.799,0:01:32.280
something useful can go all the way to

0:01:30.119,0:01:35.570
kind of compete full ratification of the

0:01:32.280,0:01:39.090
signal kind of like an absolute value

0:01:35.570,0:01:41.280
value it's well yeah

0:01:39.090,0:01:43.799
the previous the previous activation was

0:01:41.280,0:01:46.530
is user using the discriminatory in

0:01:43.799,0:01:48.990
again such that we always have gradients

0:01:46.530,0:01:51.329
going backwards for the generator and

0:01:48.990,0:01:54.000
also these activation was necessary in

0:01:51.329,0:01:55.409
order to train the very skinny network I

0:01:54.000,0:01:57.960
show at the beginning of the class

0:01:55.409,0:01:59.700
because again having like a very very

0:01:57.960,0:02:02.850
skinny network it was basically

0:01:59.700,0:02:04.829
impossible to get gradients flowing back

0:02:02.850,0:02:07.500
because we were like ending up in one of

0:02:04.829,0:02:09.179
the quadrants without you know where

0:02:07.500,0:02:12.030
everything was zero out and then nothing

0:02:09.179,0:02:12.800
would have been actually trained if you

0:02:12.030,0:02:15.740
wouldn't have

0:02:12.800,0:02:17.390
use you know this activation function

0:02:15.740,0:02:19.730
that allows me to get some kind of

0:02:17.390,0:02:21.350
gradients even if we are in the regions

0:02:19.730,0:02:28.250
where we are trying to suppress the

0:02:21.350,0:02:32.180
output so yeah right so a prelude is

0:02:28.250,0:02:34.310
it's similar except now the the slope

0:02:32.180,0:02:38.930
and they get your side can be just about

0:02:34.310,0:02:40.820
anything and okay what's what's

0:02:38.930,0:02:42.620
interesting about those all those

0:02:40.820,0:02:44.590
functions that we just saw is that they

0:02:42.620,0:02:47.090
are scale invariant in the sense that

0:02:44.590,0:02:51.550
they you know you can multiply the

0:02:47.090,0:02:52.700
signal by two and the output will not be

0:02:51.550,0:02:53.840
changed

0:02:52.700,0:02:55.280
yeah I mean it would be multiplied by

0:02:53.840,0:02:58.100
two but otherwise unchanged so they are

0:02:55.280,0:02:59.840
equivalent to scale there's no sort of

0:02:58.100,0:03:02.330
intrinsic scale in those in those

0:02:59.840,0:03:05.260
functions right because there's only one

0:03:02.330,0:03:07.610
non-linearity and it's a sharp one so

0:03:05.260,0:03:09.530
now we're getting into functions where

0:03:07.610,0:03:11.810
the scale matters so the amplitude of

0:03:09.530,0:03:14.840
the incoming signal will affect the type

0:03:11.810,0:03:17.090
of non the type of non-linearity that

0:03:14.840,0:03:19.280
you're going to get and one of those is

0:03:17.090,0:03:21.290
the the soft press so soft rice it's

0:03:19.280,0:03:24.110
sort of a differentiable version of

0:03:21.290,0:03:25.880
radio if you want it's it's kind of the

0:03:24.110,0:03:26.480
soft version of positive part and it's

0:03:25.880,0:03:29.300
usually

0:03:26.480,0:03:29.780
permit rised as you can see at the top

0:03:29.300,0:03:32.060
here

0:03:29.780,0:03:34.670
whatever Bay dialog 1 plus X financial

0:03:32.060,0:03:38.060
beta X so it's it's kind of like the log

0:03:34.670,0:03:41.420
some exponential that we've been using a

0:03:38.060,0:03:43.940
lot for service purpose except here one

0:03:41.420,0:03:45.440
of the terms in the sum is equal to 1

0:03:43.940,0:03:49.970
which is kind of like exponential zero

0:03:45.440,0:03:52.760
if you want so that looks like kind of a

0:03:49.970,0:03:54.290
function that so synthetic ly is the

0:03:52.760,0:03:56.390
identity function for large policy

0:03:54.290,0:03:58.100
values and asymptotically zero for

0:03:56.390,0:04:00.560
negative values so the approximate sir

0:03:58.100,0:04:03.260
value it has a scale parameter though

0:04:00.560,0:04:05.690
this this beta parameter the larger beta

0:04:03.260,0:04:08.330
the more the function will look like a

0:04:05.690,0:04:10.400
value so the cake will be kind of the

0:04:08.330,0:04:14.360
corner will be kind of sharper if petah

0:04:10.400,0:04:17.380
goes to infinity but that that function

0:04:14.360,0:04:17.380
has a scale

0:04:18.260,0:04:23.710
now you can prioritize those functions

0:04:21.020,0:04:26.540
in in various ways and this is sort of a

0:04:23.710,0:04:29.360
another example of

0:04:26.540,0:04:33.320
kind of a soft version of value if you

0:04:29.360,0:04:35.150
want where we're here you use value as a

0:04:33.320,0:04:38.750
basis and then you add a small constant

0:04:35.150,0:04:40.880
to it that kind of makes it smooth you

0:04:38.750,0:04:41.930
know I can't tell you that

0:04:40.880,0:04:44.840
any of those has any particular

0:04:41.930,0:04:47.240
advantage over the others it really

0:04:44.840,0:04:48.500
depends on the only problem but they

0:04:47.240,0:04:51.380
they all have kind of similar properties

0:04:48.500,0:04:56.660
if you want this also you can make sort

0:04:51.380,0:05:01.610
of continuously closer to value that's

0:04:56.660,0:05:04.820
yet ok so when when difference here in

0:05:01.610,0:05:07.660
this case is that this guy actually goes

0:05:04.820,0:05:10.070
negative right so unlike the value that

0:05:07.660,0:05:12.830
has its minimum at zero this it's

0:05:10.070,0:05:17.900
horizontal asymptote at zero this guy

0:05:12.830,0:05:19.520
goes below zero and that may or may or

0:05:17.900,0:05:21.650
may not be advantageous depending on the

0:05:19.520,0:05:23.780
application you have sometimes it's

0:05:21.650,0:05:26.450
advantageous because it allows the

0:05:23.780,0:05:28.790
system to basically make the average of

0:05:26.450,0:05:32.510
the output zero which is advantageous

0:05:28.790,0:05:34.040
for certain types of for grading design

0:05:32.510,0:05:36.200
convergence the weights that are

0:05:34.040,0:05:37.790
connected to units like this will see

0:05:36.200,0:05:40.010
both both positive and negative values

0:05:37.790,0:05:43.820
which will then converge faster than if

0:05:40.010,0:05:46.310
you only see positive values so it's

0:05:43.820,0:05:48.500
been the same here and it's just kind of

0:05:46.310,0:05:50.360
a differently you know a different

0:05:48.500,0:05:55.790
permutation of kind of the same thing if

0:05:50.360,0:05:56.960
you want with different properties so of

0:05:55.790,0:05:58.730
course there's tons of variations of

0:05:56.960,0:06:03.050
this you know with various parameters

0:05:58.730,0:06:05.440
with different properties and you know

0:06:03.050,0:06:09.820
some of them that have particular

0:06:05.440,0:06:13.370
properties that can relate them to

0:06:09.820,0:06:14.990
Gaussian distributions for example this

0:06:13.370,0:06:18.770
is not the cumulative distribution of a

0:06:14.990,0:06:20.510
Gaussian but okay so those those were

0:06:18.770,0:06:22.910
things that have one kinks in them and

0:06:20.510,0:06:25.310
if the kink is sharp there's no scale if

0:06:22.910,0:06:26.840
the King has some scale in it there is

0:06:25.310,0:06:29.240
some scale but it's still sort of a

0:06:26.840,0:06:30.710
single kick non-linearity now we're

0:06:29.240,0:06:33.080
getting into nonlinearities I have two

0:06:30.710,0:06:34.670
kings okay so this one is basically a

0:06:33.080,0:06:37.400
saturating value

0:06:34.670,0:06:39.750
I know sure white saturates at six you

0:06:37.400,0:06:41.670
know why not but

0:06:39.750,0:06:44.850
why not parameterize this a little

0:06:41.670,0:06:46.230
better so here's a smooth function that

0:06:44.850,0:06:50.490
you're familiar with because it's used

0:06:46.230,0:06:51.720
in in recurrent Nets in gated recurrent

0:06:50.490,0:06:54.600
Nets and then STM

0:06:51.720,0:06:56.730
in soft max you know basically this is a

0:06:54.600,0:07:00.900
a two-way soft max you can think of it

0:06:56.730,0:07:03.800
this way and this is just a function

0:07:00.900,0:07:06.240
that goes kind of smoothly between 0 & 1

0:07:03.800,0:07:08.780
it's sometimes called a Fermi Dirac

0:07:06.240,0:07:12.170
function as well because it derives from

0:07:08.780,0:07:15.240
some work in physics it's equal physics

0:07:12.170,0:07:17.090
and then there is the hyperbola tangent

0:07:15.240,0:07:19.830
that we also talked about it's basically

0:07:17.090,0:07:21.690
identical to the sigmoid except it's

0:07:19.830,0:07:24.360
centered so it goes between minus 1 and

0:07:21.690,0:07:26.100
plus 1 and it's a little you know it's

0:07:24.360,0:07:28.710
twice the amplitude and the gain is a

0:07:26.100,0:07:29.010
little different but it plays the same

0:07:28.710,0:07:31.080
role

0:07:29.010,0:07:34.050
the advantage of hyperbolic tangent is

0:07:31.080,0:07:36.990
that the output is you can expect the

0:07:34.050,0:07:40.680
output to not have zero mean but get

0:07:36.990,0:07:43.500
close to having zero mean and again

0:07:40.680,0:07:45.360
that's advantageous for the weights that

0:07:43.500,0:07:46.890
follow because they see positive and

0:07:45.360,0:07:50.310
negative values and they tend to

0:07:46.890,0:07:52.310
converge faster that's the case so I

0:07:50.310,0:07:54.840
used to be a big fan of those

0:07:52.310,0:07:57.390
unfortunately if you stack a lot of

0:07:54.840,0:08:02.130
sigmoids in many layers you know in a

0:07:57.390,0:08:05.160
neural net you you you can tend to not

0:08:02.130,0:08:07.140
learn very efficiently you have to be

0:08:05.160,0:08:09.030
very careful about normalization if you

0:08:07.140,0:08:11.010
want the system to to converge if you

0:08:09.030,0:08:13.800
have many layers so in that sense the

0:08:11.010,0:08:18.630
the single kick functions are better for

0:08:13.800,0:08:21.660
deeper networks so it's signed this is

0:08:18.630,0:08:23.460
basically a bit like the sigmoid except

0:08:21.660,0:08:25.560
that it's it doesn't get to the

0:08:23.460,0:08:28.950
asymptotes as fast so it doesn't get

0:08:25.560,0:08:31.169
stuck towards the asymptotes as quickly

0:08:28.950,0:08:34.740
so one problem with hyperbolic tangent

0:08:31.169,0:08:37.440
and and the sigmoid is that we get close

0:08:34.740,0:08:40.589
to the asymptotes the the gradient goes

0:08:37.440,0:08:43.979
to zero fairly quickly and so if the

0:08:40.589,0:08:47.070
weights of a unit become too large they

0:08:43.979,0:08:48.690
kind of saturate this unit and the

0:08:47.070,0:08:50.279
gradients get very small and then the

0:08:48.690,0:08:54.029
the unit doesn't run very

0:08:50.279,0:08:57.240
very quickly anymore it's a problem that

0:08:54.029,0:09:00.779
exists both sorry both in sigmoids and

0:08:57.240,0:09:03.389
hyperbole tension and so such sign is a

0:09:00.779,0:09:05.790
function of that was proposed by your

0:09:03.389,0:09:08.519
venue and so these collaborators and it

0:09:05.790,0:09:12.230
kind of saturates slower and so it

0:09:08.519,0:09:14.579
doesn't have the that same problem I

0:09:12.230,0:09:17.819
mean it has the problem also but not to

0:09:14.579,0:09:21.649
the same extent okay and this is kind of

0:09:17.819,0:09:23.819
the opposite heart tangent hard 10h I

0:09:21.649,0:09:27.149
don't know if it deserves that name but

0:09:23.819,0:09:29.720
it's basically just a ramp okay

0:09:27.149,0:09:32.100
and that works surprisingly well

0:09:29.720,0:09:35.310
particularly if your weights are somehow

0:09:32.100,0:09:40.050
kept within a kind of small value so the

0:09:35.310,0:09:42.569
the units don't saturate too much it's

0:09:40.050,0:09:46.579
surprising how well it works and you

0:09:42.569,0:09:46.579
know people use this and service context

0:09:46.790,0:09:52.350
but that's sort of you know non-standard

0:09:49.170,0:09:53.850
so hard threshold is very rarely used

0:09:52.350,0:09:56.459
because you can't really propagate

0:09:53.850,0:09:58.680
gradient to it okay and this is really

0:09:56.459,0:10:00.959
what kept people from inventing back

0:09:58.680,0:10:03.120
prop in the 60s and 70s which is that

0:10:00.959,0:10:05.779
they were using binary neurons and so

0:10:03.120,0:10:09.930
they didn't think of the whole idea of

0:10:05.779,0:10:12.029
gradients because of that okay those

0:10:09.930,0:10:16.439
other functions are rarely used in the

0:10:12.029,0:10:18.569
context of neural nets or at least for

0:10:16.439,0:10:20.730
kind of activation function in a

0:10:18.569,0:10:23.759
traditional neuron that they used mostly

0:10:20.730,0:10:26.220
for sometimes for things like sparse

0:10:23.759,0:10:29.370
coding so one step in sparse coding

0:10:26.220,0:10:32.279
consists in to compute the value of the

0:10:29.370,0:10:33.660
latent variable consistent shrinking all

0:10:32.279,0:10:35.879
the values in the latent variable is the

0:10:33.660,0:10:38.129
agent vector by some value and you do

0:10:35.879,0:10:40.019
this with a shrink function a shrinkage

0:10:38.129,0:10:41.730
function this is kind of a soft version

0:10:40.019,0:10:44.160
of shrinkage function the hard version

0:10:41.730,0:10:47.519
is here I mean it's called soft shrink

0:10:44.160,0:10:50.699
soft shrink but it actually has corners

0:10:47.519,0:10:51.930
in it the reason it's called soft shrink

0:10:50.699,0:10:54.029
is because there is a hard shrink that

0:10:51.930,0:10:55.470
looks different did that show you in a

0:10:54.029,0:10:59.819
minute

0:10:55.470,0:11:02.540
so this basically just changes variable

0:10:59.819,0:11:05.120
by a constant toward zero right

0:11:02.540,0:11:10.460
and if he goes below zero it connect its

0:11:05.120,0:11:16.910
client at zero if it if it's brought too

0:11:10.460,0:11:18.500
long and so if the this is basically

0:11:16.910,0:11:20.600
just the identity function to which you

0:11:18.500,0:11:23.570
subtract hyperbola tangent to make it

0:11:20.600,0:11:25.430
look like a shrink shrink basically

0:11:23.570,0:11:27.560
design if we if we try to get the

0:11:25.430,0:11:29.060
whatever value close to zero and they

0:11:27.560,0:11:31.670
actually are forced to zero basically

0:11:29.060,0:11:33.860
right right so small values are forced

0:11:31.670,0:11:35.810
to zero others are shrunk toward zero

0:11:33.860,0:11:37.850
but you know it's a large enough they're

0:11:35.810,0:11:42.080
not going to get to zero so again that's

0:11:37.850,0:11:44.390
used mostly as you know you can think of

0:11:42.080,0:11:45.140
it as a step of gradient for an l1

0:11:44.390,0:11:48.290
criterion

0:11:45.140,0:11:50.360
okay so if you have a variable you have

0:11:48.290,0:11:52.940
an l1 cost function on it and you take a

0:11:50.360,0:11:55.060
step in the negative gradient of the one

0:11:52.940,0:11:58.580
so every one cost is an absolute value

0:11:55.060,0:12:00.590
this will cause the variable to kind of

0:11:58.580,0:12:02.570
go towards zero by a constant which is

0:12:00.590,0:12:05.900
the slope of that at one criterion and

0:12:02.570,0:12:07.730
to kind of stay at zero you know coming

0:12:05.900,0:12:10.520
from either side it doesn't kind of

0:12:07.730,0:12:12.290
overshoot if you want and so that that's

0:12:10.520,0:12:13.880
an on-ear function you use and that's

0:12:12.290,0:12:17.300
one of the steps in the East algorithm

0:12:13.880,0:12:20.660
that is used for inference in in sparse

0:12:17.300,0:12:23.990
coding but again it's it's really used

0:12:20.660,0:12:28.130
in sort of regular neural nets unless

0:12:23.990,0:12:34.490
your encoder is kind of used as kind of

0:12:28.130,0:12:36.680
a estimation of sparse coding this is

0:12:34.490,0:12:40.010
the hard shrink so hard shrink basically

0:12:36.680,0:12:45.260
clients every value smaller than lambda

0:12:40.010,0:12:48.500
to zero okay so if a value is smaller

0:12:45.260,0:12:50.150
than lambda or larger than - from there

0:12:48.500,0:12:51.560
sort of between minus lambda lambda when

0:12:50.150,0:12:55.730
lambda is some constant is just set it

0:12:51.560,0:12:57.590
to zero again it's used for things like

0:12:55.730,0:12:59.720
you know certain types of sparse coding

0:12:57.590,0:13:04.250
but rarely as an activation function in

0:12:59.720,0:13:06.500
the night so log sigmoid is mostly used

0:13:04.250,0:13:08.930
in cost functions not really as an

0:13:06.500,0:13:11.920
activation function either but it's a

0:13:08.930,0:13:15.320
useful function to have if you want to

0:13:11.920,0:13:19.300
plug this into into a loss function and

0:13:15.320,0:13:19.300
we'll see we'll see that ten minute

0:13:19.730,0:13:23.089
so something we've seen this is the same

0:13:21.470,0:13:27.589
as softmax except you have minus signs

0:13:23.089,0:13:29.990
so this is sort of more so that so those

0:13:27.589,0:13:31.490
are multi-dimensional nonlinearities all

0:13:29.990,0:13:33.110
right you you have a vector in and you

0:13:31.490,0:13:35.930
get a vector out which is at same size

0:13:33.110,0:13:38.660
as the input vector and we know about

0:13:35.930,0:13:40.790
softmax is you know exponential X I

0:13:38.660,0:13:44.060
divided by sum over J exponential X J

0:13:40.790,0:13:45.800
this is such min where you put a minus

0:13:44.060,0:13:47.660
sign in front of the X so you view the

0:13:45.800,0:13:51.790
X's if you want as energies instead of

0:13:47.660,0:13:54.230
scores as penalties instead of scores

0:13:51.790,0:13:58.160
it's a good way of turning a bunch of

0:13:54.230,0:13:59.750
numbers to something that looks a bit

0:13:58.160,0:14:02.569
like a probability distribution which

0:13:59.750,0:14:05.899
means numbers between 0 and 1 that's 1

0:14:02.569,0:14:10.190
to 1 and that's the softmax which we all

0:14:05.899,0:14:13.009
know so locks off max again it's not

0:14:10.190,0:14:14.959
very much used as a non-linearity within

0:14:13.009,0:14:17.300
the neural net but it's used a lot at

0:14:14.959,0:14:18.920
the output as kind of one piece of a

0:14:17.300,0:14:23.870
loss function it will see this in a

0:14:18.920,0:14:26.839
minute ok so those questions yes we have

0:14:23.870,0:14:29.810
a question so for pre low I'm not sure I

0:14:26.839,0:14:32.000
understand number one why we want the

0:14:29.810,0:14:34.430
same value for all channels and number

0:14:32.000,0:14:37.250
two how learning a it would actually be

0:14:34.430,0:14:39.860
advantages you could have a different a

0:14:37.250,0:14:41.480
for different channels so different

0:14:39.860,0:14:43.430
units can have a different a it could be

0:14:41.480,0:14:48.019
you could use it this as a as a

0:14:43.430,0:14:50.089
parameter very every unit or not you

0:14:48.019,0:14:51.860
could be shared that's gonna up to you

0:14:50.089,0:14:53.240
it could be shared at the level or a

0:14:51.860,0:14:55.069
feature map it accomplished on that or

0:14:53.240,0:14:57.199
it could be share all future maps or it

0:14:55.069,0:14:59.240
could be individual to every unit if you

0:14:57.199,0:15:00.740
really want to preserve the coalitional

0:14:59.240,0:15:02.899
nature of a commercial net you probably

0:15:00.740,0:15:04.250
want to have the same a for every unit

0:15:02.899,0:15:05.420
in the future that but you think you can

0:15:04.250,0:15:08.510
have different aids for different

0:15:05.420,0:15:11.480
feature Maps okay well it's the second

0:15:08.510,0:15:13.970
question why learning actually a

0:15:11.480,0:15:16.459
specific value would be advantages like

0:15:13.970,0:15:19.759
why are we learning a you can learn it

0:15:16.459,0:15:21.949
or not you can fix it the the reason for

0:15:19.759,0:15:24.170
fixing it would be you know not

0:15:21.949,0:15:27.750
necessarily to kind of have sort of more

0:15:24.170,0:15:29.880
powerful non-linearity but to kind of

0:15:27.750,0:15:32.190
ensure that the non-linearity gives you

0:15:29.880,0:15:37.200
a non 0 gradient even if it's even if

0:15:32.190,0:15:41.430
it's in the negative region so you know

0:15:37.200,0:15:43.820
runnable not nor lib not renewable so to

0:15:41.430,0:15:46.200
make it learnable allows the system to

0:15:43.820,0:15:49.470
basically turn a non-linearity into

0:15:46.200,0:15:50.970
either the linear mapping which of

0:15:49.470,0:15:54.270
course is not particularly interesting

0:15:50.970,0:15:57.330
but why not value or something like a

0:15:54.270,0:16:01.160
full rectification okay where a would be

0:15:57.330,0:16:03.720
minus 1 in the in the negative part

0:16:01.160,0:16:04.860
which you know can be it can be

0:16:03.720,0:16:06.150
interesting for certain types of

0:16:04.860,0:16:08.310
application so for example if you have

0:16:06.150,0:16:10.440
accomplish on that that has an edge

0:16:08.310,0:16:13.080
detector an edge detector as a priority

0:16:10.440,0:16:14.550
right it's it's got press coefficients

0:16:13.080,0:16:17.580
on one side minus coefficients on the

0:16:14.550,0:16:19.800
other side and so it's gonna react so if

0:16:17.580,0:16:23.490
you have an edge in an image that goes

0:16:19.800,0:16:25.140
from say dr. bright the you know the

0:16:23.490,0:16:26.940
composition will react positively to

0:16:25.140,0:16:31.110
this one but if you have another edge

0:16:26.940,0:16:34.350
from from you know in the opposite

0:16:31.110,0:16:36.510
direction then the the react the the

0:16:34.350,0:16:39.210
filter will react negatively now if you

0:16:36.510,0:16:41.370
want your filter to react to an edge

0:16:39.210,0:16:44.100
regardless of its priority you rectify

0:16:41.370,0:16:46.320
it okay so that would be kind of just

0:16:44.100,0:16:48.600
absolute value now you could of course

0:16:46.320,0:16:50.610
bake this in you know how to use a

0:16:48.600,0:16:52.890
prelude you can just use the absolute

0:16:50.610,0:16:55.170
value probably a better idea is to use a

0:16:52.890,0:16:57.480
square actually so you take the square

0:16:55.170,0:16:59.730
of square non-linearity it's not

0:16:57.480,0:17:00.990
implemented as kind of a neural net

0:16:59.730,0:17:02.430
non-linearity but you know in the

0:17:00.990,0:17:06.000
functional form of by torture just right

0:17:02.430,0:17:06.420
square and that's it hope answered the

0:17:06.000,0:17:10.380
question

0:17:06.420,0:17:14.180
any other question on this topic I have

0:17:10.380,0:17:17.329
a question it seems to me like these

0:17:14.180,0:17:21.230
nonlinearities are trying to basically

0:17:17.329,0:17:25.290
make a linear function nonlinear and the

0:17:21.230,0:17:27.360
tweak in the in the lines denote like

0:17:25.290,0:17:33.120
the change in that function so they can

0:17:27.360,0:17:37.440
we think of this as if we want to model

0:17:33.120,0:17:38.340
of curve in the line should we have

0:17:37.440,0:17:42.990
learnable

0:17:38.340,0:17:45.930
on both like before before the 0 and

0:17:42.990,0:17:48.330
after the 0 on the x-axis like well so

0:17:45.930,0:17:50.550
yeah I mean there is diminishing return

0:17:48.330,0:17:52.560
so the question is you know how complex

0:17:50.550,0:17:54.450
do you want your non-linearity to be so

0:17:52.560,0:17:58.200
you could imagine of course primate

0:17:54.450,0:18:00.120
rising an entire nonlinear function you

0:17:58.200,0:18:02.520
know with sprite parameters or busy

0:18:00.120,0:18:05.580
curves or something like this right or I

0:18:02.520,0:18:07.890
don't know chebyshev polynomials you

0:18:05.580,0:18:09.750
know I mean you can probably try any any

0:18:07.890,0:18:12.180
mapping you want right you can imagine

0:18:09.750,0:18:16.080
those those parameters could be part of

0:18:12.180,0:18:17.730
the learning process however you know

0:18:16.080,0:18:20.880
what is the advantage of doing this

0:18:17.730,0:18:23.220
versus just you know having more units

0:18:20.880,0:18:24.660
in your in your system and relying on

0:18:23.220,0:18:28.080
the fact that multiple units will be

0:18:24.660,0:18:31.080
added in the end to approximate the

0:18:28.080,0:18:33.600
function you want generally it really

0:18:31.080,0:18:35.580
depends on what like if you want to to

0:18:33.600,0:18:37.260
do regression in a fairly low

0:18:35.580,0:18:39.510
dimensional space so perhaps you want

0:18:37.260,0:18:43.080
some parameterize nonlinearities that

0:18:39.510,0:18:44.940
might help you might have like you might

0:18:43.080,0:18:48.390
want to have a collection of different

0:18:44.940,0:18:50.280
nonlinearities with maybe things like

0:18:48.390,0:18:53.250
like gbj polynomials if you want to do

0:18:50.280,0:18:54.750
good approximation approximations but

0:18:53.250,0:18:56.370
for like you know high dimensional tasks

0:18:54.750,0:18:59.010
like image recognition or things like

0:18:56.370,0:19:01.620
this you just want a non-linearity and

0:18:59.010,0:19:04.050
it works better if the non-linearity is

0:19:01.620,0:19:05.700
monotonic otherwise it creates all kinds

0:19:04.050,0:19:07.650
of issues because you could have two

0:19:05.700,0:19:09.510
points that will produce the same output

0:19:07.650,0:19:12.500
and so it's you land videos for the

0:19:09.510,0:19:14.910
system to learn the right function there

0:19:12.500,0:19:16.590
so you want it it's it's much better if

0:19:14.910,0:19:18.810
the function is monotonic and almost all

0:19:16.590,0:19:20.820
the functions here are monotonic except

0:19:18.810,0:19:23.940
if you have a negative a here in the in

0:19:20.820,0:19:28.830
the prelude case there's big advantage

0:19:23.940,0:19:30.480
to having monotonic functions but in

0:19:28.830,0:19:32.520
principle you could probably try as you

0:19:30.480,0:19:34.140
know any function you want people are

0:19:32.520,0:19:36.510
played with this you know they're not

0:19:34.140,0:19:39.140
very popular because mostly they don't

0:19:36.510,0:19:43.830
seem to be bringing a huge advantage in

0:19:39.140,0:19:47.580
in the kind of applications that people

0:19:43.830,0:19:49.740
use a large neural nets for other

0:19:47.580,0:19:52.560
questions another question is going to

0:19:49.740,0:19:55.830
be keen commercial smooth yeah

0:19:52.560,0:19:57.690
I should never have any application with

0:19:55.830,0:20:01.340
the choice so funny

0:19:57.690,0:20:04.710
yeah teas made of bacon packed the only

0:20:01.340,0:20:08.370
thing I'm aware of is using a single

0:20:04.710,0:20:12.000
function silica double King for DPL

0:20:08.370,0:20:13.260
networks healthily in better well so

0:20:12.000,0:20:16.020
here's a party with double kick double

0:20:13.260,0:20:18.630
kick has a built-in scale in it which

0:20:16.020,0:20:21.300
means if you're it's a waste of the

0:20:18.630,0:20:23.820
incoming layer are multiplied by two or

0:20:21.300,0:20:25.590
the signal amplitude is multiplied by 2

0:20:23.820,0:20:27.870
the result on the output would be

0:20:25.590,0:20:29.940
completely different right yeah because

0:20:27.870,0:20:33.150
you will be you know the signal would be

0:20:29.940,0:20:35.250
more in the non-linearity so so you'll

0:20:33.150,0:20:37.890
get completely different behavior of

0:20:35.250,0:20:39.390
your layer whereas if you have a

0:20:37.890,0:20:40.590
function with only one King if you

0:20:39.390,0:20:43.110
multiply the input by two with the

0:20:40.590,0:20:45.690
output it's also multiplied by two in a

0:20:43.110,0:20:49.380
modulo bias but the signal device is

0:20:45.690,0:20:51.240
fine so what I mean to ask you something

0:20:49.380,0:20:53.670
of a situation with a choice of

0:20:51.240,0:20:55.440
activation function made a big

0:20:53.670,0:20:58.410
difference in the performance of the

0:20:55.440,0:21:04.410
model except for the in networks using

0:20:58.410,0:21:07.890
today move instead of sigmoid there is

0:21:04.410,0:21:12.060
no sort of general answer to this like

0:21:07.890,0:21:13.620
if you're going to use attention you

0:21:12.060,0:21:16.020
have to use softmax I mean you have no

0:21:13.620,0:21:17.460
choice right I mean it's not like you

0:21:16.020,0:21:18.930
have to use shaft softmax but you want

0:21:17.460,0:21:21.300
to have something where you get

0:21:18.930,0:21:25.080
coefficients right to to kind of focus

0:21:21.300,0:21:26.460
the attention of the system on or to

0:21:25.080,0:21:30.030
kind of spread the attention of the

0:21:26.460,0:21:31.470
system and not allow it to cheat which

0:21:30.030,0:21:33.090
is to pay attention to multiple things

0:21:31.470,0:21:36.180
at one time you have to have some sort

0:21:33.090,0:21:37.350
of normalization of the of the

0:21:36.180,0:21:40.980
coefficients that come out of the

0:21:37.350,0:21:42.750
attention system right so so normally in

0:21:40.980,0:21:44.480
most attention systems like in

0:21:42.750,0:21:47.460
transformers and stuff the the

0:21:44.480,0:21:48.750
coefficients are passed through softmax

0:21:47.460,0:21:52.610
so you get a bunch of coefficients that

0:21:48.750,0:21:54.630
are between 0 & 1 & 7 to 1 and so that

0:21:52.610,0:21:58.080
causes the system to have to pay

0:21:54.630,0:21:59.370
attention to you know a small number of

0:21:58.080,0:22:01.050
things right you can only concentrate

0:21:59.370,0:22:06.009
the coefficients on a small number of

0:22:01.050,0:22:08.289
items and it has to spread it right

0:22:06.009,0:22:11.779
there are other ways to do normalization

0:22:08.289,0:22:13.940
you you can do and in fact is something

0:22:11.779,0:22:18.049
that's wrong with softmax normalization

0:22:13.940,0:22:20.509
for for for transformers or for

0:22:18.049,0:22:22.159
attention which is that if you want a

0:22:20.509,0:22:23.529
coefficient coming out with softmax to

0:22:22.159,0:22:26.840
be close to zero

0:22:23.529,0:22:29.570
you need the input to be close to minus

0:22:26.840,0:22:32.419
infinity okay or to be considerably

0:22:29.570,0:22:35.240
smaller than the largest one right when

0:22:32.419,0:22:38.389
you go into the softmax when I put the

0:22:35.240,0:22:39.769
largest input is going to cause the

0:22:38.389,0:22:42.200
corresponding output to be the to be

0:22:39.769,0:22:43.580
large if you want that I'd put to be

0:22:42.200,0:22:46.129
close to one and all the other ones to

0:22:43.580,0:22:48.019
be close to zero you basically want this

0:22:46.129,0:22:53.379
input to be extremely large and all the

0:22:48.019,0:22:59.659
other ones to be large and negative okay

0:22:53.379,0:23:02.149
now that you know that that can be a

0:22:59.659,0:23:05.600
problem when the what you are computing

0:23:02.149,0:23:08.090
the input are our dot products because

0:23:05.600,0:23:13.369
the result is that you know the easiest

0:23:08.090,0:23:14.690
way for a system to to produce a small

0:23:13.369,0:23:16.340
dot product is to have two vectors that

0:23:14.690,0:23:18.590
are orthogonal to each other which gives

0:23:16.340,0:23:20.090
the dot product is zero if you insist

0:23:18.590,0:23:20.480
that the dot product should be very very

0:23:20.090,0:23:24.259
small

0:23:20.480,0:23:26.090
then either you have to make the so you

0:23:24.259,0:23:28.070
have to make the two vectors basically

0:23:26.090,0:23:31.580
point in opposite directions and you

0:23:28.070,0:23:36.499
have to make them very long and that's

0:23:31.580,0:23:39.769
not so great and so using softmax for

0:23:36.499,0:23:42.409
attention basically limits the the the

0:23:39.769,0:23:43.639
contrast that you're gonna have between

0:23:42.409,0:23:45.249
two coefficients which is not

0:23:43.639,0:23:50.600
necessarily a good thing

0:23:45.249,0:23:56.450
so you know same thing for ASTM gates

0:23:50.600,0:23:57.499
gated recurrent Nets etc you you need

0:23:56.450,0:23:59.029
Sigma it's there because you need

0:23:57.499,0:24:01.039
coefficients there are between 0 and 1

0:23:59.029,0:24:04.549
you know that either

0:24:01.039,0:24:06.590
we set the memory cell or make it a

0:24:04.549,0:24:09.350
fast-food so that it keeps it so it's

0:24:06.590,0:24:12.200
previous memory or kind of right the the

0:24:09.350,0:24:13.660
new input in it so there it's nice to

0:24:12.200,0:24:14.770
have

0:24:13.660,0:24:17.680
an output that varies continuously

0:24:14.770,0:24:21.010
between zero and one there you have no

0:24:17.680,0:24:24.490
choice so I mean I don't think you can

0:24:21.010,0:24:26.350
say just you know in generic term you

0:24:24.490,0:24:27.670
know this this non-linearity is better

0:24:26.350,0:24:29.110
than this other one there are certain

0:24:27.670,0:24:31.180
cases where it learns better though

0:24:29.110,0:24:33.970
certain cases where it relieves you from

0:24:31.180,0:24:35.980
having to initialize properly the

0:24:33.970,0:24:37.690
certain cases where it works better if

0:24:35.980,0:24:39.160
you have lots of layers like you know

0:24:37.690,0:24:40.900
single click functions work better if

0:24:39.160,0:24:43.750
you have lots of layers better than

0:24:40.900,0:24:48.130
Sigma large functions there's no kind of

0:24:43.750,0:24:50.110
logical answer I had a question just

0:24:48.130,0:24:54.190
regarding the general differences

0:24:50.110,0:24:57.010
between a nonlinear activation that has

0:24:54.190,0:25:00.130
kinks versus a smooth nonlinear

0:24:57.010,0:25:03.970
activation yeah is there sort of any

0:25:00.130,0:25:06.340
general reason or rule to why we would

0:25:03.970,0:25:08.950
prefer to have kinks in the function or

0:25:06.340,0:25:11.620
not it's a matter of scale invariant of

0:25:08.950,0:25:14.170
scale equivariance so if the kick is

0:25:11.620,0:25:15.610
hard again you multiply the input by 2

0:25:14.170,0:25:18.730
the output is multiplied by 2 but

0:25:15.610,0:25:22.660
otherwise unchanged okay if you have a

0:25:18.730,0:25:28.210
smooth transition if you multiply the

0:25:22.660,0:25:31.390
input by let's say 100 the output now

0:25:28.210,0:25:33.220
will look like you had a hard kick okay

0:25:31.390,0:25:35.860
because the the smooth part now it's

0:25:33.220,0:25:39.340
become shrunk by a factor of 100 if you

0:25:35.860,0:25:42.370
divide the input by 100 now the the cake

0:25:39.340,0:25:45.130
becomes a very very smooth sort of

0:25:42.370,0:25:47.710
convex functions okay so it changes the

0:25:45.130,0:25:50.050
behavior but by changing the scale the

0:25:47.710,0:25:52.810
input you change the behavior of the of

0:25:50.050,0:25:56.680
the unit and that might be a problem

0:25:52.810,0:25:58.360
sometimes because when you when you

0:25:56.680,0:26:00.460
train a multi-layer neural net and you

0:25:58.360,0:26:04.680
have two layers that are one after the

0:26:00.460,0:26:07.390
other you don't have a good control for

0:26:04.680,0:26:09.370
like how big the weights of this layer

0:26:07.390,0:26:11.050
are relative to that other way so

0:26:09.370,0:26:13.030
imagine you have a two layer Network

0:26:11.050,0:26:14.230
where you don't have a non-linearity in

0:26:13.030,0:26:19.510
the middle so the system is completely

0:26:14.230,0:26:22.060
linear right if the network has arrived

0:26:19.510,0:26:24.100
at the solution you can multiply the

0:26:22.060,0:26:26.590
incoming the the first layer weight

0:26:24.100,0:26:27.520
matrix by two divide the second weight

0:26:26.590,0:26:29.380
matrix by two

0:26:27.520,0:26:31.300
and overall the network will have

0:26:29.380,0:26:34.150
exactly the same output okay you won't

0:26:31.300,0:26:36.370
have changed anything and what that

0:26:34.150,0:26:39.250
means is that when you do training there

0:26:36.370,0:26:41.350
is nothing that forces the system to

0:26:39.250,0:26:42.910
have a particular scale for the weight

0:26:41.350,0:26:45.190
matrices all right

0:26:42.910,0:26:47.230
so now if you put a non-linearity in the

0:26:45.190,0:26:49.050
middle and you still don't have it

0:26:47.230,0:26:52.030
because train for the system to kind of

0:26:49.050,0:26:53.820
you know have scales for the first leg

0:26:52.030,0:26:57.700
of weight versus the second leg away

0:26:53.820,0:26:58.990
that you know you'd better have a

0:26:57.700,0:27:02.590
non-linearity that doesn't care about

0:26:58.990,0:27:04.950
scale okay so if you have non-linearity

0:27:02.590,0:27:07.270
that does care about scales about scale

0:27:04.950,0:27:10.030
then your network doesn't have a choice

0:27:07.270,0:27:11.470
of what size weight matrix it can use in

0:27:10.030,0:27:14.170
the first layer because that will

0:27:11.470,0:27:15.820
completely change the behavior and you

0:27:14.170,0:27:17.740
may want to have large weights for some

0:27:15.820,0:27:20.980
other reason which will saturate the

0:27:17.740,0:27:23.560
non-linearity and then kind of create

0:27:20.980,0:27:27.040
you know vanishing gradient issues so I

0:27:23.560,0:27:29.350
it's not entirely clear you know what

0:27:27.040,0:27:31.060
why is it that you know did networks

0:27:29.350,0:27:34.000
work better with single King functions

0:27:31.060,0:27:35.290
but it's probably due to that scale

0:27:34.000,0:27:38.140
invariance property let's get a quick

0:27:35.290,0:27:40.060
variance property now there would be

0:27:38.140,0:27:42.190
other ways of fixing this fixing this

0:27:40.060,0:27:43.810
problem which would be to basically set

0:27:42.190,0:27:45.340
a heart scale on the weights of every

0:27:43.810,0:27:47.850
layer so you couldn't like normalize the

0:27:45.340,0:27:50.860
weights of the layers so that the the

0:27:47.850,0:27:52.540
the variance that of things that go into

0:27:50.860,0:27:53.560
unit you know it's always constant in

0:27:52.540,0:27:54.910
fact that's a little bit with batch

0:27:53.560,0:27:56.830
normalization does so the various

0:27:54.910,0:27:58.930
normalization schemes they do that to

0:27:56.830,0:28:02.530
some extent you know they could mean at

0:27:58.930,0:28:05.560
zero and and the variance is constant so

0:28:02.530,0:28:07.570
so now the variance of the amplitude of

0:28:05.560,0:28:10.870
the output doesn't depend on the size of

0:28:07.570,0:28:16.020
the weights because it's not wise so you

0:28:10.870,0:28:19.150
know that is partially why things like

0:28:16.020,0:28:21.670
like like that norm and good norm and

0:28:19.150,0:28:28.270
things like this help is because they

0:28:21.670,0:28:32.200
can fix the scale a little bit but then

0:28:28.270,0:28:34.600
if you fix the scale then with something

0:28:32.200,0:28:37.420
like that norm you don't the system now

0:28:34.600,0:28:38.860
doesn't have any way of choosing which

0:28:37.420,0:28:41.320
part of the non-linearity it's going to

0:28:38.860,0:28:44.169
use in the to cake function system

0:28:41.320,0:28:45.820
okay so things like group normalization

0:28:44.169,0:28:49.450
or batch normalization are incompatible

0:28:45.820,0:28:51.159
with kin sigmoids if you want share a

0:28:49.450,0:28:54.490
sigmoid you don't want normalization

0:28:51.159,0:28:57.899
just before it see that provides some

0:28:54.490,0:29:01.720
really good intuition thank you okay I

0:28:57.899,0:29:03.549
have one more question I noticed in a

0:29:01.720,0:29:06.159
soft malfunction some people use the

0:29:03.549,0:29:07.840
temperature coefficient so in what cases

0:29:06.159,0:29:10.929
would we want to use their temperature

0:29:07.840,0:29:12.580
and why would we use it well to so to

0:29:10.929,0:29:14.350
some extent the temperature is redundant

0:29:12.580,0:29:18.059
with incoming weights so if you have

0:29:14.350,0:29:20.590
weighted sounds coming into your softmax

0:29:18.059,0:29:22.750
you know having a data parameter in your

0:29:20.590,0:29:24.460
softmax equal to two instead of one it's

0:29:22.750,0:29:26.730
the same as just making your waste twice

0:29:24.460,0:29:30.639
as big it has exactly the same effect

0:29:26.730,0:29:32.320
okay so that beta parameter is redundant

0:29:30.639,0:29:34.629
with the size of the weights but again

0:29:32.320,0:29:36.009
if you were or the size of the weighted

0:29:34.629,0:29:37.480
sum the variance of the weighted stones

0:29:36.009,0:29:39.700
if you want but again if you have a

0:29:37.480,0:29:41.529
batch normalization in there then the

0:29:39.700,0:29:45.190
temperature parameter matters because

0:29:41.529,0:29:48.659
now the input variances are fixed so so

0:29:45.190,0:29:51.460
now the the temperature matters

0:29:48.659,0:29:54.909
temperature basically new controls how

0:29:51.460,0:29:58.389
hard the the you know the the

0:29:54.909,0:30:00.190
distribution on the output will be so

0:29:58.389,0:30:02.470
with a very very large beta you

0:30:00.190,0:30:03.940
basically will have one of the outputs

0:30:02.470,0:30:05.379
equal to one and all the other ones

0:30:03.940,0:30:07.269
really close to zero I mean very close

0:30:05.379,0:30:09.850
to one and very close to zero where beta

0:30:07.269,0:30:11.470
is small then this software in the limit

0:30:09.850,0:30:13.029
of beta equal to zero it's more like an

0:30:11.470,0:30:15.690
average actually that you get like soft

0:30:13.029,0:30:19.779
max behaves a little bit like an average

0:30:15.690,0:30:22.210
so you know beta goes infinity at it so

0:30:19.779,0:30:24.159
behaves a bit like AG Max and there goes

0:30:22.210,0:30:30.700
to zero it behaves of it like reckon

0:30:24.159,0:30:32.440
average so so if you have some sort of

0:30:30.700,0:30:34.149
normalization before the softmax then

0:30:32.440,0:30:36.419
tuning is parameter allows you to

0:30:34.149,0:30:39.039
control this could have hardness and

0:30:36.419,0:30:42.850
what people do sometimes in certain

0:30:39.039,0:30:47.679
scenarios is that they start with a

0:30:42.850,0:30:49.000
relatively low beta so that the the

0:30:47.679,0:30:51.279
numbers that are produced are kind of

0:30:49.000,0:30:53.740
soft so you get gradients everywhere you

0:30:51.279,0:30:54.480
know it's kind of well-behaved in terms

0:30:53.740,0:30:57.179
of gradient

0:30:54.480,0:30:59.429
and then as running proceeds if you want

0:30:57.179,0:31:01.139
kind of harder decisions in your

0:30:59.429,0:31:03.389
attention mechanism or whatever you

0:31:01.139,0:31:05.580
increase data and so that makes the

0:31:03.389,0:31:07.379
system kind of make harder decisions it

0:31:05.580,0:31:08.789
doesn't run as well anymore but it's you

0:31:07.379,0:31:10.529
know presumably after a few iterations

0:31:08.789,0:31:13.379
it's kind of in the right ballpark so

0:31:10.529,0:31:15.899
you can sort of sharpen the the

0:31:13.379,0:31:18.509
decisions there but can be increasing

0:31:15.899,0:31:21.929
data it's useful for example in a

0:31:18.509,0:31:24.509
mixture of mixture of experts and you

0:31:21.929,0:31:28.320
know self attention systems are kind of

0:31:24.509,0:31:31.169
you can think of as sort of a form of a

0:31:28.320,0:31:32.399
weird form of mixture of experts so a

0:31:31.169,0:31:35.070
mixture of experts you know you have

0:31:32.399,0:31:36.659
multiple sub networks and their outputs

0:31:35.070,0:31:38.549
are can linearly combine with

0:31:36.659,0:31:41.730
coefficients that are the output of the

0:31:38.549,0:31:45.450
softmax itself could you know controlled

0:31:41.730,0:31:46.739
by another neural net so if you want

0:31:45.450,0:31:48.539
kind of a soft mixture you have a low

0:31:46.739,0:31:50.309
beta and as you increase their to

0:31:48.539,0:31:52.739
infinity basically you're going to

0:31:50.309,0:31:55.019
select one of the experts and ignore all

0:31:52.739,0:31:57.090
the other ones there might be useful for

0:31:55.019,0:31:58.830
example if you want to train a mixture

0:31:57.090,0:32:00.960
of experts or an attention mechanism but

0:31:58.830,0:32:03.480
in the end you want to save computation

0:32:00.960,0:32:05.159
by just determining which expert do I

0:32:03.480,0:32:07.139
need to compute and just not computing

0:32:05.159,0:32:08.460
the other ones so in that case you want

0:32:07.139,0:32:10.980
those coefficients to be basically

0:32:08.460,0:32:13.350
either one or zero and and you can train

0:32:10.980,0:32:17.820
the system progressively to do this by

0:32:13.350,0:32:19.649
increasing increasing data this is

0:32:17.820,0:32:21.749
called the physicists have a name for

0:32:19.649,0:32:22.859
this because it uses kind of tricks or

0:32:21.749,0:32:27.299
various other things that's called

0:32:22.859,0:32:30.480
annealing it has the same meaning as so

0:32:27.299,0:32:34.429
annealing comes from metalwork right you

0:32:30.480,0:32:37.289
you're making a steel or something and

0:32:34.429,0:32:41.580
you make a sword or something right and

0:32:37.289,0:32:43.919
you heat it up and then you you you you

0:32:41.580,0:32:47.940
you cool it and depending on whether you

0:32:43.919,0:32:49.649
cool it quickly or slowly you'll you

0:32:47.940,0:32:52.080
change the crystal no crystalline

0:32:49.649,0:32:54.389
structure of the of the metal so this

0:32:52.080,0:32:56.190
idea of annealing of progressively

0:32:54.389,0:32:58.320
lowering the temperature correspond to

0:32:56.190,0:33:00.600
this increasing this beta beta is like

0:32:58.320,0:33:04.570
an inverse temperature it's akin to an

0:33:00.600,0:33:09.270
inverse temperature any other question

0:33:04.570,0:33:18.850
I think we are good all right okay so

0:33:09.270,0:33:20.620
next topic is lost functions so PI torch

0:33:18.850,0:33:25.420
has a whole bunch of loss functions as

0:33:20.620,0:33:28.570
you might have seen and of course there

0:33:25.420,0:33:32.110
are things are simple ones like like

0:33:28.570,0:33:33.310
mean square error so I don't need to

0:33:32.110,0:33:35.020
explain to you what it is you know

0:33:33.310,0:33:38.740
compute the square of the error between

0:33:35.020,0:33:41.830
the desired output Y and actually I put

0:33:38.740,0:33:46.630
X and if it's already bet with n samples

0:33:41.830,0:33:49.240
then you have you know n losses one for

0:33:46.630,0:33:50.920
each of the samples in the batch and you

0:33:49.240,0:33:52.660
can you can tell this last function to

0:33:50.920,0:33:56.290
either keep that vector or to kind of

0:33:52.660,0:34:02.590
reduce it by academia or some okay

0:33:56.290,0:34:04.180
pretty simple mmm here's a different

0:34:02.590,0:34:06.010
loss that's the everyone knows so this

0:34:04.180,0:34:08.020
is basically the absolute value of the

0:34:06.010,0:34:10.780
difference between between the desired

0:34:08.020,0:34:12.840
output and the actual output and you

0:34:10.780,0:34:15.970
want to use this to do what's called

0:34:12.840,0:34:18.429
robust regression so if you want small

0:34:15.970,0:34:20.350
errors to count a lot and large errors

0:34:18.429,0:34:22.030
to count but you know not as much as if

0:34:20.350,0:34:23.620
you use the square

0:34:22.030,0:34:26.230
perhaps because you have noise in your

0:34:23.620,0:34:27.820
data so you know that you have a bunch

0:34:26.230,0:34:29.590
of data points you're trying to kind of

0:34:27.820,0:34:33.370
train and know on that to something to

0:34:29.590,0:34:35.470
kind of you know fit a curve or you know

0:34:33.370,0:34:36.850
do regression but you know that you have

0:34:35.470,0:34:38.260
a few outliers so you have a few points

0:34:36.850,0:34:40.000
there are you know very far away from

0:34:38.260,0:34:42.490
what they should be just because you

0:34:40.000,0:34:44.500
know the system as noise or something or

0:34:42.490,0:34:45.970
the data was collected with with some

0:34:44.500,0:34:47.740
noise so you want the system to be

0:34:45.970,0:34:49.840
robust to that noise you don't want the

0:34:47.740,0:34:53.800
cost function to increase too quickly as

0:34:49.840,0:34:59.230
the points are far away from you know

0:34:53.800,0:35:01.180
the kind of the general curve so at one

0:34:59.230,0:35:03.070
loss will be more robust now the problem

0:35:01.180,0:35:05.890
with that one loss is that it's not

0:35:03.070,0:35:07.210
differentiable at the bottom and so you

0:35:05.890,0:35:11.110
know you have to kind of be careful when

0:35:07.210,0:35:14.170
you get to the bottom of how you how you

0:35:11.110,0:35:16.390
do the the gradient that's basically

0:35:14.170,0:35:19.779
done with this soft shrink essentially

0:35:16.390,0:35:26.710
that's that's the

0:35:19.779,0:35:31.239
everyone knows now to correct for that

0:35:26.710,0:35:33.759
people have come up with various ways of

0:35:31.239,0:35:36.579
kind of making the l1 notes for bus for

0:35:33.759,0:35:38.529
large losses but then still smooth at

0:35:36.579,0:35:40.630
the bottom kind of behaving like squad

0:35:38.529,0:35:43.259
error at the bottom so an example of

0:35:40.630,0:35:46.089
this is is this particular function

0:35:43.259,0:35:48.309
smooth say one loss it's basically a one

0:35:46.089,0:35:50.619
far away and it's sort of l2 nearby and

0:35:48.309,0:35:53.619
that presents sometimes that's called a

0:35:50.619,0:35:59.140
Google loss some people call this also

0:35:53.619,0:36:02.470
elastic Network because it's an old

0:35:59.140,0:36:04.749
paper from the 1980s or 1990s that kind

0:36:02.470,0:36:08.589
of proposed this this kind of objective

0:36:04.749,0:36:12.339
function for different purpose so that's

0:36:08.589,0:36:16.390
useful that was advertised by was

0:36:12.339,0:36:18.220
gaerste confess our CNN paper for and

0:36:16.390,0:36:20.140
it's used quite a bit in Cuba division

0:36:18.220,0:36:26.880
for survivors purposes again it's for

0:36:20.140,0:36:29.680
protecting against outliers sharper

0:36:26.880,0:36:35.579
jesus also sharper results now when we

0:36:29.680,0:36:42.880
do like image prediction sharper than

0:36:35.579,0:36:45.160
using the MSE not particularly I mean

0:36:42.880,0:36:49.119
it's it's just like the MSE for small

0:36:45.160,0:36:51.989
errors okay so that doesn't make any

0:36:49.119,0:36:54.609
difference but it it doesn't

0:36:51.989,0:36:57.430
or maybe I misunderstood what your point

0:36:54.609,0:36:59.769
was sorry I was trying to compare the l1

0:36:57.430,0:37:02.739
versus the in the l2 that the l2 gives

0:36:59.769,0:37:04.359
us like usually blurry blurry

0:37:02.739,0:37:07.119
predictions whenever we try to do

0:37:04.359,0:37:09.400
prediction by using like the l2

0:37:07.119,0:37:11.440
minimizing the l2 weather whereas like

0:37:09.400,0:37:14.259
people are minimizing the l1 in order to

0:37:11.440,0:37:18.130
have like sharper overall predictions

0:37:14.259,0:37:19.869
okay so if you take if you take a bunch

0:37:18.130,0:37:22.170
of points okay if you take a bunch of

0:37:19.869,0:37:26.200
y-values okay and you ask the question

0:37:22.170,0:37:28.479
what value so you take a bunch of points

0:37:26.200,0:37:30.710
on on why okay yeah you ask the question

0:37:28.479,0:37:32.840
what value of y minimizes

0:37:30.710,0:37:35.710
the squirrel Ellis the answer is that

0:37:32.840,0:37:38.930
it's the average of all the whites okay

0:37:35.710,0:37:40.460
okay so if you so if for a single X you

0:37:38.930,0:37:44.570
have a whole bunch of wise which means

0:37:40.460,0:37:47.090
you have noise in your data your system

0:37:44.570,0:37:50.840
will want to produce the average of all

0:37:47.090,0:37:52.340
the ways that you're observing okay and

0:37:50.840,0:37:54.560
if the Y you're observing is not a

0:37:52.340,0:37:57.200
single value but is I don't know an

0:37:54.560,0:37:59.060
image the average of a bunch of images

0:37:57.200,0:38:00.730
is a blurry image okay

0:37:59.060,0:38:06.170
that's why you get those very effects

0:38:00.730,0:38:09.860
now with air one the value of y that

0:38:06.170,0:38:11.540
minimizes the l1 norm the l1 distance so

0:38:09.860,0:38:13.610
basically the sum of the absolute values

0:38:11.540,0:38:14.960
of the differences between the value

0:38:13.610,0:38:17.830
you're considering and all the points

0:38:14.960,0:38:23.420
all the white points that's the median

0:38:17.830,0:38:26.330
okay so it's it's a given point all

0:38:23.420,0:38:32.300
right uh-huh and I see media and of

0:38:26.330,0:38:34.280
course it's not blurry it's just an

0:38:32.300,0:38:41.360
image although it's kind of difficult to

0:38:34.280,0:38:42.650
define in multiple dimensions but so one

0:38:41.360,0:38:45.560
problem with this loss is that it has a

0:38:42.650,0:38:48.440
scale right so here the transition here

0:38:45.560,0:38:49.970
is at point five but why should it be at

0:38:48.440,0:38:52.430
point five you know it could be it

0:38:49.970,0:38:59.840
depends what the scale of your of your

0:38:52.430,0:39:01.700
errors are okay negative exactly who

0:38:59.840,0:39:03.080
lost this is really not the negative of

0:39:01.700,0:39:06.190
likelihood loss I'm not sure why it's

0:39:03.080,0:39:08.990
called this way by torch but basically

0:39:06.190,0:39:11.810
here imagine that you have an X vector

0:39:08.990,0:39:14.780
coming out okay and you your loss

0:39:11.810,0:39:17.740
function is there is one correct X okay

0:39:14.780,0:39:19.880
so imagine each X correspond to a score

0:39:17.740,0:39:22.550
for lies a multi-class classification

0:39:19.880,0:39:25.420
right so you have a desired class which

0:39:22.550,0:39:27.680
is one particular index in that vector

0:39:25.420,0:39:29.200
okay now what you want is you want to

0:39:27.680,0:39:32.390
make that score as large as possible

0:39:29.200,0:39:35.870
okay if those scores are likelihoods

0:39:32.390,0:39:38.600
then this is minimum negative log

0:39:35.870,0:39:40.880
likelihood if those scores are log

0:39:38.600,0:39:42.380
likelihoods and this is maximum

0:39:40.880,0:39:43.530
likelihood or minimum negative log

0:39:42.380,0:39:45.690
likelihood

0:39:43.530,0:39:47.610
okay but there is nothing in this module

0:39:45.690,0:39:49.380
that actually specifies that the elds

0:39:47.610,0:39:52.410
have to be log likelihoods so this is

0:39:49.380,0:39:55.020
just you know make my desired component

0:39:52.410,0:39:58.440
as large as possible that's it

0:39:55.020,0:40:02.070
if you put negative signs in front so

0:39:58.440,0:40:04.590
now you you can interpret the axis as

0:40:02.070,0:40:06.630
energies as opposed to scores okay

0:40:04.590,0:40:10.520
they're not called six-course they're

0:40:06.630,0:40:13.440
like they're like penalties if you want

0:40:10.520,0:40:17.070
but it's the same so the formula here

0:40:13.440,0:40:20.160
says you know just pick the X that

0:40:17.070,0:40:23.250
happens to be the correct one for one

0:40:20.160,0:40:25.920
sample in the batch and make that score

0:40:23.250,0:40:29.340
as large as possible now this particular

0:40:25.920,0:40:32.630
one allows you to give a different way

0:40:29.340,0:40:36.990
to different to different categories

0:40:32.630,0:40:38.280
which is W those w's it's a it's a

0:40:36.990,0:40:40.530
weight vector that gives a way to each

0:40:38.280,0:40:43.140
of the each of the categories it's

0:40:40.530,0:40:49.800
useful in a lot of cases particularly if

0:40:43.140,0:40:51.390
you have widely different frequencies

0:40:49.800,0:40:54.000
for the for the categories you might

0:40:51.390,0:40:56.990
want to increase the weight of samples

0:40:54.000,0:41:00.660
for which you have a small number of

0:40:56.990,0:41:02.510
examples I mean for categories for which

0:41:00.660,0:41:06.150
you have a small number of samples

0:41:02.510,0:41:08.580
however I'm actually not a big fan of

0:41:06.150,0:41:12.240
this of this I think it's a much better

0:41:08.580,0:41:16.590
idea to just increase the frequency of

0:41:12.240,0:41:18.390
the of the samples from the classes for

0:41:16.590,0:41:20.280
the class I have you know that appears

0:41:18.390,0:41:22.590
rarely so that you equalize the

0:41:20.280,0:41:26.840
frequency the frequencies of the classes

0:41:22.590,0:41:29.670
when you train it's much better because

0:41:26.840,0:41:33.180
it exploits stochastic gradient in a

0:41:29.670,0:41:36.600
better way okay so so the bottom line of

0:41:33.180,0:41:40.260
that is if you have let me actually draw

0:41:36.600,0:41:42.060
a picture of this so let's say you have

0:41:40.260,0:41:45.000
a problem where you have tons of samples

0:41:42.060,0:41:47.310
for category one and then the small

0:41:45.000,0:41:48.810
number of samples for category two in a

0:41:47.310,0:41:53.400
tiny number of samples for category

0:41:48.810,0:41:55.500
three you could so let's say you know

0:41:53.400,0:41:56.880
here you have I don't know a thousand

0:41:55.500,0:41:59.670
samples and here you have

0:41:56.880,0:42:03.089
500 samples and you have I don't know

0:41:59.670,0:42:06.119
200 samples right so you could do is

0:42:03.089,0:42:10.890
using this this kind of weight function

0:42:06.119,0:42:12.869
you could give this a weight of of 1 and

0:42:10.890,0:42:14.970
this guy weight of 2 and this guy weight

0:42:12.869,0:42:17.099
of 5 and then you can equalize the

0:42:14.970,0:42:18.180
weights if you want it's really better

0:42:17.099,0:42:19.650
to make sure that the way it's

0:42:18.180,0:42:23.279
normalized to 1 that would be probably a

0:42:19.650,0:42:26.549
better idea but what I recommend is not

0:42:23.279,0:42:34.589
that what I recommend is when you pick

0:42:26.549,0:42:36.660
your samples you basically pick one

0:42:34.589,0:42:38.369
sample from class 1 and then one sample

0:42:36.660,0:42:40.529
from class 2 and super from class 3 and

0:42:38.369,0:42:43.950
then you know you keep doing this during

0:42:40.529,0:42:46.950
your training session and when you get

0:42:43.950,0:42:50.789
to the end of class 3 you go back to the

0:42:46.950,0:42:52.200
beginning ok so you keep going here but

0:42:50.789,0:42:55.200
here you go back to the first sample

0:42:52.200,0:42:58.380
keep going here go back and now you have

0:42:55.200,0:43:03.119
the second sample ok and now you get to

0:42:58.380,0:43:05.490
the end of class to go back to the start

0:43:03.119,0:43:09.779
ok so the next sample is going to be

0:43:05.490,0:43:13.380
here here and here and then the next one

0:43:09.779,0:43:17.009
here here and here here here and then

0:43:13.380,0:43:19.380
this guy wraps around again etc right so

0:43:17.009,0:43:21.420
you basically have equal probability

0:43:19.380,0:43:23.519
equal frequencies for all the categories

0:43:21.420,0:43:28.589
but just going through those kind of

0:43:23.519,0:43:29.789
circular buffers more often for

0:43:28.589,0:43:33.420
categories for which you have fewer

0:43:29.789,0:43:37.470
samples ok once you should absolutely

0:43:33.420,0:43:40.710
never do is equalize the frequencies by

0:43:37.470,0:43:42.869
by just not using all the samples in

0:43:40.710,0:43:45.029
categories that are frequent I mean

0:43:42.869,0:43:46.680
that's horrible you should never let any

0:43:45.029,0:43:51.509
data on the floor it's never any reason

0:43:46.680,0:43:52.680
to leave that on the floor ok now here's

0:43:51.509,0:43:54.359
a problem with this the problem with

0:43:52.680,0:43:56.609
this is that after you've trained your

0:43:54.359,0:43:59.609
your neuron that to do this you know one

0:43:56.609,0:44:02.430
that does not know about the relative

0:43:59.609,0:44:05.250
likelihood the relative frequencies of

0:44:02.430,0:44:07.230
the samples and so let's say this is a

0:44:05.250,0:44:09.970
system that those medical diagnoses it

0:44:07.230,0:44:14.260
doesn't know that the common cold is

0:44:09.970,0:44:19.990
a way more frequent than you know lung

0:44:14.260,0:44:22.869
cancer or something right so what you

0:44:19.990,0:44:25.210
need to do in the end is do a pass a few

0:44:22.869,0:44:28.150
passes perhaps where you can fine tune

0:44:25.210,0:44:30.609
your system so that with the actual

0:44:28.150,0:44:31.750
frequencies of the categories and the

0:44:30.609,0:44:33.790
effect of this is going to be for the

0:44:31.750,0:44:38.490
system to adapt the biases at the output

0:44:33.790,0:44:40.869
layer so that the likelihood of you know

0:44:38.490,0:44:43.390
diagnosis corresponds to the the

0:44:40.869,0:44:45.760
frequency of it right it's gonna favor

0:44:43.390,0:44:47.200
things that are more frequent the reason

0:44:45.760,0:44:50.109
why I don't want to do this during the

0:44:47.200,0:44:53.560
entire training is because if you train

0:44:50.109,0:44:55.000
a multi-layer net the the the system

0:44:53.560,0:44:57.940
basically never develops the right

0:44:55.000,0:45:00.180
features for rare cases and they have

0:44:57.940,0:45:05.560
spoken about this already in the class

0:45:00.180,0:45:09.130
in in past weeks to kind of recycle the

0:45:05.560,0:45:10.660
example of medical school you you don't

0:45:09.130,0:45:14.080
spend when you go to medical school you

0:45:10.660,0:45:16.089
don't spend time studying the food that

0:45:14.080,0:45:18.250
is proportional to the frequency of the

0:45:16.089,0:45:21.099
food with respect to very rare diseases

0:45:18.250,0:45:23.050
for example right you spend basically

0:45:21.099,0:45:24.820
the same time studying all the diseases

0:45:23.050,0:45:26.800
in fact you spend more time studying

0:45:24.820,0:45:30.070
complicated one which usually tend to be

0:45:26.800,0:45:31.750
rarer and that's because you need to

0:45:30.070,0:45:33.640
develop the features for it okay and

0:45:31.750,0:45:35.440
then you need to kind of correct for the

0:45:33.640,0:45:39.160
fact that you know those rare diseases

0:45:35.440,0:45:44.950
are rare so you don't do that that you

0:45:39.160,0:45:46.900
know you don't suspect the diagnosis for

0:45:44.950,0:45:54.910
rare diseases very often because you

0:45:46.900,0:45:58.300
know it's rare okay so that's all for

0:45:54.910,0:46:01.869
for weights question tributo so you'll

0:45:58.300,0:46:05.260
be using this a lot of course and cross

0:46:01.869,0:46:08.650
country pillows is a kind of merging of

0:46:05.260,0:46:10.900
two things merging of lot softmax

0:46:08.650,0:46:13.599
function and negative low likelihood

0:46:10.900,0:46:16.150
loss okay and so and the reason why you

0:46:13.599,0:46:18.510
want to have this is for numerical

0:46:16.150,0:46:18.510
reasons

0:46:18.910,0:46:24.560
so the locks off max is you know

0:46:23.150,0:46:26.660
basically the softmax followed by log

0:46:24.560,0:46:29.690
right so you first compute the softmax

0:46:26.660,0:46:31.369
then you do the log if you do softmax

0:46:29.690,0:46:35.210
and then log and you back propagate

0:46:31.369,0:46:36.680
through this you might have gradients in

0:46:35.210,0:46:41.119
the middle between the log and the

0:46:36.680,0:46:44.780
softmax that end up being infinite so

0:46:41.119,0:46:48.160
for example if if the the maximum value

0:46:44.780,0:46:48.160
of one of the stars max is close to one

0:46:48.280,0:46:53.330
and some of the other ones are close to

0:46:50.990,0:46:55.430
zero you take the log you get something

0:46:53.330,0:46:56.540
that's close to minus infinity you back

0:46:55.430,0:46:59.090
propagate through the log you get

0:46:56.540,0:47:04.280
something that's close to infinity okay

0:46:59.090,0:47:07.040
because the the slope of vlog goes to

0:47:04.280,0:47:09.380
zero is very very close to infinity but

0:47:07.040,0:47:10.849
now you multiply this by a soft max that

0:47:09.380,0:47:12.260
is saturated so it's multiplied by

0:47:10.849,0:47:13.730
something that's very close to zero so

0:47:12.260,0:47:16.849
in the end you get a reasonable number

0:47:13.730,0:47:19.280
but because the intermediate numbers are

0:47:16.849,0:47:20.960
close to infinity or 0 you multiply plus

0:47:19.280,0:47:22.940
something that's close to plus infinity

0:47:20.960,0:47:25.130
by something that's close to 0 you get

0:47:22.940,0:47:26.750
numerical issues so you don't want to

0:47:25.130,0:47:28.550
separate log and soft max you want to do

0:47:26.750,0:47:30.680
lots of Max in one in one go it

0:47:28.550,0:47:33.700
simplifies the formula it makes the

0:47:30.680,0:47:37.660
whole thing much more stable numerically

0:47:33.700,0:47:40.520
and for similar reasons you also want to

0:47:37.660,0:47:42.740
merge lots of Max and get you really

0:47:40.520,0:47:43.609
good loss so basically if you have locks

0:47:42.740,0:47:45.589
off Max and you get you've

0:47:43.609,0:47:47.390
log-likelihood loss it says I got a

0:47:45.589,0:47:48.890
bunch of weighted sums and a percent

0:47:47.390,0:47:51.760
through soft max I'm going to take the

0:47:48.890,0:47:54.950
log of those and then I want to make the

0:47:51.760,0:47:57.950
the output of the log short max for the

0:47:54.950,0:47:59.420
correct class as large as possible okay

0:47:57.950,0:48:01.430
that's what the negative load actually

0:47:59.420,0:48:02.869
lost does it wants to make the score of

0:48:01.430,0:48:06.349
the correct class as large as possible

0:48:02.869,0:48:08.750
we saw that just a minute ago right when

0:48:06.349,0:48:10.640
you back propagate through the blocks

0:48:08.750,0:48:12.230
off max as a consequence it's going to

0:48:10.640,0:48:13.640
make the score of all the other classes

0:48:12.230,0:48:19.580
as small as possible

0:48:13.640,0:48:21.380
right because of the normalization and

0:48:19.580,0:48:26.109
so that you know that's why sometimes

0:48:21.380,0:48:28.940
the the whole idea of sort of building a

0:48:26.109,0:48:30.440
network by you know modules

0:48:28.940,0:48:31.589
sometimes there is an advantage instead

0:48:30.440,0:48:44.219
of merging in modules into

0:48:31.589,0:48:46.710
single wine by end right so so the cross

0:48:44.219,0:48:50.269
entropy does in fact this explains a

0:48:46.710,0:48:55.410
little bit you know those numerical

0:48:50.269,0:48:58.410
simplifications so the loss you know

0:48:55.410,0:49:01.430
takes an X Factor and a category a

0:48:58.410,0:49:04.049
desired category a class okay and

0:49:01.430,0:49:06.749
computes the negative log of the softmax

0:49:04.049,0:49:11.279
applied to the vector of scores but the

0:49:06.749,0:49:15.359
one that's on the numerator numerator

0:49:11.279,0:49:18.029
here is the the X of the index of the

0:49:15.359,0:49:20.969
correct class okay so that's that's your

0:49:18.029,0:49:22.440
loss the negative log of exponential the

0:49:20.969,0:49:23.819
score of the correct class divided by

0:49:22.440,0:49:27.930
the sum of the Exponential's all the

0:49:23.819,0:49:31.680
scores okay you can think of the X's as

0:49:27.930,0:49:38.759
negative energies okay it's completely

0:49:31.680,0:49:40.589
equivalent now when you do the math and

0:49:38.759,0:49:42.329
you simplify your the log and the

0:49:40.589,0:49:44.130
Exponential's gonna simplify and so you

0:49:42.329,0:49:45.450
just get the score of the correct class

0:49:44.130,0:49:48.299
the negative score of the correct class

0:49:45.450,0:49:51.059
okay so to make that small you make the

0:49:48.299,0:49:52.710
score large and then press the log of

0:49:51.059,0:49:54.509
the sum of the Exponential's of the

0:49:52.710,0:49:58.739
scores of all the other class to make

0:49:54.509,0:50:01.529
that small you make all the edges small

0:49:58.739,0:50:04.619
negative as far as fat you know as

0:50:01.529,0:50:06.029
negative as possible okay so this will

0:50:04.619,0:50:07.469
make the score of the correct class

0:50:06.029,0:50:13.769
large like this core of everything else

0:50:07.469,0:50:17.460
small again like in the aisle you can

0:50:13.769,0:50:18.989
you can have a weight per category also

0:50:17.460,0:50:23.759
there is a physical interpretation right

0:50:18.989,0:50:25.499
of the cross entropy right okay so why

0:50:23.759,0:50:26.999
is it called course entropy because it

0:50:25.499,0:50:27.660
is the cross entropy between two

0:50:26.999,0:50:29.460
distributions

0:50:27.660,0:50:31.049
it's the KL divergence really between

0:50:29.460,0:50:33.150
two distributions

0:50:31.049,0:50:37.079
it doesn't appear clearly here in this

0:50:33.150,0:50:39.329
formula but think of the softmax applied

0:50:37.079,0:50:41.339
to the X vector as a distribution okay

0:50:39.329,0:50:43.380
so take the X Factor's this course

0:50:41.339,0:50:46.890
rather than to a softmax you get a bunch

0:50:43.380,0:50:49.320
of numbers between 0 & 1 that's 1 2

0:50:46.890,0:50:52.020
and now you have a desired distribution

0:50:49.320,0:50:53.460
and the desired distribution the target

0:50:52.020,0:50:56.700
distribution if you want is one in which

0:50:53.460,0:50:59.520
all the wrong categories had zero and

0:50:56.700,0:51:01.410
the correct category has one okay now

0:50:59.520,0:51:04.230
compute the KL divergence between those

0:51:01.410,0:51:11.040
two distributions okay so it's the sum

0:51:04.230,0:51:14.550
over indices of the correct probability

0:51:11.040,0:51:19.280
okay which is zero for except for one

0:51:14.550,0:51:23.430
term times the ratio between the log of

0:51:19.280,0:51:24.750
the the probability that the system

0:51:23.430,0:51:29.150
produces and the correct probability

0:51:24.750,0:51:31.440
which is one okay so all of those terms

0:51:29.150,0:51:33.480
you know reduce to kind of a single term

0:51:31.440,0:51:36.630
which is just the one for which the

0:51:33.480,0:51:38.610
correct probability term is one okay so

0:51:36.630,0:51:41.010
we end up with this with this term it's

0:51:38.610,0:51:44.280
just a negative log of the softmax

0:51:41.010,0:51:46.230
output for the correct class okay we can

0:51:44.280,0:51:48.590
use this as a cross entropy between the

0:51:46.230,0:51:51.180
distribution produced by the system and

0:51:48.590,0:51:53.850
the one hot vector corresponding to the

0:51:51.180,0:51:55.970
desired distribution if you want okay so

0:51:53.850,0:51:58.350
now there is a there would be another

0:51:55.970,0:52:01.140
can a more sophisticated version of this

0:51:58.350,0:52:02.910
which would be the actual KL divergence

0:52:01.140,0:52:04.620
between the distribution produced by the

0:52:02.910,0:52:06.300
system and a distribution that you

0:52:04.620,0:52:07.950
propose whatever it is a target

0:52:06.300,0:52:09.570
distribution which now is not binary

0:52:07.950,0:52:11.820
it's not the one hard vector anymore but

0:52:09.570,0:52:14.040
it's just a vector of numbers and that's

0:52:11.820,0:52:21.960
called the KL divergence in fact it's

0:52:14.040,0:52:24.240
we'll see it in a minute so Kerala

0:52:21.960,0:52:25.620
vergence is a kind of you know it's not

0:52:24.240,0:52:29.870
a distance because it's not symmetric

0:52:25.620,0:52:31.980
but it's a sort of a divergence between

0:52:29.870,0:52:34.950
between distributions discrete

0:52:31.980,0:52:40.400
distributions okay so this one is a bit

0:52:34.950,0:52:45.480
of a kind of a extension if you want of

0:52:40.400,0:52:48.110
lakhs of Max and it's a version of it

0:52:45.480,0:52:50.610
that is applicable for very very large

0:52:48.110,0:52:53.580
categorization so if you have many many

0:52:50.610,0:52:56.100
many categories what you might want to

0:52:53.580,0:52:58.690
do is kind of cut some corners you don't

0:52:56.100,0:53:02.920
want to compute the giant softmax over

0:52:58.690,0:53:04.390
million categories or maybe even more so

0:53:02.920,0:53:06.010
there you can sort of basically ignore

0:53:04.390,0:53:10.390
the ones that are small and you know

0:53:06.010,0:53:14.740
kind of use tricks to kind of you know

0:53:10.390,0:53:16.270
improve the speed of the of the

0:53:14.740,0:53:17.619
computation and this is what this does

0:53:16.270,0:53:19.300
I'm not going to go into the details

0:53:17.619,0:53:21.400
exactly what it does because actually I

0:53:19.300,0:53:22.839
don't know the details but but it's

0:53:21.400,0:53:24.490
basically an efficient approximation

0:53:22.839,0:53:34.089
softmax for a very very large number of

0:53:24.490,0:53:37.390
categories so this is a special case of

0:53:34.089,0:53:40.000
course entropy when you only have two

0:53:37.390,0:53:43.630
categories and in that case it kind of

0:53:40.000,0:53:45.310
reduces to something simple so this does

0:53:43.630,0:53:47.140
not include stuff max this is just a

0:53:45.310,0:53:50.589
quest entropy when you have two

0:53:47.140,0:53:55.000
categories and as I as I said before the

0:53:50.589,0:54:02.859
the the cross entropy loss is the sum of

0:53:55.000,0:54:05.050
our categories of the probability I mean

0:54:02.859,0:54:06.849
some of our indices or some of our

0:54:05.050,0:54:08.560
categories of the probability for the

0:54:06.849,0:54:12.750
target the target probability for that

0:54:08.560,0:54:19.300
category times the ratio between the log

0:54:12.750,0:54:21.550
of the probability for of produced by

0:54:19.300,0:54:24.250
the system divided by the probability of

0:54:21.550,0:54:27.220
the target category and if you work it

0:54:24.250,0:54:28.780
out for two categories necessarily one

0:54:27.220,0:54:33.130
score is one minus the other one if you

0:54:28.780,0:54:33.970
have two exclusive categories and and it

0:54:33.130,0:54:39.609
comes down to this

0:54:33.970,0:54:43.630
okay now this presupposes that x and y

0:54:39.609,0:54:44.829
is x and y are kind of probabilities

0:54:43.630,0:54:45.400
they have to be between strictly between

0:54:44.829,0:54:47.589
0 & 1

0:54:45.400,0:54:48.849
I mean not strictly but well kind of

0:54:47.589,0:54:55.540
strictly because otherwise the logs

0:54:48.849,0:54:58.109
gonna blow up here is the KL divergence

0:54:55.540,0:55:01.650
process I was to tell you about earlier

0:54:58.109,0:55:04.589
so here it's the

0:55:01.650,0:55:10.019
I mean it it's it's real turn here in a

0:55:04.589,0:55:13.140
funny form but it's basically the here

0:55:10.019,0:55:13.980
again it's it sort of assumes this is

0:55:13.140,0:55:15.359
another one I was telling you about

0:55:13.980,0:55:19.410
earlier actually this one is also a

0:55:15.359,0:55:23.029
simplified one when you have a a one hot

0:55:19.410,0:55:28.799
distribution for the target so why is

0:55:23.029,0:55:30.240
it's a category but it has the

0:55:28.799,0:55:32.069
disadvantage of not being merged with

0:55:30.240,0:55:37.259
something like softmax or log softmax so

0:55:32.069,0:55:42.420
it may reach I mean it may have kind of

0:55:37.259,0:55:46.970
numerical issues again it assumes x and

0:55:42.420,0:55:46.970
y are you know distributions

0:55:48.749,0:55:56.910
this is values personal loss okay so

0:55:54.089,0:56:00.509
this version of binary choice entropy

0:55:56.910,0:56:02.910
here takes scores that haven't gone

0:56:00.509,0:56:06.980
through sigmoid so this one does not

0:56:02.910,0:56:09.660
assume that X the x's are between 0 & 1

0:56:06.980,0:56:12.210
it just takes you know values whatever

0:56:09.660,0:56:13.710
they are and it it passes them to a

0:56:12.210,0:56:17.430
sigmoid to make sure there are between 0

0:56:13.710,0:56:21.690
& 1 strictly ok and so that is more

0:56:17.430,0:56:23.369
likely to be a numerically stable it's a

0:56:21.690,0:56:25.529
bit the same ideas kind of emerging

0:56:23.369,0:56:33.480
locks off max and you get your lucky

0:56:25.529,0:56:36.239
hood very yeah same thing here that's

0:56:33.480,0:56:37.950
what I was talking okay margin losses so

0:56:36.239,0:56:43.799
this is sort of a important category of

0:56:37.950,0:56:50.579
losses those losses basically say if I

0:56:43.799,0:56:53.970
have in this case two inputs the last

0:56:50.579,0:56:56.579
function here says I want one input to

0:56:53.970,0:56:59.279
be larger than the other one by at least

0:56:56.579,0:57:02.160
two margin okay so imagine the two

0:56:59.279,0:57:03.420
inputs or scores for two categories you

0:57:02.160,0:57:04.680
want the score for the correct category

0:57:03.420,0:57:06.390
to be larger than the score for the

0:57:04.680,0:57:08.579
incorrect category but it hits some

0:57:06.390,0:57:12.779
margin that you passed through the

0:57:08.579,0:57:14.730
system and that's the the formula you

0:57:12.779,0:57:16.890
see down there so it's basically a him

0:57:14.730,0:57:19.260
okay and it takes the difference between

0:57:16.890,0:57:20.820
the two scores and so why is a binary

0:57:19.260,0:57:22.760
variable this plus one or minus one and

0:57:20.820,0:57:26.700
it controls whether you want X to be

0:57:22.760,0:57:28.430
larger than x1 to be larger than x2 or

0:57:26.700,0:57:31.109
whether you want x2 to be larger than x1

0:57:28.430,0:57:32.910
okay we basically give you two scores

0:57:31.109,0:57:37.109
and you tell it which one you want to be

0:57:32.910,0:57:39.150
the larger score and then the cost

0:57:37.109,0:57:40.410
function says you know if this one is

0:57:39.150,0:57:43.380
larger than that one by at least a

0:57:40.410,0:57:45.869
margin then the cost is zero if it's if

0:57:43.380,0:57:47.609
if it's smaller than the margin or if

0:57:45.869,0:57:49.740
it's in the other direction and the cost

0:57:47.609,0:58:00.150
increases linearly okay so that's called

0:57:49.740,0:58:02.550
a hingeless okay so that's very useful

0:58:00.150,0:58:14.160
for a number of different things we've

0:58:02.550,0:58:17.880
we've seen an example of this in so yeah

0:58:14.160,0:58:19.940
for example so this is sort of a margin

0:58:17.880,0:58:22.619
ranking class so you have two values but

0:58:19.940,0:58:24.390
there are sort of is a simplified

0:58:22.619,0:58:26.340
version of it I mean there's a simpler

0:58:24.390,0:58:29.430
version of it which I don't have here

0:58:26.340,0:58:35.660
for some reason we only have an X okay

0:58:29.430,0:58:35.660
so basically the loss is max of 0 and

0:58:36.140,0:58:42.600
minus X times the margin and it just

0:58:39.630,0:58:46.020
wants to make to make X smaller than the

0:58:42.600,0:58:48.180
margin right and so this is sort of a

0:58:46.020,0:58:51.000
special case where you have a ranking

0:58:48.180,0:58:52.590
between two scores of two categories so

0:58:51.000,0:58:56.280
here is how you would use this for

0:58:52.590,0:58:59.490
classification you would basically run

0:58:56.280,0:59:01.770
your classifier you would get scores ok

0:58:59.490,0:59:04.410
idea so before you do any non-linearity

0:59:01.770,0:59:07.020
weighted sums and then you know the

0:59:04.410,0:59:09.570
correct category so you say I want this

0:59:07.020,0:59:13.050
correct category to have a high score

0:59:09.570,0:59:15.470
and then what you do is you take another

0:59:13.050,0:59:18.480
category that has the most offending

0:59:15.470,0:59:21.180
score so either another category so a

0:59:18.480,0:59:23.070
category that is incorrect that has a

0:59:21.180,0:59:25.260
higher score than the correct one or

0:59:23.070,0:59:27.850
that has a lower score but the lowest

0:59:25.260,0:59:30.220
score is too close ok so you take the

0:59:27.850,0:59:34.240
the category that whose core is the

0:59:30.220,0:59:37.960
closest to the to the correct one or who

0:59:34.240,0:59:40.030
score is higher than the correct one and

0:59:37.960,0:59:42.550
you feed those two scores to the last

0:59:40.030,0:59:43.510
function like this so basically it's

0:59:42.550,0:59:44.890
going to push up the score in the

0:59:43.510,0:59:46.690
correct category push down the score the

0:59:44.890,0:59:48.070
incorrect category until the difference

0:59:46.690,0:59:52.630
is that eastern margin equal to the

0:59:48.070,0:59:55.750
margin okay that's you know a perfectly

0:59:52.630,0:59:57.160
good way of training something in the

0:59:55.750,0:59:58.510
context of an energy-based model for

0:59:57.160,1:00:02.310
example that's the sort of things you

0:59:58.510,1:00:06.310
might want to do you might want to say X

1:00:02.310,1:00:08.230
1/4 minus x1 is the energy I mean X you

1:00:06.310,1:00:10.330
know minus X 1 will be the energy of the

1:00:08.230,1:00:13.540
correct answer and minus X 2 would be

1:00:10.330,1:00:16.900
the energy of the incorrect answer like

1:00:13.540,1:00:19.240
an a contrastive term an incorrect

1:00:16.900,1:00:20.500
answer and you want to push down the

1:00:19.240,1:00:22.060
energy of the correct answer which are

1:00:20.500,1:00:23.760
the energy of incorrect answer so that

1:00:22.060,1:00:27.870
the difference is at least some margin

1:00:23.760,1:00:29.950
okay can use this kind of laws for that

1:00:27.870,1:00:31.630
the to play Clause is gonna be

1:00:29.950,1:00:34.450
refinement on this so this is used a lot

1:00:31.630,1:00:37.420
for metric learning for the kind of

1:00:34.450,1:00:39.930
Sammys nets that each n was each and

1:00:37.420,1:00:46.290
mister I was talking about last week and

1:00:39.930,1:00:46.290
and there the idea is let's say I have a

1:00:47.010,1:00:53.620
distance so let's say I have three

1:00:48.910,1:00:55.180
samples I have one sample and another

1:00:53.620,1:00:57.040
sample is very similar to it I've run

1:00:55.180,1:00:58.750
them through to commercial Nets I get

1:00:57.040,1:01:01.210
two vectors I compute the distance

1:00:58.750,1:01:04.180
between those two vectors D of a IP I

1:01:01.210,1:01:05.620
for example okay I want to make this

1:01:04.180,1:01:08.320
distance as small as possible because

1:01:05.620,1:01:10.540
that's the core example and then I take

1:01:08.320,1:01:12.700
two samples that I know are semantically

1:01:10.540,1:01:15.430
different ok the image of a cat and

1:01:12.700,1:01:17.230
whatever table and I want to make the

1:01:15.430,1:01:18.670
vector as far away from each other so I

1:01:17.230,1:01:24.910
compute the distance I want to make this

1:01:18.670,1:01:27.100
distance large alright now I can insist

1:01:24.910,1:01:28.450
that the first is done B zero and it can

1:01:27.100,1:01:29.590
insist that the second distance be

1:01:28.450,1:01:32.790
larger than the margin that would be

1:01:29.590,1:01:35.260
kind of a margin loss type type thing

1:01:32.790,1:01:37.840
but what I can do is one of those

1:01:35.260,1:01:39.700
triplet margin notes where I say the

1:01:37.840,1:01:41.520
only thing I care about is that the

1:01:39.700,1:01:43.500
distance that I get for the good

1:01:41.520,1:01:45.750
is smaller than the distance that I get

1:01:43.500,1:01:47.580
for the bad pit I don't care if the

1:01:45.750,1:01:49.619
distance is small I just wanted to be

1:01:47.580,1:01:54.450
smaller than the distance for the bad

1:01:49.619,1:02:01.440
pair okay and that's what those ranking

1:01:54.450,1:02:02.670
goes to a bunch of those were I mean one

1:02:01.440,1:02:06.660
of the first I think that was proposed

1:02:02.670,1:02:08.820
was by Chisholm Weston and Sammy bengio

1:02:06.660,1:02:12.390
back when Jaden Weston was today Google

1:02:08.820,1:02:15.869
and they used this to train kind of an

1:02:12.390,1:02:17.369
image search system for Google so back

1:02:15.869,1:02:19.350
then I'm not sure is true anymore but

1:02:17.369,1:02:22.080
back then you would type a query on

1:02:19.350,1:02:24.780
Google Google would encode that query

1:02:22.080,1:02:26.610
into a vector then we compare this to a

1:02:24.780,1:02:30.440
whole bunch of vectors describing images

1:02:26.610,1:02:33.420
that I've been previously indexed and

1:02:30.440,1:02:35.040
and then would kind of retrieve the

1:02:33.420,1:02:37.590
images whose vector were close to the

1:02:35.040,1:02:40.260
one that that you had and the way you

1:02:37.590,1:02:41.850
train those those the networks that

1:02:40.260,1:02:45.180
compute those vectors in that case back

1:02:41.850,1:02:46.740
then it was in your networks actually as

1:02:45.180,1:02:50.900
you train them with those trip pillows

1:02:46.740,1:02:53.310
okay so you said good hits for my search

1:02:50.900,1:02:55.020
should should have a distance between

1:02:53.310,1:02:57.119
the vectors that is smaller than any bad

1:02:55.020,1:02:58.500
hit and I don't care if the distance is

1:02:57.119,1:03:07.590
small I just wanted to be smaller than

1:02:58.500,1:03:10.140
for that it's any question that's kind

1:03:07.590,1:03:14.070
of a graphical explanation of this where

1:03:10.140,1:03:17.310
P is a positive sample so it's you know

1:03:14.070,1:03:18.690
similar to a so a is the sample you

1:03:17.310,1:03:21.240
considered P is kind of a positive

1:03:18.690,1:03:23.790
sample and N is a negative sample

1:03:21.240,1:03:27.869
contrast example you want to push anyway

1:03:23.790,1:03:30.119
and bring P closer and as soon as P is

1:03:27.869,1:03:38.220
closer than n by some margin you you

1:03:30.119,1:03:39.930
stop pushing and pulling your soft

1:03:38.220,1:03:41.790
versions of this and in fact you can

1:03:39.930,1:03:43.580
think of nce the the kind of loss

1:03:41.790,1:03:46.920
function that hn was talking about as

1:03:43.580,1:03:50.040
kind of a soft version of that where you

1:03:46.920,1:03:51.450
basically you have a bunch of positives

1:03:50.040,1:03:54.650
and bunch of negatives or you have one

1:03:51.450,1:03:57.140
positive and bunch of negatives and

1:03:54.650,1:04:01.040
you run them through softmax and you say

1:03:57.140,1:04:03.820
I want this the you know e to the minus

1:04:01.040,1:04:07.280
distance for the correct one to be

1:04:03.820,1:04:11.360
smaller than you know e to the minus the

1:04:07.280,1:04:13.370
other one so it kind of you pushes the

1:04:11.360,1:04:15.170
positive closer to you and pushes the

1:04:13.370,1:04:18.260
other ones further to you but now with

1:04:15.170,1:04:19.730
some sort of stuff maxi that sort of

1:04:18.260,1:04:26.000
exponential decay as opposed to sort of

1:04:19.730,1:04:27.440
a hard margin so in PI torch you have

1:04:26.000,1:04:29.930
things that allow you to have multi

1:04:27.440,1:04:33.290
labels so this allows you to basically

1:04:29.930,1:04:37.310
give multiple correct outputs so instead

1:04:33.290,1:04:38.750
of you know this is a ranking Mouse but

1:04:37.310,1:04:40.550
it's serving sitting that there is only

1:04:38.750,1:04:42.680
one correct category and you know you

1:04:40.550,1:04:44.120
you you want a high score for the

1:04:42.680,1:04:46.430
correct category and that's core for

1:04:44.120,1:04:48.020
everything else here you can have a

1:04:46.430,1:04:50.360
number of categories for which you want

1:04:48.020,1:04:51.680
high scores and then all the other ones

1:04:50.360,1:04:54.200
will get pushed away

1:04:51.680,1:04:57.350
all right we'll get the scores will be

1:04:54.200,1:04:59.810
pushed down so here it's a it's a hinge

1:04:57.350,1:05:04.880
loss but you do a sum of those this

1:04:59.810,1:05:07.520
hinge loss overall categories and and

1:05:04.880,1:05:09.320
for each category if the category is a

1:05:07.520,1:05:12.170
desired one you push it up if it's a non

1:05:09.320,1:05:17.510
desire one you push it down which is

1:05:12.170,1:05:19.340
what this video formula says and of

1:05:17.510,1:05:20.870
course you have the soft version of this

1:05:19.340,1:05:26.000
which I'm not going to go into the

1:05:20.870,1:05:32.800
details of and the multi margin version

1:05:26.000,1:05:35.060
of it so this pushing and pulling for

1:05:32.800,1:05:37.750
metric learning for embedding for Sammis

1:05:35.060,1:05:40.400
nets that I would say telling you about

1:05:37.750,1:05:43.250
it's actually kind of all implemented if

1:05:40.400,1:05:46.700
you want in one of those engine vending

1:05:43.250,1:05:50.480
laws so engine 30 loss is a loss for

1:05:46.700,1:05:51.920
Siamese nets that kind of pushes things

1:05:50.480,1:05:55.070
are semantically similar to you and push

1:05:51.920,1:05:56.810
away things that are not okay so the Y

1:05:55.070,1:05:58.670
variable indicates whether the pair you

1:05:56.810,1:06:00.590
are or whether the score you are giving

1:05:58.670,1:06:01.730
to the system is one that should be

1:06:00.590,1:06:05.780
pushed up a window should be pushed down

1:06:01.730,1:06:07.520
and it it chooses the hinge loss that

1:06:05.780,1:06:12.230
makes the score

1:06:07.520,1:06:15.550
positive if Y is plus one and it makes a

1:06:12.230,1:06:19.240
score negative by some margin Delta if

1:06:15.550,1:06:19.240
if Y is minus one

1:06:25.480,1:06:29.890
very often when you are doing

1:06:28.000,1:06:31.720
Siamese Nets the way you compute the

1:06:29.890,1:06:32.950
similarity between two vectors is not

1:06:31.720,1:06:36.250
through Euclidean distance between

1:06:32.950,1:06:39.160
cosine distance so the one minus the

1:06:36.250,1:06:40.900
cosine between the of the angle between

1:06:39.160,1:06:42.790
the two vectors this is basically a

1:06:40.900,1:06:45.940
normalized Euclidean distance if you

1:06:42.790,1:06:47.950
want you can think of it this way the

1:06:45.940,1:06:51.070
advantage of this is that whenever you

1:06:47.950,1:06:52.210
can push the distance whenever your two

1:06:51.070,1:06:54.220
vectors and you want to make the

1:06:52.210,1:06:55.690
distance as large as possible there's a

1:06:54.220,1:06:57.730
very easy way for the system to get away

1:06:55.690,1:07:00.310
with it by making the the two vectors

1:06:57.730,1:07:02.650
very large very long you know not

1:07:00.310,1:07:04.390
pointing in the same direction and make

1:07:02.650,1:07:06.790
them very very long so now the distance

1:07:04.390,1:07:08.109
would be large but of course that's not

1:07:06.790,1:07:09.790
what you want you don't want the system

1:07:08.109,1:07:11.950
to just make the the vectors bigger you

1:07:09.790,1:07:14.320
wanted to actually rotate the vector in

1:07:11.950,1:07:15.820
the right direction so you you normalize

1:07:14.320,1:07:17.170
the vectors and then computer normalize

1:07:15.820,1:07:19.390
Euclidean distance and that's basically

1:07:17.170,1:07:22.030
what this does and what this does is

1:07:19.390,1:07:24.369
that it for positive cases it tries to

1:07:22.030,1:07:28.240
make the vectors as aligned with each

1:07:24.369,1:07:31.720
other as possible and for negative pairs

1:07:28.240,1:07:34.089
it tries to make the cosine smaller than

1:07:31.720,1:07:35.890
the particular margin the margin in that

1:07:34.089,1:07:40.210
case which should probably be something

1:07:35.890,1:07:42.910
that kind of is close to zero so you

1:07:40.210,1:07:44.950
want the the cosine you know in a high

1:07:42.910,1:07:47.500
dimensional space there's a lot of space

1:07:44.950,1:07:49.300
near the equator of the sphere of the

1:07:47.500,1:07:50.950
high dimensions here okay so all your

1:07:49.300,1:07:54.040
points now are normalized so this here

1:07:50.950,1:07:55.540
what you want is samples that are

1:07:54.040,1:07:57.790
symmetrically similar to you should be

1:07:55.540,1:07:59.589
close to you the samples that are

1:07:57.790,1:08:01.060
dissimilar should be orthogonal you

1:07:59.589,1:08:03.640
don't want them to be Oppo opposed

1:08:01.060,1:08:05.800
because there is only one point in the

1:08:03.640,1:08:09.330
South Pole whereas on the equator is a

1:08:05.800,1:08:11.589
very very high large space the entire

1:08:09.330,1:08:15.250
sphere minus one dimension basically

1:08:11.589,1:08:17.259
okay so you can make the margin just you

1:08:15.250,1:08:18.969
know some small so small positive value

1:08:17.259,1:08:21.100
and and then you get the entire equator

1:08:18.969,1:08:22.480
essentially of this here which contains

1:08:21.100,1:08:25.350
you almost the entire volume of the

1:08:22.480,1:08:25.350
sphere and I dimension

1:08:29.230,1:08:32.889
since you see loss this is a little more

1:08:30.969,1:08:37.269
complicated because that's a task that

1:08:32.889,1:08:39.880
is basically uses structure prediction

1:08:37.269,1:08:42.940
what's called structure prediction so

1:08:39.880,1:08:46.420
this is I sort of briefly talked about

1:08:42.940,1:08:48.909
it very quickly a few weeks ago was

1:08:46.420,1:08:54.309
something very similar to this so this

1:08:48.909,1:08:56.949
is it also is applicable when you your

1:08:54.309,1:09:00.909
output is a sequence of vectors and

1:08:56.949,1:09:05.230
scores where the vectors correspond to

1:09:00.909,1:09:07.299
scores of categories okay and so you

1:09:05.230,1:09:09.639
have so your system computes a vector of

1:09:07.299,1:09:11.710
such score so imagine for example speech

1:09:09.639,1:09:15.069
recognition system speech recognition

1:09:11.710,1:09:17.889
system every 10 milliseconds gives you a

1:09:15.069,1:09:20.099
vector of probabilities for what the

1:09:17.889,1:09:22.179
sound being pronounced right now is and

1:09:20.099,1:09:23.589
the number of categories usually is

1:09:22.179,1:09:26.380
quite large on the order of a few

1:09:23.589,1:09:28.119
thousand okay so give you basically a

1:09:26.380,1:09:31.150
softmax vector of a size you know

1:09:28.119,1:09:34.619
typically three thousand let's say one

1:09:31.150,1:09:36.730
of those every 10 milliseconds all right

1:09:34.619,1:09:38.409
and what you like you know you have a

1:09:36.730,1:09:41.319
desired output and the desired output is

1:09:38.409,1:09:42.609
what word was being pronounced and a

1:09:41.319,1:09:44.079
word that's being pronounced that

1:09:42.609,1:09:48.519
corresponds to kind of a particular

1:09:44.079,1:09:50.349
sequence of sounds if you want that you

1:09:48.519,1:09:53.380
might you might know so what you need

1:09:50.349,1:09:55.750
now is a course that basically is row if

1:09:53.380,1:10:00.820
that sequence looks like like that

1:09:55.750,1:10:04.869
sequence but what you might allow is for

1:10:00.820,1:10:10.090
the input sequence to repeat some of the

1:10:04.869,1:10:13.630
sounds if you want right so so for

1:10:10.090,1:10:16.630
example you know my my cost to make the

1:10:13.630,1:10:18.579
target might be the word seven they say

1:10:16.630,1:10:20.440
and it's pronounced really quickly 7 so

1:10:18.579,1:10:22.420
you basically have you know a very small

1:10:20.440,1:10:25.059
number of samples of each sound in the

1:10:22.420,1:10:26.769
sequence but then perhaps the the person

1:10:25.059,1:10:28.750
who is pronouncing the the word now that

1:10:26.769,1:10:31.809
user as a training sample pronounced it

1:10:28.750,1:10:35.559
very slowly like seven right so now the

1:10:31.809,1:10:38.289
the first the first takes you know

1:10:35.559,1:10:40.030
several several frames of 10

1:10:38.289,1:10:43.050
milliseconds that should all be mapped

1:10:40.030,1:10:43.050
to the the same

1:10:43.980,1:10:50.560
instance of the earth in the indian foot

1:10:47.560,1:10:55.960
and i do that picture before but i gonna

1:10:50.560,1:11:04.630
do it again right so the you have let's

1:10:55.960,1:11:06.730
see you have a sequence of scores coming

1:11:04.630,1:11:07.810
out of soft max's let's say it's

1:11:06.730,1:11:12.640
actually better if there are energies

1:11:07.810,1:11:20.260
but firstly TC they need to be and then

1:11:12.640,1:11:25.530
you have the target sequence and i think

1:11:20.260,1:11:28.870
of this as some sort of matrix and each

1:11:25.530,1:11:30.850
entry in that matrix basically measures

1:11:28.870,1:11:34.930
the distance between the two vectors

1:11:30.850,1:11:37.570
that are here okay so when i'm treating

1:11:34.930,1:11:40.180
the matrix indicates how this vector

1:11:37.570,1:11:41.290
looks like that vector for example with

1:11:40.180,1:11:44.920
the course entropy or something like

1:11:41.290,1:11:49.770
that okay or is quite error it doesn't

1:11:44.920,1:11:49.770
matter what the last function is so now

1:11:49.800,1:12:05.290
if this is the word seven pronounced

1:11:59.910,1:12:10.570
slowly okay and this has perhaps only

1:12:05.290,1:12:15.190
one instance of each sound you want all

1:12:10.570,1:12:20.490
of those you know you would want all of

1:12:15.190,1:12:27.250
those vectors corresponding to the e to

1:12:20.490,1:12:29.440
be mapped to that vector here okay so

1:12:27.250,1:12:33.010
you want to compute that cost of you

1:12:29.440,1:12:33.430
know confusing that those all of those I

1:12:33.010,1:12:35.470
mean

1:12:33.430,1:12:37.510
map matching those ease to that to that

1:12:35.470,1:12:39.100
e now of course here the system could

1:12:37.510,1:12:41.680
use the correct answer so you don't have

1:12:39.100,1:12:43.420
much of a problem but if the target is

1:12:41.680,1:12:47.590
seven but the word that was pronounced

1:12:43.420,1:12:51.780
here or the output that was produced by

1:12:47.590,1:12:54.010
the system does not correspond to seven

1:12:51.780,1:12:55.210
that's that's that's when you run into

1:12:54.010,1:12:57.489
into trouble so

1:12:55.210,1:13:01.420
here what you do is you find the best

1:12:57.489,1:13:03.370
mapping from the input sequence to the

1:13:01.420,1:13:06.190
output sequence okay so the s gets

1:13:03.370,1:13:09.940
mapped to the s the e to the e the V to

1:13:06.190,1:13:12.460
the V the east to the e and the end to

1:13:09.940,1:13:14.170
the N so you get this kind of path if

1:13:12.460,1:13:15.880
you want that think of this as a path in

1:13:14.170,1:13:17.350
the graph

1:13:15.880,1:13:20.350
and the way you determine this is

1:13:17.350,1:13:22.210
basically by using a dynamic programming

1:13:20.350,1:13:23.890
algorithm the short path algorithm that

1:13:22.210,1:13:27.430
figures out how do I get from here to

1:13:23.890,1:13:30.480
here you know path that minimizes the

1:13:27.430,1:13:34.150
sum of the distance distances between

1:13:30.480,1:13:35.890
the the all the vectors of the distances

1:13:34.150,1:13:40.360
between the vectors of you know all the

1:13:35.890,1:13:41.890
points are going through ok so there's a

1:13:40.360,1:13:44.290
optimization respect to a latent

1:13:41.890,1:13:45.670
variable if you want okay and CGC

1:13:44.290,1:13:47.500
basically decide for you right so you

1:13:45.670,1:13:50.230
give it two sequences and it computes

1:13:47.500,1:13:52.510
the distance between them and you know

1:13:50.230,1:13:58.960
kind of the best kind of mapping between

1:13:52.510,1:14:01.989
the two by allowing basically to to map

1:13:58.960,1:14:03.550
multiple input vectors to kind of a

1:14:01.989,1:14:06.100
single one on the output it cannot

1:14:03.550,1:14:09.489
expand it it can only kind of reduce if

1:14:06.100,1:14:10.719
you want and then that's done in a way

1:14:09.489,1:14:13.989
that you can back propagate gradient to

1:14:10.719,1:14:15.310
it we'll come back to this two more

1:14:13.989,1:14:21.820
things like this at the end if you can

1:14:15.310,1:14:24.880
oops so this is what this the target is

1:14:21.820,1:14:26.260
assumed to be many-to-one the alignment

1:14:24.880,1:14:27.340
of the input to the target is assumed to

1:14:26.260,1:14:29.020
be miniature when which leave is the

1:14:27.340,1:14:31.480
length of the target sequence such that

1:14:29.020,1:14:33.010
it must be smaller than the length of

1:14:31.480,1:14:36.400
the input that's for the reason I just

1:14:33.010,1:14:37.930
explained okay so it's basically

1:14:36.400,1:14:41.080
differentiable time-warping you could

1:14:37.930,1:14:43.420
think of it this way or sort of a module

1:14:41.080,1:14:46.330
that does that any time marking or

1:14:43.420,1:14:48.400
dynamic programming and it's still

1:14:46.330,1:14:50.590
differentiable the idea for this goes

1:14:48.400,1:14:54.340
back in the early 90s in the Lobo two

1:14:50.590,1:14:57.730
species actually that's very old very a

1:14:54.340,1:15:00.550
good paper or resource to learn more

1:14:57.730,1:15:03.010
about that dynamic programming algorithm

1:15:00.550,1:15:05.980
there actually that's kind of what I'm

1:15:03.010,1:15:08.099
gonna talk about next I may not have

1:15:05.980,1:15:12.329
time to go through it but

1:15:08.099,1:15:16.730
I'll try to okay but basically the last

1:15:12.329,1:15:17.909
part of the energy based model tutorial

1:15:16.730,1:15:20.010
okay

1:15:17.909,1:15:24.239
so the initial base model tutorial the

1:15:20.010,1:15:27.420
2006 paper that we give you a reference

1:15:24.239,1:15:31.110
a link to a tutorial on energy based

1:15:27.420,1:15:35.719
models the this the second part is all

1:15:31.110,1:15:38.219
about this kind of stuff essentially

1:15:35.719,1:15:40.230
okay so it's more energy based models

1:15:38.219,1:15:42.510
but now in getting more of a supervised

1:15:40.230,1:15:48.270
context if you want

1:15:42.510,1:15:52.050
so preliminary so before I get to this I

1:15:48.270,1:15:53.250
want to come back to the sort of more

1:15:52.050,1:16:01.949
general formulation of energy based

1:15:53.250,1:16:03.840
models and the idea that so if you want

1:16:01.949,1:16:06.300
to kind of define energy based models in

1:16:03.840,1:16:10.469
the proper way these are the conditional

1:16:06.300,1:16:13.079
versions you have a a training set a

1:16:10.469,1:16:15.690
bunch of pairs x y y I for I equals 1 to

1:16:13.079,1:16:18.869
P you have a loss function also the last

1:16:15.690,1:16:20.670
functional L of ENS so it takes the

1:16:18.869,1:16:23.820
energy function computed by the system

1:16:20.670,1:16:26.550
okay and the training set and it gives

1:16:23.820,1:16:28.139
you a scalar value now you can you can

1:16:26.550,1:16:30.659
think of this as a functional functional

1:16:28.139,1:16:32.219
is a function of a function ok but in

1:16:30.659,1:16:34.889
fact because the energy function itself

1:16:32.219,1:16:36.510
is parametrized by parameter W you can

1:16:34.889,1:16:38.969
turn this functional into a loss

1:16:36.510,1:16:40.469
function which is not just a function of

1:16:38.969,1:16:45.119
W another function of the energy

1:16:40.469,1:16:46.739
function okay and of course the the set

1:16:45.119,1:16:49.650
of energy functions is called epsilon

1:16:46.739,1:16:53.909
here it's family tries by the parameter

1:16:49.650,1:16:55.800
W which is taken within the set so

1:16:53.909,1:16:57.630
training consistent of course minimizing

1:16:55.800,1:16:59.489
the the loss function or with respect to

1:16:57.630,1:17:02.400
W and finding the W that minimizes it

1:16:59.489,1:17:03.960
and and so one question you might ask

1:17:02.400,1:17:05.429
yourself you know I went to a whole

1:17:03.960,1:17:07.619
bunch of objective function loss

1:17:05.429,1:17:08.880
functions here and the question is if

1:17:07.619,1:17:11.360
you are in an energy based framework

1:17:08.880,1:17:14.670
what loss functions are good ones and

1:17:11.360,1:17:16.199
what all functions are bad ones how do

1:17:14.670,1:17:18.929
you characterize a loss function that

1:17:16.199,1:17:20.990
actually will do something useful for

1:17:18.929,1:17:23.480
you ok

1:17:20.990,1:17:26.090
so here is a general formulation of the

1:17:23.480,1:17:29.300
last function it's it's an average over

1:17:26.090,1:17:31.790
training samples so here I'm kind of

1:17:29.300,1:17:34.070
assuming that it's invariant under

1:17:31.790,1:17:36.440
permutation of the samples so an average

1:17:34.070,1:17:38.510
is as good as any other aggregation

1:17:36.440,1:17:40.580
go-getting function so it's the average

1:17:38.510,1:17:44.150
of our training samples of a person for

1:17:40.580,1:17:46.160
loss function capital L and it takes the

1:17:44.150,1:17:50.000
desired answer Y which could be just a

1:17:46.160,1:17:52.310
category or it could be a whole image or

1:17:50.000,1:17:56.180
whatever and it takes the energy

1:17:52.310,1:17:59.650
function where X the X variable X I is

1:17:56.180,1:18:03.200
is equal to X I the ice training sample

1:17:59.650,1:18:08.300
the Y variable is undetermined okay so e

1:18:03.200,1:18:10.370
of W Y and X I is basically the entire

1:18:08.300,1:18:13.250
shape of the energy function for values

1:18:10.370,1:18:15.200
of Y over values of Y for a given X okay

1:18:13.250,1:18:19.130
X equal to X I and you can have a

1:18:15.200,1:18:22.790
regularizer if you want okay so here

1:18:19.130,1:18:24.710
this is a loss functional again again of

1:18:22.790,1:18:26.090
course we have to design this loss

1:18:24.710,1:18:27.980
function all so that it makes the energy

1:18:26.090,1:18:31.520
of correct answers small and the energy

1:18:27.980,1:18:36.230
of incorrect answers large in some ways

1:18:31.520,1:18:38.060
right ok now we're going to go through a

1:18:36.230,1:18:42.020
bunch of different types of loss

1:18:38.060,1:18:45.050
functions so one thing we could do is

1:18:42.020,1:18:47.480
say my loss function is just going to be

1:18:45.050,1:18:50.180
the energy of the correct answer so I'm

1:18:47.480,1:18:52.580
gonna place myself in the context of

1:18:50.180,1:18:53.960
energy based model my system produces

1:18:52.580,1:18:57.200
scores I interpret those scores as

1:18:53.960,1:19:01.540
energies so high is bad good is good

1:18:57.200,1:19:06.080
that means low is good as opposed to

1:19:01.540,1:19:13.460
positive scores and what I'm just going

1:19:06.080,1:19:14.750
to do is define my energy functional as

1:19:13.460,1:19:16.820
unfortunately the energy function of the

1:19:14.750,1:19:18.170
function of Y as simply the energy that

1:19:16.820,1:19:24.950
my model gives to the correct answer

1:19:18.170,1:19:26.570
okay so basically I give it an X and I

1:19:24.950,1:19:27.890
give it the correct answer Y and as a

1:19:26.570,1:19:30.170
system what energy do you give to that

1:19:27.890,1:19:33.110
pair and then I try to make that energy

1:19:30.170,1:19:34.510
as small as possible okay so you have

1:19:33.110,1:19:36.700
this landscape of energy

1:19:34.510,1:19:39.250
is here now we honor I showed you this

1:19:36.700,1:19:40.900
slide in the context of unsupervised

1:19:39.250,1:19:42.550
super Ronnie here I'm showing to you in

1:19:40.900,1:19:44.710
the context of supervised Ronnie so

1:19:42.550,1:19:47.650
imagine that one of the variables is X

1:19:44.710,1:19:49.180
and the other variable is y okay and the

1:19:47.650,1:19:50.290
blue beads are training samples and you

1:19:49.180,1:19:54.940
want to make the energy of the blue

1:19:50.290,1:19:56.170
beads as far as possible so you're

1:19:54.940,1:19:58.330
pulling down on the blue beads but

1:19:56.170,1:19:59.770
you're not doing anything else and so as

1:19:58.330,1:20:01.510
a result depending on the architecture

1:19:59.770,1:20:03.310
or your network if your network is not

1:20:01.510,1:20:07.300
designed properly or if it's designing a

1:20:03.310,1:20:08.650
bit in no particular way it could very

1:20:07.300,1:20:10.570
well be that the energy function is

1:20:08.650,1:20:11.950
going to become flat everywhere okay

1:20:10.570,1:20:13.630
you're just trying to make the energy of

1:20:11.950,1:20:14.860
the correct answer small are you not

1:20:13.630,1:20:16.930
telling the system the energy of

1:20:14.860,1:20:19.720
everything else should be higher and so

1:20:16.930,1:20:24.670
the system might just collapse

1:20:19.720,1:20:26.500
all right so energy loss is not good in

1:20:24.670,1:20:29.100
that sense but there are certain

1:20:26.500,1:20:32.560
situations where it's applicable because

1:20:29.100,1:20:35.500
if the shape of the energy function is

1:20:32.560,1:20:37.600
such that it cannot make the you can

1:20:35.500,1:20:40.450
only make the energy of a single answer

1:20:37.600,1:20:43.360
small all the other ones being larger

1:20:40.450,1:20:45.370
then you need to have a quadratic term

1:20:43.360,1:20:47.890
okay and we've seen this in the context

1:20:45.370,1:20:49.720
of supervised learning people are

1:20:47.890,1:20:55.930
completely lost about the loss

1:20:49.720,1:20:57.760
functional right okay so this is a

1:20:55.930,1:21:01.570
function Al and it's a function of

1:20:57.760,1:21:03.400
another function e okay

1:21:01.570,1:21:06.040
so it's called a functional because it's

1:21:03.400,1:21:07.360
a function of a function right it's not

1:21:06.040,1:21:10.260
a function of a point it's a function of

1:21:07.360,1:21:15.730
a function now if that second function

1:21:10.260,1:21:17.020
is permit rised by a parameter W then

1:21:15.730,1:21:18.430
you can say that the last function is

1:21:17.020,1:21:21.250
actually a function of that parameter W

1:21:18.430,1:21:23.110
now it becomes a regular function okay

1:21:21.250,1:21:24.670
that's what I had can you can you write

1:21:23.110,1:21:27.100
it down

1:21:24.670,1:21:31.060
it's basically written here okay you can

1:21:27.100,1:21:33.520
either write the functional as if I can

1:21:31.060,1:21:34.990
find it as L of e and s so that's a

1:21:33.520,1:21:39.700
functional because it's a function of e

1:21:34.990,1:21:44.080
which itself is a function okay but e

1:21:39.700,1:21:45.610
itself is a function of W and so if I

1:21:44.080,1:21:47.110
write the last function directly as a

1:21:45.610,1:21:49.290
function of W now it's just a regular

1:21:47.110,1:21:49.290
function

1:21:50.190,1:21:59.920
okay yeah I mean I asked the question

1:21:57.550,1:22:12.460
that was asked in the chat I yeah I was

1:21:59.920,1:22:17.530
kind of you I know before that you know

1:22:12.460,1:22:21.550
for okay we've seen the negative log

1:22:17.530,1:22:25.380
likelihood loss before I talked about

1:22:21.550,1:22:28.060
this so this is a loss function that

1:22:25.380,1:22:31.210
tries to make the energy of the correct

1:22:28.060,1:22:32.680
answer so look at the rectangle in red

1:22:31.210,1:22:35.740
tries to make the energy of the correct

1:22:32.680,1:22:37.570
answer as low as possible and then you

1:22:35.740,1:22:40.090
have the second term whatever beta log

1:22:37.570,1:22:45.730
sum over all why's of e to the minus

1:22:40.090,1:22:49.330
beta e of WI X and this one is trying to

1:22:45.730,1:22:51.940
make the energy of all Y's for this

1:22:49.330,1:22:53.320
given X as large as possible okay

1:22:51.940,1:22:55.720
because the best way to make this term

1:22:53.320,1:22:58.150
small is to make those energies large

1:22:55.720,1:23:03.370
because they enter in there as a

1:22:58.150,1:23:06.340
negative of negative its financial okay

1:23:03.370,1:23:07.870
so this has this kind of pushing down on

1:23:06.340,1:23:13.360
the correct answer pushing up on

1:23:07.870,1:23:16.360
incorrect answer behavior and we've seen

1:23:13.360,1:23:20.110
before we just talked about margin loss

1:23:16.360,1:23:21.190
and and other types of losses here is

1:23:20.110,1:23:22.900
something that's called a perceptron

1:23:21.190,1:23:26.080
loss because it's basically very similar

1:23:22.900,1:23:28.900
to I mean it's exactly the same as the

1:23:26.080,1:23:31.510
loss that was used for the perceptron 60

1:23:28.900,1:23:34.480
years ago over 60 years ago so this one

1:23:31.510,1:23:40.480
says I want to make the energy of the

1:23:34.480,1:23:43.410
correct answer small and the same time I

1:23:40.480,1:23:46.930
want to make the energy of the smallest

1:23:43.410,1:23:49.660
the smallest energy for all answers as

1:23:46.930,1:23:51.370
large as possible okay so pick the Y

1:23:49.660,1:23:54.280
that has the smallest energy in your

1:23:51.370,1:23:56.020
system make that as large as you can the

1:23:54.280,1:23:57.760
same time picks the correct energy make

1:23:56.020,1:24:00.690
that as small as you can now there is a

1:23:57.760,1:24:00.690
point at which the

1:24:00.710,1:24:06.260
answer with the correct energy is going

1:24:02.360,1:24:07.370
to equal to the correct answer and so

1:24:06.260,1:24:11.410
that difference can never be negative

1:24:07.370,1:24:13.970
okay because the first term is

1:24:11.410,1:24:17.620
necessarily one term in that minimum and

1:24:13.970,1:24:22.010
so the difference is at best zero and

1:24:17.620,1:24:25.370
for every other cases is in spot this

1:24:22.010,1:24:27.250
trick is positive it's only zero when

1:24:25.370,1:24:31.550
the system gives you the correct answer

1:24:27.250,1:24:33.740
okay but this objective function does

1:24:31.550,1:24:36.950
not prevent the system from giving the

1:24:33.740,1:24:38.510
very same energy to every answer okay so

1:24:36.950,1:24:40.730
in that sense it's a bad energy it's a

1:24:38.510,1:24:43.670
bad loss function so bad a loss function

1:24:40.730,1:24:45.560
because it it says I want the energy of

1:24:43.670,1:24:47.720
the correct answer to be small I want

1:24:45.560,1:24:49.700
the energy of all the other answers to

1:24:47.720,1:24:51.170
be large but I don't insist that there

1:24:49.700,1:24:52.970
is any difference between them so the

1:24:51.170,1:24:59.000
system can choose to make every answer

1:24:52.970,1:25:01.070
the same energy and that's a class okay

1:24:59.000,1:25:02.510
so the spectrum loss is not good it's

1:25:01.070,1:25:06.380
actually only good for linear systems

1:25:02.510,1:25:11.150
but it's not good for as a objective

1:25:06.380,1:25:12.530
function for nonlinear systems so here

1:25:11.150,1:25:16.670
is a way to design an objective function

1:25:12.530,1:25:18.140
that will always be good and you you

1:25:16.670,1:25:19.790
take the energy of the correct answer

1:25:18.140,1:25:21.460
and you take the energy of the most

1:25:19.790,1:25:24.260
offending incorrect answer which means

1:25:21.460,1:25:25.850
the value of y that is incorrect but at

1:25:24.260,1:25:30.920
the same time is the lowest energy of

1:25:25.850,1:25:33.590
all the incorrect answers okay and your

1:25:30.920,1:25:35.600
system will work if that difference is

1:25:33.590,1:25:37.520
negative in other words if the energy of

1:25:35.600,1:25:39.410
the correct answer is smaller than the

1:25:37.520,1:25:42.580
energy of the most of any correct answer

1:25:39.410,1:25:45.710
but at least some quantity some margin

1:25:42.580,1:25:47.930
okay so as long as your objective

1:25:45.710,1:25:49.550
function will you design it ensures that

1:25:47.930,1:25:50.900
the energy of the correct answer is

1:25:49.550,1:25:52.580
smaller than the energy of the most of

1:25:50.900,1:25:55.300
any correct answer by at least a margin

1:25:52.580,1:26:02.510
nonzero margin then you'll find your

1:25:55.300,1:26:04.610
loss function is good okay so things

1:26:02.510,1:26:07.580
like kyndra's are good the heat loss

1:26:04.610,1:26:09.950
basically says and we talked about this

1:26:07.580,1:26:11.210
just just before I want the energy of

1:26:09.950,1:26:12.410
the correct answer to be smaller than

1:26:11.210,1:26:12.889
the energy of the most offending

1:26:12.410,1:26:16.280
incorrect

1:26:12.889,1:26:18.679
which is denoted why I bar here but at

1:26:16.280,1:26:21.260
least M okay this is what this house

1:26:18.679,1:26:22.610
function does it's a hinge loss and it

1:26:21.260,1:26:26.210
wants to push down the energy of this

1:26:22.610,1:26:31.340
guy below the energy of that guy by at

1:26:26.210,1:26:33.889
least this margin so this has a margin M

1:26:31.340,1:26:36.230
and this will you know if you train a

1:26:33.889,1:26:38.409
system with this loss a well and he can

1:26:36.230,1:26:42.260
run the task he will run the task and

1:26:38.409,1:26:44.960
probably produce the good answers the

1:26:42.260,1:26:47.210
heat loss the soft hinge loss which is

1:26:44.960,1:26:50.270
in the context of Nigeria's models is

1:26:47.210,1:26:51.830
expressed this way basically instead of

1:26:50.270,1:26:53.179
feeling the difference between the

1:26:51.830,1:26:55.670
energies of the correct answer and the

1:26:53.179,1:26:59.330
most offending incorrect one into a

1:26:55.670,1:27:02.600
hinge it sits it feeds it to a soft

1:26:59.330,1:27:06.080
hinge okay which we talked about to 30

1:27:02.600,1:27:08.989
minutes ago and there this was also very

1:27:06.080,1:27:11.300
margin the modulation will be how how to

1:27:08.989,1:27:16.250
pick em say that question would be how

1:27:11.300,1:27:20.210
to pick em it's arbitrary

1:27:16.250,1:27:22.850
you can set n to 1 you can set m to 1/10

1:27:20.210,1:27:24.020
I mean it's kind of arbitrary because it

1:27:22.850,1:27:25.880
will just determine the size of the

1:27:24.020,1:27:33.429
weights of your last layer that's all it

1:27:25.880,1:27:33.429
does okay so it's basically up to you

1:27:34.119,1:27:38.869
yeah so the soft hinge loss has an

1:27:37.190,1:27:39.830
infinite margin it wants the difference

1:27:38.869,1:27:42.590
between those two energies to be

1:27:39.830,1:27:44.540
infinite but the the Stroop sort of

1:27:42.590,1:27:45.980
decreases exponentially so it's it's

1:27:44.540,1:27:50.060
never going to get there because you

1:27:45.980,1:27:56.119
know the gradients get very small as the

1:27:50.060,1:27:58.760
difference increases here's another

1:27:56.119,1:28:01.250
example of a margin loss the square loss

1:27:58.760,1:28:05.090
the square the square square square loss

1:28:01.250,1:28:07.639
okay so this is it also tries to make

1:28:05.090,1:28:10.340
the energy of the correct answer squared

1:28:07.639,1:28:14.030
as small as possible and then it has a

1:28:10.340,1:28:15.679
square hinge to push away to push up the

1:28:14.030,1:28:19.550
energy of the most offending incorrect

1:28:15.679,1:28:21.469
answers okay and again that works and

1:28:19.550,1:28:22.940
this is very similar to the kind of laws

1:28:21.469,1:28:25.849
that people use inside these nets and

1:28:22.940,1:28:27.650
stuff like that that you've heard about

1:28:25.849,1:28:28.929
there's a whole menagerie of such losses

1:28:27.650,1:28:31.429
which I'm not going to go through

1:28:28.929,1:28:33.110
there's actually a whole table here

1:28:31.429,1:28:36.290
which is also in this paper the tutorial

1:28:33.110,1:28:40.010
energy these models and what's indicated

1:28:36.290,1:28:42.260
on the on the right side is whether they

1:28:40.010,1:28:44.000
have a margin or not so the energy loss

1:28:42.260,1:28:47.239
does not have a margin it doesn't push

1:28:44.000,1:28:49.070
up anything so no margin it doesn't it's

1:28:47.239,1:28:51.139
not it doesn't work always you have to

1:28:49.070,1:28:53.810
design the machine so that this just may

1:28:51.139,1:28:57.110
work for that that system the perceptual

1:28:53.810,1:28:58.489
noise does not work in general it only

1:28:57.110,1:29:00.920
works it works if you have a linear

1:28:58.489,1:29:02.750
parameterization of your energy as a

1:29:00.920,1:29:03.920
function of the parameters but that's a

1:29:02.750,1:29:08.510
special case and that's the case for the

1:29:03.920,1:29:10.730
perceptron and then some of them have a

1:29:08.510,1:29:12.280
finite margin like the hinge loss and so

1:29:10.730,1:29:22.310
on then I have an infinite margin like

1:29:12.280,1:29:24.679
the log the soft hinge if you want much

1:29:22.310,1:29:26.810
of how much of those losses some of

1:29:24.679,1:29:29.119
those were used were invented in the

1:29:26.810,1:29:33.500
context of discriminative learning for

1:29:29.119,1:29:35.300
speech recognition systems but not they

1:29:33.500,1:29:38.230
were invented before people in machine

1:29:35.300,1:29:41.210
learning actually got interested in this

1:29:38.230,1:29:43.190
position would be like how you find the

1:29:41.210,1:29:45.500
Y bar so if you have like a discreet

1:29:43.190,1:29:47.869
code we can find simply like you know

1:29:45.500,1:29:55.130
the minimum value but otherwise are we

1:29:47.869,1:29:59.570
running gradient descent right so if Y

1:29:55.130,1:30:01.040
is continuous then there is no kind of

1:29:59.570,1:30:03.800
clear definition for what is the most

1:30:01.040,1:30:06.290
offending incorrect answer okay you will

1:30:03.800,1:30:10.250
have to define some sort of distance

1:30:06.290,1:30:12.280
around the correct answer above which

1:30:10.250,1:30:14.619
you consider an answer to be incorrect

1:30:12.280,1:30:17.570
okay so for example you are in a

1:30:14.619,1:30:19.429
continuous energy landscape is one-one

1:30:17.570,1:30:20.900
training sample here you want to make

1:30:19.429,1:30:23.690
that the energy of that training sample

1:30:20.900,1:30:25.460
small easy enough compute the energy

1:30:23.690,1:30:27.320
through your neural net push it down

1:30:25.460,1:30:29.300
back propagate update the weight so that

1:30:27.320,1:30:31.820
the energy goes down easy enough now the

1:30:29.300,1:30:33.590
incorrect answer if you if you take an

1:30:31.820,1:30:38.659
answer that just kind of epsilon outside

1:30:33.590,1:30:40.699
of that and you push up you know you

1:30:38.659,1:30:42.139
your energy surface might be a little

1:30:40.699,1:30:44.150
stiff because it's completely computed

1:30:42.139,1:30:46.040
by a paralyzed neural net so that may

1:30:44.150,1:30:49.310
not be possible so you probably want to

1:30:46.040,1:30:51.050
have a incorrect answer that's you know

1:30:49.310,1:30:55.190
quite a bit outside that you're gonna

1:30:51.050,1:30:56.840
you know push up and so that's how you

1:30:55.190,1:30:58.130
define you know the the whole question

1:30:56.840,1:31:00.980
is how you define the contrasts each

1:30:58.130,1:31:03.710
sample that you gonna push up and and

1:31:00.980,1:31:08.350
the at all of those objective functions

1:31:03.710,1:31:14.330
here those loss functions use a single

1:31:08.350,1:31:17.270
you know Y bar negative sample but there

1:31:14.330,1:31:20.290
is no single simple single correct way

1:31:17.270,1:31:23.360
of picking this Y bar you can imagine

1:31:20.290,1:31:26.000
you know particularly in the kind of in

1:31:23.360,1:31:30.800
the sort of continuous case or in the

1:31:26.000,1:31:33.429
case where Y is either very very large

1:31:30.800,1:31:37.880
or continuous and high dimensional

1:31:33.429,1:31:39.739
there's no simple way to pick to pick y

1:31:37.880,1:31:41.810
bar you know a lot of discussions we've

1:31:39.739,1:31:43.699
had about contrasting methods that I

1:31:41.810,1:31:46.190
shan't talk to that first time use nets

1:31:43.699,1:31:48.219
and that we talked about before where

1:31:46.190,1:31:50.570
basically how do you pick a y bar in the

1:31:48.219,1:31:55.520
South supervised case so supervised do

1:31:50.570,1:31:58.960
not have an X right and you know this

1:31:55.520,1:31:58.960
many ways you can you can pick it up

1:31:59.889,1:32:04.310
it's only obvious how to pick it up in

1:32:02.179,1:32:06.469
kind of small cases I just want to point

1:32:04.310,1:32:09.409
out the formula here at the bottom so

1:32:06.469,1:32:13.790
this is a kind of you can think of this

1:32:09.409,1:32:20.690
as sort of a general form of of sort of

1:32:13.790,1:32:22.699
hinge type contrasted losses where you

1:32:20.690,1:32:26.179
have an H function here think of it as a

1:32:22.699,1:32:30.230
hinge for some type okay and instead of

1:32:26.179,1:32:32.020
that hinge you have the energy of the

1:32:30.230,1:32:34.790
correct answer so that's the energy of

1:32:32.020,1:32:36.139
WY ixi so this is your training sample

1:32:34.790,1:32:39.290
as the energy of the system gives to the

1:32:36.139,1:32:45.260
training sample the second term is the

1:32:39.290,1:32:48.170
energy of some other answer Y ok for the

1:32:45.260,1:32:51.199
same X training sample and then there is

1:32:48.170,1:32:52.220
a margin but that margin C is actually a

1:32:51.199,1:32:54.530
function of

1:32:52.220,1:32:56.210
why I and why and you might imagine the

1:32:54.530,1:32:59.930
margin is actually also a function of X

1:32:56.210,1:33:01.280
and X I so basically you determine a

1:32:59.930,1:33:05.770
margin as a function of the distance

1:33:01.280,1:33:08.800
between the between the Y's okay

1:33:05.770,1:33:11.840
and you feed that to let's say a hinge

1:33:08.800,1:33:13.820
now the thing is this loss function is

1:33:11.840,1:33:15.290
summed over all wise here is a discreet

1:33:13.820,1:33:18.470
song because Y is discrete but you could

1:33:15.290,1:33:22.340
imagine an integral okay so this kind of

1:33:18.470,1:33:23.270
loss says you know I have an energy for

1:33:22.340,1:33:25.340
my correct answer

1:33:23.270,1:33:28.100
I have energies for every other answer

1:33:25.340,1:33:31.010
in my space and I want to push out the

1:33:28.100,1:33:33.380
energy of all other answers but the

1:33:31.010,1:33:35.630
amount by which I want to make them

1:33:33.380,1:33:40.120
higher the margin depends on the

1:33:35.630,1:33:43.070
distance between between y and y y bar

1:33:40.120,1:33:46.310
or in this case between y I which is

1:33:43.070,1:33:48.830
this and Y which is the other the other

1:33:46.310,1:33:51.020
wise okay so you can imagine that this

1:33:48.830,1:33:53.030
margin will come you know we become

1:33:51.020,1:33:54.260
smaller and smaller as the 2y is gonna

1:33:53.030,1:33:56.180
get closer to each other in this case

1:33:54.260,1:33:57.950
you don't push up too much four things

1:33:56.180,1:34:00.470
are too close and you you push up in

1:33:57.950,1:34:02.240
proportion to the distance of of the Y

1:34:00.470,1:34:05.390
you know whatever distance you you think

1:34:02.240,1:34:09.640
is appropriate this is of course a more

1:34:05.390,1:34:12.830
difficult loss function to to optimize

1:34:09.640,1:34:14.480
and out of time so I might talk about

1:34:12.830,1:34:16.640
the structure prediction issue that I

1:34:14.480,1:34:21.560
said I was going to talk about at a

1:34:16.640,1:34:24.709
later time any more question

1:34:21.560,1:34:24.709
[Music]

1:34:25.110,1:34:31.300
the contrast method you'll what though

1:34:28.780,1:34:36.660
as the cell supervised learning papers

1:34:31.300,1:34:39.490
the are you usually dump take the random

1:34:36.660,1:34:41.770
take it and and images as a negative

1:34:39.490,1:34:43.780
examples do you have any idea be used

1:34:41.770,1:34:46.330
these functions anyone's tried

1:34:43.780,1:34:49.030
experimented with ease either use what

1:34:46.330,1:34:51.280
kind of function these loss functions

1:34:49.030,1:34:54.760
that you explain to us now

1:34:51.280,1:34:56.350
so most most of them use the basically

1:34:54.760,1:34:59.290
the negative log likelihood loss here

1:34:56.350,1:35:02.560
which in this panel is called NLM mi

1:34:59.290,1:35:04.780
okay so NCE that you you heard about

1:35:02.560,1:35:06.700
from your shine that's for Jones right

1:35:04.780,1:35:08.350
there trying to make the distance

1:35:06.700,1:35:11.920
between the samples as small as possible

1:35:08.350,1:35:13.420
and then the contrastive term is you

1:35:11.920,1:35:15.310
know it's basically your log softmax of

1:35:13.420,1:35:18.610
the distances so when you compute the

1:35:15.310,1:35:20.080
log softmax you think of distance of a

1:35:18.610,1:35:21.310
distance as an energy and then you

1:35:20.080,1:35:25.810
compute the log softmax of those

1:35:21.310,1:35:35.530
energies you get this formula here in

1:35:25.810,1:35:40.150
the second lashline called mi random

1:35:35.530,1:35:43.900
what is a negative example so that we

1:35:40.150,1:35:47.710
use proximal well so basically you can't

1:35:43.900,1:35:51.520
compute this integral over all Y so or

1:35:47.710,1:35:54.580
this sum if y is discrete and so you

1:35:51.520,1:35:58.480
basically approximate the Sun by you

1:35:54.580,1:36:00.220
know a few terms that you pick randomly

1:35:58.480,1:36:02.080
right yeah let's go take a look

1:36:00.220,1:36:04.440
I mean basically if you want to do this

1:36:02.080,1:36:06.940
properly you have to pick those samples

1:36:04.440,1:36:09.940
according to the rule of Monte Carlo

1:36:06.940,1:36:11.770
sampling but it doesn't matter I mean

1:36:09.940,1:36:14.290
that's that's why I hard- mining is hard

1:36:11.770,1:36:16.360
okay that's why what makes the

1:36:14.290,1:36:18.520
difference between moco world seemed

1:36:16.360,1:36:20.590
clear etc is how you pick those negative

1:36:18.520,1:36:24.580
samples that's why I said there is no

1:36:20.590,1:36:27.400
kind of in cases where the white space

1:36:24.580,1:36:29.560
is is high dimensional is there's no you

1:36:27.400,1:36:30.280
know predefined way of taking negative

1:36:29.560,1:36:32.500
samples

1:36:30.280,1:36:37.929
essentially it's only classification

1:36:32.500,1:36:42.440
that is easy as other people

1:36:37.929,1:36:45.619
other losses yeah I mean there are a lot

1:36:42.440,1:36:48.530
of people using the square square or the

1:36:45.619,1:36:51.289
the the syringe you know with the

1:36:48.530,1:36:53.599
difference of energies so some of the

1:36:51.289,1:36:57.050
systems are used by at least at some

1:36:53.599,1:36:58.880
point the the system that deep phase

1:36:57.050,1:37:01.940
which is the the face recognition system

1:36:58.880,1:37:05.539
that used by Facebook to to tag people

1:37:01.940,1:37:08.079
it used a convolutional net trained in

1:37:05.539,1:37:10.579
supervised mode with a certain number of

1:37:08.079,1:37:12.949
categories basically images from I don't

1:37:10.579,1:37:14.809
know million people or something but

1:37:12.949,1:37:17.659
then there was a fine-tuning phase that

1:37:14.809,1:37:20.900
use metric running basically Chinese

1:37:17.659,1:37:22.579
nets where you show two photos of the

1:37:20.900,1:37:24.380
same person and you say those are the

1:37:22.579,1:37:25.639
same person and then two photos of

1:37:24.380,1:37:28.039
different people and you push them apart

1:37:25.639,1:37:30.650
and that used they tried different

1:37:28.039,1:37:32.750
objective functions but I think they

1:37:30.650,1:37:34.250
were using the square square laws at

1:37:32.750,1:37:35.929
some point maybe the squares financial

1:37:34.250,1:37:42.559
and now entirely sure what they're using

1:37:35.929,1:37:46.570
now but you know it's one of those what

1:37:42.559,1:37:49.849
topics you covered in the next lecture

1:37:46.570,1:37:52.280
okay so we're gonna have two guest

1:37:49.849,1:37:55.329
lectures so next which is Michael Lewis

1:37:52.280,1:37:58.780
Michael Lewis is a research scientist at

1:37:55.329,1:38:02.539
Facebook where research in Seattle and

1:37:58.780,1:38:04.599
he is a specialist of natural language

1:38:02.539,1:38:08.119
processing and translation so it's gonna

1:38:04.599,1:38:11.119
you know tell you all the interesting

1:38:08.119,1:38:13.940
tidbits about sequence to sequence about

1:38:11.119,1:38:19.039
transformers about NLP and about

1:38:13.940,1:38:21.289
translation okay and you know he knows a

1:38:19.039,1:38:22.880
lot you know much better the details

1:38:21.289,1:38:25.099
about this and I do so he's the right

1:38:22.880,1:38:26.900
person to talk about this we're gonna

1:38:25.099,1:38:29.210
have another guest lecture is going to

1:38:26.900,1:38:32.000
be exactly what I saw he he's you know

1:38:29.210,1:38:36.650
one of the world specialist of graph

1:38:32.000,1:38:38.239
neural nets and so this is the the whole

1:38:36.650,1:38:40.519
idea of you know how do you apply your

1:38:38.239,1:38:44.179
nets you know you can think of an image

1:38:40.519,1:38:46.550
as a function on a regular grid okay

1:38:44.179,1:38:48.380
every pixel is a location on a regular

1:38:46.550,1:38:50.460
grid you can think of an image as a

1:38:48.380,1:38:52.140
function on that grid okay

1:38:50.460,1:38:54.450
so a grid is a graph of a particular

1:38:52.140,1:38:57.510
type and the image is just a function on

1:38:54.450,1:39:01.200
the graph you can think of I don't know

1:38:57.510,1:39:04.560
a video as you know a regular 3d grid

1:39:01.200,1:39:06.540
where you have space and time and you

1:39:04.560,1:39:10.110
know most natural signals you can think

1:39:06.540,1:39:13.110
of them as functions on kind of regular

1:39:10.110,1:39:15.570
graphs okay what about the case where

1:39:13.110,1:39:18.330
the function you're interested in is not

1:39:15.570,1:39:20.280
on a is not on the euclidean graph if

1:39:18.330,1:39:23.910
you want so let's imagine for example

1:39:20.280,1:39:26.550
you take a photo with a panoramic camera

1:39:23.910,1:39:29.090
okay 360 camera right so it's a camera

1:39:26.550,1:39:32.010
that basically takes a spherical image

1:39:29.090,1:39:34.320
okay so now your your pixels live on the

1:39:32.010,1:39:39.390
sphere how do you go to the convolution

1:39:34.320,1:39:41.130
on us here okay so you want to run your

1:39:39.390,1:39:43.380
commercial net on this image that now

1:39:41.130,1:39:47.250
lives on the sphere

1:39:43.380,1:39:50.640
you can't use the standard ways of

1:39:47.250,1:39:52.410
completing correlations so you have to

1:39:50.640,1:39:54.710
figure out how to compute coalition's on

1:39:52.410,1:39:56.730
the sphere right so that's an example

1:39:54.710,1:40:01.020
now here's something a little more

1:39:56.730,1:40:04.800
complicated imagine now that you have a

1:40:01.020,1:40:06.480
3d scanner and you're capturing I don't

1:40:04.800,1:40:11.040
know a dancer you know someone kind of

1:40:06.480,1:40:12.570
in front of a 3d scanner and that person

1:40:11.040,1:40:13.469
has a particular pose they'd say like

1:40:12.570,1:40:18.530
this okay

1:40:13.469,1:40:20.910
and and then you take another 3d picture

1:40:18.530,1:40:24.060
3d data from another person and that

1:40:20.910,1:40:26.120
other person is you know in another post

1:40:24.060,1:40:30.030
that person has a different body shape

1:40:26.120,1:40:32.820
she's in a different body pose and now

1:40:30.030,1:40:34.230
what you want is you want to be able to

1:40:32.820,1:40:36.780
map one on to the other you want to be

1:40:34.230,1:40:38.520
able to say like you know what is the

1:40:36.780,1:40:40.500
hand in the first person what is where

1:40:38.520,1:40:43.170
is the hand in the second person so what

1:40:40.500,1:40:46.080
you have to do now is basically have a

1:40:43.170,1:40:48.660
neural net that takes into account a 3d

1:40:46.080,1:40:53.700
mesh that represents the geometry of a

1:40:48.660,1:40:55.830
hand and trainee to tell you it's a hand

1:40:53.700,1:40:57.300
so that when you apply it to the hand

1:40:55.830,1:40:59.010
itself it's a hand when you apply to the

1:40:57.300,1:41:01.290
other parts of the body and tells you

1:40:59.010,1:41:03.480
it's something else but the data you

1:41:01.290,1:41:05.160
have is not an image it's a 3d mesh

1:41:03.480,1:41:07.050
okay the match may have different

1:41:05.160,1:41:09.420
resolutions the triangles make here

1:41:07.050,1:41:11.310
occur at different places so how you

1:41:09.420,1:41:14.030
define your coalitions on the domain

1:41:11.310,1:41:17.100
like this that is independent of the

1:41:14.030,1:41:19.410
resolution of the mesh and only kind of

1:41:17.100,1:41:21.330
depends on the shape so that you can

1:41:19.410,1:41:24.890
classify your hand regardless of the

1:41:21.330,1:41:27.480
orientation the size the conformation

1:41:24.890,1:41:28.739
and the body shape of the person you

1:41:27.480,1:41:32.930
know things like that right

1:41:28.739,1:41:37.500
so here's another example that's perhaps

1:41:32.930,1:41:39.900
more interesting you you want to do you

1:41:37.500,1:41:43.170
want to train something like like a

1:41:39.900,1:41:46.940
Siamese net but you want to train this

1:41:43.170,1:41:49.080
aim is not to tell you whether when

1:41:46.940,1:41:51.239
molecule is going to stick to another

1:41:49.080,1:41:55.080
molecule right so you give two molecules

1:41:51.239,1:41:57.590
to your neural net and your neural net

1:41:55.080,1:42:01.890
produces two vectors if those two

1:41:57.590,1:42:05.270
molecules stick together it gives you

1:42:01.890,1:42:07.440
two vectors whose distance is small okay

1:42:05.270,1:42:09.060
and if they don't stick together then

1:42:07.440,1:42:10.380
the distance is large okay so you can

1:42:09.060,1:42:12.300
think of the distance as kind of the

1:42:10.380,1:42:16.230
negative free energy of the binding the

1:42:12.300,1:42:21.540
binding energy of the two you know the

1:42:16.230,1:42:23.550
two the two molecules right or the you

1:42:21.540,1:42:30.720
know the the free energy you know minus

1:42:23.550,1:42:32.370
a constant each one so so you would

1:42:30.720,1:42:34.220
train this as a sign is net but then the

1:42:32.370,1:42:37.110
problem is how you represent a molecule

1:42:34.220,1:42:38.250
to a network knowing that it's the same

1:42:37.110,1:42:39.750
network you're going to apply to this

1:42:38.250,1:42:41.310
molecule on that molecule and the two

1:42:39.750,1:42:43.020
molecules don't have the same shape they

1:42:41.310,1:42:43.980
don't have the same length

1:42:43.020,1:42:45.570
they don't have the same number of atoms

1:42:43.980,1:42:48.270
they don't have the same like how do you

1:42:45.570,1:42:50.430
represent a molecule the best way to

1:42:48.270,1:42:51.960
represent a molecule is as a graph it's

1:42:50.430,1:42:54.660
basically a graph whose structure

1:42:51.960,1:42:59.100
changes with the molecule and this graph

1:42:54.660,1:43:01.980
is annotated by the identity of the

1:42:59.100,1:43:04.170
atoms at each site maybe by the location

1:43:01.980,1:43:07.080
in 3d space for their relative location

1:43:04.170,1:43:11.010
maybe by the angle of the bounds between

1:43:07.080,1:43:12.480
two successive atoms or the binding

1:43:11.010,1:43:15.900
energy of that particular bond or things

1:43:12.480,1:43:17.300
like this so so the best way to

1:43:15.900,1:43:19.760
represent a molecule is by

1:43:17.300,1:43:22.310
by representing as a graph basically and

1:43:19.760,1:43:24.610
there's a here's another example perhaps

1:43:22.310,1:43:31.040
more relevant to something like Facebook

1:43:24.610,1:43:33.770
let's say let's say I want to kind of

1:43:31.040,1:43:38.540
infer or let's say Amazon or something I

1:43:33.770,1:43:40.220
want to infer what type of let's let's

1:43:38.540,1:43:41.990
say an Amazon right and I have a

1:43:40.220,1:43:44.240
customer and that customer has bought a

1:43:41.990,1:43:46.400
whole bunch of different things and that

1:43:44.240,1:43:48.050
customer has commented a whole bunch of

1:43:46.400,1:43:49.700
different things I could think of kind

1:43:48.050,1:43:53.720
of encoding this as a vector but it

1:43:49.700,1:43:56.120
would be a vector of of variable size

1:43:53.720,1:43:57.770
because you know people buy different

1:43:56.120,1:43:59.360
numbers of things and stuff like that so

1:43:57.770,1:44:01.370
I would need to so find a way to

1:43:59.360,1:44:02.840
aggregate that data so that everybody

1:44:01.370,1:44:05.140
can be represented by the same fixed

1:44:02.840,1:44:08.060
size vector but what if instead I

1:44:05.140,1:44:09.440
represent the the person and although

1:44:08.060,1:44:12.800
all the things that that person has

1:44:09.440,1:44:15.410
bought and all the you know reviews that

1:44:12.800,1:44:18.080
person etc as a graph essentially and

1:44:15.410,1:44:21.050
then I represent what I feed to the

1:44:18.080,1:44:24.080
neural net is the graph with values on

1:44:21.050,1:44:25.550
the nodes and perhaps the arcs if I have

1:44:24.080,1:44:26.930
a way of representing a graph so that

1:44:25.550,1:44:28.610
you can connect a neuron that

1:44:26.930,1:44:30.790
independently of the shape of the graph

1:44:28.610,1:44:33.290
then I can do this kind of application

1:44:30.790,1:44:35.570
and so this is what graph neural nets

1:44:33.290,1:44:37.430
are about it's a very very hot topic at

1:44:35.570,1:44:39.100
the moment it's extremely promising for

1:44:37.430,1:44:41.750
a lot of applications particularly in

1:44:39.100,1:44:44.690
biomedicine you know in chemistry in

1:44:41.750,1:44:47.650
material science but also in social

1:44:44.690,1:44:49.940
science for social network analysis and

1:44:47.650,1:44:52.100
you know all kinds of all kinds of

1:44:49.940,1:44:54.710
applications computer graphics you know

1:44:52.100,1:44:56.660
okay good stuff so it's it's really cool

1:44:54.710,1:44:58.760
exactly it's really one of the one of

1:44:56.660,1:45:00.260
the experts on this topic so I'm really

1:44:58.760,1:45:02.360
happy that he accepted to give us a talk

1:45:00.260,1:45:04.460
it's not gonna be easy for them for him

1:45:02.360,1:45:06.680
because he was in Singapore so he's

1:45:04.460,1:45:08.120
gonna be fine the morning for him that's

1:45:06.680,1:45:13.040
right well I'm giving a lecture in a

1:45:08.120,1:45:15.290
couple days in in Hong Kong so I think

1:45:13.040,1:45:17.780
so actually he's from Nanyang

1:45:15.290,1:45:21.020
Technological illogical university and

1:45:17.780,1:45:27.620
university NTU right until you into you

1:45:21.020,1:45:28.280
yeah yeah correct alright so that was it

1:45:27.620,1:45:31.010
sorry

1:45:28.280,1:45:33.440
there was one more questions yeah

1:45:31.010,1:45:35.300
was really interesting professor I had

1:45:33.440,1:45:38.180
one no question I was reading this term

1:45:35.300,1:45:39.770
called normalizing flows yeah and I

1:45:38.180,1:45:42.620
don't understand what they are could you

1:45:39.770,1:45:44.800
just give some intuition into why why

1:45:42.620,1:45:49.250
people are excited about it

1:45:44.800,1:45:50.690
right so nominally flows so it's not a

1:45:49.250,1:45:51.910
key to have a lot of experience with but

1:45:50.690,1:45:56.210
you know I've read the papers

1:45:51.910,1:46:02.420
it's so was proposed by the nino Rezende

1:45:56.210,1:46:04.850
and check here at defined a while ago

1:46:02.420,1:46:08.230
and a while back maybe five years ago or

1:46:04.850,1:46:10.340
so and it's a sort of a density

1:46:08.230,1:46:13.190
estimation method so it's a little bit

1:46:10.340,1:46:18.290
like like Gans it has a bit of the same

1:46:13.190,1:46:20.120
spirit as Gans the and it get it gets

1:46:18.290,1:46:22.190
inspiration from ICA independent

1:46:20.120,1:46:24.080
component analysis although it's not

1:46:22.190,1:46:25.430
kind of explicit in the original paper

1:46:24.080,1:46:29.720
but here's the basic idea

1:46:25.430,1:46:31.400
this guy is you want to train a neuron

1:46:29.720,1:46:33.290
that to transform a known distribution

1:46:31.400,1:46:34.700
from which you can sample into a

1:46:33.290,1:46:37.550
distribution that happens to be the

1:46:34.700,1:46:39.830
distribution of your data okay so let's

1:46:37.550,1:46:41.870
imagine that you have a latent variable

1:46:39.830,1:46:44.630
z that you sample from a Gaussian

1:46:41.870,1:46:47.240
distribution and you run it through a

1:46:44.630,1:46:49.460
function or uniform distribution over a

1:46:47.240,1:46:51.680
domain okay you ready to a function

1:46:49.460,1:46:53.060
implemented by an overnight and you want

1:46:51.680,1:46:54.680
to train this new on that so that the

1:46:53.060,1:46:56.390
distribution you get at the output is

1:46:54.680,1:46:59.720
the one you want that corresponds to

1:46:56.390,1:47:10.180
your data okay

1:46:59.720,1:47:16.910
and so the let me give you a very simple

1:47:10.180,1:47:19.190
example so let's say let's say I have a

1:47:16.910,1:47:23.210
variable Z and I've observed variable Y

1:47:19.190,1:47:26.450
and I sample my variable Z with the

1:47:23.210,1:47:28.000
uniform distribution okay between say

1:47:26.450,1:47:29.949
zero and one

1:47:28.000,1:47:34.540
okay

1:47:29.949,1:47:38.860
and what I want on the output is I don't

1:47:34.540,1:47:40.120
know say Gaussian it's kind of stupid to

1:47:38.860,1:47:42.070
want to get in but it's say I want to

1:47:40.120,1:47:45.160
guess because I could sample from a

1:47:42.070,1:47:49.000
Gaussian easily so what I need to do is

1:47:45.160,1:47:50.590
kind of transform this uniform

1:47:49.000,1:47:52.120
distribution into a Gaussian by a

1:47:50.590,1:48:01.140
mapping and the mapping is going to be a

1:47:52.120,1:48:08.860
function like it's gonna be if function

1:48:01.140,1:48:15.910
okay zero is here kind of like this if

1:48:08.860,1:48:18.160
you want okay and and this is the

1:48:15.910,1:48:23.710
inverse of the integral of the Gaussian

1:48:18.160,1:48:25.600
distribution okay so if I take the

1:48:23.710,1:48:32.739
derivative of this function okay so I

1:48:25.600,1:48:37.860
don't it let me kind of draw this it's a

1:48:32.739,1:48:40.179
little difficult to see but if I map

1:48:37.860,1:48:43.000
okay the derivative of this function

1:48:40.179,1:48:45.580
here will indicate how much I stretch a

1:48:43.000,1:48:48.550
little piece here into a piece here

1:48:45.580,1:48:50.350
right so the larger the derivative the

1:48:48.550,1:48:53.860
more I stretch right if the slope here

1:48:50.350,1:48:55.090
is 1 then this piece of the distribution

1:48:53.860,1:49:00.610
here is not going to stretch it's going

1:48:55.090,1:49:02.739
to be kind of passed unchanged okay and

1:49:00.610,1:49:05.140
the larger the district the the slope

1:49:02.739,1:49:06.820
the more I stretch the distribution I

1:49:05.140,1:49:08.650
stretch a little piece here and

1:49:06.820,1:49:10.590
therefore I kind of distribute all the

1:49:08.650,1:49:13.620
samples that fall into this little

1:49:10.590,1:49:19.570
location here are stretched time over a

1:49:13.620,1:49:21.699
large region right and so what I need to

1:49:19.570,1:49:24.250
do is design this function in such a way

1:49:21.699,1:49:26.380
that it stretches my input distribution

1:49:24.250,1:49:29.020
so that that distribution get

1:49:26.380,1:49:33.100
transformed into the output distribution

1:49:29.020,1:49:34.870
I want alright so there is a formula

1:49:33.100,1:49:36.160
that says so in multi dimension it's a

1:49:34.870,1:49:38.380
little more complicated than this but it

1:49:36.160,1:49:42.160
says that the the distribution you're

1:49:38.380,1:49:42.999
going to get on Y is going to be equal

1:49:42.160,1:49:45.599
to the

1:49:42.999,1:49:48.210
distribution that you started with in Z

1:49:45.599,1:49:52.690
multiplied by the inverse of the

1:49:48.210,1:50:06.210
determinant of the Jacobian of this F

1:49:52.690,1:50:09.550
function so this is f minus 1 so it's

1:50:06.210,1:50:15.150
actually the original formula is this

1:50:09.550,1:50:17.530
one but those two things are equal okay

1:50:15.150,1:50:18.880
so if you take so this is for a

1:50:17.530,1:50:23.769
multi-dimensional vector function right

1:50:18.880,1:50:28.090
so it has a Jacobian to map Z 2 y and so

1:50:23.769,1:50:33.610
if you take the determinant of the

1:50:28.090,1:50:36.489
inverse Jacobian of that function which

1:50:33.610,1:50:40.780
is a scalar value indicates by how much

1:50:36.489,1:50:45.639
the distribution gets stretched or

1:50:40.780,1:50:47.289
compressed in that case at Q so in that

1:50:45.639,1:50:48.940
case here is the it's the compression

1:50:47.289,1:50:53.229
ratio it's the inverse of the derivative

1:50:48.940,1:50:57.579
so it's the compression right and so the

1:50:53.229,1:51:00.820
more you compress here the model the

1:50:57.579,1:51:03.309
probability will be high more P of Y

1:51:00.820,1:51:07.719
will be large the density P of Y for

1:51:03.309,1:51:17.789
this Y will be large for a given Q so

1:51:07.719,1:51:24.840
this is for y equals f of Z okay

1:51:17.789,1:51:28.409
so the big question of homology flow

1:51:24.840,1:51:30.840
methods is its how you how you do this

1:51:28.409,1:51:32.880
right given a number of samples of P of

1:51:30.840,1:51:36.659
Y and given that you sample your

1:51:32.880,1:51:39.570
distribution Q when you have your

1:51:36.659,1:51:40.800
distribution Q you sample from it you

1:51:39.570,1:51:44.909
know how do you kind of minimize an

1:51:40.800,1:51:47.250
objective function that knowing that

1:51:44.909,1:51:49.500
that you know the P you get that the

1:51:47.250,1:51:51.650
output is equal to the Q you put at the

1:51:49.500,1:51:54.059
input multiplied by this inverse

1:51:51.650,1:51:55.829
determinant of the Jacobian of the F

1:51:54.059,1:51:57.119
function what you have to find is the F

1:51:55.829,1:51:59.670
function so basically have to

1:51:57.119,1:52:01.289
differentiate so basically compute a

1:51:59.670,1:52:02.940
distance between those you know

1:52:01.289,1:52:05.670
divergence KL divergence for example

1:52:02.940,1:52:09.420
between py and the thing on the right

1:52:05.670,1:52:10.500
side of the of the equal sign and you

1:52:09.420,1:52:12.900
have to differentiate this with respect

1:52:10.500,1:52:15.599
to the parameters of F so you have to

1:52:12.900,1:52:19.199
basically propagate through the inverse

1:52:15.599,1:52:20.119
gradient of the Jacobian of F right it's

1:52:19.199,1:52:23.309
not easy

1:52:20.119,1:52:26.719
very often what people do is that they

1:52:23.309,1:52:30.329
write f as a succession a very simple F

1:52:26.719,1:52:35.340
that only modify the the distribution

1:52:30.329,1:52:38.250
just a little bit so f very often is you

1:52:35.340,1:52:40.409
know something like the identity plus

1:52:38.250,1:52:43.800
some deviation the bit like ResNet if

1:52:40.409,1:52:45.900
you want and then you stack lots and

1:52:43.800,1:52:50.280
lots of layers of that and the problem

1:52:45.900,1:52:52.070
becomes simpler because when the when

1:52:50.280,1:52:55.199
those those functions do a little bit of

1:52:52.070,1:52:58.320
modification then the a lot of those

1:52:55.199,1:53:00.539
kind of issues can it becomes become

1:52:58.320,1:53:03.090
simple the the determinant here kind of

1:53:00.539,1:53:06.150
simplifies okay that's a very sort of AD

1:53:03.090,1:53:13.559
try K level description of normalizing

1:53:06.150,1:53:16.070
flow there is yeah interesting papers

1:53:13.559,1:53:19.130
about this in recent recent years on

1:53:16.070,1:53:21.269
even recent months on so using this for

1:53:19.130,1:53:23.130
like particle physics and stuff like

1:53:21.269,1:53:28.639
that Cramer at NYU is actually a kind of

1:53:23.130,1:53:32.090
a specialist of that thank you so much

1:53:28.639,1:53:35.330
right any other question

1:53:32.090,1:53:36.080
okay that was it great thank you very

1:53:35.330,1:53:39.290
much everyone

1:53:36.080,1:53:39.610
yeah see you tomorrow guys all right

1:53:39.290,1:53:42.790
bye-bye

1:53:39.610,1:53:42.790
take care


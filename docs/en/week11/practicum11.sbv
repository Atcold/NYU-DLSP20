0:00:00.319,0:00:07.080
okay so let's get started with today lesson and let's see what Yann likes to

0:00:07.080,0:00:13.349
do research on alright so today we're going to be talking about model

0:00:13.349,0:00:18.180
predictive policy learning with uncertainty regularization for driving

0:00:18.180,0:00:24.420
in dense traffic what a mouthful the nice part is that in roughly 50 minutes

0:00:24.420,0:00:28.289
you're going to be able to understand every word in this title and you

0:00:28.289,0:00:33.149
actually should be able to even be ready to implement this because we basically

0:00:33.149,0:00:37.620
know all the basic components that we have cover so far and so this is just

0:00:37.620,0:00:42.809
you know put together something but how perhaps not in a trivial way but but I

0:00:42.809,0:00:48.539
don't think it's too crazy so this is work done by my friend and colleague me

0:00:48.539,0:00:57.379
Kyle enough myself and then Yong here at grant a few years back I think it was

0:00:57.379,0:01:05.059
2019 so last year I think oh maybe the year before I don't know all right

0:01:10.140,0:01:15.330
so let's see how you can learn how to drive the model free reinforcement

0:01:15.330,0:01:21.960
learning way right so pay attention to the car here in in the back this display

0:01:21.960,0:01:26.490
guy so let's say I'd like to train this this guy with model free reinforcement

0:01:26.490,0:01:32.540
learning so how would you go about that so you would have to try

0:01:32.840,0:01:41.359
um to do things that are maybe not good and then if you hear uh shouldn't do

0:01:41.359,0:01:47.359
those things right because it's not good and so you have to die a few times right

0:01:47.359,0:01:52.340
before actually learning not to die but that's arguably not the way you learn

0:01:52.340,0:01:58.009
how to drive right especially if you're driving your your parents car and you

0:01:58.009,0:02:02.060
don't really want to crash your parents car before learning how to not crash

0:02:02.060,0:02:08.630
your parents car right so let's figure out a more principled way to learn how

0:02:08.630,0:02:12.739
to drive a car right so I would argue here and this is just you know my

0:02:12.739,0:02:17.150
intuition is that if you if you have a car now you're driving a hundred

0:02:17.150,0:02:24.290
kilometers per hour which is like thirty meters per second if you look 30 meters

0:02:24.290,0:02:33.769
in front of you that will mean that you look one second in the future right and

0:02:33.769,0:02:38.389
therefore you can see that you know the road center turns slightly to the left

0:02:38.389,0:02:42.799
in one second in the future therefore I would like to change the steering wheel

0:02:42.799,0:02:47.540
right now such that in one second I will be you know following the ended

0:02:47.540,0:02:52.849
trajectory and where the street takes me to okay so here I'm just trying to you

0:02:52.849,0:02:57.799
know push forward the idea that we need to look in the future in order to be

0:02:57.799,0:03:05.780
able to make you know some sort of nice like you know plan of action right so

0:03:05.780,0:03:11.540
you'd like to figure out that if something is not good you may not want

0:03:11.540,0:03:21.739
to do it so it's that you don't get into trouble so but hmm was the main problem

0:03:21.739,0:03:29.720
here others other people are problem right so here you have that you know the

0:03:29.720,0:03:36.169
eCos around you are quite not deterministic therefore it might be

0:03:36.169,0:03:41.620
slightly hard to take an account every possible thing that might happen right

0:03:41.620,0:03:48.040
so let me give you like an introduction here or a small recap of the what are

0:03:48.040,0:03:53.050
the main components in this system here so we have a agent here represented by

0:03:53.050,0:04:01.030
this pink brain which gets as input a state sₜ  and produces in action is an

0:04:01.030,0:04:06.190
output which is aₜ which is for example my steering control and my acceleration

0:04:06.190,0:04:13.810
or braking signal moreover I observe a cost which is you know a consequence of

0:04:13.810,0:04:19.780
taking a specific action a given that I find myself in state sₜ  ok so if I

0:04:19.780,0:04:25.000
should be okay let me see there are chat messages here let me see what's going on

0:04:25.000,0:04:30.720
okay people are laughing cool alright on the other side you had the real world

0:04:30.720,0:04:36.789
the real world given a internal state you get a new action and then you

0:04:36.789,0:04:40.720
produce the new state ok and also you produce what is the result this

0:04:40.720,0:04:45.430
consequence city to provide to your agent and so this is how you know how

0:04:45.430,0:04:49.870
you can have like a network interacting with a real world you take actions given

0:04:49.870,0:04:54.160
a specific state and the word give you gives you the next state and the next

0:04:54.160,0:05:00.570
consequence so this is model free because you interact with the real world

0:05:00.570,0:05:06.729
but then the nice part is that you can do interaction with the model of the

0:05:06.729,0:05:11.289
world okay so under on the left hand side instead of trying things in the

0:05:11.289,0:05:15.490
real world and you know try to cook and you burn something and then that's not

0:05:15.490,0:05:21.400
good you burnt also your hand burn my arm nasty making cookies maybe you would

0:05:21.400,0:05:27.789
like to try in your mind first how can I make cookies without getting burnt maybe

0:05:27.789,0:05:32.410
don't touch the oven with your hands right that would be very

0:05:32.410,0:05:40.960
smart option right all right so how how can we get how can we think right how

0:05:40.960,0:05:47.110
can we ponder how can we make this kind of interaction between my actions and

0:05:47.110,0:05:52.330
the expected consequences you know of taking a specific action with actually

0:05:52.330,0:05:57.400
without actually taking the action right so how can I avoid to burn myself again

0:05:57.400,0:06:02.320
tonight while baking without actually getting burned right so you want to

0:06:02.320,0:06:08.170
think ahead don't touch what's hot hmm okay all

0:06:08.170,0:06:12.700
right so how do we train this word model the same way we trained it last week

0:06:12.700,0:06:18.760
right so we start with initial state we provide some you know action which was

0:06:18.760,0:06:22.990
random last week and this week instead is not for example the action here might

0:06:22.990,0:06:29.140
be the action taken by some expert that we observed like mom cooking for you of

0:06:29.140,0:06:34.120
dead don't know and you can see what is the the current state and then you can

0:06:34.120,0:06:39.970
have the next state dinner is ready mmm I'm gonna gray I'm actually hungry

0:06:39.970,0:06:45.670
p.m. all right more or you also had the consequence cₜ they're going to be you

0:06:45.670,0:06:51.310
know your mouth is watering anyhow you're gonna be providing now like a

0:06:51.310,0:06:55.330
distance between this state that are observed in the real world in the state

0:06:55.330,0:07:00.220
that are provided to you by the model of the world and then you have an MSc loss

0:07:00.220,0:07:04.060
right so that's just regression so you try to regress what is the next state

0:07:04.060,0:07:09.190
given that you start from the initial same state and you provide like specific

0:07:09.190,0:07:15.040
action okay so far this is something we already seen last week just give you an

0:07:15.040,0:07:21.790
overview again right all right keep coin if you don't complain so let's figure

0:07:21.790,0:07:28.270
out here what we can do for example in my case in this case I don't have my

0:07:28.270,0:07:34.390
model out putting a cost or I don't have the word out put in a cost and more

0:07:34.390,0:07:42.820
specifically I have that in my case I will have my brain here my agent that

0:07:42.820,0:07:49.690
again takes a state and provides an action which is feeding this model and

0:07:49.690,0:07:55.360
then I have my cost is going to be a differentiable function of the state

0:07:55.360,0:08:00.760
okay which is exactly what we had seen last week right with the final you know

0:08:00.760,0:08:04.720
destination to be you know the specific thing that you gramm run back

0:08:04.720,0:08:08.110
propagation through time and back propagation through time would be

0:08:08.110,0:08:12.760
unrolling this loop right so you go forward you go like that and then you do

0:08:12.760,0:08:20.560
a back problem okay all right cool so let me introduce to you right now the

0:08:20.560,0:08:26.500
data set right so far littles you know just like setting up the problem so here

0:08:26.500,0:08:31.720
it's my actual real case scenario I have six cameras seven cameras mounted on it

0:08:31.720,0:08:39.940
on the top of a 30-story building facing the i-80 interstate segment on the of

0:08:39.940,0:08:44.980
the highway array and so here I have these cameras recording these cars first

0:08:44.980,0:08:48.340
part is going to be fixing the perspective right such that I can have

0:08:48.340,0:08:53.500
like a hovering view where I cat op down view moreover here we extract some

0:08:53.500,0:08:57.940
bounding boxes for each baked-on right so there is detection there is

0:08:57.940,0:09:01.480
regression of the you know well detection and figuring out the size of

0:09:01.480,0:09:04.960
this bounding boxes and then there is tracking because these bounding boxes

0:09:04.960,0:09:10.210
are following my cars and so you can see here the red track on the left hand side

0:09:10.210,0:09:17.530
or you can see like the the red pickup car a pickup truck that is you know from

0:09:17.530,0:09:23.290
both views or from the top camera down to one single view here I can input

0:09:23.290,0:09:31.140
these cars into my you know Python program item game emulator game engine

0:09:31.140,0:09:36.340
and I can draw this representation in this case you can still see the red

0:09:36.340,0:09:41.980
tractor the pickup pickup pickup truck and then on the bus on the right hand

0:09:41.980,0:09:48.460
side so here for every each and every vehicle the blue the cyan one I have two

0:09:48.460,0:09:53.320
vectors the vector Pₜ  which is representing the position of the vehicle

0:09:53.320,0:09:56.350
the we are part vₜ  which is the velocity

0:09:56.350,0:10:02.589
which is you know vector representing the vₓ and vy component and then

0:10:02.589,0:10:08.470
moreover I given that I know what is the kinematics of driving a car or a bicycle

0:10:08.470,0:10:14.170
I can invert the kinematics of these cars that have been driven by you know

0:10:14.170,0:10:20.769
experts and I can figure out what the actions are that you know what the

0:10:20.769,0:10:26.800
actions of the driver are in the sense that if the car moves with a rectilinear

0:10:26.800,0:10:32.950
uniform motion you have no action right so if you don't apply any acceleration

0:10:32.950,0:10:39.010
which is longitudinal or transverse you have you know the car keeps going so if

0:10:39.010,0:10:43.290
you have a rectilinear uniform motion there is no action every time you try to

0:10:43.290,0:10:48.459
diverge from this you know motion for example to accelerate you break or you

0:10:48.459,0:10:54.870
steer then you have some basically you know action involved right you can

0:10:54.870,0:11:02.079
invert the kinematic model in order to figure out what the action is cool so

0:11:02.079,0:11:06.089
here again in this representation which is the thing that I call machine

0:11:06.089,0:11:11.620
representation I have again the same vehicle there the other pickup car and

0:11:11.620,0:11:17.920
one on the right hand side so in this case each vehicle here has a bounding

0:11:17.920,0:11:24.220
box which is representing my viewable area so each car can only view that kind

0:11:24.220,0:11:30.190
of box around itself and so for example here I can extract the first box where

0:11:30.190,0:11:35.470
April replace myself in the centre and I movie I move myself to the blue channel

0:11:35.470,0:11:42.220
again such that i'ma I'm not like I make myself different from the others and the

0:11:42.220,0:11:45.430
red lane the red channel represents the lane and the green channel represents

0:11:45.430,0:11:51.579
the others wake on other vehicles you have the other guy here this one here

0:11:51.579,0:11:59.649
that one and the last one okay and here you can see again the pickup pickup

0:11:59.649,0:12:04.899
truck there the red one and then here you have a piece of the bus so these are

0:12:04.899,0:12:08.660
my images I T or serve Asians right these are

0:12:08.660,0:12:13.880
representing two things the state of the lanes of the street basically and the

0:12:13.880,0:12:16.850
second part is going to be they represent the traffic situation

0:12:16.850,0:12:24.020
surrounding me so overall the set of Pₜ the position

0:12:24.020,0:12:30.830
within the velocity and I team this observation represent my state sₜ so as

0:12:30.830,0:12:37.310
T represents the current state at a specific time T right for my given

0:12:37.310,0:12:44.690
vehicle so far any question how is it clear I mean we already seen basically

0:12:44.690,0:12:48.860
everything so far last week and I just gave you like an overview about the

0:12:48.860,0:12:53.750
specific data sets so there are no new concept so far so it should be clear

0:12:53.750,0:12:59.570
right yeah no okay you're very quiet today

0:12:59.570,0:13:07.490
maybe it's my audio on okay you're not texting so I expect you to be okay yes

0:13:07.490,0:13:13.130
it's clear okay thank you all right cool so the cost right so we define before we

0:13:13.130,0:13:18.800
said before that my cost is going to be a function of my state so let's see how

0:13:18.800,0:13:23.120
I compute this cost there are two different costs there is a lane cost

0:13:23.120,0:13:30.110
that is basically telling me whether I am on the lane like within the lane on

0:13:30.110,0:13:36.230
like inside the lane or I am going off lane off road and the other one is going

0:13:36.230,0:13:41.480
to be a cost that is gonna be telling me how close I am to other vehicles so the

0:13:41.480,0:13:46.880
first one it looks like this on my y-axis so the x-axis the direction of

0:13:46.880,0:13:52.760
motion y-axis is gonna be the one that is you know 90 degrees to the left in

0:13:52.760,0:13:57.680
their form you can think about having a potential it is like a house on top of

0:13:57.680,0:14:03.500
you if you overlay this with the red channel there is some intersection on

0:14:03.500,0:14:08.570
the left hand side this intersection the height of that intersection will go to

0:14:08.570,0:14:13.520
zero if you are exactly in the center of the two lanes if you are shifted the

0:14:13.520,0:14:18.050
words one side you're gonna get you know some nonzero intersection and if you're

0:14:18.050,0:14:21.040
exactly on top of the lane you get is actually the

0:14:21.040,0:14:26.980
on the on the on the top of this triangle right on the other side you

0:14:26.980,0:14:30.699
have the proximity cost so I have exactly the same but for the other

0:14:30.699,0:14:35.880
vehicles night so in this case I have one longitudinal sorry transverse

0:14:35.880,0:14:43.509
potential I have one longitudinal potential which is changing the length

0:14:43.509,0:14:48.790
with the speed so the faster I go and the more I would like to look ahead and

0:14:48.790,0:14:54.550
behind in this case in in the slower I go I can you know I don't really care so

0:14:54.550,0:14:58.720
much about things that too far I just look close to myself so we could plug

0:14:58.720,0:15:03.250
these two things in my environment you can see now that there is an

0:15:03.250,0:15:08.860
intersection for example here that is pretty high for the purple because we

0:15:08.860,0:15:13.930
are exactly in front of in front of us but then the orange is quite low because

0:15:13.930,0:15:19.000
it's you know further away in the front so you can simply do the multiplication

0:15:19.000,0:15:26.740
of the two you can get what is my current proximity cost okay so how does

0:15:26.740,0:15:32.980
this look right now I can show you for example for a situation where we go at

0:15:32.980,0:15:36.610
20 kilometers per hour you have all these vehicles are very close to each

0:15:36.610,0:15:40.209
other and then if you go at 50 kilometers per hour you know on average

0:15:40.209,0:15:44.980
every one is a bit further away so if you multiply that potential that is in

0:15:44.980,0:15:49.870
the y and the other one that was in the x you get something that looks like this

0:15:49.870,0:15:55.060
okay and in the case that we go at a higher speed you cannot get something

0:15:55.060,0:15:58.839
like this because I gain the extension in the X direction depends on the speed

0:15:58.839,0:16:05.230
and you can tell like that is the right one is further far reaching front and

0:16:05.230,0:16:13.480
back cool so how do we get this final cost well my final cost right now at

0:16:13.480,0:16:19.060
least as we what we establish this paper we just multiplied this potential mask

0:16:19.060,0:16:24.130
with the green channel and then we pick the max basically we figure out which

0:16:24.130,0:16:29.889
one is the closest car or close the same the the part closest to us belongs to

0:16:29.889,0:16:34.960
some other object and so if you multiply element wise multiply

0:16:34.960,0:16:39.880
these two guys and then you take the max you get a number and the cool part is

0:16:39.880,0:16:45.520
that differentiable right so now you can run gradients through on the network

0:16:45.520,0:16:51.400
such that you know you can compute some actions such that that value overall is

0:16:51.400,0:16:59.020
going to be reduced and it goes to zero such that you avoid collisions all right

0:16:59.020,0:17:04.270
so let me give you now the outline of the talk of the lesson for today so

0:17:04.270,0:17:09.520
first we said we had to learn how to mimic the word rain so that was pretty

0:17:09.520,0:17:15.040
abstract so far now we can start getting concrete you know tools and information

0:17:15.040,0:17:20.500
so first we want to learn and mimic the real world second part we'd like to use

0:17:20.500,0:17:27.220
the you know the learned model of the environment in order to train this agent

0:17:27.220,0:17:34.030
by you know making thinking how to drive so first you learn how other vehicles

0:17:34.030,0:17:38.710
behave in the real world second part given that you have an understanding of

0:17:38.710,0:17:43.540
how other people interact you try to think what would happen if I you know

0:17:43.540,0:17:49.810
perform such an such action in this in this condition finally we can figure out

0:17:49.810,0:17:55.690
what is a nice way to evaluate a possible way to evaluate this policy

0:17:55.690,0:18:00.970
back in the real world right so once you've thought how to drive let's figure

0:18:00.970,0:18:07.690
out whether you can drive cool so let's get started with the first part the

0:18:07.690,0:18:14.680
world model predicting what's next given history and action so this is gonna be

0:18:14.680,0:18:18.760
something that you already had seen a few lessons back but let me give you

0:18:18.760,0:18:26.310
again like a full picture here so we have a world model which is fed with my

0:18:26.310,0:18:32.020
s1 to T which is a sequence of States and each state represent is represented

0:18:32.020,0:18:37.780
by a position vector P T a velocity vector in V T and these context images I

0:18:37.780,0:18:42.850
T these observations so this is a set of things of course as you can tell you

0:18:42.850,0:18:47.860
will have different you know in different input branches in your net

0:18:47.860,0:18:50.710
right because there are different kind of data one is

0:18:50.710,0:18:53.559
you know four-dimensional vector the other is an image you'd like to use a

0:18:53.559,0:19:00.820
convolutional net moreover this word modern gets an action and the word model

0:19:00.820,0:19:06.220
will produce a prediction for the next action on the other side you had the

0:19:06.220,0:19:10.780
real word which is telling you well these happen instead okay so that's a

0:19:10.780,0:19:15.970
target okay so how do we train this stuff as we said it's just a regression

0:19:15.970,0:19:22.840
problem so we just train with MSE right so we have my state like a sequence of

0:19:22.840,0:19:29.350
state an action we provide this to this predictor module the predictor gives me

0:19:29.350,0:19:36.550
some kind of hidden representation of the you know of whatever future then I

0:19:36.550,0:19:40.450
have a decoder which is decoding this hidden representation of the future and

0:19:40.450,0:19:45.360
this one this one should give me a prediction right so this is pretty

0:19:45.360,0:19:49.960
straightforward right you have a predictor that predicts the past into

0:19:49.960,0:19:53.559
the hidden representation of the future they have a decoder which is decoding

0:19:53.559,0:20:00.700
the hidden representation of the future into the actual future so we have a

0:20:00.700,0:20:05.200
target on the other side so all you need to do is going to be having this tool

0:20:05.200,0:20:12.010
going inside an MSc then you minimize the MSE by training these two modules so

0:20:12.010,0:20:19.380
does it work what's the action here so the action here is for example the

0:20:19.380,0:20:25.059
acceleration and the steering steering command that the we have observed in our

0:20:25.059,0:20:30.730
data set right so my state s1:t are a sequence of you know observations of

0:20:30.730,0:20:35.530
position velocities and context images in the action are the action taken by

0:20:35.530,0:20:43.030
the driver that I obtained by inverting the kinematic model of the car Pₜ is the

0:20:43.030,0:20:48.670
position of the car and Vₜ is the velocity of the car so a position

0:20:48.670,0:20:57.190
velocity in the 80 is the acceleration right so Pₜ, x and y position, Vₜ (x,y)

0:20:57.190,0:21:05.440
velocity 80 x and y acceleration basically is it preferred

0:21:05.440,0:21:11.289
to add the decoder instead simply using a predictor to improve the accuracy so

0:21:11.289,0:21:19.480
the predictor predicts what's gonna be the hidden state of the future okay so

0:21:19.480,0:21:24.190
the predictor gets the past and compress it and tries to give you what is the

0:21:24.190,0:21:29.169
future you know the future hidden representation then you have this code

0:21:29.169,0:21:33.340
you'd like to decode it right this is just you know one neural net but we'd

0:21:33.340,0:21:43.779
like to separate those into blocks okay so the action at eighty is calculated

0:21:43.779,0:21:48.789
from st plus one yeah so these actions are the ground

0:21:48.789,0:21:53.710
truth action coming from the yeah from the ground truth thanks how do we

0:21:53.710,0:21:58.619
calculate st we don't how do we get actual sto

0:21:58.619,0:22:05.200
the actual st i show you before right the the camera are checking the the cars

0:22:05.200,0:22:10.029
going on the highway i get the bounding boxes i track the bounding boxes and

0:22:10.029,0:22:16.450
therefore i have all those you know positions x and y's over time and then

0:22:16.450,0:22:22.659
those are DX the Pₜ  positions you can also compute the velocity right you know

0:22:22.659,0:22:27.879
position time t plus 1 minus position at timesₜ divided by time you have the

0:22:27.879,0:22:33.759
velocity wait I'm still a little confused on the role of the decoder is

0:22:33.759,0:22:38.230
it just converting the vector representation of the future from the

0:22:38.230,0:22:41.169
predictor into actual real-world predictions yeah that's correct

0:22:41.169,0:22:49.090
so the decoder is just like semantic the subdivision right now it's just one

0:22:49.090,0:22:53.769
neural net and you can think about the neural net has having an encoder and

0:22:53.769,0:22:58.119
decoder all the time right you can decide where you want to have the hidden

0:22:58.119,0:23:04.090
representation how do we hold on how do we determine the dimensionality of a

0:23:04.090,0:23:09.849
fpred output well it depends on your network right so whatever your network

0:23:09.849,0:23:14.000
is outputting in my case I think it 128 dimensional

0:23:14.000,0:23:22.310
vector foresty since there are variable number of cars surrounding our agent

0:23:22.310,0:23:28.360
vehicle then okay that's a good question is the size of sₜ  variable. So sₜ  

0:23:28.360,0:23:35.610
represents the position and velocity of myself and then i have an image

0:23:35.610,0:23:42.330
which shows me the occupancy grid basically shows me what the surrounding

0:23:42.330,0:23:49.009
right so I show you here this is my image iₜ  which represents the

0:23:49.009,0:23:53.909
configuration of the lanes of the street and what is the configuration of the

0:23:53.909,0:23:57.809
vehicles you have not different number of vehicles other weight vehicles in the

0:23:57.809,0:24:01.649
left image and the right image nevertheless an image can just show your

0:24:01.649,0:24:07.950
any number of vehicles right so that's very cute way of using images just for

0:24:07.950,0:24:15.359
the fact that I don't need to have a variable length set of you know things

0:24:15.359,0:24:19.889
right otherwise you should have used some kind of you know attention or asset

0:24:19.889,0:24:24.929
network or some other you know crafty things this one you know you can use

0:24:24.929,0:24:29.519
images is a mean of representing information these are not natural images

0:24:29.519,0:24:34.409
right these are completely synthetic images but I can use them in order to

0:24:34.409,0:24:40.590
cope with the fact that I have a different number of items nearby me yeah

0:24:40.590,0:24:45.889
I think I answer everyone yeah that's a boolean green yeah yeah it

0:24:50.330,0:24:56.600
is a boolean grid right so my image has RGB and each of them are or 0 or 1 in

0:24:56.600,0:25:02.059
this case they are I you can see the pixels and they are a little bit blurry

0:25:02.059,0:25:09.200
because there is like I think some up sampling with by linear by linear up

0:25:09.200,0:25:15.200
sampling here in this case let me think yeah I think there is something like

0:25:15.200,0:25:23.269
that maybe I should be a boolean grid that's correct I think here we do okay

0:25:23.269,0:25:26.480
there is down sampling this there is down sampling so these images actually

0:25:26.480,0:25:31.190
are larger in there are binary but then there are too large so it's so that we

0:25:31.190,0:25:37.340
actually make them forth and by doing the fourth you know scaling down they

0:25:37.340,0:25:40.880
start looking like a little blurring right now otherwise if you have a car

0:25:40.880,0:25:45.830
that turns you're gonna have all that kind of staircase right right now

0:25:45.830,0:25:52.750
instead if you have this kind of blurred version it's no like that stir-crazy

0:25:52.750,0:26:01.159
okay too many questions with with a differentiable cost function do we back

0:26:01.159,0:26:06.200
propagate from the end of the trajectory all the way back yeah it's gonna be yeah

0:26:06.200,0:26:10.039
sure that's correct we are gonna see this in the second part right this is

0:26:10.039,0:26:17.630
first part where I train a regression network so here it was the first model

0:26:17.630,0:26:22.399
right so you have like a encoder decoder which is not because it's a predictor

0:26:22.399,0:26:27.710
predictor decoder just from because of the fact that the amount stuff on the

0:26:27.710,0:26:30.769
left hand side is from the past right that's why you need a predictor that

0:26:30.769,0:26:35.929
gives you the next thing in line but otherwise you can think about an encoder

0:26:35.929,0:26:41.680
but it's not correct way of thinking just you know similar

0:26:41.680,0:26:45.910
does it work so on the left hand side you have the actual future the thing

0:26:45.910,0:26:49.930
that really happened on the right hand side you get the deterministic you know

0:26:49.930,0:26:54.610
predictor decoder and the network that I just show you train with MSC in order to

0:26:54.610,0:26:58.720
replicate the thing on the left this is from the testing set of course and it's

0:26:58.720,0:27:02.410
been trained on the training set so on the top right you're gonna see the

0:27:02.410,0:27:07.240
frames you're gonna have a 10 frame per second and the direction of motion is

0:27:07.240,0:27:11.710
going forward and the blue guy is our self the green guys are the others and

0:27:11.710,0:27:20.340
the red are the lanes so you can see here that after 3 seconds for 5 seconds

0:27:20.340,0:27:28.830
everything gets quite [ __ ] up yep nothing works

0:27:28.830,0:27:34.990
ok how nice I just taught you something that doesn't work how happy are you are

0:27:34.990,0:27:39.880
you happy no I can't hear you but okay I mean yeah

0:27:39.880,0:27:46.600
thank you for the no the not happy okay alright so what's happening okay who can

0:27:46.600,0:27:49.870
tell them you should know right what's happening because Jana has been talking

0:27:49.870,0:27:56.760
about this stuff all along this past three weeks so what's the problem here

0:27:57.000,0:28:02.290
latent variables your cadet the solution what's the problem okay

0:28:02.290,0:28:09.370
the MSE loss yeah I was the actual problem oh okay someone answer is

0:28:09.370,0:28:14.350
someone answer LM whose LM I don't know its average in future outcomes yeah yes

0:28:14.350,0:28:21.400
yes is someone answer stop okay this is basically everything that can happen

0:28:21.400,0:28:26.800
from that initial point in time you know you have everything else therefore it

0:28:26.800,0:28:30.850
looks like that you know every kind of image looks up like a blur the image

0:28:30.850,0:28:37.090
right so again you had this example from Yann if you have a pencil on on a plane

0:28:37.090,0:28:41.260
and I since I can't really draw in 3d I'm gonna give you the top-down view on

0:28:41.260,0:28:46.480
the right-hand side let's say you make it falling right one two three four

0:28:46.480,0:28:52.990
times five six whatever and if you compute what is the average right

0:28:52.990,0:28:59.260
folding location well this is since is a you know (x,y) is just you know coordinate

0:28:59.260,0:29:04.990
the average final location is like oh the pen never fed and it's really wrong

0:29:04.990,0:29:09.400
right otherwise if you actually use a pixel space you would have like the

0:29:09.400,0:29:14.680
overlay anyhow the problem is this one right you have a multi possible future

0:29:14.680,0:29:22.570
and then you only try to regress the average teacher right how do we fix it I

0:29:22.570,0:29:28.290
really told me we latent variable the ACS no no no no I want latent variable

0:29:28.290,0:29:35.140
latent variable this is okay cool this is the the energy base stuff he likes a

0:29:35.140,0:29:40.560
lot and we like it all right because we like young

0:29:46.890,0:29:52.120
so okay and actually you already know everything here so this is the original

0:29:52.120,0:29:56.170
Network I just show you before it doesn't work and let's fix this so in

0:29:56.170,0:29:59.770
the center we're going to be summing something and with some this

0:29:59.770,0:30:05.940
low-dimensional latent variable in green zₜ  which goes through a you know

0:30:05.940,0:30:11.980
expansion expansion module such that he fixed the image the dimensionality so

0:30:11.980,0:30:17.260
where's this zₜ  coming from well I guess you can tell so that zₜ  is going to be

0:30:17.260,0:30:22.840
chosen such that the prediction is minimized you know that the MSE is

0:30:22.840,0:30:27.700
minimized for this specific peak of the prediction right so you can do inference

0:30:27.700,0:30:31.480
right so you train everything it's a real train actually because we trained a

0:30:31.480,0:30:35.230
deterministic one now you can do inference of the latent variable such

0:30:35.230,0:30:40.540
that you can still get the MSE to zero by doing gradient descent into latent

0:30:40.540,0:30:44.110
space right so you can change that set into two to chain chain chain chain

0:30:44.110,0:30:48.120
chain until the MSE dies this is very expensive because you have

0:30:50.450,0:30:53.960
two degrees in the same thing to the damn thing there otherwise you can just

0:30:53.960,0:30:58.160
plug this you know you can actually predict that latent how do you do that

0:30:58.160,0:31:02.390
with an encoder and there you go where did the encoder is so the encoder gets

0:31:02.390,0:31:10.360
the future state and gives you the mean and variance from which you sample the

0:31:10.360,0:31:15.740
what is it Irish variational predictive network actually is a variational

0:31:15.740,0:31:21.050
conditional predictive network because you actually start from an actual action

0:31:21.050,0:31:29.750
aₜ. So aₜ is the condition is it advisable to help the output fed into a

0:31:29.750,0:31:35.570
encoder for the latent variable during training oh never mind okay all right

0:31:35.570,0:31:39.890
yeah that's that's how yeah okay that was the answer yeah so you don't have to

0:31:39.890,0:31:45.860
all right this is just a very convenient way to get that said zt guess in the

0:31:45.860,0:31:49.010
next next lab when we do the energy based model we're going to do first

0:31:49.010,0:31:52.130
inference right but inference is not take you forever because every time you

0:31:52.130,0:31:56.420
have to try try try try try try a new z then where you start with why is

0:31:56.420,0:32:02.320
there an arrow from st+1 to fenc because unless you know what will happen

0:32:08.539,0:32:14.749
how will you find that zₜ right ? So that zₜ is the missing information that you

0:32:14.749,0:32:18.979
can't have from the past because something happened you know now my

0:32:18.979,0:32:24.589
roommate is gonna be coming naked inside oh okay now it doesn't do that things

0:32:24.589,0:32:29.749
you know that's unpredictable part and may never happen before yeah hopefully

0:32:29.749,0:32:36.499
likely okay the point was that given that I have no idea about what's gonna

0:32:36.499,0:32:42.559
happen next in the future you know a meteorite crashes here you can't really

0:32:42.559,0:32:46.759
predict the unpredictable unpredictable part right you don't know what's going

0:32:46.759,0:32:51.169
on therefore during training I look in the

0:32:51.169,0:32:55.849
future what's happening huh I see something as I get information from the

0:32:55.849,0:33:01.039
future and from that information I can predict the latent variable okay so

0:33:01.039,0:33:06.009
someone's gonna say oh you're cheating right because you look in the future and

0:33:06.009,0:33:12.229
you don't have the future at testing time right when you drive you don't have

0:33:12.229,0:33:16.489
access to the future but since your training here you can kind of cheat and

0:33:16.489,0:33:23.029
look look what's happened there but we can fix this how do we fix that so this

0:33:23.029,0:33:28.190
is the posterior “blah” of a variational encoder and you fix it this way right

0:33:28.190,0:33:34.969
you enforce the posterior the decoder they saw the encoder there to be giving

0:33:34.969,0:33:39.079
you a distribution that is as close as possible to the prior to that K ed right

0:33:39.079,0:33:48.349
and so in this case you learn how to predict basically mean meaningful latent

0:33:48.349,0:33:52.729
variable zₜ trying to you know disconnect as well from that kind of future.

0:33:52.790,0:34:06.960
I'm not I'm not really understanding the fexp part fexp means a expansion so that zt

0:34:06.960,0:34:11.370
is the latent might be you know 16 dimensional vector very tiny thing and

0:34:11.370,0:34:17.400
the fpred must can easily be like some kind of spatial information right so

0:34:17.400,0:34:22.710
given that is the state is an image are gonna be like you know throughout New

0:34:22.710,0:34:26.190
York neural-net I'm gonna be making it convolutional nets a bit smaller it's

0:34:26.190,0:34:30.679
gonna be still spatial I won't be collapsing in one vector such that my X

0:34:30.679,0:34:37.500
fexp be explained expander will expand my 16 dimensional vector may be in two

0:34:37.500,0:34:44.070
sixteenth lanes of the same size of this fpred hidden representation okay such

0:34:44.070,0:34:47.520
that I can sum them together could you repeat how you're putting the

0:34:47.520,0:34:52.770
restriction on the model not being able to look at actually interstates so right

0:34:52.770,0:34:59.280
now only by looking in the future you can figure out what's happening okay so

0:34:59.280,0:35:05.400
this fenc is trained to produce the latent variable which is minimizing that

0:35:05.400,0:35:11.100
MSC so that's not the variational at encoder encoder there on the top part

0:35:11.100,0:35:17.040
yeah it's gonna be trying to predict the latent that really gives you you know

0:35:17.040,0:35:22.230
zero prediction error but then on the other side you also enforce that encoder

0:35:22.230,0:35:28.140
to give you something that is close to a normal to to the prior feed over there

0:35:28.140,0:35:33.000
so the fact that there is like a KL term between the posterior and the Q and the

0:35:33.000,0:35:39.420
P allows you later on to sample from the prior when you are actually doing you

0:35:39.420,0:35:43.590
know you're using this network. So you build a prior distribution and then

0:35:43.590,0:35:47.400
inference / test time instead of looking at the actual future

0:35:47.400,0:35:52.740
state you sample from the distribution label you just learn you just assemble

0:35:52.740,0:35:55.860
from the prior distribution so this is going to be a fixed prior you fix the

0:35:55.860,0:35:59.700
prior which is a normal distribution even forced the encoder you know to

0:35:59.700,0:36:05.760
stick with this kind of prior or you can even learn what is the distribution of

0:36:05.760,0:36:09.480
those latent variables you can do many things this is what actually we managed

0:36:09.480,0:36:14.309
to to get better best result with okay look do you

0:36:14.309,0:36:18.209
enforce the encoder to give you something that looks like a prior which

0:36:18.209,0:36:25.439
is a Gaussian you know independent Gaussian and then from independent in

0:36:25.439,0:36:31.670
also I saw what's called I saw something I forgot that you know the unitary

0:36:31.670,0:36:37.499
identity matrix right for the covariance and so later on we are gonna be just

0:36:37.499,0:36:41.279
sampling from that prior distribution to get late into that to look you know

0:36:41.279,0:36:47.069
reasonable okay thanks sure okay there are many more questions how do the

0:36:47.069,0:36:53.869
latent variable prevent the averaging oh well that's actually so basically

0:36:53.869,0:36:59.189
whenever okay so let's say we train the brains on the

0:36:59.189,0:37:03.539
bottom the deterministic part right so at the end you're gonna get a prediction

0:37:03.539,0:37:08.279
for the you know predicted state for the future state which is looking like some

0:37:08.279,0:37:17.609
kind of possible but you know average future now for a specific future you can

0:37:17.609,0:37:21.959
figure out now you know by doing gradient descent you can minimize that

0:37:21.959,0:37:27.719
MSE by changing that additional latent variable this is how latent variable

0:37:27.719,0:37:33.660
models work right so for every training sample you have one latent variable

0:37:33.660,0:37:38.729
which is gonna give you exactly zero and the seed loss you can train first with

0:37:38.729,0:37:42.359
all of them such that you can get a very initial starting point which is the

0:37:42.359,0:37:46.890
average prediction and then you can refine that average average prediction

0:37:46.890,0:37:52.259
by adding this additional latent variable and that value for the latent

0:37:52.259,0:37:56.789
variable can be found for example by doing gradient descent meaning you

0:37:56.789,0:38:02.130
minimize the MSE like you minimize this MSE loss by getting gradients coming

0:38:02.130,0:38:06.839
down here the gradient comes here and then you get gradients here so you can

0:38:06.839,0:38:15.809
do that equal you know exact gets Z minus ETA gradient of the loss with

0:38:15.809,0:38:22.309
respect to to the Z right got it okay awesome

0:38:23.619,0:38:27.109
no that wasn't someone else saying got it okay

0:38:27.109,0:38:31.789
someone had a question before okay I don't know oh no it was the answer I

0:38:31.789,0:38:35.359
gave so basically we are adding the fpred

0:38:35.359,0:38:42.559
fpred our prediction of what will happen with fexp the representation of the

0:38:42.559,0:38:50.630
what actually happens now fexp is not what actually happens fexp is what I couldn't

0:38:50.630,0:38:59.059
figure out that would have happened okay so fpred output it's what I I can

0:38:59.059,0:39:04.519
you know I would guess you know my best prediction for what happens tomorrow is

0:39:04.519,0:39:12.739
going to be that the Sun will rise might not be the case so the the Zed that

0:39:12.739,0:39:20.690
comes down to the fexp will add that component that will you know manage

0:39:20.690,0:39:27.979
to fix my broken prediction right so the lower branch will try to do as the best

0:39:27.979,0:39:33.589
job it can without having knowledge of the future and the other one allows me

0:39:33.589,0:39:38.029
to refine my prediction to be actually correct okay but in this case we have

0:39:38.029,0:39:43.489
access to the actual future so it's kind of cheating nevertheless you enforce

0:39:43.489,0:39:49.609
that this generation of latent variable is going to be as close as possible to

0:39:49.609,0:39:52.479
my prior distribution I hope it makes more sense but maybe we

0:39:56.480,0:40:02.720
can go further and then you may get a little bit clearer clear mind well last

0:40:02.720,0:40:08.170
question is Syd Garon okay starting to make sense answer is it guaranteed that

0:40:08.170,0:40:13.550
there exists a Zed which gives us M s is zero I think I'm not sure about

0:40:13.550,0:40:21.050
guarantees but if your network is reasonably well behaved I guess yeah

0:40:21.050,0:40:29.990
you're guaranteed you know if your network capacity is can over fit but you

0:40:29.990,0:40:34.490
can't over fit because you know there are noise in the data you can reduce you

0:40:34.490,0:40:40.850
can zero out that noise by adding this additional term so it's guaranteed if

0:40:40.850,0:40:49.280
the network is properly sized yeah okay cool so inference how do we drive okay

0:40:49.280,0:40:55.670
we already spoiled okay more question training time we try to learn q(z) that

0:40:55.670,0:41:03.710
close to prior petesy test time okay yeah I'm getting there hold on all right

0:41:03.710,0:41:07.300
test time not test driving time right how do the hell do you use this stuff

0:41:07.300,0:41:12.350
variation of predicted condition and predicted net for inference cool so we

0:41:12.350,0:41:15.530
had this lower branch in this lower dimensional latent variable not a no

0:41:15.530,0:41:21.440
sixteen dimensional vector and where it comes from we sample from prior right

0:41:21.440,0:41:26.600
because we enforce that the encoder there was trying to you know shoot it

0:41:26.600,0:41:32.870
towards this distribution hmm cool then what do you do next you get the

0:41:32.870,0:41:37.940
prediction you put it back so you do a outer aggressive step right you get next

0:41:37.940,0:41:43.490
prediction network next yeah correct okay no one wrote anything

0:41:43.490,0:41:47.900
but I know you understood and so you keep feeding this stuff does it work yes

0:41:47.900,0:41:52.550
it works so here you can see again a comparison between the actual future on

0:41:52.550,0:41:56.390
the left hand side the deterministic branch that is just just the lower

0:41:56.390,0:42:03.260
branch strain as before and then here I give you four different draws from an

0:42:03.260,0:42:15.440
art and from a end distribution was called I can I can tell the help normal

0:42:15.440,0:42:21.440
distribution so you have you know 200 times 4 so we have 800 samples from a

0:42:21.440,0:42:28.670
normal distribution of size I don't know 16 and I feed you know on the first 200

0:42:28.670,0:42:33.530
values to the first model then I start again from from same past and I feed to

0:42:33.530,0:42:39.890
new 100 no no 200 200 new values today to the Leighton there then I have third

0:42:39.890,0:42:44.450
time I get the same initial condition and then I feed this sequence another

0:42:44.450,0:42:50.360
third sequence of 200 variables pay attention here to the car on the

0:42:50.360,0:42:53.810
right-hand side of me that isn't a circle white circle and then the guy

0:42:53.810,0:42:59.710
behind that guy right in this square okay and so if you show if you see here

0:42:59.710,0:43:03.710
you get basically that all these different predictions will predict a

0:43:03.710,0:43:08.450
different location for the car going there the dead one in the circle and a

0:43:08.450,0:43:12.500
car behind the one one in the square also has completely no arbitrary

0:43:12.500,0:43:16.730
prediction so this super cool right right now you have that each of these

0:43:16.730,0:43:22.370
you know possible futures you know are completely elucidated by my network so

0:43:22.370,0:43:26.210
this stuff doesn't exist but we have a network that generates

0:43:26.210,0:43:32.360
future how cool is this right so before maybe we had you know limited amount of

0:43:32.360,0:43:38.750
training data now we have a network that is just generating futures from the Hut

0:43:38.750,0:43:43.610
like a magician right so this is super super super cool you have infinite

0:43:43.610,0:43:48.140
amount of data which is you know completely different from what actually

0:43:48.140,0:43:52.880
happened the actual future this data comes from only observational data so

0:43:52.880,0:43:56.190
that we have seen in the actual reality but is

0:43:56.190,0:44:02.880
applied to this specific initial condition so what next now you can use

0:44:02.880,0:44:08.910
this huge amount of data to train this policy right this network that allows

0:44:08.910,0:44:14.130
you to control our agent such that it minimizes losses the loss that the cost

0:44:14.130,0:44:19.470
for the going over these lanes and the cost for you know colliding against

0:44:19.470,0:44:25.069
other vehicles okay the cool part is that okay I can tell

0:44:30.630,0:44:35.819
the design I should yeah I can tell you the cool part is that this future here

0:44:35.819,0:44:41.279
these multiple futures come from the specific sequence of latent variable you

0:44:41.279,0:44:49.500
feed to this network right what if you perform gradient ascent in

0:44:49.500,0:44:54.210
the latent space you get your initial you know sample from this normal

0:44:54.210,0:45:01.200
distribution then you tweak these values such that you [ __ ] up the hardness

0:45:01.200,0:45:05.460
like you increase the hardness you don't [ __ ] up you increase the hardness I like

0:45:05.460,0:45:09.029
to swear you know but you increase the hardness for example you try to increase

0:45:09.029,0:45:15.450
the proximity cost right so you get the sequence of latent where other cars are

0:45:15.450,0:45:20.579
gonna be like kamikaze like they are gonna be driving into you and like crazy

0:45:20.579,0:45:24.960
so that's the super cool you have a network that gives you futures that you

0:45:24.960,0:45:29.990
want right okay I'm already too excited I will not be able to sleep tonight

0:45:29.990,0:45:35.910
okay question if we try to sample that from Pisa during test time it is

0:45:35.910,0:45:41.640
possible for us to look for a specific future for example I would like to know

0:45:41.640,0:45:50.430
the solution of turning left not moving forward so here here I don't let me

0:45:50.430,0:45:52.609
think right right so this predictive network

0:45:55.420,0:46:00.190
you feed the action right so this is a condition a predictive network so we

0:46:00.190,0:46:03.940
have the action here on the bottom part so you actually display the whole point

0:46:03.940,0:46:08.110
you can take different actions and the whole future will change based on the

0:46:08.110,0:46:11.950
action you take right I was mentioning here we have a initial state that's you

0:46:11.950,0:46:15.940
know the initial condition and then given different latent you're gonna have

0:46:15.940,0:46:19.690
different kind of behavior of the other vehicles and then you can decide to

0:46:19.690,0:46:24.790
tweak this latent by for example doing gradient ascend in the latest pace by

0:46:24.790,0:46:29.500
increasing that kind of collision term such that you have in our crazy cars

0:46:29.500,0:46:33.880
coming at you but nevertheless as you pointed out here you can also figure out

0:46:33.880,0:46:38.530
what are the outcomes of changing the action right that's exactly how we're

0:46:38.530,0:46:44.020
going to be training our policy that is actually the whole thing right okay

0:46:44.020,0:46:54.600
issues first issue so given given that you actually have access to the future

0:46:54.600,0:47:00.310
if you turn slightly to the left everything is gonna turn to the right

0:47:00.310,0:47:05.400
right because if you drive you know you turn to the left and how's gonna be like

0:47:05.400,0:47:12.880
like that right so if I turn to the left you just turn to the right and turning

0:47:12.880,0:47:19.990
to the right is gonna be contributing in a huge way to the MSE right so right now

0:47:19.990,0:47:26.020
you can I get basically that these MSE lofts can be minimized if you were

0:47:26.020,0:47:31.570
encoder and they're in the latent variable is gonna tell my bottom part

0:47:31.570,0:47:35.830
that everything is turned to turn to the right but this is absolutely not what we

0:47:35.830,0:47:40.660
want right because we know how to tell everything turns to the right because

0:47:40.660,0:47:44.950
that's deterministic right given the past given that I turned to the left the

0:47:44.950,0:47:49.330
steering wheel everything turns to the right I don't care I don't want to look

0:47:49.330,0:47:54.400
in the future to see everything turns to the right regardless of what I'm doing

0:47:54.400,0:47:59.200
with my steering wheel right so this it's a very very huge terrible problem

0:47:59.200,0:48:03.970
because the network basically was learning to cheat right and was figuring

0:48:03.970,0:48:10.870
out that we were turning without us telling the system we turned

0:48:10.870,0:48:17.230
so that was terrible because basically this big arrow here is a leak of

0:48:17.230,0:48:23.410
information and therefore it was not any more sensitive to the current action I

0:48:23.410,0:48:29.110
was providing to my predictor right this was a very nice big nightmare how do you

0:48:29.110,0:48:32.080
now kill that big arrow right so how do I

0:48:32.080,0:48:41.740
get my predictive network to actually care about the action I take so let me

0:48:41.740,0:48:46.450
show you the EDD problem here so here we have the real sequence of latent the one

0:48:46.450,0:48:50.980
that I actually have computed from in minimizing the MSE yeah that the answer

0:48:50.980,0:48:55.300
that's correct so here I have the real sequence of latent and the real sequence

0:48:55.300,0:48:58.570
of actions that are taken by the DEA agent

0:48:58.570,0:49:01.830
and so here you can see is actually speed up for time such that you can see

0:49:01.830,0:49:11.590
there is some kind of you know you know turning then you can see here random

0:49:11.590,0:49:15.910
variables but the real sequence of action you can see now that things are

0:49:15.910,0:49:21.370
kind of turning right you can see see things are turning on the left on the

0:49:21.370,0:49:26.020
last one there you have the real sequence of latent but simple actions

0:49:26.020,0:49:28.620
and you can clearly see the last pile at the

0:49:33.080,0:49:40.640
last point that the turning came from the mostly from the latent right so the

0:49:40.640,0:49:47.990
latent burial 1/10 encodes the rotation in the action that it's written a tilde

0:49:47.990,0:49:53.840
and those are same sample at random right so well symbol from other from

0:49:53.840,0:49:58.730
other episodes so the problem here is that the fact that we were turning can

0:49:58.730,0:50:01.060
we learn so you'd like to learn that thing you

0:50:07.310,0:50:12.110
would like to reject the fact that we were turning right so the fact that the

0:50:12.110,0:50:16.790
thing turns should be completely explainable by the action right and I

0:50:16.790,0:50:23.000
think yeah I don't know anyhow let me show you how we fix the problem

0:50:23.000,0:50:28.250
sorry explain again it was very unclear when you were saying first and last what

0:50:28.250,0:50:34.930
you were referring to okay I try again so here we have four different

0:50:34.930,0:50:40.580
rectangles right in the the last one on the right hand side you have the real

0:50:40.580,0:50:45.140
sequence of latent variable which are this the latent variable that allows me

0:50:45.140,0:50:51.470
to get the precise the correct future right so those are the latent variable

0:50:51.470,0:50:56.780
coming from the and from the encoder in the variational encoder and have a have

0:50:56.780,0:50:59.900
the real sequence of actions taken by the expert okay

0:50:59.900,0:51:05.750
the further the two blue the one that have this blue here and this one here

0:51:05.750,0:51:11.240
are you know simple simple latent variable that the till that means they

0:51:11.240,0:51:15.380
are sample so they are randomly sample but I have the real sequence of actions

0:51:15.380,0:51:20.090
and so I would expect to see the steering and the last one on the left

0:51:20.090,0:51:24.770
hand side I have the real sequence of latent but then I have you know

0:51:24.770,0:51:30.920
arbitrary actions and so if I show you again the animation you're gonna see

0:51:30.920,0:51:37.220
here there is you know some amount of steering involved okay and you know the

0:51:37.220,0:51:43.340
steering comes from the actions then on the other two examples here I show you

0:51:43.340,0:51:47.510
that the same actions are not providing the same amount of steering now that I

0:51:47.510,0:51:52.700
had sample different latent variables nevertheless if I use the you know exact

0:51:52.700,0:51:57.470
same latent variable all the steering happens because of the latent variable

0:51:57.470,0:52:04.760
so my decoder will tell me if in my network that things were turning just

0:52:04.760,0:52:08.450
because they have been encoded in the latent variable rather than in the

0:52:08.450,0:52:10.810
action all I can I think I was clear clear but

0:52:15.640,0:52:19.810
let me show you how we fix this maybe oh is it is it clear what I'm

0:52:19.810,0:52:24.250
trying to show you yeah that was much better so okay thank you see

0:52:24.250,0:52:30.640
but repeating we say repetita iuvant in Latin repetition helps that's why you

0:52:30.640,0:52:35.860
had to practice practice practice okay how do you fix the problem the problem

0:52:35.860,0:52:40.030
is that we have a not a memory leak but information leak right the information

0:52:40.030,0:52:45.940
leaks from the from the future well it's a bit late all right so how do we fix

0:52:45.940,0:52:52.230
this problem we fix this problem by simply dropping out this latent and

0:52:52.230,0:52:57.520
picking it and sampling from the prior distribution at random okay so we don't

0:52:57.520,0:53:03.040
always rely on the output of the posterior network the encoder but

0:53:03.040,0:53:06.940
sometimes we pick from the posterior from the prior in this way you can't

0:53:06.940,0:53:10.900
encode the rotation anymore in the latent variable because sometimes it

0:53:10.900,0:53:16.270
will be missing and therefore the network will be having to you know

0:53:16.270,0:53:22.060
exploit the action you provide so in this case the purple on the right hand

0:53:22.060,0:53:27.190
side I show you two different sets of latent variable but the real actions

0:53:27.190,0:53:31.840
okay and this network has been trained with this dropout trick in so in this

0:53:31.840,0:53:36.430
case you can see that the rotation is actually encoded by the action and no

0:53:36.430,0:53:42.600
longer by the latent variables as it was the case before so we fix this problem I

0:53:42.600,0:53:48.700
think I should be speeding slightly a little because we're kind of running way

0:53:48.700,0:53:53.610
too late so sorry I didn't know sit notice

0:53:59.880,0:54:06.599
but okay what how do we train the agent well this is pretty much what I said

0:54:06.599,0:54:11.430
before on the right hand side we learned so far the the model of the world from

0:54:11.430,0:54:16.019
the real world on the left hand side you're gonna be training this agent

0:54:16.019,0:54:23.819
through using this predictive model so we have the agent which picks a initial

0:54:23.819,0:54:27.599
state the initial condition right position velocity in those context

0:54:27.599,0:54:33.029
images and gives you a control an action which is the acceleration in the

0:54:33.029,0:54:39.109
longitudinal direction and the acceleration in the transverse direction

0:54:39.109,0:54:45.569
alright so how does it work so this is how we train we have a state we feed a

0:54:45.569,0:54:50.190
state to the policy you get an action then you feed both of them to the world

0:54:50.190,0:54:54.900
model right what does the model tell you well you need to provide some you know

0:54:54.900,0:55:00.619
latent variable we also some possible way that the future can you know evolve

0:55:00.619,0:55:08.460
then you get a prediction cool you feed a prediction to these laws where the

0:55:08.460,0:55:15.390
lost is gonna be my cost of the task I'm trying to accomplish in my case is gonna

0:55:15.390,0:55:19.170
be the summation of the proximity cost the one that is telling me how close I

0:55:19.170,0:55:24.359
am to other vehicles plus some kind of you know cost associated to the beam in

0:55:24.359,0:55:29.249
the center of the lane cool so in the next state I I send it to the network

0:55:29.249,0:55:34.019
then the policy I get the next action I feed both of them to the world model you

0:55:34.019,0:55:39.259
get the latent variable you get a new prediction you send this to the loss and

0:55:39.259,0:55:47.069
more prediction like policy world model latent variable next guy loss and finish

0:55:47.069,0:55:55.339
you do a back propagation to train the policy model and doesn't work [ __ ]

0:55:56.450,0:56:03.740
what happened so we are basically falling outside a manifold the policy

0:56:03.740,0:56:10.290
manages to you know crank up those actions and give predictions that are

0:56:10.290,0:56:19.050
all black and all black is good because zero cost right so that's bad okay so

0:56:19.050,0:56:22.950
here we actually went outside the road and here we actually collided in two

0:56:22.950,0:56:27.780
other vehicles so let's maybe try to imitate other vehicles so how do you do

0:56:27.780,0:56:32.609
that well you can say that the loss is gonna be you know the task that we were

0:56:32.609,0:56:38.970
trying to accomplish plus some you know regularizer which is you know expert

0:56:38.970,0:56:42.119
regularizer what is this stuff so here you try also

0:56:42.119,0:56:48.750
to get the prediction by you know that you would get by taking a specific

0:56:48.750,0:56:53.670
action as close as possible to the action the actual future so you do that

0:56:53.670,0:56:58.380
for everyone but in this case you actually have to kind of remove this

0:56:58.380,0:57:03.180
latent variable because the latent variable gives you a specific prediction

0:57:03.180,0:57:09.660
it works better if you just work with the average prediction we trained

0:57:09.660,0:57:13.730
evaluation of the encoder to just remove it hmm does it work actually yeah

0:57:13.730,0:57:18.420
imitating the expert work so this is kind of imitation learning but imitation

0:57:18.420,0:57:23.099
learning with you know model day so I mean model basing imitation learning you

0:57:23.099,0:57:28.530
use your brain in order to try to imitate others all right can we do

0:57:28.530,0:57:33.990
better yes we can do better and that's going to be the end of the class so how

0:57:33.990,0:57:38.330
can we add a different kind of manifold attractor what am I talking about here

0:57:38.330,0:57:46.320
so forward model uncertainty so my predictive model outputs a prediction

0:57:46.320,0:57:51.599
given you know a state and the action and then I have my cost here so this is

0:57:51.599,0:57:58.210
for example my cost the point is that you know there are

0:57:58.210,0:58:04.570
always this cost can go whenever you're outside the training region the one with

0:58:04.570,0:58:08.380
the red points right so if you are within the red points as I show you in

0:58:08.380,0:58:13.180
the lab number three when we were doing regression if you're within this

0:58:13.180,0:58:17.619
training region you have zero variance right across those points is it go away

0:58:17.619,0:58:24.430
from those training region the variance increases right guess what the variance

0:58:24.430,0:58:31.660
is differentiable let's run within descent so we can try to run gradient

0:58:31.660,0:58:39.150
descent over the variance and yeah so you minimize the variance by using SGD

0:58:39.150,0:58:46.330
so this is my uncertainty regularizer so I have my predicted my policy which

0:58:46.330,0:58:52.180
is feeding my you know action to the world model you get this latent variable

0:58:52.180,0:58:57.700
here and you get a prediction also you get the task cost here which is you know

0:58:57.700,0:59:04.270
the minimization of the lane and proximity cost yep in there you actually

0:59:04.270,0:59:08.440
can get several models or you can use some the drop out trick we haven't

0:59:08.440,0:59:14.320
talked about but you can leave the drop out on during inference such that you

0:59:14.320,0:59:19.559
can have multiple predictions and now you can compute the variance

0:59:19.559,0:59:24.989
right of these predictions and you multiply by a thing whatever a lambda

0:59:24.989,0:59:31.680
scaler and then my model uncertainty regularizer leave some of the two and

0:59:31.680,0:59:38.689
you get the final loss to optimize and that's it so that's the whole model

0:59:38.689,0:59:42.390
where the final loss is going to be my ctask

0:59:42.390,0:59:48.479
plus this uncertainty this slides are going to be in the next slide I give you

0:59:48.479,0:59:54.059
the links so in this one I show you in pink that they actually managed to learn

0:59:54.059,0:59:58.380
how to drive by minimizing the uncertainty of the predictive model so

0:59:58.380,1:00:02.939
the action takes an taken by the policy are minimizing the uncertainty with

1:00:02.939,1:00:08.880
which the predictive network makes predictions right what a mouthful but it

1:00:08.880,1:00:16.109
works very well and evaluation I'll show you just very quickly here in the yellow

1:00:16.109,1:00:22.109
is the original car and the blue is the car controlled by our policy which got

1:00:22.109,1:00:27.779
lost because they already the other guy got you know went to a different path so

1:00:27.779,1:00:32.579
our blue guy here has to survive this jungle of other cars they cannot see us

1:00:32.579,1:00:37.589
right so in this case we are slightly ahead slightly behind slightly we are

1:00:37.589,1:00:42.209
accelerating a lot and we survived in the other case is well we are the blue

1:00:42.209,1:00:48.209
guy the yellow one is the original guy in the in the data and oh we again

1:00:48.209,1:00:53.099
managed to diverge from the original trajectory but we still survive until

1:00:53.099,1:01:00.599
the end and slides as we were mentioning so okay this is like a summary of the

1:01:00.599,1:01:04.410
whole thing model predictive policy learning with uncertainty repolarization

1:01:04.410,1:01:10.349
for driving during dense traffic you should have understand everything now so

1:01:10.349,1:01:14.910
model prediction like okay and just you have the four different points that are

1:01:14.910,1:01:19.890
uncertain in the regularizer letting dropout large-scale data set and then

1:01:19.890,1:01:26.900
you have the additional cost for you know trying to mimic the experts

1:01:26.900,1:01:32.210
information and these are the links for everything so that's the title that's me

1:01:32.210,1:01:37.339
collaborators are well main authors we are both first autors Mikael and myself

1:01:37.339,1:01:43.190
and then division slides are here the article is here the code is available on

1:01:43.190,1:01:50.779
github on my GitHub and this is a website and we also have a poster sorry

1:01:50.779,1:01:56.390
it'll be running so so so late but I hope you really enjoyed this small

1:01:56.390,1:02:02.450
project of ours there were so many questions I didn't plan there is another

1:02:02.450,1:02:07.839
explanation of this there is another explanation of this project on my

1:02:07.839,1:02:15.260
YouTube Israel is 20 minutes long maybe I've done a better job than today but

1:02:15.260,1:02:20.630
maybe I did a better job today since I was answering your questions so if there

1:02:20.630,1:02:26.480
are no other questions we gonna see each other next week and again if someone can

1:02:26.480,1:02:33.200
help out with the review of last scribes from the lab would be very helpful or

1:02:33.200,1:02:40.730
because I I have no idea how to deal with this otherwise is the mall is a

1:02:40.730,1:02:44.720
word model frozen when training the agent yeah as as we have seen last week

1:02:44.720,1:02:49.670
do we still have time for questions of course any time since when you're

1:02:49.670,1:02:56.710
training the network since it's kind of like a recurrent architecture are we

1:02:56.710,1:03:06.789
worried about like vanishing gradients and how do you with with such a complex

1:03:06.789,1:03:13.579
model how do you like address that all right so it's not that complex as in

1:03:13.579,1:03:18.559
this is two layer neural net so the policy is very stupid policy like it's

1:03:18.559,1:03:24.980
very tiny the word model is a bit larger but nevertheless we have batch norm

1:03:24.980,1:03:31.970
which is keep you know helping out sending things through like the

1:03:31.970,1:03:36.280
gradients throw and then I think we also used the Samsung

1:03:36.280,1:03:40.869
receiver connections because those are units yeah so you know the gradients

1:03:40.869,1:03:48.940
didn't give us big issues for training this model also we are doing 1330 times

1:03:48.940,1:03:53.020
steps in the future so it was like three seconds in the future given that we

1:03:53.020,1:03:57.369
start from I think two seconds in the past so we're always like five seconds

1:03:57.369,1:04:03.299
window temporal window for training the system I didn't quite understand how the

1:04:03.299,1:04:12.760
when we're doing the latent dropout how that works how that helps us with I

1:04:12.760,1:04:18.039
guess would you call it like like disentangling the action versus the

1:04:18.039,1:04:23.589
latent right so the problem was here right so the problem was that the fact

1:04:23.589,1:04:29.650
that we have access to the future if we make a small tiny change in my steering

1:04:29.650,1:04:34.539
steering wheel control everything will change which is a very large change

1:04:34.539,1:04:40.329
right and that MSE will to minimize that MSE you know a lot of information will

1:04:40.329,1:04:45.460
go through that way because it's a big change right so it's very is a brutal

1:04:45.460,1:04:49.809
change now and you know if that latent variable that way little Bible will try

1:04:49.809,1:04:55.569
to to acknowledge the fact that everything changes an 80 just changed

1:04:55.569,1:05:00.490
slightly right because it's just a tiny change of the steering wheel the problem

1:05:00.490,1:05:04.599
is that if you're forward if your predictive model on this is called

1:05:04.599,1:05:11.200
forward model is no longer looking at what is the steering wheel then you know

1:05:11.200,1:05:15.849
you can steer around but the network will not care about what is your control

1:05:15.849,1:05:20.380
may instead be you want to have a network that if you steer to the left

1:05:20.380,1:05:27.039
he's gonna tell you everything steers to the right by the fact that you steer to

1:05:27.039,1:05:31.930
the left so you had to disentangle this and to fix that the thing that really

1:05:31.930,1:05:37.869
worked very well was this one which was basically half of the time or maybe we

1:05:37.869,1:05:42.069
had some scheduling I forgot but you know some some times instead of you know

1:05:42.069,1:05:47.420
sampling the latent variable while training these automation of

1:05:47.420,1:05:52.790
encoder instead of sampling it all the time from your encoder sometimes it's

1:05:52.790,1:05:59.960
just simple from the prior distribution in this way rotation in the future

1:05:59.960,1:06:05.680
cannot be explained by the latent variable therefore there must be a path

1:06:05.680,1:06:16.310
that connects the action to the future state right so if you break this arrow

1:06:16.310,1:06:20.360
you have to break this arm if I do this switching between this one and the other

1:06:20.360,1:06:24.890
you basically Blake you break this arrow for you know a fraction of the

1:06:24.890,1:06:30.050
iterations and therefore the arrow will not actually subsist because it cannot

1:06:30.050,1:06:34.730
use it right to make a prediction if sometimes it happens sometimes it

1:06:34.730,1:06:39.080
doesn't happen the network always sees this action right this action is gonna

1:06:39.080,1:06:44.090
be always turning left turn right that are mystically determing the fact that

1:06:44.090,1:06:50.320
is steer to the left to the right this is kind of a way to make the network

1:06:50.320,1:06:56.140
like see the action when it's performing the training yeah this was a big issue

1:06:56.140,1:07:00.470
without these things it doesn't work because you would have a network which

1:07:00.470,1:07:05.030
is not carrying at all something nice you can do is gonna be like adversarial

1:07:05.030,1:07:09.920
adversarial something you want to want to get a network which is gonna be

1:07:09.920,1:07:14.960
killing you like if the future changed because of the action or not and it said

1:07:14.960,1:07:19.700
the prior distribution for the latent variable is just like a Gaussian normal

1:07:19.700,1:07:25.220
okay all right cool thank you so much Yann keeps saying that we sample from

1:07:25.220,1:07:30.530
zero that was the previous previous version where we actually didn't even

1:07:30.530,1:07:37.280
have a variational encoder and we had just a encoder which was encoding the

1:07:37.280,1:07:42.320
mean so we we didn't have the same thing module we didn't have the variance just

1:07:42.320,1:07:45.890
we had an encoder which was just giving me this latent so that was the previous

1:07:45.890,1:07:52.970
version no sampling no V just fenc you encode this guy and so sometimes you

1:07:52.970,1:07:57.339
were getting it from the encoder and sometimes you were picking it from zero

1:07:57.339,1:08:01.930
because we trained this first branch with this guy that was not here right so

1:08:01.930,1:08:11.200
we first trained this stuff deterministically which is or is it here

1:08:11.200,1:08:16.659
so initially we trained these which means set Z to 0 right so if you set the

1:08:16.659,1:08:21.369
Z to 0 there you go you get this back deterministic so basically whenever you

1:08:21.369,1:08:26.980
do this dropouts trick you switch back and forth between the deterministic and

1:08:26.980,1:08:31.710
the stochastic version of the predictive model

1:08:31.710,1:08:38.589
kula sounds crazy I think well yeah I I think you should be able to digest this

1:08:38.589,1:08:46.390
stuff because you're good you're good students it might interest you more

1:08:46.390,1:08:52.660
questions yeah al yeah so I wanted I just wanted to check my understanding of

1:08:52.660,1:08:57.310
latent variable models it seems that different models use

1:08:57.310,1:09:02.290
latent variables for different objectives like the auto encoder uses it

1:09:02.290,1:09:07.449
to learn a low dimensional representation then we AE the

1:09:07.449,1:09:11.770
variational auto encoder builds on it to regularize the latent space so that it

1:09:11.770,1:09:18.069
can randomly sample from the Nathan space and then GANs and the model that

1:09:18.069,1:09:24.370
we just studied they use it to introduce randomness so that when we have the same

1:09:24.370,1:09:33.120
input and different outcomes the network doesn't have to output an average of the

1:09:33.120,1:09:41.560
predictions and so basically here we are using latent space to provide more

1:09:41.560,1:09:46.719
dimension led to the input to introduce randomness to the input so in this case

1:09:46.719,1:09:56.350
we have this latent variable to account for what cannot be account from the past

1:09:56.350,1:10:02.080
array so the point is that you know not everything is predictable and so if you

1:10:02.080,1:10:06.130
cannot predict everything you're gonna make some errors so this latent variable

1:10:06.130,1:10:11.550
allow you to tune your algorithm to actually you know zero out that error

1:10:11.550,1:10:16.120
and later on you can sample this latent variable in order to get you know proper

1:10:16.120,1:10:19.179
predictions so without the laden bag what you are getting that you know

1:10:19.179,1:10:25.660
masked version of the prediction it's blurry prediction instead in order to

1:10:25.660,1:10:32.020
get you know crisp predictions you want to refine your average prediction to the

1:10:32.020,1:10:38.500
specific case you have at hand right got it so it is basically using it to add

1:10:38.500,1:10:41.860
dimensionality to the input to make things more specific

1:10:41.860,1:10:44.740
I wouldn't say dimensionizer to the import

1:10:44.740,1:10:48.850
I would say you add latent variables in order to provide the missing information

1:10:48.850,1:10:54.630
that would be required for you to make a proper prediction got it

1:10:54.630,1:11:03.360
good thank you of course more questions I know it's so it's so late I'm taking

1:11:03.360,1:11:13.860
this was very extreme extremely lengthy class I feel sorry more questions I

1:11:13.860,1:11:18.730
can't see you hold on stop share screen okay

1:11:18.730,1:11:28.990
Oh 50 people still here okay what do you want to know else I also want to check

1:11:28.990,1:11:34.540
my understanding for later was correct so today tomorrow will tell us something

1:11:34.540,1:11:40.960
that we cannot predict from the future but you just said if we turn lab

1:11:40.960,1:11:45.490
everything would be like to our right but you don't want me to tell this

1:11:45.490,1:11:51.490
information you want this information all it comes from our action yeah

1:11:51.490,1:11:56.290
exactly because if you if you if you steer to their left everything will turn

1:11:56.290,1:11:59.650
to the right right if you if you're riding a bicycle and you're doing like

1:11:59.650,1:12:04.660
that you're gonna go you exactly know how you will evolve right you don't know

1:12:04.660,1:12:08.350
if you're gonna be crashing into someone and that's gonna be taken care by the

1:12:08.350,1:12:14.890
latent variable and which is gonna be you know patching your wrong prediction

1:12:14.890,1:12:20.740
due to unforeseen events but everything that is you know predictable should be

1:12:20.740,1:12:25.300
done by the deterministic branch such that you use the past information to

1:12:25.300,1:12:28.450
determine what's happening next but then you can't really know so it's

1:12:28.450,1:12:35.560
like you know you're gonna be me trying to touch the calm web web-cam here but

1:12:35.560,1:12:40.630
then there is some wind because I open the window so I go here and it goes

1:12:40.630,1:12:45.700
there I go here and it goes here right so if there is a additional component it

1:12:45.700,1:12:50.770
is like moving my hand I will have to deal with that for the specific case so

1:12:50.770,1:12:57.310
I can only go here given that I know that my finger will go forward from the

1:12:57.310,1:13:00.190
past to the future I exactly nowhere to go but if there are

1:13:00.190,1:13:05.350
additional you know inputs that are not under my control then you will not be

1:13:05.350,1:13:11.470
able to make accurate predictions unless you have some knowledge it's like okay

1:13:11.470,1:13:16.630
like going back to the example Yann makes all the time with a pencil right so the

1:13:16.630,1:13:20.160
pen is gonna be falling one direction let me get the pens

1:13:20.160,1:13:25.570
get the pen if you let it go it's gonna be falling in one direction goes another

1:13:25.570,1:13:32.020
direction goes the third direction right so you can first learn you know massive

1:13:32.020,1:13:37.450
Network which is learning the dynamics of falling right so you learn how a pen

1:13:37.450,1:13:42.970
falls down the only little information you know - you need to know that you

1:13:42.970,1:13:47.320
can't know in advance is in which direction it will fall so you're gonna

1:13:47.320,1:13:54.700
have the big ass Network learning the folding dynamics and then the last

1:13:54.700,1:13:58.900
minimal amount of information that is missing is in which direction you should

1:13:58.900,1:14:05.770
actually you know initiate this folding trajectory right so this is how it works

1:14:05.770,1:14:12.460
bid network to learn the big bulk of prediction little tiny little variable

1:14:12.460,1:14:15.640
right so this latent variable again I said is something like 16 dimensions

1:14:15.640,1:14:20.980
it's very tiny like a very short vector which is just providing you the missing

1:14:20.980,1:14:28.060
information to make a sharp prediction okay okay so to avoid the latent

1:14:28.060,1:14:32.950
variable to tell us information that should be predictable we are trying to

1:14:32.950,1:14:40.060
approach the regularization term the KO term from with the help of prior okay so

1:14:40.060,1:14:49.510
yes so the KL term it's necessary for you otherwise things will explode as we

1:14:49.510,1:14:53.950
have seen in class right if you don't have the KL this prediction will go as

1:14:53.950,1:14:59.140
far as possible because they don't want to overlap the second term is going to

1:14:59.140,1:15:02.680
be the actual network will try to collapse them right so whenever you

1:15:02.680,1:15:08.230
introduce this KL you enforce the bubble to be having variants of one and also

1:15:08.230,1:15:12.460
all the mean to be close to zero such that you feel that you know

1:15:12.460,1:15:18.910
fear with little bubbles and then later on given that now all these fears are

1:15:18.910,1:15:23.469
within this bubble you can simple from you know a Gaussian distribution like a

1:15:23.469,1:15:30.610
normal distribution from the same size so that's why we use the KL the part

1:15:30.610,1:15:34.780
that allows you to be you know if you if you have like if you put a lot of

1:15:34.780,1:15:39.160
strength on the KL you are going to be also reducing the amount of information

1:15:39.160,1:15:47.260
so you can use that KL as a term to reduce the amount of information coming

1:15:47.260,1:15:53.410
from the future so that's definitely one way of regularizing the the latent

1:15:53.410,1:15:59.290
variable the other way is also the other the other trick we are using is the drop

1:15:59.290,1:16:03.340
out streak for the latent which is sometimes you know sampling completely

1:16:03.340,1:16:11.140
from the prior and such that now the backbone the the big massive network the

1:16:11.140,1:16:16.239
deterministic one cannot rely on information it is you know not always

1:16:16.239,1:16:22.150
present right okay and all this happen in the training stage of the predictive

1:16:22.150,1:16:30.820
model yeah okay okay yeah thank you thank you so much yeah yeah could you so

1:16:30.820,1:16:37.510
regarding that forward model uncertainty is that uncertainty that comes from like

1:16:37.510,1:16:44.860
trying to predict more in the future or so the uncertainty there I I think I

1:16:44.860,1:16:49.510
went a little faster the uncertainty coming there is the fact that you know

1:16:49.510,1:16:55.150
whenever you have your policy trying to control the other guy you can

1:16:55.150,1:16:59.560
you know press a lot the acceleration or you can you know steer completely like

1:16:59.560,1:17:03.070
crazy and if you give yeah you know some

1:17:03.070,1:17:09.640
actions that are far from the training domain of that policy different policies

1:17:09.640,1:17:16.360
will reply will will respond in a different way right because if you train

1:17:16.360,1:17:24.160
several models on the same training data and then you test this multiple models

1:17:24.160,1:17:28.270
train on this data outside the training interval let's say let's

1:17:28.270,1:17:34.120
say this is oh I cannot speak with a pen in the mouth interval right and then we

1:17:34.120,1:17:39.010
had the you know a your test on our fryer here right so if you in this

1:17:39.010,1:17:43.630
training interval here all the model will agree and therefore the variance

1:17:43.630,1:17:49.750
across this training interval will be very tiny as you go away from the

1:17:49.750,1:17:55.060
training interval the variance will increase right the nice part is now that

1:17:55.060,1:18:00.160
given that this variance is you know it's a scalar value you can minimize the

1:18:00.160,1:18:06.790
variance by having the action going from clear draw here because here the

1:18:06.790,1:18:09.160
variance is tiny right the variance here is minimum and

1:18:09.160,1:18:14.920
here the variance is very large so you can have actions that might migrate in

1:18:14.920,1:18:21.820
your action space such that the variance is minimized right so your variance now

1:18:21.820,1:18:30.610
is your loss you do gradient descent in action space for variance minimization

1:18:30.610,1:18:35.110
and therefore you can travel no in this action space which actually is to the

1:18:35.110,1:18:38.260
arc is actually two dimensional right it's so cool so you have you know X

1:18:38.260,1:18:43.630
Direction y direction you have the day the hill in this case you can plot the

1:18:43.630,1:18:48.400
variance right so if the variance goes up here you can go in the center down

1:18:48.400,1:18:54.190
such that in this region where the variance is low you know that all these

1:18:54.190,1:19:00.190
different models will agree for the future predictions therefore and those

1:19:00.190,1:19:09.970
actions are coming from you know safe realm right the actions are now coming

1:19:09.970,1:19:14.560
from more like the training points rather than outside point a training

1:19:14.560,1:19:18.490
region let's say this way yeah right okay thank you

1:19:18.490,1:19:23.140
of course more questions ask me everything

1:19:23.140,1:19:29.340
Emma Emma how do you say MEA all right mark

1:19:29.340,1:19:34.409
you're done am I gonna be cooking dinner I have one question

1:19:34.409,1:19:39.090
so suppose we change the context of this problem from regression to some

1:19:39.090,1:19:43.020
classification problem for example water predict are we going to crash or not or

1:19:43.020,1:19:46.440
if you want to predict is there high traffic or not then will the

1:19:46.440,1:19:49.290
effectiveness of latent variables still be present

1:19:49.290,1:19:54.510
like would latent variables improve the model even if it were a classification

1:19:54.510,1:19:59.099
setting and more because both from what I've observed it seems like latent

1:19:59.099,1:20:03.929
variables always improve the performance in like a generation when we want to

1:20:03.929,1:20:07.739
generate something like in generate an image or a music synthesis or some

1:20:07.739,1:20:12.150
regression prediction but in a classification setting how effective

1:20:12.150,1:20:18.210
will they be so we are actually working on this right now we are working on

1:20:18.210,1:20:21.020
classification and maybe we can hear about more about

1:20:25.500,1:20:30.059
this next week when we're going to be talking about language this is actually

1:20:30.059,1:20:35.000
was another student of Leon is working on using latent variable models for

1:20:35.000,1:20:41.489
performing classification that allows you to pick for multiple multiple

1:20:41.489,1:20:46.889
options so I guess you're gonna hear about this one

1:20:46.889,1:20:49.760
not not today or you can ask yuan next time but again

1:20:52.380,1:20:58.040
we're gonna be I think we make it we can mention this next week

1:20:59.289,1:21:03.280
okay thanks all right that's it all right have a good night

1:21:06.040,1:21:11.860
have enjoy your meal I stay safe wash your hands and the pen I don't was not

1:21:11.860,1:21:15.340
sanitized but I haven't used it so good point

1:21:15.340,1:21:21.600
don't touch your face don't on stay safe why

1:21:21.600,1:21:26.010
[Music] how can you get more out of this course

1:21:26.010,1:21:30.180
right overall so let me give you a few suggestions first comprehension if

1:21:30.180,1:21:34.110
something was still not clear just ask me the question a section below the

1:21:34.110,1:21:38.340
video I will answer every question so you will get it eventually if you'd like

1:21:38.340,1:21:43.830
to I get more news about the field things I do on in terms of educational

1:21:43.830,1:21:48.300
content and things I find interesting you can follow up on Twitter and there

1:21:48.300,1:21:52.200
you have my handle I've seen that if you'd like to have updates about newer

1:21:52.200,1:21:56.160
videos don't forget to subscribe to the channel and activate the notification

1:21:56.160,1:22:00.780
bell if you actually like this video don't forget to put a thumbs up it helps

1:22:00.780,1:22:05.220
as well recommending this video to other people if you'd like to search the

1:22:05.220,1:22:09.330
content of this lesson we have our English transcription which is connected

1:22:09.330,1:22:13.620
directly to this video so every title in the transcription is clickable if you

1:22:13.620,1:22:16.950
click on the title you get the right director to the correct location on the

1:22:16.950,1:22:20.310
video in the same way each section of the video is the same title is in

1:22:20.310,1:22:25.020
transcription so you can go back and forth maybe English is not your first

1:22:25.020,1:22:30.960
language. Parli italiano? ¿Hables Español? 你会说中文吗 ? speak Korean have no idea how

1:22:30.960,1:22:36.180
to speak Korean well we have several translations of this material available

1:22:36.180,1:22:40.050
on at the website so and we are also looking for more translations if you can

1:22:40.050,1:22:44.910
help as well it's really important that you actually try to do some of the

1:22:44.910,1:22:48.900
exercises and you play around with the notebooks and the source code we

1:22:48.900,1:22:53.130
provided in order to internalize and understand better the concepts we

1:22:53.130,1:22:58.890
explained during the lessons contribute this is really giving you the

1:22:58.890,1:23:03.060
opportunity to show your contribution for example you find some typos in the

1:23:03.060,1:23:07.740
write-up so you find some bugs in the notebooks you can fix those and you know

1:23:07.740,1:23:12.810
be part of this whole project by sending me a poor request on GitHub or letting

1:23:12.810,1:23:18.920
me know otherwise and that was it so see you next time bye bye

0:00:00.000,0:00:04.740
I guess we can get started alright so

0:00:03.659,0:00:07.379
today we're going to talk about back

0:00:04.740,0:00:08.849
prop and I'm sure for some of you a lot

0:00:07.379,0:00:11.790
of this is gonna look familiar

0:00:08.849,0:00:14.719
I'm going to start with some sort of

0:00:11.790,0:00:17.460
ring pressure is about busy concepts and

0:00:14.719,0:00:20.640
in talked about sort of a more general

0:00:17.460,0:00:25.560
formulation of back prop a little later

0:00:20.640,0:00:27.930
and then tomorrow alfredo will go

0:00:25.560,0:00:29.099
through like how you use Otto grad and

0:00:27.930,0:00:33.840
things like this in PI torch

0:00:29.099,0:00:35.910
okay so basic concepts we have

0:00:33.840,0:00:39.540
parameterize models so parameterize

0:00:35.910,0:00:41.579
models are nothing more than functions

0:00:39.540,0:00:44.040
that depends on that depend on two

0:00:41.579,0:00:46.289
parameters an input and a trainable

0:00:44.040,0:00:47.969
parameter and there is no conceptual

0:00:46.289,0:00:50.460
difference between the parameter and the

0:00:47.969,0:00:52.410
input they're just they're both

0:00:50.460,0:00:57.120
parameters of the of the deterministic

0:00:52.410,0:01:01.500
function the the thing is though the

0:00:57.120,0:01:03.210
parameter is shared across training

0:01:01.500,0:01:04.530
samples whereas the samples of course

0:01:03.210,0:01:09.540
are different for every training samples

0:01:04.530,0:01:11.520
so in things like in most deep learning

0:01:09.540,0:01:14.070
for Mark's the parameter is actually

0:01:11.520,0:01:15.540
implicit to the to the parameterize

0:01:14.070,0:01:17.430
function when you call the function you

0:01:15.540,0:01:20.580
don't actually pass the parameter value

0:01:17.430,0:01:23.600
it's sort of stored inside if you if at

0:01:20.580,0:01:28.860
least in the object-oriented versions of

0:01:23.600,0:01:30.600
models but just need to remember that

0:01:28.860,0:01:32.130
you know you have primary trans model is

0:01:30.600,0:01:34.200
just a prioritized function it takes an

0:01:32.130,0:01:37.829
input and it has a parameter vector it

0:01:34.200,0:01:39.810
produces an output in simple supervised

0:01:37.829,0:01:41.549
running this output goes into a cost

0:01:39.810,0:01:44.490
function that compares the output of the

0:01:41.549,0:01:47.159
model with the output you want here is

0:01:44.490,0:01:49.649
called C the prediction the output of

0:01:47.159,0:01:51.720
the module is called Y bar and the C

0:01:49.649,0:01:53.970
function compared Y in Y bar where why

0:01:51.720,0:02:00.090
is he at what you want Y bar is he I put

0:01:53.970,0:02:02.189
the output you get so I'm giving here

0:02:00.090,0:02:03.540
two very simple examples parameterized

0:02:02.189,0:02:06.450
functions which I'm sure you're familiar

0:02:03.540,0:02:07.770
with the first one is a linear model so

0:02:06.450,0:02:09.810
a linear model just computes a weighted

0:02:07.770,0:02:10.720
sum of the components of its input

0:02:09.810,0:02:13.540
vector multiply

0:02:10.720,0:02:15.610
by weights and if you do linear

0:02:13.540,0:02:19.060
regression with queloz

0:02:15.610,0:02:20.500
the the C function is just the squared

0:02:19.060,0:02:25.240
distance you create a distance between

0:02:20.500,0:02:28.060
the Y vector and the Y bar vector when y

0:02:25.240,0:02:31.570
bar can be vectors or scalars or tensors

0:02:28.060,0:02:33.610
or whatever doesn't matter things with

0:02:31.570,0:02:36.550
numbers in them basically or things that

0:02:33.610,0:02:40.180
you can compute distances between that's

0:02:36.550,0:02:42.060
actually all you need technically but

0:02:40.180,0:02:45.280
who needs a slightly more complicated

0:02:42.060,0:02:47.740
prioritized function here down on the

0:02:45.280,0:02:52.990
bottom which actually computes nearest

0:02:47.740,0:02:55.630
neighbor so here there is the input X

0:02:52.990,0:03:01.300
and then W is a matrix each row of the

0:02:55.630,0:03:06.990
matrix is indexed by the index K and to

0:03:01.300,0:03:13.630
compute the output we actually I put the

0:03:06.990,0:03:16.600
number K that corresponds to the row of

0:03:13.630,0:03:18.160
W that is closest to X okay so we

0:03:16.600,0:03:22.060
compute the distance between X and a

0:03:18.160,0:03:24.340
particular row of W which is wk and then

0:03:22.060,0:03:26.530
we value overall case and we figure out

0:03:24.340,0:03:28.450
which of those difference is smallest

0:03:26.530,0:03:33.010
okay

0:03:28.450,0:03:35.140
and we output that K so augment that's

0:03:33.010,0:03:40.269
what the argument function does right it

0:03:35.140,0:03:42.010
returns the the value of the argument

0:03:40.269,0:03:43.600
that minimizes the function right so

0:03:42.010,0:03:46.540
it's a function of K and that returns

0:03:43.600,0:03:49.269
the K that minimizes that function the

0:03:46.540,0:03:50.860
point I'm making with this which is a

0:03:49.269,0:03:54.280
complicated way of explaining nearest

0:03:50.860,0:03:55.810
neighbor is that the the type of

0:03:54.280,0:03:57.220
competition that takes place in your

0:03:55.810,0:03:58.600
parametrize model could be very

0:03:57.220,0:04:00.519
complicated it doesn't have to be just

0:03:58.600,0:04:02.250
like a neuron that something that you

0:04:00.519,0:04:04.420
compute with weighted sums and and

0:04:02.250,0:04:06.370
anomaly era T's it can be something

0:04:04.420,0:04:07.000
complicated and involves a minimization

0:04:06.370,0:04:09.070
of something else

0:04:07.000,0:04:11.920
I mean what could be the minima with

0:04:09.070,0:04:14.820
some function okay and we'll come back

0:04:11.920,0:04:17.820
to this in a few weeks

0:04:14.820,0:04:17.820
yes

0:04:23.550,0:04:30.300
you can denote it in a different way I

0:04:26.229,0:04:34.919
could have written W the W matrix

0:04:30.300,0:04:37.419
multiplied by let's say the Z vector and

0:04:34.919,0:04:41.020
the Z vector would be constrained to be

0:04:37.419,0:04:43.599
a a one hut and in which case we would

0:04:41.020,0:04:46.690
select a column of W and you could do a

0:04:43.599,0:04:47.949
mean over this is a vector okay then it

0:04:46.690,0:04:49.840
would be a different notation but kind

0:04:47.949,0:04:51.460
of a different effect a similar effect

0:04:49.840,0:04:53.860
but then you know I would have to read

0:04:51.460,0:04:56.139
to write to another equation like you

0:04:53.860,0:05:00.190
know Z is one hot and explain what that

0:04:56.139,0:05:03.360
means let me forget about the notation

0:05:00.190,0:05:05.139
you know just remember the fact that

0:05:03.360,0:05:06.970
there could be something complicated

0:05:05.139,0:05:08.289
going on in this parameterize function

0:05:06.970,0:05:12.729
it's not necessarily just you know a

0:05:08.289,0:05:15.210
very simple very simple thing so what

0:05:12.729,0:05:17.440
I've done with this diagram here is that

0:05:15.210,0:05:20.110
I've introduced a sort of way of

0:05:17.440,0:05:21.610
denoting of sort of writing neural nets

0:05:20.110,0:05:25.300
and various other models as block

0:05:21.610,0:05:30.039
diagrams and I'm using three different

0:05:25.300,0:05:33.909
types of symbols here or for really the

0:05:30.039,0:05:35.530
bubbles represent variables the bubbles

0:05:33.909,0:05:37.630
that I've filled up represent variables

0:05:35.530,0:05:39.250
that are observed so X is an observed

0:05:37.630,0:05:40.509
variable is the input to your system so

0:05:39.250,0:05:44.650
you observe it on the training set and

0:05:40.509,0:05:47.349
the test set and whatever Y bar is a

0:05:44.650,0:05:48.909
computed variable so it's it's something

0:05:47.349,0:05:50.919
that's just you know produced by a

0:05:48.909,0:05:52.389
deterministic function you can just

0:05:50.919,0:05:56.860
compute from the observe variable

0:05:52.389,0:05:58.330
through a deterministic function so Y

0:05:56.860,0:06:00.190
similarly is an observed variable

0:05:58.330,0:06:01.389
because on the or it's observed on the

0:06:00.190,0:06:02.740
training sites it's not observed on the

0:06:01.389,0:06:05.710
test set but during training it's

0:06:02.740,0:06:07.750
observed and then you have two types of

0:06:05.710,0:06:10.599
functional modules one type is those

0:06:07.750,0:06:12.569
kind of blue round shaped modules which

0:06:10.599,0:06:18.219
represent deterministic functions and

0:06:12.569,0:06:21.000
the the round side indicates in which

0:06:18.219,0:06:25.060
direction it's easy to compute okay so

0:06:21.000,0:06:26.770
here you can compute Y bar from X it's

0:06:25.060,0:06:29.520
considerably more complicated to compute

0:06:26.770,0:06:29.520
X from Y bar

0:06:30.250,0:06:33.610
if I give you a white bar but you have a

0:06:32.230,0:06:37.870
hard time giving me an X that

0:06:33.610,0:06:40.110
corresponds to it okay and then you have

0:06:37.870,0:06:42.780
another type of module which are usually

0:06:40.110,0:06:45.160
used to represent cost functions

0:06:42.780,0:06:46.830
represented by squares red squares to

0:06:45.160,0:06:49.750
make them more visible in this case and

0:06:46.830,0:06:53.410
they have an implicit output which is a

0:06:49.750,0:06:55.390
scalar output a single number and they

0:06:53.410,0:06:57.490
can take multiple inputs and basically

0:06:55.390,0:07:00.190
compute a single number usually a

0:06:57.490,0:07:02.670
distance between the inputs or something

0:07:00.190,0:07:02.670
similar to that

0:07:03.570,0:07:09.460
so with also basic symbols you can you

0:07:07.060,0:07:12.940
know you can represent sort of standard

0:07:09.460,0:07:14.560
supervised learning systems for those of

0:07:12.940,0:07:17.290
you who are familiar with graphical

0:07:14.560,0:07:19.810
models this this is a similar notation

0:07:17.290,0:07:23.190
that is used in what's called factor

0:07:19.810,0:07:24.940
graphs where the squares where factors

0:07:23.190,0:07:26.140
factor graphs don't have those

0:07:24.940,0:07:27.460
deterministic functions because they

0:07:26.140,0:07:30.250
don't care about in which you know which

0:07:27.460,0:07:38.230
way dependencies can be computed but in

0:07:30.250,0:07:40.960
our case it's really important okay so

0:07:38.230,0:07:42.430
loss functions so loss functions are

0:07:40.960,0:07:45.940
things that we minimize during training

0:07:42.430,0:07:50.320
and these two types of rust is the per

0:07:45.940,0:07:53.380
sample loss so in this case L of X Y W

0:07:50.320,0:07:55.570
so you give it a sample a pair x and y

0:07:53.380,0:07:57.850
and evaluate the parameter and it

0:07:55.570,0:07:59.350
computes just the scalar value okay in

0:07:57.850,0:08:01.870
all case here we use a very simple loss

0:07:59.350,0:08:03.310
which is just equal to the cost module

0:08:01.870,0:08:05.410
the output of the customer you Allah

0:08:03.310,0:08:07.330
will put on top over system this is kind

0:08:05.410,0:08:11.350
of the standard sort of supervised

0:08:07.330,0:08:15.160
learning paradigm here where the loss is

0:08:11.350,0:08:17.080
simply the average I mean the first

0:08:15.160,0:08:19.780
sample loss is just the output of the

0:08:17.080,0:08:23.410
cost function that we we put in it's not

0:08:19.780,0:08:25.090
always the case and then the loss that

0:08:23.410,0:08:26.830
we actually minimize to retraining is

0:08:25.090,0:08:30.430
the average loss over a training set so

0:08:26.830,0:08:34.659
training set s is a set of pairs X Y P Y

0:08:30.430,0:08:36.940
of P for P equals 0 to P minus 1 and the

0:08:34.659,0:08:38.469
overall loss which depends of course on

0:08:36.940,0:08:40.240
the training set and the parameter

0:08:38.469,0:08:42.219
values is the

0:08:40.240,0:08:44.669
average of the per sample loss overall

0:08:42.219,0:08:44.669
the samples

0:08:49.529,0:09:00.509
and I forgot to say in the first son

0:08:53.009,0:09:02.249
here it's X Y belongs to s so machine

0:09:00.509,0:09:04.110
learning is all about optimizing

0:09:02.249,0:09:06.170
functions most of the time minimizing

0:09:04.110,0:09:09.660
functions sometimes maximizing functions

0:09:06.170,0:09:11.759
occasionally finding Nash equilibria

0:09:09.660,0:09:13.170
between two functions as in the case of

0:09:11.759,0:09:17.459
Ganz but most of the time we minimize

0:09:13.170,0:09:19.680
functions and we do this with gradient

0:09:17.459,0:09:21.389
based methods not necessarily gradient

0:09:19.680,0:09:22.680
descent but gradient based methods what

0:09:21.389,0:09:26.819
is a gradient based method gradient

0:09:22.680,0:09:28.470
based method is a method that finds the

0:09:26.819,0:09:29.910
minimum of a function assuming that you

0:09:28.470,0:09:33.059
get easily compute the gradient of that

0:09:29.910,0:09:35.819
function so that assumes the function is

0:09:33.059,0:09:37.290
more or less differentiable it doesn't

0:09:35.819,0:09:40.199
actually have to be everywhere

0:09:37.290,0:09:42.300
differentiable technically it needs to

0:09:40.199,0:09:43.949
be continuous and it needs to be almost

0:09:42.300,0:09:47.009
everywhere differentiable otherwise you

0:09:43.949,0:09:50.990
run into trouble but it can have kinks

0:09:47.009,0:09:50.990
in it as long as they're not too nasty

0:09:51.170,0:09:58.259
and grainy descent of course as you

0:09:54.300,0:09:59.790
probably know consists in computing the

0:09:58.259,0:10:04.559
the gradient so you see a function here

0:09:59.790,0:10:09.000
at the top it's got a minimum at the top

0:10:04.559,0:10:11.370
right i drawn the lines of equal cost of

0:10:09.000,0:10:13.470
that function and the arrows that you

0:10:11.370,0:10:16.819
see are the gradient vectors the

0:10:13.470,0:10:19.199
gradient is pointing up okay at every

0:10:16.819,0:10:22.740
every location and the gradient is

0:10:19.199,0:10:24.059
always orthogonal to the lines of equal

0:10:22.740,0:10:28.079
cost okay

0:10:24.059,0:10:33.569
equal altitude if you want okay so when

0:10:28.079,0:10:35.519
in this set is like you know being in

0:10:33.569,0:10:37.230
the mountain in the fog and it's night

0:10:35.519,0:10:38.370
and you can't see anything but you want

0:10:37.230,0:10:39.720
to go down to the village and so you

0:10:38.370,0:10:41.879
look around you and you look for the

0:10:39.720,0:10:45.149
direction of steepest descent and you

0:10:41.879,0:10:47.399
take a step okay so the algorithm here

0:10:45.149,0:10:50.339
at the top the division vector which is

0:10:47.399,0:10:52.829
your position is replaced by the double

0:10:50.339,0:10:55.139
your current W vector minus some

0:10:52.829,0:10:56.699
constant times the gradient vector and

0:10:55.139,0:11:00.230
the gradient vector points up so when

0:10:56.699,0:11:02.910
you do - you're kind of walking downhill

0:11:00.230,0:11:06.360
in the direction of steepest descent

0:11:02.910,0:11:09.330
now this is if ADA is a scalar constant

0:11:06.360,0:11:13.350
but in sophisticated algorithms that can

0:11:09.330,0:11:15.270
actually be a matrix so if it's a matrix

0:11:13.350,0:11:18.860
if it's a positive semi definite matrix

0:11:15.270,0:11:21.300
it will still go down go downhill

0:11:18.860,0:11:23.580
except that not necessarily in the

0:11:21.300,0:11:24.720
direction of steepest descent in fact

0:11:23.580,0:11:27.450
the direction of steepest descent is not

0:11:24.720,0:11:28.980
necessarily the one you want to go to if

0:11:27.450,0:11:31.980
you have a situation like the one at the

0:11:28.980,0:11:34.200
top where the the value is a little Ella

0:11:31.980,0:11:35.640
gated the gradient actually does not

0:11:34.200,0:11:39.360
point towards the minimum it point

0:11:35.640,0:11:40.920
started you know off off center and so

0:11:39.360,0:11:42.030
it should go directly to the minimum you

0:11:40.920,0:11:43.200
don't want to follow the gradient you

0:11:42.030,0:11:47.490
want to be a little smarter than this

0:11:43.200,0:11:49.950
and by using sort of idea that all

0:11:47.490,0:11:52.950
matrices you can actually you could in

0:11:49.950,0:11:57.270
principle do this with so called second

0:11:52.950,0:12:01.470
order methods which are still gradient

0:11:57.270,0:12:03.420
based methods but they're kind of

0:12:01.470,0:12:06.210
impractical in most cases we'll talk

0:12:03.420,0:12:10.560
about some issues with this in a few

0:12:06.210,0:12:13.320
weeks now there are algorithms that are

0:12:10.560,0:12:15.060
not gradient based optimization

0:12:13.320,0:12:17.550
algorithms are not gradient based so

0:12:15.060,0:12:19.920
when your function is not differentiable

0:12:17.550,0:12:22.020
when it's like a golf course you know

0:12:19.920,0:12:25.080
it's flat and it's got a hole in it or

0:12:22.020,0:12:25.980
when it's kind of staircase like where

0:12:25.080,0:12:28.380
the gradient doesn't give you an

0:12:25.980,0:12:31.590
information useful information or when

0:12:28.380,0:12:34.320
it may be differentiable but you don't

0:12:31.590,0:12:35.610
know the function you don't you know you

0:12:34.320,0:12:37.800
don't write you didn't write the program

0:12:35.610,0:12:39.930
that actually computes it because that

0:12:37.800,0:12:43.890
function might might be the entire

0:12:39.930,0:12:46.230
environment around you then you cannot

0:12:43.890,0:12:49.070
compute the gradient efficiently ok so

0:12:46.230,0:12:52.680
then you have to resort to other methods

0:12:49.070,0:12:55.920
methods are called zeroth order methods

0:12:52.680,0:12:57.270
or gradient free methods and there's a

0:12:55.920,0:13:00.630
whole bunch of whole family of those

0:12:57.270,0:13:03.540
functions of those methods which I'm not

0:13:00.630,0:13:04.950
going to talk about ok at all

0:13:03.540,0:13:09.060
deep learning is all about gradient

0:13:04.950,0:13:10.170
based methods that said if you're

0:13:09.060,0:13:12.210
interested in reinforcement learning

0:13:10.170,0:13:13.709
most of reinforcement learning actually

0:13:12.210,0:13:16.410
uses

0:13:13.709,0:13:18.899
great at estimation without the gradient

0:13:16.410,0:13:23.790
what you want is I don't know you want

0:13:18.899,0:13:27.380
to get a robot to learn to ride a bike

0:13:23.790,0:13:30.360
and once in a while the robot Falls and

0:13:27.380,0:13:32.940
you don't have a gradient for the

0:13:30.360,0:13:34.889
objective function that says don't fall

0:13:32.940,0:13:37.980
or the objective function that measures

0:13:34.889,0:13:41.160
how long you the bike stays up without

0:13:37.980,0:13:43.649
falling nobody tells you what to do to

0:13:41.160,0:13:45.570
minimize that cost function right so you

0:13:43.649,0:13:48.510
try things you can't compute a gradient

0:13:45.570,0:13:50.010
of that function okay so in RL your cost

0:13:48.510,0:13:52.320
function is not differentiable most of

0:13:50.010,0:13:53.519
the time but the network that computes

0:13:52.320,0:13:57.389
the output that goes into the

0:13:53.519,0:13:59.970
environment is differentiable okay so

0:13:57.389,0:14:02.130
from that point on that's that's great

0:13:59.970,0:14:02.610
in this only the cost is not

0:14:02.130,0:14:05.910
differentiable

0:14:02.610,0:14:08.970
okay so there'll be a situation like the

0:14:05.910,0:14:12.149
diagram I showed earlier imagine here

0:14:08.970,0:14:13.680
that G is differentiable you can compute

0:14:12.149,0:14:15.209
the gradient of the output of G with

0:14:13.680,0:14:16.680
respect to the parameters and its input

0:14:15.209,0:14:18.120
and everything but C is not

0:14:16.680,0:14:20.010
differentiable in fact it's completely

0:14:18.120,0:14:21.839
unknown the only thing you know about C

0:14:20.010,0:14:23.550
is that if you give it a Y bar and Y it

0:14:21.839,0:14:26.010
tells you the value but doesn't give you

0:14:23.550,0:14:27.990
the gradient okay that's kind of what RL

0:14:26.010,0:14:29.880
is there are the things about RL okay

0:14:27.990,0:14:32.850
but that's the basic difference between

0:14:29.880,0:14:37.850
reinforcement learning and supervised

0:14:32.850,0:14:37.850
learning yeah

0:14:43.930,0:14:48.410
well the reward is just the output of C

0:14:46.370,0:14:52.040
that's all okay so it's use a black box

0:14:48.410,0:14:53.839
and what you get is the output of C you

0:14:52.040,0:14:55.250
don't you don't get a Y either right so

0:14:53.839,0:14:58.399
you're not being told what the correct

0:14:55.250,0:15:00.680
answer is you just talk about black box

0:14:58.399,0:15:02.060
you give it a white bar and it gives

0:15:00.680,0:15:05.029
your sink that's it

0:15:02.060,0:15:10.730
you can't compute the gradient of C with

0:15:05.029,0:15:12.230
respect to Y bar that's right so what

0:15:10.730,0:15:13.819
you do is you change Y by a little bit

0:15:12.230,0:15:16.100
and you see the C go up or down

0:15:13.819,0:15:19.819
two goes down you kind of reinforce that

0:15:16.100,0:15:21.769
he goes up you do something else okay so

0:15:19.819,0:15:23.689
basically you're telling the system how

0:15:21.769,0:15:25.550
good it is doing without turning it the

0:15:23.689,0:15:28.180
correct answer and it doesn't have

0:15:25.550,0:15:30.829
access to acquit yeah

0:15:28.180,0:15:34.129
okay so what that tells you is that RL

0:15:30.829,0:15:35.779
is horribly inefficient right because

0:15:34.129,0:15:41.120
you don't have a gradient so you have to

0:15:35.779,0:15:43.339
try you know if the if the output Y bar

0:15:41.120,0:15:46.550
is row dimensional then you know it's

0:15:43.339,0:15:48.920
okay right you can you can try to make

0:15:46.550,0:15:51.560
it larger smaller things like that but

0:15:48.920,0:15:54.889
it's not too bad if Y bar is a high

0:15:51.560,0:15:56.959
dimensional vector there's like such a

0:15:54.889,0:16:00.259
huge space to search it's probably no

0:15:56.959,0:16:02.209
way you're gonna find an optimal value

0:16:00.259,0:16:06.740
for it unless you try lots and lots of

0:16:02.209,0:16:10.370
different times right so that's a huge

0:16:06.740,0:16:12.439
problem with RL and I would take weeks

0:16:10.370,0:16:14.839
in RL actually I'm not gonna talk about

0:16:12.439,0:16:18.649
hourly this course actually except today

0:16:14.839,0:16:21.290
maybe but a very properly technique in

0:16:18.649,0:16:23.899
RL is so-called actual critic methods

0:16:21.290,0:16:27.050
and a critic method basically consists

0:16:23.899,0:16:29.959
in having a second C module which you

0:16:27.050,0:16:32.059
know which is a trainable module and you

0:16:29.959,0:16:34.699
train you see your own C module which is

0:16:32.059,0:16:36.769
differentiable to approximate the one to

0:16:34.699,0:16:39.439
approximate the cost function the value

0:16:36.769,0:16:42.949
function that you get the rewards the

0:16:39.439,0:16:45.230
reward function you get so reward is the

0:16:42.949,0:16:49.730
inverse of a cost okay so I mean this is

0:16:45.230,0:16:52.490
a negative of a cost and you get more

0:16:49.730,0:16:53.900
like your punishment actually but so

0:16:52.490,0:16:56.050
then that that's a way

0:16:53.900,0:16:58.250
of making the cost function

0:16:56.050,0:16:59.870
differentiable or at least approximating

0:16:58.250,0:17:02.600
it by a differentiable function and that

0:16:59.870,0:17:08.690
you can just use back pop so the you

0:17:02.600,0:17:11.839
know AC a AC and a AAA C are versions of

0:17:08.690,0:17:21.110
this actor critic Adventures actor

0:17:11.839,0:17:23.410
critic etc okay right so what you have

0:17:21.110,0:17:28.670
to know how to do is compute the

0:17:23.410,0:17:30.080
gradient of your per sample you of your

0:17:28.670,0:17:32.690
objective function with respect to the

0:17:30.080,0:17:36.110
parameters in practice we'll use the

0:17:32.690,0:17:38.720
caste gradient as you probably aware so

0:17:36.110,0:17:40.250
instead of computing the gradient of the

0:17:38.720,0:17:41.900
entire objective function which is the

0:17:40.250,0:17:48.650
average of all samples we just take one

0:17:41.900,0:17:49.670
sample compute the loss L Big L compute

0:17:48.650,0:17:51.860
the gradient of these charts with

0:17:49.670,0:17:53.660
respect to their parameters and then

0:17:51.860,0:18:00.200
take one step in the negative gradient

0:17:53.660,0:18:02.960
direction okay so that's the the second

0:18:00.200,0:18:05.090
formula here W is replaced by W minus

0:18:02.960,0:18:07.820
some step size times the gradient of the

0:18:05.090,0:18:12.970
per sample loss function with respect to

0:18:07.820,0:18:12.970
the parameter for a given sample XP YP

0:18:13.840,0:18:32.960
yep yep okay so in practice people use

0:18:29.780,0:18:35.300
batches so instead of doing this on a

0:18:32.960,0:18:36.560
single sample so first of all if you do

0:18:35.300,0:18:38.600
this on a single sample you're going to

0:18:36.560,0:18:40.070
get a very noisy trajectory you're gonna

0:18:38.600,0:18:42.220
get trajectory like the one you see here

0:18:40.070,0:18:44.720
at the bottom where instead of the

0:18:42.220,0:18:47.300
parameter vector directly kind of going

0:18:44.720,0:18:50.090
downhill it's going to oscillate so some

0:18:47.300,0:18:51.590
people say it should not be called SGD

0:18:50.090,0:18:53.030
which means to get the gradient descent

0:18:51.590,0:18:54.500
because it's not actually a descent

0:18:53.030,0:18:56.390
algorithm should be called stochastic

0:18:54.500,0:18:59.390
gradient optimization but it's

0:18:56.390,0:19:00.690
stochastic so it's very noisy every

0:18:59.390,0:19:02.250
sample you get is going to

0:19:00.690,0:19:03.990
in a different direction it's just the

0:19:02.250,0:19:08.160
average that pulls you towards the

0:19:03.990,0:19:09.780
minimum of the average so it looks

0:19:08.160,0:19:12.240
inefficient but in fact it's actually

0:19:09.780,0:19:14.040
fast it's much faster than batch

0:19:12.240,0:19:16.770
gradient at least in the context of

0:19:14.040,0:19:19.830
machine learning when the samples had

0:19:16.770,0:19:22.770
some redundancy between them and it goes

0:19:19.830,0:19:24.510
faster to do stochastic gradient so to

0:19:22.770,0:19:26.070
the the question of batching so what

0:19:24.510,0:19:28.440
people do most of the time is that they

0:19:26.070,0:19:30.510
compute the average of the gradient over

0:19:28.440,0:19:34.920
a batch of samples not a single sample

0:19:30.510,0:19:37.950
and then do one step and the only reason

0:19:34.920,0:19:44.300
for doing this this has nothing to do

0:19:37.950,0:19:46.500
with you know algorithmic convergence

0:19:44.300,0:19:48.900
efficacy or do you think the only reason

0:19:46.500,0:19:51.360
for doing this is because the kind of

0:19:48.900,0:19:55.500
hardware we that is given to us there

0:19:51.360,0:19:59.940
are is our disposal GPUs and multi-core

0:19:55.500,0:20:02.910
CPUs is more efficient if you if you

0:19:59.940,0:20:05.130
have batches it's easier to paralyze so

0:20:02.910,0:20:07.920
you get more efficient computation use

0:20:05.130,0:20:10.050
of your hardware if you use batches it's

0:20:07.920,0:20:13.200
a bad reason for batching but we have no

0:20:10.050,0:20:14.730
choice until you know someone builds a

0:20:13.200,0:20:19.080
piece of hardware that actually is

0:20:14.730,0:20:21.780
properly designed the reason we have to

0:20:19.080,0:20:24.050
do this again is for is because the

0:20:21.780,0:20:27.540
chips that we have you know in DDR GPUs

0:20:24.050,0:20:31.440
are so you know heavily paralyzed but

0:20:27.540,0:20:37.400
they depart lies in a simple way and the

0:20:31.440,0:20:37.400
simplest way to paralyze is to batch yes

0:20:40.700,0:20:48.630
yeah but okay so so here is why

0:20:46.320,0:20:51.330
stochastic gradient is is better right

0:20:48.630,0:20:53.790
if I give you a million samples but in

0:20:51.330,0:20:56.310
these million samples I actually only

0:20:53.790,0:20:58.290
have actually 10,000 different samples

0:20:56.310,0:21:00.270
and I repeat those 10,000 samples 100

0:20:58.290,0:21:01.950
times and I shuffle them and I give you

0:21:00.270,0:21:03.930
this training sample and I give you it's

0:21:01.950,0:21:06.000
a million samples you don't know it's

0:21:03.930,0:21:08.040
actually only 10,000 samples repeated

0:21:06.000,0:21:08.250
100 times if you use batch gradient you

0:21:08.040,0:21:09.840
can

0:21:08.250,0:21:12.930
compute a hundred times the same

0:21:09.840,0:21:14.970
quantities and average them so gonna

0:21:12.930,0:21:17.490
spend how many times more computation

0:21:14.970,0:21:19.100
than necessary well she used to catch

0:21:17.490,0:21:21.150
the gradient by the time you've seen

0:21:19.100,0:21:23.370
20,000 samples you voted on two

0:21:21.150,0:21:26.340
iterations your two passes through your

0:21:23.370,0:21:28.440
entire training set okay so it'll be at

0:21:26.340,0:21:31.620
least a hundred times more efficient so

0:21:28.440,0:21:34.890
the question is you know what's you know

0:21:31.620,0:21:38.820
how much can you average in a batch

0:21:34.890,0:21:40.410
without using you know efficiency in the

0:21:38.820,0:21:41.880
redundancy and some people have done

0:21:40.410,0:21:44.970
experiments with this is some empirical

0:21:41.880,0:21:48.480
evidence that the number of samples that

0:21:44.970,0:21:50.730
you can put in a batch is roughly equal

0:21:48.480,0:21:53.040
to the number of categories you have if

0:21:50.730,0:21:54.930
you do classification between one and

0:21:53.040,0:21:56.730
two times the number of categories you

0:21:54.930,0:21:59.820
have so if you train on imagenet you

0:21:56.730,0:22:01.560
have 1,000 categories you can you can

0:21:59.820,0:22:04.160
have batches up to about two thousand

0:22:01.560,0:22:13.680
and beyond this you start losing

0:22:04.160,0:22:15.360
conversion speed that means are

0:22:13.680,0:22:17.670
completely random basically I mean that

0:22:15.360,0:22:19.470
never occurs right because think about

0:22:17.670,0:22:21.720
okay think about the refirming scenario

0:22:19.470,0:22:23.310
that you can be that training set and

0:22:21.720,0:22:25.500
what I do is that I split it in half and

0:22:23.310,0:22:27.390
I use the first as a training set and

0:22:25.500,0:22:29.010
the second one the second half as the

0:22:27.390,0:22:32.250
validation set if there is zero we

0:22:29.010,0:22:33.990
doesn't see in your in your in your set

0:22:32.250,0:22:36.980
that means my machine is not gonna work

0:22:33.990,0:22:39.720
at all right it's not gonna be able to

0:22:36.980,0:22:41.430
generalize on the second half right so

0:22:39.720,0:22:43.110
if there is any possibility of

0:22:41.430,0:22:56.320
generalization there has to be some

0:22:43.110,0:22:57.759
redundancy okay so let's start

0:22:56.320,0:23:01.269
and talk about traditional neural nets

0:22:57.759,0:23:04.809
okay traditional neural nets are

0:23:01.269,0:23:06.730
basically interspersed layers of linear

0:23:04.809,0:23:11.259
operations and point where's nonlinear

0:23:06.730,0:23:13.090
operations so the linear operation you

0:23:11.259,0:23:16.269
have an input vector you compute a

0:23:13.090,0:23:18.759
weighted sum of that vector with a bunch

0:23:16.269,0:23:21.820
of weights and in this case here we have

0:23:18.759,0:23:23.320
six inputs with three hidden units in

0:23:21.820,0:23:25.480
the first layer so there's three

0:23:23.320,0:23:28.320
different sets of ways with which we

0:23:25.480,0:23:31.750
compute weighted sums of the six inputs

0:23:28.320,0:23:34.000
the conceptually the operation to go

0:23:31.750,0:23:36.159
from the input the six dimensional input

0:23:34.000,0:23:37.570
vector to the three dimensional weighted

0:23:36.159,0:23:39.340
sum is just a matrix vector

0:23:37.570,0:23:43.179
multiplication right take the input

0:23:39.340,0:23:46.090
vector multiplied by a matrix form by

0:23:43.179,0:23:48.639
the weights it's going to be a three by

0:23:46.090,0:23:50.529
six matrix right and so you multiply

0:23:48.639,0:23:54.730
this by six dimensional vector you get a

0:23:50.529,0:23:56.200
three dimensional vector okay so that's

0:23:54.730,0:23:57.759
the first type of operation in a case

0:23:56.200,0:23:59.350
called neural net and the second type of

0:23:57.759,0:24:01.389
operation is that you take all the

0:23:59.350,0:24:03.029
components of the the vector the

0:24:01.389,0:24:05.559
weighted sums and you pass them through

0:24:03.029,0:24:07.240
simple nonlinearities in this case this

0:24:05.559,0:24:11.500
is called a value it's called a half

0:24:07.240,0:24:12.789
wave rectification in engineering you

0:24:11.500,0:24:15.000
know there's different names for it but

0:24:12.789,0:24:20.500
basically it's the positive part

0:24:15.000,0:24:22.960
mathematically so it's equal to identity

0:24:20.500,0:24:25.269
when X when the argument is positive and

0:24:22.960,0:24:28.889
is equal to zero and the argument is

0:24:25.269,0:24:31.779
negative and then you repeat the process

0:24:28.889,0:24:33.519
so the third stage is again a linear

0:24:31.779,0:24:35.649
stage multiply that three dimensional

0:24:33.519,0:24:37.330
vector by a matrix in this case a two by

0:24:35.649,0:24:39.029
three matrix you get a two dimensional

0:24:37.330,0:24:44.169
vector pass the components two

0:24:39.029,0:24:45.759
nonlinearities okay I could insert a two

0:24:44.169,0:24:48.450
layer Network because I think what

0:24:45.759,0:24:50.679
matters are the pairs linear non linear

0:24:48.450,0:24:52.509
okay so most people recall this a two

0:24:50.679,0:24:54.070
layer Network some people recall it's a

0:24:52.509,0:24:55.960
3 layer Network because they count the

0:24:54.070,0:24:58.360
variables but I don't think that's fair

0:24:55.960,0:25:00.990
you do this but you know what you don't

0:24:58.360,0:25:00.990
want to do this

0:25:01.680,0:25:07.080
if there is no nonlinearities in the in

0:25:04.080,0:25:10.110
the middle as I said last week you might

0:25:07.080,0:25:12.780
as well have a single layer because the

0:25:10.110,0:25:14.370
product of two linear functions is a

0:25:12.780,0:25:17.430
linear function and so you can collapse

0:25:14.370,0:25:19.140
them into a single one basically a

0:25:17.430,0:25:29.670
single matrix that is the product of the

0:25:19.140,0:25:38.340
two matrices okay so here it is a little

0:25:29.670,0:25:42.080
more detail the sum of unit I is so s I

0:25:38.340,0:25:45.210
which is a weighted sum for unit I is is

0:25:42.080,0:25:50.190
the sum over all the predecessors of I

0:25:45.210,0:25:51.870
which is denoted up of I okay so the J

0:25:50.190,0:25:55.490
index goes over all the predecessors of

0:25:51.870,0:25:57.750
I of W IJ x zj where Z J is the output

0:25:55.490,0:26:00.360
the J side pull from the previous layer

0:25:57.750,0:26:05.490
this is a sort of stack you know kind of

0:26:00.360,0:26:08.250
regular layered neural net and then you

0:26:05.490,0:26:10.020
take a particular si and you pass it

0:26:08.250,0:26:14.240
through one of those nonlinear functions

0:26:10.020,0:26:14.240
F and you get Zi

0:26:21.110,0:26:25.400
talk about early computer gradients and

0:26:24.680,0:26:28.310
things like this

0:26:25.400,0:26:29.450
and these two forms of it okay there is

0:26:28.310,0:26:32.600
an intuitive form that I'm going to

0:26:29.450,0:26:34.550
explain right now which does not even

0:26:32.600,0:26:37.880
require you to know what a derivative is

0:26:34.550,0:26:40.370
funnily enough and then there is a

0:26:37.880,0:26:41.900
slightly more general form there's an

0:26:40.370,0:26:46.340
even more general form that maybe I'll

0:26:41.900,0:26:48.770
talk about next week okay so let's say

0:26:46.340,0:26:53.480
we have a big network we have a cost

0:26:48.770,0:26:55.280
function and so or thing has an X on a Y

0:26:53.480,0:26:56.720
and it's gonna cost coming out but in

0:26:55.280,0:26:58.130
fact you don't need to make this

0:26:56.720,0:26:59.510
assumption the only assumption you need

0:26:58.130,0:27:01.190
to make is that you have some primary

0:26:59.510,0:27:02.240
choice function that produces a scalar

0:27:01.190,0:27:07.760
on the output that's it

0:27:02.240,0:27:09.980
okay and somewhere in that network you

0:27:07.760,0:27:12.440
have a nonlinear function H I called it

0:27:09.980,0:27:15.140
F in the previous slide but i call it h

0:27:12.440,0:27:16.700
here so it takes a one of those weighted

0:27:15.140,0:27:18.410
sum s you pass it through this H

0:27:16.700,0:27:20.960
function and then it produces one of

0:27:18.410,0:27:22.430
those Z Z variables okay I'm not putting

0:27:20.960,0:27:28.370
an index here because you know it's just

0:27:22.430,0:27:29.630
I'm taking one particular you know one

0:27:28.370,0:27:31.730
of those functions outside of the

0:27:29.630,0:27:34.670
network and I do the the rest of the

0:27:31.730,0:27:36.950
network as kind of a black box so let's

0:27:34.670,0:27:38.720
assume that okay so we're going to use

0:27:36.950,0:27:47.270
chain rule chain rule if you remember

0:27:38.720,0:27:52.430
from kindergarten okay high school okay

0:27:47.270,0:27:55.430
college if you have two functions that

0:27:52.430,0:27:58.310
fill each other your G of H of s and you

0:27:55.430,0:28:02.060
want to differentiate it so G of H of s

0:27:58.310,0:28:04.100
prime is equal to the derivative of G at

0:28:02.060,0:28:08.110
point H of s multiplied by the

0:28:04.100,0:28:11.530
derivative of H at Point s all right

0:28:08.110,0:28:11.530
this going back

0:28:12.250,0:28:19.330
okay but if you want through you know

0:28:17.530,0:28:22.210
couple years of college you can write

0:28:19.330,0:28:26.289
this the way Newton wrote it or earlier

0:28:22.210,0:28:29.350
or whatever with the infinitesimal

0:28:26.289,0:28:31.950
quantities so you can write D C over D s

0:28:29.350,0:28:35.230
which means the derivative at 0 s by 2 s

0:28:31.950,0:28:37.780
is equal to D C over DZ times DZ over

0:28:35.230,0:28:40.390
the s ok so it's the derivative of C

0:28:37.780,0:28:42.280
with respect to Z Z multiplied by the

0:28:40.390,0:28:44.679
derivative of Z with respect to s and

0:28:42.280,0:28:46.750
the nice the nice reason for writing it

0:28:44.679,0:28:48.760
like this is that it's obvious you can

0:28:46.750,0:28:52.330
simplify by DZ right you have dizzy at

0:28:48.760,0:28:54.100
the bottom and at the top and so by

0:28:52.330,0:28:57.970
simplifying by DZ this is the second

0:28:54.100,0:29:01.000
line you get DC over the s ok so you

0:28:57.970,0:29:02.409
kind of split the derivative by having

0:29:01.000,0:29:03.669
certain intermediate variable that you

0:29:02.409,0:29:06.370
put at the bottom and at the top right

0:29:03.669,0:29:09.880
it's very simple manipulation simple

0:29:06.370,0:29:12.429
equally palacios now DZ over D s is just

0:29:09.880,0:29:14.919
the derivative of Z with respect to s

0:29:12.429,0:29:20.440
with Z is equal to H of s so that's just

0:29:14.919,0:29:24.669
H prime of s ok so D Co VDS is equal to

0:29:20.440,0:29:27.010
DT over D G times H prime of s so if if

0:29:24.669,0:29:32.230
someone gives you the derivative of the

0:29:27.010,0:29:34.539
cost function with respect to Z you

0:29:32.230,0:29:36.220
multiplied by the derivative of your

0:29:34.539,0:29:37.450
nonlinear function and you get the

0:29:36.220,0:29:41.559
derivative of the cost function with

0:29:37.450,0:29:43.780
respect to s ok so imagine you have a

0:29:41.559,0:29:46.990
chain of those functions in your network

0:29:43.780,0:29:49.090
you can back propagate by multiplying by

0:29:46.990,0:29:51.789
the derivatives of all those H functions

0:29:49.090,0:29:55.960
H functions when after the other all the

0:29:51.789,0:29:58.090
way back to the to the bottom ok so

0:29:55.960,0:29:58.720
basically if you want to compute a

0:29:58.090,0:30:01.510
gradient

0:29:58.720,0:30:03.460
you basically have to use a network that

0:30:01.510,0:30:04.960
looks very much like this one except you

0:30:03.460,0:30:07.120
have signals that go backwards and

0:30:04.960,0:30:10.419
wherever you had an H function what you

0:30:07.120,0:30:13.720
have now is derivative coming from the

0:30:10.419,0:30:15.700
top ok so scalar just the same as e

0:30:13.720,0:30:18.669
you're multiplied by the derivative of

0:30:15.700,0:30:20.140
the H function and then you get the

0:30:18.669,0:30:23.409
derivative of the cost function with

0:30:20.140,0:30:24.450
respect to the input variable to H which

0:30:23.409,0:30:26.980
is s

0:30:24.450,0:30:28.840
so basically what you have now is a

0:30:26.980,0:30:34.360
basically a transform network that

0:30:28.840,0:30:36.700
computes your gradient okay now you can

0:30:34.360,0:30:39.100
convince yourself of this because if you

0:30:36.700,0:30:43.650
don't really sort of completely grok

0:30:39.100,0:30:46.510
chain world which I hope you do but

0:30:43.650,0:30:49.960
imagine that you are you know twiddling

0:30:46.510,0:30:53.400
s by little okay we're going to we're

0:30:49.960,0:30:57.160
going to perturb s by Diaz

0:30:53.400,0:31:00.880
so as we go through h h as a stroke

0:30:57.160,0:31:03.580
which is s prime of s so Z is going to

0:31:00.880,0:31:05.850
be perturbed by D s times that

0:31:03.580,0:31:10.560
derivative right D s times H prime of s

0:31:05.850,0:31:14.020
okay this is what is being written here

0:31:10.560,0:31:19.060
for tubing s by D s were perturb Z by DZ

0:31:14.020,0:31:22.810
equal D s times H prime of s this will

0:31:19.060,0:31:24.910
perturb CV by the small perturbation DZ

0:31:22.810,0:31:27.220
times the gradient of I mean the

0:31:24.910,0:31:31.090
derivative of C with respect to Z right

0:31:27.220,0:31:34.690
which is DC or dizzy so basically we get

0:31:31.090,0:31:36.610
that DC equals DZ the perturbation of Z

0:31:34.690,0:31:38.740
so patch will show C is equal to the

0:31:36.610,0:31:41.080
perturbation with Z DZ times the

0:31:38.740,0:31:45.360
gradient of other derivative of single

0:31:41.080,0:31:48.190
spec to Z okay but we've computed this

0:31:45.360,0:31:52.180
DZ before we know is the s time H prime

0:31:48.190,0:31:56.050
of s so we just substitute in and then

0:31:52.180,0:31:59.200
we pass the s on the other side and we

0:31:56.050,0:32:01.450
get simply that DT over D is equal to C

0:31:59.200,0:32:03.220
over DZ times H prime of s okay we just

0:32:01.450,0:32:05.380
we derive chain rule okay I've done

0:32:03.220,0:32:06.580
nothing more than really IV general but

0:32:05.380,0:32:08.440
it's a lot more intuitive if you think

0:32:06.580,0:32:10.480
of it in terms of you know twitting

0:32:08.440,0:32:11.650
things around and sometimes it's useful

0:32:10.480,0:32:15.180
when you're writing the backup function

0:32:11.650,0:32:15.180
for a module to think in those terms

0:32:15.270,0:32:19.240
okay because sometimes it's easier to

0:32:17.950,0:32:21.630
think about it in those terms than to

0:32:19.240,0:32:26.380
actually write down the equations

0:32:21.630,0:32:28.270
alright now we had two types of modules

0:32:26.380,0:32:30.750
in orbit you know or you know on that

0:32:28.270,0:32:34.810
the other one is a linear module okay

0:32:30.750,0:32:35.960
and for this one I'm gonna again use the

0:32:34.810,0:32:40.190
internet work as a black

0:32:35.960,0:32:42.890
except for just three connections going

0:32:40.190,0:32:46.760
from AZ variable to a bunch of s

0:32:42.890,0:32:50.330
variables okay so an SRO is a weighted

0:32:46.760,0:32:53.000
sum so s 0 for example is going to take

0:32:50.330,0:32:54.950
Z the Z at the bottom here by

0:32:53.000,0:32:56.420
multiplying it but it's only W which I

0:32:54.950,0:32:59.530
call W 0 okay

0:32:56.420,0:33:02.540
I draw up all indices that are annoying

0:32:59.530,0:33:07.990
and so then I can ask again the question

0:33:02.540,0:33:15.740
if I if I twiddle Z by how much will CB

0:33:07.990,0:33:18.080
twiddled ok so if I twiddle Z s 0 is

0:33:15.740,0:33:20.690
going to be twiddle by Z times W 0 right

0:33:18.080,0:33:24.920
because Z is multiplied by W 0 so if I

0:33:20.690,0:33:29.390
so it W 0 is is 2 and I twiddle Z by by

0:33:24.920,0:33:32.120
DZ the output after the weight is going

0:33:29.390,0:33:36.020
to be twice okay it's gonna be trolled

0:33:32.120,0:33:38.000
by twice the value but now Z actually

0:33:36.020,0:33:42.110
influences several variables in this

0:33:38.000,0:33:44.180
case 3 so it's also going to cause a

0:33:42.110,0:33:46.360
probation for s of 1 and s of to the

0:33:44.180,0:33:50.420
prohibition for s oh one is going to be

0:33:46.360,0:34:00.190
d ZW 1 and for s of 2 is going to be d

0:33:50.420,0:34:00.190
ZW 2 okay so the overall perturbation

0:34:01.000,0:34:10.310
now okay so so we get D Z times W 0 that

0:34:07.250,0:34:14.210
is the prohibition for a 0 DZ times W 1

0:34:10.310,0:34:17.360
for s 1 DZ times W 2 4 is 2 but now s 0

0:34:14.210,0:34:22.010
s 1 and s 2 are going to influence C and

0:34:17.360,0:34:24.890
the question is by how much so C is

0:34:22.010,0:34:27.230
going to vary by whatever a 0 was

0:34:24.890,0:34:32.900
varying times the derivative of C with

0:34:27.230,0:34:34.760
respect to a 0 right but also C is also

0:34:32.900,0:34:36.830
going to vary because s 1 is varying and

0:34:34.760,0:34:38.330
also because s 2 is varying if the

0:34:36.830,0:34:39.360
variations are small enough then the

0:34:38.330,0:34:41.460
overall value

0:34:39.360,0:34:44.010
variation is just the sum of the three

0:34:41.460,0:34:45.990
variations okay

0:34:44.010,0:34:50.490
and so what you have here at the bottom

0:34:45.990,0:34:52.679
is that the entire variation of the cost

0:34:50.490,0:34:56.060
is going to be equal to the variation of

0:34:52.679,0:34:58.590
Z multiplied by W zero multiplet which

0:34:56.060,0:35:00.360
is the version of a zero and then you're

0:34:58.590,0:35:02.510
going to have to multiply that by the

0:35:00.360,0:35:06.360
derivative of C with respect to a 0

0:35:02.510,0:35:08.940
which is DC over da 0 ok so what you see

0:35:06.360,0:35:16.530
here at the last in the last equation is

0:35:08.940,0:35:18.780
exactly that and and you have to sum the

0:35:16.530,0:35:22.530
contributions from the three components

0:35:18.780,0:35:26.310
ok so this year which is easy and is DC

0:35:22.530,0:35:29.940
over D s 0 times W 0 plus DC over the s

0:35:26.310,0:35:31.190
1 times W 1 plus GC over the as 2 times

0:35:29.940,0:35:34.050
W 2

0:35:31.190,0:35:36.930
okay when you have a branch like this

0:35:34.050,0:35:39.150
you perturb the input variable all

0:35:36.930,0:35:42.300
branches or perturb and you have to sum

0:35:39.150,0:35:43.830
up the results on the cost function okay

0:35:42.300,0:35:48.330
which you assume you know others are the

0:35:43.830,0:35:53.930
DC over the whatever variables any

0:35:48.330,0:35:53.930
question is it clear Iko principle

0:35:57.980,0:36:03.600
okay what does that mean like look at

0:36:00.660,0:36:05.790
this formula here it says if I have the

0:36:03.600,0:36:10.020
gradient of or if I have the derivative

0:36:05.790,0:36:12.690
of C the derivative derivative of C with

0:36:10.020,0:36:14.820
respect to s 0 s 1 and s 2 okay all

0:36:12.690,0:36:16.680
three of them then I compute the

0:36:14.820,0:36:19.380
weighted sum of those derivatives with

0:36:16.680,0:36:21.120
the weights going up but I'm using them

0:36:19.380,0:36:22.590
going down and that gives me the

0:36:21.120,0:36:25.950
derivative of the cost function with

0:36:22.590,0:36:28.710
respect to Z which feeds those three

0:36:25.950,0:36:31.340
weights so basically when you back

0:36:28.710,0:36:33.960
propagate through a neural net you

0:36:31.340,0:36:42.720
compute weighted sum of gradients using

0:36:33.960,0:36:43.980
the weights backwards ok all right so

0:36:42.720,0:36:45.180
this is to give a little bit of

0:36:43.980,0:36:48.740
intuition but there is a much more

0:36:45.180,0:36:51.510
general general formulation for this

0:36:48.740,0:36:54.000
before we do this let's let's write it

0:36:51.510,0:36:56.490
this way okay do it one step at a time

0:36:54.000,0:36:58.050
so conceptually you know on that the way

0:36:56.490,0:37:02.010
you want to see it is more something

0:36:58.050,0:37:05.880
like this where you have at least a

0:37:02.010,0:37:08.160
traditional neural net we have an input

0:37:05.880,0:37:11.190
variable you multiply the input variable

0:37:08.160,0:37:13.620
by the first matrix W 0 that gives you s

0:37:11.190,0:37:15.420
1 and then pass that through a

0:37:13.620,0:37:17.760
non-linearity that gives you Z 1 then

0:37:15.420,0:37:19.890
multiply that by the weight matrix W 1

0:37:17.760,0:37:22.980
that gives us two paths out to a

0:37:19.890,0:37:26.070
non-linearity that gives you Z 2 linear

0:37:22.980,0:37:34.010
again bah-bah-bah how many how many how

0:37:26.070,0:37:36.840
many layer neural net is this three yes

0:37:34.010,0:37:39.960
layer is gonna repair in your nonlinear

0:37:36.840,0:37:42.840
right most modern neural Nets don't

0:37:39.960,0:37:45.570
actually have clear linear nonlinear

0:37:42.840,0:37:51.450
separations they're like more complex

0:37:45.570,0:37:54.000
things okay so so SK plus one basically

0:37:51.450,0:37:57.780
equals W K times Z K where wk is a

0:37:54.000,0:38:01.110
matrix DK is a vector XK plus 1 is a

0:37:57.780,0:38:04.620
vector and then Z K equals h HK where H

0:38:01.110,0:38:09.060
is kind of application of the scalar h

0:38:04.620,0:38:10.290
function to every component so if you

0:38:09.060,0:38:13.470
write this in Python

0:38:10.290,0:38:14.820
you write something like this there's

0:38:13.470,0:38:16.830
many ways to write it in pi torch you

0:38:14.820,0:38:18.810
can you can write it from scratch she

0:38:16.830,0:38:20.160
can write it in a functional way or you

0:38:18.810,0:38:22.770
can write it this way which is more like

0:38:20.160,0:38:29.000
object-oriented and it kind of hides a

0:38:22.770,0:38:32.550
video complexity for you so your torch

0:38:29.000,0:38:34.760
you for import an end from torch you

0:38:32.550,0:38:39.180
make a sort of an input which is some

0:38:34.760,0:38:40.530
300 310 sir you can't how many elements

0:38:39.180,0:38:42.330
it has and that's going to be the size

0:38:40.530,0:38:45.240
of your input layer we're going to turn

0:38:42.330,0:38:47.070
it into a vector okay but not yet and

0:38:45.240,0:38:51.240
then you define a class for your neural

0:38:47.070,0:38:54.540
net so the the constructor is going to

0:38:51.240,0:38:57.630
just initialize three linear layers so

0:38:54.540,0:38:59.640
linear layers need to in this case these

0:38:57.630,0:39:02.970
separate objects because they contain a

0:38:59.640,0:39:04.260
vector for the parameter the values

0:39:02.970,0:39:08.850
don't need to be separate objects could

0:39:04.260,0:39:10.230
do not actually have parameters okay so

0:39:08.850,0:39:12.450
that's the complexity that's hidden in

0:39:10.230,0:39:13.650
those NN linear so an engineer actually

0:39:12.450,0:39:15.210
does a little bit more than just

0:39:13.650,0:39:21.060
multiplying the matrix it also adds a

0:39:15.210,0:39:22.770
bias factor but that's okay so you

0:39:21.060,0:39:24.510
initialize those layers with the right

0:39:22.770,0:39:27.240
sizes that you passed as argument to the

0:39:24.510,0:39:28.890
constructor and then you define a

0:39:27.240,0:39:31.410
forward function which is you know how

0:39:28.890,0:39:34.350
you can compute the output as a function

0:39:31.410,0:39:37.020
of the input and so the the first one

0:39:34.350,0:39:38.610
here X dot view minus one just flattens

0:39:37.020,0:39:41.910
the input tensor into its into a vector

0:39:38.610,0:39:45.990
and then you apply the n zero module to

0:39:41.910,0:39:48.900
X you get s1 then apply the value the

0:39:45.990,0:39:52.830
non-linearity to s1 you get Z one etc

0:39:48.900,0:39:57.990
etc and then you return s3 okay and the

0:39:52.830,0:40:01.290
beauty of pie charts which Alfredo will

0:39:57.990,0:40:03.000
explain to you they hatch tomorrow is

0:40:01.290,0:40:05.420
that you don't need to worry about

0:40:03.000,0:40:07.730
computing the gradient because as you

0:40:05.420,0:40:11.100
you've written the forward function and

0:40:07.730,0:40:12.690
patrasche knows what this looks like and

0:40:11.100,0:40:15.330
he knows how to back propagate gradient

0:40:12.690,0:40:16.920
to it it knows how to transform the

0:40:15.330,0:40:19.110
graph that corresponds to your forward

0:40:16.920,0:40:20.520
function into a graph that corresponds

0:40:19.110,0:40:21.490
to the back function so you're not too

0:40:20.520,0:40:23.109
worried about it

0:40:21.490,0:40:24.520
but you still need to know how to keep

0:40:23.109,0:40:27.849
your gradients because sometimes you

0:40:24.520,0:40:29.230
have to write your own module you invent

0:40:27.849,0:40:31.630
this new type of neural net that's got

0:40:29.230,0:40:32.950
this new you know like you know multi

0:40:31.630,0:40:38.099
head multi tail

0:40:32.950,0:40:40.150
you know memory attention STM whatever

0:40:38.099,0:40:42.010
you know you have to write your own

0:40:40.150,0:40:44.500
thing and you basically you might have

0:40:42.010,0:40:52.320
to write your own and CUDA kernel or

0:40:44.500,0:41:13.570
whatever right but it's pretty simple

0:40:52.320,0:41:16.180
yes so as I said if you don't have a

0:41:13.570,0:41:17.740
non-linearity the whole thing is linear

0:41:16.180,0:41:20.920
so it doesn't there's no point having

0:41:17.740,0:41:22.270
layers okay now you have to think you

0:41:20.920,0:41:24.190
know what is the simplest non-linearity

0:41:22.270,0:41:26.349
you can think of it's gonna be a point

0:41:24.190,0:41:28.240
where is you know component or as

0:41:26.349,0:41:29.619
non-linearity was it was the simplest

0:41:28.240,0:41:31.089
component wise non-linearity you can

0:41:29.619,0:41:31.599
think of something that has a single

0:41:31.089,0:41:33.280
kink

0:41:31.599,0:41:34.839
now the funny thing is we're talking

0:41:33.280,0:41:36.250
about gradient baserunning this is not

0:41:34.839,0:41:40.900
even differentiable right because it's

0:41:36.250,0:41:43.750
got a kink but if you're a mathematician

0:41:40.900,0:41:46.750
and you obsessive-compulsive about it

0:41:43.750,0:41:51.790
you would call this not gradient but a

0:41:46.750,0:41:59.430
sub gradient but you know how many

0:41:51.790,0:42:01.540
mathematicians are there here yes

0:41:59.430,0:42:04.180
there's several sub gradient so here a

0:42:01.540,0:42:08.440
function that has a kink in it at this

0:42:04.180,0:42:11.650
point any slope that is between this one

0:42:08.440,0:42:14.200
and that one is correct it's fine okay

0:42:11.650,0:42:15.430
all of those are good sub gradients and

0:42:14.200,0:42:16.540
so the question is you should use

0:42:15.430,0:42:18.400
something in the kind of somewhere in

0:42:16.540,0:42:20.410
the middle or just zero it doesn't

0:42:18.400,0:42:23.670
matter because it's just one point so it

0:42:20.410,0:42:23.670
has no impact no practical impact

0:42:31.460,0:42:38.369
okay so here's the strategy more general

0:42:34.470,0:42:43.349
form we're going from the kind of

0:42:38.369,0:42:48.990
specific to the strategy more general so

0:42:43.349,0:42:50.880
here's a form of chain rule for modules

0:42:48.990,0:42:53.460
that may have multiple outputs and

0:42:50.880,0:42:55.910
multiple inputs or may have inputs that

0:42:53.460,0:42:59.670
are vectors and outputs that are vectors

0:42:55.910,0:43:03.299
okay I don't I don't give them different

0:42:59.670,0:43:08.880
symbols here the the basic formula DC

0:43:03.299,0:43:11.940
over dzf in this case equal this here it

0:43:08.880,0:43:15.180
is e G times DZ g or DZ f holds right

0:43:11.940,0:43:17.970
this is the same chain rule formula that

0:43:15.180,0:43:22.680
we were previously for scalar functions

0:43:17.970,0:43:23.790
it also applies to vector functions okay

0:43:22.680,0:43:26.579
but there's one thing that we need to

0:43:23.790,0:43:30.540
remember the gradient of the scalar

0:43:26.579,0:43:32.490
function with respect to a vector is a

0:43:30.540,0:43:35.329
vector of the same size as the vector

0:43:32.490,0:43:37.650
with respect to which you differentiate

0:43:35.329,0:43:40.500
make sure if you write it this way and

0:43:37.650,0:43:42.690
you want the notations to be consistent

0:43:40.500,0:43:45.510
it's a row vector it's not a column

0:43:42.690,0:43:48.150
vector anymore okay so we'll take a

0:43:45.510,0:43:51.589
scalar function which depends on the

0:43:48.150,0:43:53.880
vector therefore a column vector

0:43:51.589,0:43:56.940
differentiate this scalar function with

0:43:53.880,0:43:58.829
respect to the to this column vector

0:43:56.940,0:44:00.900
what you get is a row vector that's the

0:43:58.829,0:44:02.460
gradient it's not the gradient

0:44:00.900,0:44:05.520
technically the gradient is once you

0:44:02.460,0:44:13.020
transpose it but it's DC over DZ F

0:44:05.520,0:44:14.880
that's the that's the notation and you

0:44:13.020,0:44:19.280
can see that it kind of checks out so

0:44:14.880,0:44:24.079
let's imagine that Z G is a vector

0:44:19.280,0:44:28.040
column vector so of size DG by one and

0:44:24.079,0:44:32.880
ZF is a column vector of size DF by one

0:44:28.040,0:44:36.030
then this little general equation here

0:44:32.880,0:44:36.580
gives you a world vector size DF is

0:44:36.030,0:44:41.290
equal to

0:44:36.580,0:44:44.920
vector of size DG multiplied by a matrix

0:44:41.290,0:44:49.540
who is number of rows is DG and the real

0:44:44.920,0:44:53.740
columns is DF ok and of course the size

0:44:49.540,0:44:55.000
the last size of the vector and the

0:44:53.740,0:45:01.420
first size of the matrix have to match

0:44:55.000,0:45:03.070
if you want this product to work out so

0:45:01.420,0:45:04.720
a more convenient form for this would be

0:45:03.070,0:45:06.910
to kind of transpose everything to say

0:45:04.720,0:45:09.910
this here with ETR transpose which is

0:45:06.910,0:45:11.530
now a column vector is equal to the

0:45:09.910,0:45:14.440
transpose of the product here and that

0:45:11.530,0:45:17.850
would be the transpose of DZ over DZ f

0:45:14.440,0:45:19.810
times the transpose of d c over d GG and

0:45:17.850,0:45:23.980
that would be kind of a more convenient

0:45:19.810,0:45:30.070
form of writing it but it's kind of

0:45:23.980,0:45:32.710
simpler this way Okin system okay so

0:45:30.070,0:45:37.030
what's this funny animal here DZ g over

0:45:32.710,0:45:39.400
DZ f so we have little neural net here

0:45:37.030,0:45:43.720
that has two modules in it

0:45:39.400,0:45:45.850
FNG the output of F the of the F module

0:45:43.720,0:45:55.920
is ZF and the output of the G model is e

0:45:45.850,0:45:57.910
G okay and basically we want the

0:45:55.920,0:46:00.580
gradient of the cost function with

0:45:57.910,0:46:01.690
respect to Z F we assume we know the

0:46:00.580,0:46:03.070
gradient of this cost function is

0:46:01.690,0:46:06.670
affected Z G we know how to back

0:46:03.070,0:46:08.560
propagate to see and to compute the

0:46:06.670,0:46:09.910
gradient with respect to Z F if we know

0:46:08.560,0:46:12.580
the greater respect to Z G we need to

0:46:09.910,0:46:15.330
multiply by this matrix d zg / DZ f

0:46:12.580,0:46:21.910
which is called the Jacobian matrix of G

0:46:15.330,0:46:23.320
with respect to its input okay G has two

0:46:21.910,0:46:25.330
arguments so we can differentiate it

0:46:23.320,0:46:26.380
with respect to Z or respect to W which

0:46:25.330,0:46:30.310
is going to differentiate it with

0:46:26.380,0:46:34.210
respect to Z okay what is this matrix so

0:46:30.310,0:46:37.840
the entry I J of that matrix the

0:46:34.210,0:46:39.400
Jacobian matrix is equal to the partial

0:46:37.840,0:46:42.190
derivative of the eyes

0:46:39.400,0:46:44.650
I put okay the ice component of the

0:46:42.190,0:46:46.210
output vector of the G module with

0:46:44.650,0:46:49.760
respect to the jth component of the

0:46:46.210,0:46:53.160
input vector

0:46:49.760,0:47:00.859
so if I twiddle the J's input is gonna

0:46:53.160,0:47:03.900
make all the output to twiddle and that

0:47:00.859,0:47:14.040
basically is an entire column of the

0:47:03.900,0:47:16.700
Jacobian matrix okay

0:47:14.040,0:47:21.000
that's back problem right so if you have

0:47:16.700,0:47:25.050
a network composed of a cascade of

0:47:21.000,0:47:26.609
modules you just keep multiplying by the

0:47:25.050,0:47:28.320
Jacobian matrix of all the modules going

0:47:26.609,0:47:35.010
down and you get all the gradient

0:47:28.320,0:47:37.380
suspect to all the eternal variables now

0:47:35.010,0:47:38.460
you actually need two sets of of

0:47:37.380,0:47:39.630
gradients you need the gradient so

0:47:38.460,0:47:40.650
respect to the Statesville so the

0:47:39.630,0:47:43.530
gradients with respect to the weights

0:47:40.650,0:47:46.530
and there's a as I said a module that

0:47:43.530,0:47:48.900
has parameters has two Jacobian matrices

0:47:46.530,0:47:50.340
it has one with respect to its input

0:47:48.900,0:47:52.800
state and another one with respect to

0:47:50.340,0:47:54.869
its parameters okay so you have the two

0:47:52.800,0:47:57.300
equations here so let's say now you have

0:47:54.869,0:48:01.260
kind of a slightly more general neural

0:47:57.300,0:48:08.099
net which is a stack of you know many

0:48:01.260,0:48:10.830
modules each module is called FK so okay

0:48:08.099,0:48:15.800
it's kind of an index for that module in

0:48:10.830,0:48:18.750
its input is ZK and it's parameters WK

0:48:15.800,0:48:23.660
and the output is you know ZK plus 1 so

0:48:18.750,0:48:27.300
ZK plus 1 equals F KO z k WK very simple

0:48:23.660,0:48:29.339
so how do i compute DC / g zk which is

0:48:27.300,0:48:30.869
the gradient of the cost function or

0:48:29.339,0:48:34.310
whatever function you want to minimize

0:48:30.869,0:48:38.960
with respect to the input of module ZK

0:48:34.310,0:48:41.369
assuming I know GC / gz k plus 1 already

0:48:38.960,0:48:46.109
you just multiplied by the Jacobian

0:48:41.369,0:48:49.080
matrix of the module care which is G ZK

0:48:46.109,0:48:56.160
plus 1 over G ZK or in other words the

0:48:49.080,0:48:58.650
FK of Z k WK respect to z k okay so it's

0:48:56.160,0:49:01.040
just chain rule again D Z over G ZK

0:48:58.650,0:49:03.590
equals this

0:49:01.040,0:49:06.650
z k equals DC or DZ k plus one which i

0:49:03.590,0:49:10.730
assume i know x the Jacobian matrix of f

0:49:06.650,0:49:12.920
k with respect to z k second line is

0:49:10.730,0:49:16.430
same thing with respect to w vc over DW

0:49:12.920,0:49:19.310
k is equal to DC over DZ k plus 1 which

0:49:16.430,0:49:22.040
already had at the top and then DZ k

0:49:19.310,0:49:24.620
plus 1 over d WK which is the Jacobian

0:49:22.040,0:49:27.320
matrix of the F function with respect to

0:49:24.620,0:49:33.380
its weights to its parameters

0:49:27.320,0:49:41.860
whatever they are that's all there is to

0:49:33.380,0:49:41.860
back part we okay

0:49:42.790,0:49:46.120
any question

0:49:50.340,0:49:56.830
she's a little cute example was so

0:49:55.990,0:49:58.420
Michael Crete

0:49:56.830,0:50:02.380
so let's say we've you know one of those

0:49:58.420,0:50:05.290
simple functions here G of X W we don't

0:50:02.380,0:50:09.000
know what's inside but it's okay and it

0:50:05.290,0:50:12.160
goes to a cost function it's a graph and

0:50:09.000,0:50:16.900
through this manipulation of you know

0:50:12.160,0:50:18.640
multiplying by Jacobian matrices we can

0:50:16.900,0:50:21.190
transform this graph into the graph that

0:50:18.640,0:50:23.740
will compute the gradients going

0:50:21.190,0:50:25.900
backwards and so things like by torsion

0:50:23.740,0:50:27.160
circle do this automatically for you you

0:50:25.900,0:50:30.730
write a function it turns it into a

0:50:27.160,0:50:33.160
graph and then there is something that

0:50:30.730,0:50:35.320
turns its graph into the derivative

0:50:33.160,0:50:37.800
graph if you want that back propagates

0:50:35.320,0:50:40.510
the gradient so in this case here the

0:50:37.800,0:50:44.380
the gradient graph looks like the one at

0:50:40.510,0:50:48.100
the at the right when you start with one

0:50:44.380,0:50:53.170
at the top and then you compute the

0:50:48.100,0:50:56.830
Jacobian of C with respect to y bar you

0:50:53.170,0:51:01.330
multiply this one number by this

0:50:56.830,0:51:04.900
Jacobian Jacobian is actually a vector

0:51:01.330,0:51:09.780
okay it's a gradient it's a row vector

0:51:04.900,0:51:13.120
and that's DC over dy bar then you

0:51:09.780,0:51:14.470
multiply that but it could be enough G

0:51:13.120,0:51:16.320
with respect to its weight and you get

0:51:14.470,0:51:20.620
the gradient of respect to the weights

0:51:16.320,0:51:22.510
that's what you need to train so that's

0:51:20.620,0:51:23.890
an example of you know automatic

0:51:22.510,0:51:26.700
transformation that's what odigo a does

0:51:23.890,0:51:30.580
now where becomes complicated is when

0:51:26.700,0:51:36.630
the the graph the architecture of the

0:51:30.580,0:51:38.830
graph is not fixed but is data dependent

0:51:36.630,0:51:42.000
so let's imagine that depending on the

0:51:38.830,0:51:44.800
value of x you have a test in your

0:51:42.000,0:51:47.620
neuron that code that decides that you

0:51:44.800,0:51:50.440
know if X is a vector that is longer

0:51:47.620,0:51:51.460
than a certain length then you do

0:51:50.440,0:51:55.360
something to do one thing and it is

0:51:51.460,0:51:56.950
shorter you do another thing you know

0:51:55.360,0:51:58.480
then you're gonna have kind of

0:51:56.950,0:52:00.790
the condition Intergraph depending on

0:51:58.480,0:52:03.330
the input right you still need to

0:52:00.790,0:52:07.020
generate the graph of backpropagation

0:52:03.330,0:52:27.250
she have loops it becomes complicated

0:52:07.020,0:52:29.410
you can still do it yeah yeah it it

0:52:27.250,0:52:30.790
usually doesn't work very well if the

0:52:29.410,0:52:34.660
number of loops that you have is more

0:52:30.790,0:52:38.260
than say 50 and same 50 it could be 20

0:52:34.660,0:52:40.780
right it depends and you've probably

0:52:38.260,0:52:42.580
heard of STM and what's special about it

0:52:40.780,0:52:45.640
STM compared to regular recurrent Nets

0:52:42.580,0:52:50.620
is one way of basically making them work

0:52:45.640,0:52:59.320
for longer than like five but they don't

0:52:50.620,0:53:00.700
work very well past twenty years old the

0:52:59.320,0:53:05.140
point is that you can have a variable

0:53:00.700,0:53:06.640
number of steps it's it's specified by

0:53:05.140,0:53:09.370
program and it could be variable depends

0:53:06.640,0:53:11.310
on the size of the input a lot of them

0:53:09.370,0:53:15.010
you all know some people use nowadays a

0:53:11.310,0:53:17.110
variable size X's could be a variable

0:53:15.010,0:53:19.930
size multi-dimensional array and that

0:53:17.110,0:53:21.330
means Genie has variable size inside and

0:53:19.930,0:53:25.780
you can have you know kind of

0:53:21.330,0:53:30.430
complicated he's going on there so again

0:53:25.780,0:53:37.870
in terms of the the sizes that those

0:53:30.430,0:53:40.030
things take so DC / GW is is a row

0:53:37.870,0:53:44.050
vector which is 1 by n where n is the

0:53:40.030,0:53:46.390
number of components of W DC / dy bar is

0:53:44.050,0:53:53.020
1 by M where m is the dimension of the

0:53:46.390,0:53:55.570
output and dy bar DW is number of number

0:53:53.020,0:53:58.510
of rows is the number of outputs of G

0:53:55.570,0:54:01.360
and the number of columns is the

0:53:58.510,0:54:03.540
dimension of W which is n so it takes

0:54:01.360,0:54:03.540
out

0:54:04.810,0:54:20.950
so so fine okay now what kind of modules

0:54:16.760,0:54:24.140
are we using in neural net so as I said

0:54:20.950,0:54:25.609
the linear and rare modules or

0:54:24.140,0:54:28.010
non-linear point-wise non-linearity

0:54:25.609,0:54:29.570
modules are just two examples of things

0:54:28.010,0:54:31.220
that we use to build neural nets but

0:54:29.570,0:54:33.920
what you build deep learning systems in

0:54:31.220,0:54:35.119
general but it's tons of and tons I mean

0:54:33.920,0:54:38.780
if you look at the Python documentation

0:54:35.119,0:54:41.150
there is like a huge list of them of

0:54:38.780,0:54:43.340
such modules and the reason why you need

0:54:41.150,0:54:45.650
it a lot of them I mean most of them are

0:54:43.340,0:54:48.770
kind of can be built out of kind of

0:54:45.650,0:54:51.230
smaller like more elementary functions

0:54:48.770,0:54:53.119
but the reason why they are pre-built is

0:54:51.230,0:54:54.530
because first they have a name but and

0:54:53.119,0:54:57.590
the debug but also because they

0:54:54.530,0:55:00.710
optimized so sometimes you can kind of

0:54:57.590,0:55:02.119
write you know CUDA kernels directly or

0:55:00.710,0:55:05.930
you know they're generated by a compiler

0:55:02.119,0:55:10.760
or something so but here's a bunch of

0:55:05.930,0:55:13.930
elementary modules and I'm not sure I'm

0:55:10.760,0:55:13.930
going to be able to use my

0:55:21.940,0:55:27.290
okay let's start with a duplicate module

0:55:25.550,0:55:29.630
so what in the duplicate module is say

0:55:27.290,0:55:33.849
it's a module that takes a single takes

0:55:29.630,0:55:37.250
you know it's basically a Y connector

0:55:33.849,0:55:39.530
okay you want two people to listen to

0:55:37.250,0:55:49.760
music on your iPhone you need one of

0:55:39.530,0:55:50.990
those Y cables so the first output is

0:55:49.760,0:55:53.450
equal to the input and the second output

0:55:50.990,0:55:57.410
is also equal to the input okay

0:55:53.450,0:55:58.490
y1 equals x y2 equals x so you think you

0:55:57.410,0:55:59.510
know you would think that you don't even

0:55:58.490,0:56:01.910
either with your like this but you

0:55:59.510,0:56:06.230
actually do sometimes in fact in

0:56:01.910,0:56:07.609
patrasche this kind of implicit but but

0:56:06.230,0:56:11.480
you need to make it explicit sometimes

0:56:07.609,0:56:16.400
so whenever you have a wire that splits

0:56:11.480,0:56:19.520
into 2 or n on the way back the

0:56:16.400,0:56:21.680
gradients get summed okay and it's

0:56:19.520,0:56:24.470
exactly the same situation that I

0:56:21.680,0:56:28.810
explained earlier in fact you can

0:56:24.470,0:56:28.810
decompose this video module that I

0:56:29.500,0:56:40.040
explain here you can think of this Z

0:56:37.940,0:56:45.410
variable spitting into three wires as

0:56:40.040,0:56:47.150
one of those branch modules and as the

0:56:45.410,0:56:51.400
three wire converge you have to sum the

0:56:47.150,0:56:53.660
gradients okay which we figured out but

0:56:51.400,0:56:59.540
you can you can sort of build this into

0:56:53.660,0:57:01.940
this speed module just duplicate module

0:56:59.540,0:57:08.300
or triplicate or n ticket whatever it is

0:57:01.940,0:57:09.800
okay so whatever you copy a variable we

0:57:08.300,0:57:12.589
are whatever you use available in

0:57:09.800,0:57:15.650
multiple places you need to sum the

0:57:12.589,0:57:22.400
gradients again the auto grading in PI

0:57:15.650,0:57:26.000
tortoises for you but remember this so

0:57:22.400,0:57:29.250
add so if you have two variables and you

0:57:26.000,0:57:31.890
sum them up we

0:57:29.250,0:57:33.300
this guy the output which will by the

0:57:31.890,0:57:34.620
same quantity for you to love this guy

0:57:33.300,0:57:37.860
the output will twiddle by the same

0:57:34.620,0:57:41.340
quantity what that means is that the

0:57:37.860,0:57:42.630
gradient of whatever function you want

0:57:41.340,0:57:52.260
to minimize with respect to the output

0:57:42.630,0:57:53.400
of a sum is equal to the equal when you

0:57:52.260,0:57:57.360
have the gradient of the cost function

0:57:53.400,0:57:58.800
with respect to the some weight is the

0:57:57.360,0:58:07.710
gradient with respect to each of the two

0:57:58.800,0:58:11.130
branches that you added up it's actually

0:58:07.710,0:58:14.250
equal for both branches okay so if you

0:58:11.130,0:58:15.480
have a connection going this way and you

0:58:14.250,0:58:20.250
get a gradient from the top you just

0:58:15.480,0:58:24.230
copy the gradient okay it's because you

0:58:20.250,0:58:24.230
get the same influence from both sides

0:58:28.430,0:58:39.120
no it's independent of the value of the

0:58:32.610,0:58:49.700
inputs if you think about it but it's

0:58:39.120,0:58:49.700
pretty obvious actually let me try this

0:58:50.750,0:58:54.560
can you be able to do this

0:58:56.420,0:58:59.420
okay

0:59:16.210,0:59:29.479
this work ha only works if I mirror my

0:59:21.079,0:59:37.519
screen because I can't write on the

0:59:29.479,0:59:44.509
screen that doesn't exist okay hang on

0:59:37.519,0:59:46.779
with me for just a minute here know what

0:59:44.509,0:59:46.779
I want

1:00:04.530,1:00:07.530
okay

1:00:15.190,1:00:28.480
oops oh wow

1:00:21.740,1:00:28.480
okay all right

1:00:30.030,1:00:53.700
here we go sorry about that let's go to

1:00:40.440,1:01:03.600
a place that actually works okay you see

1:00:53.700,1:01:12.300
this works so so if y equals x 1 plus x

1:01:03.600,1:01:22.650
2 DC over the X 1 let's say is equal to

1:01:12.300,1:01:31.760
D C over D Y D Y over the X 1 okay this

1:01:22.650,1:01:39.380
we assume we know how much is this one

1:01:31.760,1:01:44.790
and of course the Y of the X 2 is also 1

1:01:39.380,1:01:49.680
okay so there you have it this here the

1:01:44.790,1:01:56.040
X 1 equals to C over D Y is your really

1:01:49.680,1:01:57.480
X to equal this you're really why just

1:01:56.040,1:01:59.630
take this already why copy it and you're

1:01:57.480,1:01:59.630
done

1:02:09.260,1:02:31.040
max that's an interesting one so y

1:02:21.950,1:02:39.170
equals max of XY next to this you're

1:02:31.040,1:02:43.990
ready x1 equal this your D Y D Y over

1:02:39.170,1:02:55.630
the X 1 right let's just chain rule

1:02:43.990,1:02:57.109
what is the worry DX 1 yep and otherwise

1:02:55.630,1:03:00.730
it's 1

1:02:57.109,1:03:03.130
yes correct so in fact you can

1:03:00.730,1:03:15.980
completely understand this graphically

1:03:03.130,1:03:23.510
basically you have X 1 this again yes

1:03:15.980,1:03:27.380
yes yes right so the answer was D Y DX 1

1:03:23.510,1:03:31.130
0 if X 2 is larger than x1 and is 1 if X

1:03:27.380,1:03:34.900
1 is larger than X 2 but intuitively is

1:03:31.130,1:03:42.160
very simple if you have variable X 1 and

1:03:34.900,1:03:42.160
variable X 2 basically the output

1:03:44.630,1:03:51.200
this max module is basically just a

1:03:46.910,1:03:55.550
switch okay I'm putting an arrow here

1:03:51.200,1:03:57.530
but it's not an arrow it's a switch okay

1:03:55.550,1:04:01.190
I can move this switch from left to

1:03:57.530,1:04:05.270
right okay I can choose to connect x1 to

1:04:01.190,1:04:07.850
Y or to connect X 2 to y now once I've

1:04:05.270,1:04:12.950
decided on which side I connect is just

1:04:07.850,1:04:14.810
a wire right regardless on how high

1:04:12.950,1:04:17.480
chose to put the switch in one position

1:04:14.810,1:04:19.970
of the other in this case I use max okay

1:04:17.480,1:04:21.590
but it's just a switch that I decide to

1:04:19.970,1:04:23.990
put on one side or the other when I

1:04:21.590,1:04:25.820
decide to put it on one side then I've

1:04:23.990,1:04:28.940
just connected x1 to Y and it's just a

1:04:25.820,1:04:31.130
wire so x2 if I twiddle it has no

1:04:28.940,1:04:32.630
influence on the output therefore the

1:04:31.130,1:04:36.830
gradient of the cost function is fed to

1:04:32.630,1:04:39.080
x2 is 0 okay and the gradient of the

1:04:36.830,1:04:40.850
cost function with respect to x1 is of

1:04:39.080,1:04:42.020
course equal to the gradient of the cost

1:04:40.850,1:04:43.960
function inspector why because it's just

1:04:42.020,1:04:45.110
a wire it's the same variable really

1:04:43.960,1:04:51.800
okay

1:04:45.110,1:04:55.190
so that generalizes to a switch a switch

1:04:51.800,1:04:59.530
of multiple variables so are the many

1:04:55.190,1:05:04.310
valuable I have if the output is

1:04:59.530,1:05:05.990
determined by you know a switch that I

1:05:04.310,1:05:08.120
can move to one of the input variables

1:05:05.990,1:05:09.380
then when I back propagate I just

1:05:08.120,1:05:11.540
propagate through the variable that was

1:05:09.380,1:05:19.170
connected and the other ones just get

1:05:11.540,1:05:20.430
zero okay it's easier to

1:05:19.170,1:05:21.930
draw this way than to actually write the

1:05:20.430,1:05:29.130
math you have to use Delta functions and

1:05:21.930,1:05:31.460
stuff is okay knocks off max that's a

1:05:29.130,1:05:31.460
fun one

1:05:40.300,1:05:52.720
oh I have to use any page but next page

1:05:49.860,1:06:00.880
actually doesn't go to the next page

1:05:52.720,1:06:14.830
okay so softmax is module where the ad

1:06:00.880,1:06:18.610
put why i is equal to e to the X I so

1:06:14.830,1:06:25.540
it's a module with which I should not

1:06:18.610,1:06:32.010
draw this way should draw this way and

1:06:25.540,1:06:38.740
it has as many outputs as it has inputs

1:06:32.010,1:06:43.960
I'm pulling this the Y I and is the X J

1:06:38.740,1:06:52.540
let's say okay or X whatever so softmax

1:06:43.960,1:06:54.850
is this okay it's a very convenient way

1:06:52.540,1:06:58.270
of transforming a bunch of numbers into

1:06:54.850,1:07:01.090
a bunch of positive numbers between 0 &

1:06:58.270,1:07:03.790
1 that sum to 1 okay when I take the

1:07:01.090,1:07:05.470
exponential so X the X J's can be any

1:07:03.790,1:07:07.390
number when I take the exponential of

1:07:05.470,1:07:09.940
those numbers I get positive numbers and

1:07:07.390,1:07:12.130
I normalize by their sum so what I get

1:07:09.940,1:07:14.440
is a bunch of numbers that are between 0

1:07:12.130,1:07:17.950
& 1 & sum to 1 which some people call a

1:07:14.440,1:07:20.800
probability distribution okay so you can

1:07:17.950,1:07:24.220
interpret why I as a vector of

1:07:20.800,1:07:28.829
probabilities over the discrete set of

1:07:24.220,1:07:36.479
outcomes what is

1:07:28.829,1:07:41.099
dog stuff max so locks off max is oops

1:07:36.479,1:07:50.430
no what I want you to do it's the log of

1:07:41.099,1:07:52.589
that so you get the log of the stuff at

1:07:50.430,1:07:55.380
the top minus the log of the stuff at

1:07:52.589,1:07:59.219
the bottom right so you get the log of

1:07:55.380,1:08:04.170
exponential X I and that's going to be X

1:07:59.219,1:08:06.209
I and there's a mistaken and then you

1:08:04.170,1:08:07.440
get the log of the sum of you get minus

1:08:06.209,1:08:09.809
the log of this some of the

1:08:07.440,1:08:17.609
Exponential's of X J's right so that

1:08:09.809,1:08:25.549
will give us X I minus log of sum over J

1:08:17.609,1:08:25.549
of e to the X J let's call log surf max

1:08:32.760,1:08:42.100
now the guy who invented softmax in 1989

1:08:38.170,1:08:46.020
or so or maybe 88 I don't remember is a

1:08:42.100,1:08:46.020
gentleman by the name of john vidal a

1:08:46.170,1:08:52.240
from britain and he regretted calling it

1:08:49.960,1:08:56.920
softmax he said it should have been

1:08:52.240,1:09:05.380
called soft egg max but it's too late

1:08:56.920,1:09:07.240
people call it soft max so here's an

1:09:05.380,1:09:08.980
interesting exercise for you I'm not

1:09:07.240,1:09:11.500
going to tell you how you back propagate

1:09:08.980,1:09:13.120
through this okay but I want you to do

1:09:11.500,1:09:16.750
the calculation it's a very good

1:09:13.120,1:09:21.070
exercise so laps off meg is actually a

1:09:16.750,1:09:22.570
module in Python but do it on your own

1:09:21.070,1:09:29.910
it's a perfect exercise

1:09:22.570,1:09:34.960
so basically compute DC or by DX K

1:09:29.910,1:09:40.420
assuming that you know although DC

1:09:34.960,1:09:42.070
already why eyes okay

1:09:40.420,1:09:48.670
so you're gonna have a bunch of detail

1:09:42.070,1:09:54.880
with you eyes so here you only have one

1:09:48.670,1:09:58.450
output actually but it's okay so what I

1:09:54.880,1:10:00.460
say there is only one Y I and you know

1:09:58.450,1:10:03.280
the gradient of the loss with respect to

1:10:00.460,1:10:07.500
this why I what is the gradient of the

1:10:03.280,1:10:07.500
loss with respect to all of the X K's

1:10:07.800,1:10:14.290
that's good exercise is it an official

1:10:12.430,1:10:19.930
homework it's an official homework

1:10:14.290,1:10:22.960
tonight okay it's more than just an

1:10:19.930,1:10:24.790
exercise you can find the answer but

1:10:22.960,1:10:26.350
it's more fun to kind of you know I mean

1:10:24.790,1:10:27.520
you're not you don't you don't run as

1:10:26.350,1:10:31.140
much if you don't kind of try about

1:10:27.520,1:10:31.140
yourself you should just look the answer

1:10:35.889,1:10:46.369
okay so the softmax analog is a

1:10:41.929,1:10:48.050
combination of modules that is very

1:10:46.369,1:10:50.449
commonly used in a multi-class

1:10:48.050,1:10:51.739
classification right so you may take an

1:10:50.449,1:10:53.320
all-night the last module would be a

1:10:51.739,1:10:55.789
soft max so we'd normalize all the

1:10:53.320,1:10:57.409
outputs make them positive made them

1:10:55.789,1:11:02.650
look like probabilities and what you

1:10:57.409,1:11:04.849
want is you want to maximize the

1:11:02.650,1:11:06.349
probability that the model gives to the

1:11:04.849,1:11:07.250
correct answer okay so you know the

1:11:06.349,1:11:10.119
correct answer is

1:11:07.250,1:11:13.460
bird bird is number four in your

1:11:10.119,1:11:14.980
categories you want the fourth I put

1:11:13.460,1:11:23.119
yourself Mac to be as high as possible

1:11:14.980,1:11:25.820
okay so that's the BB dog soft max now

1:11:23.119,1:11:27.230
if you separate this into things so if

1:11:25.820,1:11:28.940
you have soft Max and then you take the

1:11:27.230,1:11:30.800
log of the output as your cost function

1:11:28.940,1:11:34.460
the log of the correct output on your

1:11:30.800,1:11:37.820
cost function you get a you get

1:11:34.460,1:11:40.820
numerical issues because you don't get

1:11:37.820,1:11:43.309
log of zero okay so as the score gets

1:11:40.820,1:11:45.079
very very small the log kind of diverges

1:11:43.309,1:11:47.210
and you get sort of numerical problems

1:11:45.079,1:11:49.670
so we're better off writing locks off

1:11:47.210,1:11:53.860
max directly as a single module because

1:11:49.670,1:11:53.860
then that numerical issue disappears

1:11:57.590,1:12:04.590
okay that's a good question in fact it's

1:12:03.270,1:12:08.040
a very good question for the next 20

1:12:04.590,1:12:18.630
minutes not just the next 20 minutes

1:12:08.040,1:12:24.030
actually I stupidly put my pin back did

1:12:18.630,1:12:32.480
I what did you do my pen I have no idea

1:12:24.030,1:12:32.480
what I did with a pen it's here okay

1:12:35.390,1:12:40.770
okay so let's say you're gonna have any

1:12:37.710,1:12:46.350
on that and it's gonna take an X

1:12:40.770,1:12:54.660
variable and then it's gonna have w0 and

1:12:46.350,1:13:00.690
then value and then w1 okay and now we

1:12:54.660,1:13:11.690
get a bunch of scores and we're gonna

1:13:00.690,1:13:15.690
turn this into a score between 0 & 1 now

1:13:11.690,1:13:16.830
this network has only one output and so

1:13:15.690,1:13:20.850
we can only do a two class

1:13:16.830,1:13:25.440
classification and the module we're

1:13:20.850,1:13:28.440
going to put here is the sigmoid

1:13:25.440,1:13:36.450
function also called logistic function

1:13:28.440,1:13:40.650
and so this function is H of discredit s

1:13:36.450,1:13:48.420
since we've called this s before 1 over

1:13:40.650,1:13:51.360
1 plus exponential minus s okay so this

1:13:48.420,1:13:54.360
function when s is very large this

1:13:51.360,1:14:01.740
exponential is equal to close to 0 and

1:13:54.360,1:14:06.720
so H is equal to 1 and when s is very

1:14:01.740,1:14:08.520
small or highly negative then this

1:14:06.720,1:14:10.630
exponential becomes very large and so

1:14:08.520,1:14:16.989
the overall function is you

1:14:10.630,1:14:21.789
okay so that function is like this and

1:14:16.989,1:14:32.599
here it's 0.5 and the asymptote here is

1:14:21.789,1:14:40.639
plus 1 and here is just 0 a 0.5 is

1:14:32.599,1:14:45.849
unreadable ok so I could just take the

1:14:40.639,1:14:49.929
output here which I can call Y bar and

1:14:45.849,1:14:49.929
plug this through some cost function

1:14:52.420,1:15:02.809
which I compare with y okay now so Y

1:14:57.039,1:15:05.900
would be also a binary variable 0 1 now

1:15:02.809,1:15:08.179
what what do this cost function what

1:15:05.900,1:15:11.960
should it do I could use square error

1:15:08.179,1:15:17.559
right so C could be equal to the

1:15:11.960,1:15:17.559
difference between y and y bar squared

1:15:18.730,1:15:21.579
sounds perfectly reasonable doesn't work

1:15:21.199,1:15:24.010
very well

1:15:21.579,1:15:31.190
reason it doesn't work very well is that

1:15:24.010,1:15:32.510
the is that the sigmoid and people you

1:15:31.190,1:15:35.510
know in the early days of neural nets in

1:15:32.510,1:15:38.090
the 1980s we're doing this very commonly

1:15:35.510,1:15:40.460
and the network wouldn't converging I

1:15:38.090,1:15:41.210
would say neural nets don't work they

1:15:40.460,1:15:44.360
were just doing it wrong

1:15:41.210,1:15:50.900
so the problem that you have here is

1:15:44.360,1:15:54.559
that if Y is equal to 1 for one class

1:15:50.900,1:15:57.320
and 0 for the other class the system

1:15:54.559,1:15:59.480
wants to get the output equal to 1 and

1:15:57.320,1:16:01.280
it can't because it's an asymptote so it

1:15:59.480,1:16:05.659
tries to make the weights that we won

1:16:01.280,1:16:07.789
very very large so that it gets to 1 or

1:16:05.659,1:16:11.420
to 0 it has to make the weighted sum

1:16:07.789,1:16:13.760
enormous you know if it wants to get

1:16:11.420,1:16:15.349
close to the desired output the bell the

1:16:13.760,1:16:17.809
gradient is very small right because the

1:16:15.349,1:16:19.639
derivative of that sigmoid that Sigma is

1:16:17.809,1:16:20.430
very flat there so when you back

1:16:19.639,1:16:22.230
propagate

1:16:20.430,1:16:27.990
the gradient is basically zero because

1:16:22.230,1:16:32.220
the Sigma is flat so you get the

1:16:27.990,1:16:35.970
saturation problem so some people like

1:16:32.220,1:16:39.510
said you know back in the old days one

1:16:35.970,1:16:44.310
of two things either you set your

1:16:39.510,1:16:46.350
targets in between so not at the

1:16:44.310,1:16:50.640
asymptotes or you use a different loss

1:16:46.350,1:16:52.710
okay so so basically you say here is the

1:16:50.640,1:16:54.390
sigmoid function the target for a

1:16:52.710,1:16:56.700
category one is going to be at

1:16:54.390,1:16:58.260
I don't know point eight and the target

1:16:56.700,1:17:02.250
for category 2 is going to be at point

1:16:58.260,1:17:04.260
two so there those would be a table and

1:17:02.250,1:17:07.320
so the weights won't go to infinity and

1:17:04.260,1:17:10.230
you won't have those problems but here's

1:17:07.320,1:17:12.570
another idea and the other idea is just

1:17:10.230,1:17:14.450
take the log of it okay

1:17:12.570,1:17:16.980
take the log so if you think about this

1:17:14.450,1:17:20.760
the zero function here it's actually a

1:17:16.980,1:17:22.980
softmax it's a soft max between two

1:17:20.760,1:17:26.460
variables one of which is equal to minus

1:17:22.980,1:17:27.990
x the other one is equal to 1 and what

1:17:26.460,1:17:31.560
you're getting is the surf max output

1:17:27.990,1:17:39.720
from from the input that's always equal

1:17:31.560,1:17:47.310
to 1 okay then you write this function

1:17:39.720,1:17:50.430
in another way I'm going to multiply the

1:17:47.310,1:17:55.670
top and the bottom by e to the S okay so

1:17:50.430,1:17:58.620
I get e to the s divided by e to the S

1:17:55.670,1:18:06.780
plus e to the x times e to the minus s

1:17:58.620,1:18:09.060
and that's 1 okay this is our faqs it's

1:18:06.780,1:18:12.600
off max where when input is 1 the other

1:18:09.060,1:18:14.760
one is s and what I'm looking at is the

1:18:12.600,1:18:17.090
S the output corresponding to the s

1:18:14.760,1:18:17.090
input

1:18:21.880,1:18:25.969
so when Sigma it's just you know softmax

1:18:24.290,1:18:30.409
is just a generalization is assimilated

1:18:25.969,1:18:32.980
for multiple outputs now if you take a

1:18:30.409,1:18:32.980
lot of this

1:18:48.210,1:18:59.940
you get s minus log of 1 plus e to the S

1:19:06.120,1:19:09.120
okay

1:19:15.780,1:19:19.110
the question is and again this is a

1:19:17.670,1:19:21.810
special occasion softmax with only two

1:19:19.110,1:19:23.160
inputs where one is equal to one one of

1:19:21.810,1:19:30.420
the one of the two inputs is equal to

1:19:23.160,1:19:34.380
one okay so the effect of the log like

1:19:30.420,1:19:44.370
look at this this function here this

1:19:34.380,1:19:47.160
function looks like this where when s is

1:19:44.370,1:19:48.870
very large the 1 doesn't count in the

1:19:47.160,1:19:51.300
sum and so you basically a vlog of

1:19:48.870,1:19:52.710
exponential s which is just s right so

1:19:51.300,1:19:57.600
for large s is just the identity

1:19:52.710,1:20:00.960
function and for small s the 1 dominates

1:19:57.600,1:20:03.810
and so it slope 1 which is 0 as we get 0

1:20:00.960,1:20:08.970
it's kind of like a soft really kind of

1:20:03.810,1:20:11.760
thing but the point is it doesn't

1:20:08.970,1:20:16.250
saturate so you don't get those

1:20:11.760,1:20:16.250
vanishing gradient issue

1:20:34.710,1:20:43.480
yes we have a log in front so log E

1:20:37.630,1:20:45.579
power s is s oh yeah I mean sure I mean

1:20:43.480,1:20:48.039
if you could do as far as s then I was

1:20:45.579,1:20:52.329
just talking about the second term yeah

1:20:48.039,1:20:57.280
yeah I mean s - this is kind of the

1:20:52.329,1:20:58.570
other way around yeah absolutely yeah

1:20:57.280,1:21:08.699
you should take the entire the entire

1:20:58.570,1:21:08.699
function it's it's the exact opposite

1:21:21.680,1:21:33.560
okay do you have softmax also as one of

1:21:28.280,1:21:37.820
the exercises yeah right okay all right

1:21:33.560,1:21:44.030
so let's end with a few tricks practical

1:21:37.820,1:21:46.010
tricks and you will you'll see more of

1:21:44.030,1:21:50.540
them tomorrow and as you start playing

1:21:46.010,1:21:52.970
with back prop so the idea of using

1:21:50.540,1:21:54.500
value instead of hyperbole tension so

1:21:52.970,1:21:56.870
hyperbole tension is just like the Sigma

1:21:54.500,1:21:59.240
8 I just showed except that it's

1:21:56.870,1:22:01.220
multiplied by 2 and you subtract 1 so it

1:21:59.240,1:22:04.900
it goes from minus 1 to 1 instead of 0

1:22:01.220,1:22:06.280
to 1 but it's essentially the same shape

1:22:04.900,1:22:09.290
huh

1:22:06.280,1:22:13.060
yeah we talked about it last week

1:22:09.290,1:22:16.610
and they're both putting out of favor

1:22:13.060,1:22:20.000
value tends to work much better when you

1:22:16.610,1:22:22.700
have many layers and probably the reason

1:22:20.000,1:22:26.390
is that it's it's scale invariant in the

1:22:22.700,1:22:28.790
sense that or scale equivalent if if the

1:22:26.390,1:22:30.500
input is if you multiply the input by 2

1:22:28.790,1:22:33.590
the output we multiplied by 2 but

1:22:30.500,1:22:36.220
otherwise unchanged right it's got only

1:22:33.590,1:22:38.870
one kink and so it has no scale to it

1:22:36.220,1:22:41.210
whereas if you had 2 Kings then you know

1:22:38.870,1:22:42.950
the input has to have a particular the

1:22:41.210,1:22:48.320
onions to kind of fit those 2 kings in

1:22:42.950,1:22:51.170
the right place so people will use read

1:22:48.320,1:22:54.140
you the use cross-entropy does for

1:22:51.170,1:22:59.050
classification lock softmax is a special

1:22:54.140,1:22:59.050
case a simple special case of

1:22:59.290,1:23:08.140
cross-entropy laws we'll come back to

1:23:02.630,1:23:08.140
that yes

1:23:11.400,1:23:16.830
cross interview was like definitely

1:23:13.260,1:23:26.190
expect the logs on taxes but it's super

1:23:16.830,1:23:29.670
easy right yeah you want to use well so

1:23:26.190,1:23:31.770
long soft max not soft max definitely if

1:23:29.670,1:23:33.630
you feed it to a question Cheerios

1:23:31.770,1:23:37.620
function it expects outputs from a lock

1:23:33.630,1:23:39.120
soft max notice off length if you don't

1:23:37.620,1:23:42.449
know this yet you might waste a lot of

1:23:39.120,1:23:44.640
time in stochastic gradient on mini

1:23:42.449,1:23:46.469
batches we talked about this before you

1:23:44.640,1:23:48.780
shuffle the training samples we've used

1:23:46.469,1:23:52.710
the casi gradient the order of the

1:23:48.780,1:23:54.030
examples matters if you have I don't

1:23:52.710,1:23:56.159
know a 10-way classification doing

1:23:54.030,1:23:58.230
endless try to classifying the ten

1:23:56.159,1:23:59.640
digits from 0 to 9 if you put all the

1:23:58.230,1:24:02.489
zeros and all the ones then all the

1:23:59.640,1:24:04.409
tools and etc it's not going to work

1:24:02.489,1:24:06.690
because what's going to happen is that

1:24:04.409,1:24:08.760
in the first few examples of zeros the

1:24:06.690,1:24:11.100
system will adapt the biases of the last

1:24:08.760,1:24:13.020
layer to just produce the correct output

1:24:11.100,1:24:14.969
and we'll never learn what a zero looks

1:24:13.020,1:24:16.380
like and then you show one and it's

1:24:14.969,1:24:18.929
going to take just a few samples for it

1:24:16.380,1:24:21.000
to adapt to biases so that it learns to

1:24:18.929,1:24:22.350
produce one without actually looking at

1:24:21.000,1:24:24.540
the input and it's going to keep doing

1:24:22.350,1:24:27.510
this for eons and eons and it's never

1:24:24.540,1:24:30.150
gonna converge so you absolutely need to

1:24:27.510,1:24:31.469
shuffle the examples in the case of n

1:24:30.150,1:24:32.699
days but it's true also for a lot of

1:24:31.469,1:24:35.760
others

1:24:32.699,1:24:37.080
you probably want in a mini batch as I

1:24:35.760,1:24:39.480
said before it may be that you want

1:24:37.080,1:24:42.360
examples of all the categories if you

1:24:39.480,1:24:45.480
really want to use an e batch use

1:24:42.360,1:24:47.250
samples from different categories and if

1:24:45.480,1:24:49.260
you don't use a mini batch just you know

1:24:47.250,1:24:52.110
have samples of different categories one

1:24:49.260,1:24:53.760
after the other there is debate as to

1:24:52.110,1:24:56.310
whether you need to change the order of

1:24:53.760,1:24:59.610
the samples at every pass through the

1:24:56.310,1:25:00.659
samples it's not an entirely clear some

1:24:59.610,1:25:02.400
people claim it's better if you don't

1:25:00.659,1:25:05.280
some people people claim it's better if

1:25:02.400,1:25:09.929
you do with you know various theoretical

1:25:05.280,1:25:11.969
arguments for it you need to normalize

1:25:09.929,1:25:15.929
the input variables so if you look at

1:25:11.969,1:25:18.210
standard code that people publish for

1:25:15.929,1:25:20.730
training on imagenet or speech

1:25:18.210,1:25:21.909
recognition or whatever the first

1:25:20.730,1:25:24.159
operation they do is that

1:25:21.909,1:25:27.210
they normalize the inputs what do they

1:25:24.159,1:25:27.210
do they

1:25:46.890,1:26:04.180
so an image is really going to be three

1:25:52.470,1:26:05.590
planes are G and D okay so it's think of

1:26:04.180,1:26:09.510
it as a three-dimensional array where

1:26:05.590,1:26:12.430
the first dimension is color plane and

1:26:09.510,1:26:14.100
the other two dimensions are space or

1:26:12.430,1:26:18.700
sometimes the other way around

1:26:14.100,1:26:20.560
sometimes the channel is last but it is

1:26:18.700,1:26:22.750
better to think of it this way

1:26:20.560,1:26:28.120
so what you do is you take each of those

1:26:22.750,1:26:32.680
guys so let's say blue you compute the

1:26:28.120,1:26:34.390
mean of all the all the variables in

1:26:32.680,1:26:36.520
this blue image and you do this for

1:26:34.390,1:26:38.650
every single image in your training set

1:26:36.520,1:26:40.630
okay take the entire training set or a

1:26:38.650,1:26:42.610
good chunk of it and compute the mean of

1:26:40.630,1:26:44.920
all the blue

1:26:42.610,1:26:47.980
inputs for the entire training set that

1:26:44.920,1:26:51.460
gives you a single scalar right let's

1:26:47.980,1:26:55.720
call it MB so it's the mean of all the

1:26:51.460,1:26:56.740
Blues okay so interesting you can do the

1:26:55.720,1:26:58.240
same you can compute the standard

1:26:56.740,1:27:01.360
deviation where I compute the variance

1:26:58.240,1:27:04.150
of all the Blues and take the square

1:27:01.360,1:27:08.640
root that's substandard deviation Sigma

1:27:04.150,1:27:08.640
B you know same for green

1:27:11.630,1:27:19.370
same for red okay so we get six numbers

1:27:16.760,1:27:26.030
six scalar values and now what you do is

1:27:19.370,1:27:37.040
you take whatever you see an image you

1:27:26.030,1:27:43.429
take the all component IJ and to

1:27:37.040,1:27:48.889
normalize it you replace it by itself

1:27:43.429,1:27:54.679
minus the mean divided by the standard

1:27:48.889,1:27:58.460
deviation or the max of the standard

1:27:54.679,1:28:02.540
deviation and some small quantity so it

1:27:58.460,1:28:04.790
doesn't blow up what does that do for

1:28:02.540,1:28:07.690
you it normalizes the contrast and

1:28:04.790,1:28:11.630
normal eye and it makes the variant zero

1:28:07.690,1:28:14.000
this is good for various reasons in fact

1:28:11.630,1:28:15.770
it's a good idea to have variables

1:28:14.000,1:28:19.429
inside of a neural net that are zero

1:28:15.770,1:28:20.780
mean and unit variance or kind of

1:28:19.429,1:28:26.110
variance there are more or less all the

1:28:20.780,1:28:38.300
same of course you do this also for the

1:28:26.110,1:28:43.040
green and blue yeah across across many

1:28:38.300,1:28:45.050
images it's a single mean yeah I mean

1:28:43.040,1:28:47.540
there's various ways to do it some you

1:28:45.050,1:28:48.980
can do it for a single image for Google

1:28:47.540,1:28:52.100
Images you can do it which is what

1:28:48.980,1:28:55.130
batchman does you can also do it like on

1:28:52.100,1:28:58.070
a small piece of an image that's called

1:28:55.130,1:28:59.690
high pass filtering but the simplest

1:28:58.070,1:29:01.730
thing is and what almost everybody does

1:28:59.690,1:29:04.100
internet standard standard image that

1:29:01.730,1:29:05.389
pipeline or image processing image

1:29:04.100,1:29:08.800
recognition pipeline with commercial

1:29:05.389,1:29:08.800
nets for example is this

1:29:15.969,1:29:23.300
yeah the channels have very different

1:29:18.489,1:29:27.079
means and and so you know in a typical

1:29:23.300,1:29:29.780
natural image when you're in you know

1:29:27.079,1:29:31.219
outside and inside the the components

1:29:29.780,1:29:37.630
would be very different you have color

1:29:31.219,1:29:40.190
shift and the amplitude of blue is

1:29:37.630,1:29:41.949
relatively low for example if you are in

1:29:40.190,1:29:44.389
full Sun the amplitude of red is

1:29:41.949,1:29:46.119
basically non-existent if you are

1:29:44.389,1:29:49.999
underwater

1:29:46.119,1:29:52.489
so you know if if you want any kind of

1:29:49.999,1:29:54.469
signal you need to kind of normalize

1:29:52.489,1:29:57.409
this is basically like automatic gain

1:29:54.469,1:29:58.519
control the means are very different of

1:29:57.409,1:30:00.139
course because that depends on the

1:29:58.519,1:30:02.179
overall luminosity and you don't want a

1:30:00.139,1:30:03.889
system that where the recognition

1:30:02.179,1:30:05.719
depends too much on the global

1:30:03.889,1:30:07.099
illumination of your image so that's a

1:30:05.719,1:30:10.039
way of kind of getting rid of global

1:30:07.099,1:30:13.909
illumination if you want and sort of you

1:30:10.039,1:30:17.719
know kind of bad tuning of your exposure

1:30:13.909,1:30:19.369
or contrast or whatever it is but is

1:30:17.719,1:30:31.909
really good so numerical reasons for

1:30:19.369,1:30:33.679
doing this so in most precooked and

1:30:31.909,1:30:34.130
welcome back for why this is a good idea

1:30:33.679,1:30:37.340
okay

1:30:34.130,1:30:39.709
later in most pre-code code you will

1:30:37.340,1:30:40.969
also find things schedules to decrease

1:30:39.709,1:30:45.079
the running rate so the learning rate

1:30:40.969,1:30:46.789
the ADA system first of all most systems

1:30:45.079,1:30:48.499
don't use just plain stochastic gradient

1:30:46.789,1:30:51.530
they use things like atom which

1:30:48.499,1:30:54.469
automatically adapt the the step size or

1:30:51.530,1:30:57.170
other tricks they also use what's called

1:30:54.469,1:30:59.900
a momentum trick or necessary Fomento in

1:30:57.170,1:31:04.280
particular you know which atom

1:30:59.900,1:31:05.659
integrates and generally if you really

1:31:04.280,1:31:07.010
want good results you need to kind of

1:31:05.659,1:31:09.650
decrease your running way that as time

1:31:07.010,1:31:11.840
goes by and so there are kind of

1:31:09.650,1:31:13.880
standard ways of you know scheduling the

1:31:11.840,1:31:16.179
decrease of the learning rate that you

1:31:13.880,1:31:16.179
can use

1:31:21.690,1:31:27.720
occasionally not always you can use a

1:31:25.060,1:31:30.850
bit of l2 r1 regularization on the way

1:31:27.720,1:31:34.390
so what does that mean that means so l2

1:31:30.850,1:31:37.930
regularization means at every update you

1:31:34.390,1:31:42.360
multiply every way by one minus a small

1:31:37.930,1:31:42.360
constant multiplied by the learning rate

1:31:43.050,1:31:56.130
so basically and people call this way DK

1:31:57.540,1:32:09.160
says sessions call this l2

1:32:01.390,1:32:13.150
regularization this guy you have an

1:32:09.160,1:32:18.370
additional in in addition to your in

1:32:13.150,1:32:21.760
Europe in your last function in addition

1:32:18.370,1:32:24.250
to your cost you have a regularization

1:32:21.760,1:32:27.820
term that only depends on the weight the

1:32:24.250,1:32:29.920
cost depends on the sample as well right

1:32:27.820,1:32:35.530
and you have some sort of variable to

1:32:29.920,1:32:42.250
control these importance here so l2

1:32:35.530,1:32:46.990
regularization means our W equals the

1:32:42.250,1:32:50.200
square norm of W when you compute the

1:32:46.990,1:32:53.770
gradient of R with respect to a

1:32:50.200,1:32:56.310
particular component to W what you get

1:32:53.770,1:32:56.310
is to

1:33:00.969,1:33:21.480
- w hi and so in the update rule when

1:33:14.890,1:33:24.430
you do WI is replaced by WI - ADA

1:33:21.480,1:33:31.260
gradient of your overall loss with

1:33:24.430,1:33:33.550
respect to W what you get is WI - ADA

1:33:31.260,1:33:39.270
times the gradient of the cost with

1:33:33.550,1:33:52.350
respect to W - because it's a - gradient

1:33:39.270,1:33:52.350
- of WI oh you're right

1:33:54.330,1:34:10.560
which I can rewrite as so this is WI and

1:34:02.620,1:34:10.560
I can rewrite as WI times 1 minus 2

1:34:10.980,1:34:25.750
alpha minus C over D WI ok so what does

1:34:22.810,1:34:27.750
that mean you take every weight and at

1:34:25.750,1:34:33.070
every iteration you shrink it by a

1:34:27.750,1:34:36.570
constant that's slightly less than 1 and

1:34:33.070,1:34:39.310
so that's why it's called weight decay

1:34:36.570,1:34:41.590
in the absence of any gradient from

1:34:39.310,1:34:45.730
seeing the weights exponentially decay

1:34:41.590,1:34:48.940
to zero ok so what that does is that it

1:34:45.730,1:34:52.000
tries to tell the system you know

1:34:48.940,1:34:53.890
minimize my cost function but do it with

1:34:52.000,1:34:55.980
a weight vector that is as short as

1:34:53.890,1:34:55.980
possible

1:34:58.749,1:35:04.269
okay the other one is a one so l1

1:35:01.579,1:35:04.269
regularization

1:35:10.689,1:35:25.599
is basically a regularization term equal

1:35:13.090,1:35:42.340
to some of our I of absolute value of W

1:35:25.599,1:35:46.659
is for e which is yeah one norm so when

1:35:42.340,1:35:52.840
you do the the gradient a day you get WI

1:35:46.659,1:36:02.559
- yeah you see already WI and then

1:35:52.840,1:36:09.869
where's the gradient of this or - the

1:36:02.559,1:36:09.869
gradient of this that would be sine of

1:36:10.559,1:36:19.539
WI and of course you need e alpha in

1:36:13.809,1:36:23.590
front so this is a constant which is

1:36:19.539,1:36:25.090
positive which is positive if WI is

1:36:23.590,1:36:28.530
positive negative it is negative but

1:36:25.090,1:36:28.530
there's a minus sign in front so

1:36:29.400,1:36:36.219
basically here WI is being shrunk

1:36:33.519,1:36:42.610
towards zero by a constant equal to area

1:36:36.219,1:36:45.119
times alpha the state sessions converse

1:36:42.610,1:36:45.119
less soon

1:36:48.699,1:36:59.440
needs to absolute whatever okay I mean

1:36:57.099,1:37:03.070
it's some cute I quit name right this is

1:36:59.440,1:37:04.420
some sort of pun in it but and they

1:37:03.070,1:37:09.190
pronounce it let's sue for reason I

1:37:04.420,1:37:11.170
never understood and so that basically

1:37:09.190,1:37:14.559
shrinks the all the weights toward zero

1:37:11.170,1:37:16.239
by a constant and what that means is

1:37:14.559,1:37:24.869
that if a weight is not useful it's

1:37:16.239,1:37:27.280
gonna get eliminated to zero okay and

1:37:24.869,1:37:30.039
that is very interesting when you have

1:37:27.280,1:37:33.070
like a very large particularly a very

1:37:30.039,1:37:34.510
large uh like a network with a very

1:37:33.070,1:37:36.519
large number of inputs many of which are

1:37:34.510,1:37:38.499
not very useful this will basically

1:37:36.519,1:37:40.360
eliminate the inputs that are not very

1:37:38.499,1:37:42.820
useful because the weights that connect

1:37:40.360,1:37:52.360
to it will go to zero so when I did a

1:37:42.820,1:37:54.369
question okay so first of all you don't

1:37:52.360,1:37:56.829
want to use it at at the start because

1:37:54.369,1:38:00.969
there is a curious thing with neural

1:37:56.829,1:38:02.949
nets which is that the origin of weight

1:38:00.969,1:38:05.110
space is kind of a saddle point and so

1:38:02.949,1:38:06.969
if you if you crank up at one or l2

1:38:05.110,1:38:11.199
initially the weights just go to zero

1:38:06.969,1:38:13.989
and nothing works so so so it's in one

1:38:11.199,1:38:18.429
of the tricks actually I forgot a very

1:38:13.989,1:38:19.929
important one in this list which is that

1:38:18.429,1:38:20.949
the weights have to be initialized in

1:38:19.929,1:38:23.860
the neural net and they have to be

1:38:20.949,1:38:25.869
initialized properly these various

1:38:23.860,1:38:28.570
tricks that are built into PI torch to

1:38:25.869,1:38:30.429
initialize when trick is called the

1:38:28.570,1:38:33.729
climbing trick it's actually the glioma

1:38:30.429,1:38:37.449
to attract from 20 years earlier but and

1:38:33.729,1:38:43.239
the idea is it was reinvented multiple

1:38:37.449,1:38:45.760
times but the idea is you want the the

1:38:43.239,1:38:47.739
weights that they're going to a unit to

1:38:45.760,1:38:49.510
be to be random I mean you initialize

1:38:47.739,1:38:50.920
them randomly but you don't know them to

1:38:49.510,1:38:52.209
be too large or too small you want them

1:38:50.920,1:38:53.559
to be kind of roughly the right size so

1:38:52.209,1:38:54.909
that the output

1:38:53.559,1:38:57.849
is roughly the same variance as the

1:38:54.909,1:39:01.959
inputs okay so if the inputs to a unit

1:38:57.849,1:39:06.189
are independent the variance of the

1:39:01.959,1:39:09.369
output the variance of the weighted sum

1:39:06.189,1:39:11.199
will be equal to the sum of the

1:39:09.369,1:39:13.769
variances of the input multiplied by

1:39:11.199,1:39:17.739
weighted by the square of the weights

1:39:13.769,1:39:20.889
okay so if you want if you have n inputs

1:39:17.739,1:39:23.050
and you want the output to have the same

1:39:20.889,1:39:24.579
variance as the input and into weights

1:39:23.050,1:39:30.550
to be proportional to the inverse square

1:39:24.579,1:39:32.169
root of the number of inputs okay and

1:39:30.550,1:39:34.449
that's that's basically the trick so we

1:39:32.169,1:39:37.899
initialize the weights to values which

1:39:34.449,1:39:40.599
are drawn randomly with zero mean and

1:39:37.899,1:39:42.869
the variance is 1 over the square root

1:39:40.599,1:39:45.159
of the number of inputs to that unit

1:39:42.869,1:39:47.669
okay and that's you know hard you know

1:39:45.159,1:39:51.099
built into pie charts as well so

1:39:47.669,1:39:53.679
initialization is super important and it

1:39:51.099,1:40:11.079
can if you do it wrong your network is

1:39:53.679,1:40:13.449
not gonna cover yeah well I mean you

1:40:11.079,1:40:15.280
probably want to start with a alpha

1:40:13.449,1:40:17.139
equal zero and then maybe crank it up

1:40:15.280,1:40:18.249
and then it depends you know how much

1:40:17.139,1:40:20.050
you want to regularize how much is

1:40:18.249,1:40:22.829
necessary I mean a lot of people just

1:40:20.050,1:40:25.809
don't use any okay either everyone or l2

1:40:22.829,1:40:26.800
but they didn't drop out okay so the

1:40:25.809,1:40:29.859
road patch is another type of

1:40:26.800,1:40:31.749
regularization and and you can think of

1:40:29.859,1:40:33.419
it as a layer inside of a neural net

1:40:31.749,1:40:36.659
that you just insert in the neural net

1:40:33.419,1:40:39.519
where drop out does is that it randomly

1:40:36.659,1:40:44.800
it's a it's a box that has an input and

1:40:39.519,1:40:47.319
an output and it randomly sets n over

1:40:44.800,1:40:49.869
two of the outputs to zero and it's a

1:40:47.319,1:40:53.169
random draw at every new sample that you

1:40:49.869,1:40:58.199
draw that layer basically kills half of

1:40:53.169,1:41:01.289
its components okay this is crazy right

1:40:58.199,1:41:03.550
but in fact it kind of makes the other

1:41:01.289,1:41:05.400
variables more robust basically it

1:41:03.550,1:41:08.610
forces the system to not rely on

1:41:05.400,1:41:10.320
any single unit to produce an answer

1:41:08.610,1:41:12.720
it sort of distributes the information

1:41:10.320,1:41:14.220
across all the units because it knows

1:41:12.720,1:41:16.830
that you're in training you know half of

1:41:14.220,1:41:18.060
them can disappear so it tends to kind

1:41:16.830,1:41:20.670
of distribute the information better

1:41:18.060,1:41:22.830
it's a trick that you know Jeff Fenton

1:41:20.670,1:41:24.960
and his team came up with and turns out

1:41:22.830,1:41:27.030
to be you know quite efficient way of

1:41:24.960,1:41:30.720
regularizing neural nets a lot of people

1:41:27.030,1:41:32.610
use ok there's variations of it but and

1:41:30.720,1:41:34.110
we'll talk about more of those there are

1:41:32.610,1:41:36.060
in one of those papers efficient

1:41:34.110,1:41:42.690
backdrops that I wrote many years ago

1:41:36.060,1:41:45.230
and that you are invited to read ok last

1:41:42.690,1:41:47.910
thing for today even though we're late

1:41:45.230,1:41:51.180
this trick I mean it's this whole

1:41:47.910,1:41:52.470
framework of having a computer graph and

1:41:51.180,1:41:56.730
that propagating to it of course it

1:41:52.470,1:41:59.550
doesn't work just for stacked module it

1:41:56.730,1:42:01.290
works for any arrangement of module

1:41:59.550,1:42:05.930
including the ones that I dynamical that

1:42:01.290,1:42:05.930
depend on on on the inputs question

1:42:13.760,1:42:19.710
yeah so the question is why why do we

1:42:18.510,1:42:21.480
care about the fact that values are

1:42:19.710,1:42:24.539
scale equivariance if we're going to

1:42:21.480,1:42:26.670
normalize anyway the question is where

1:42:24.539,1:42:29.130
do you normalize to so if you have

1:42:26.670,1:42:30.960
sigmoids and you normalize you're

1:42:29.130,1:42:32.400
basically forcing the system if your

1:42:30.960,1:42:33.840
normal is it is a variance for example

1:42:32.400,1:42:35.039
is too small then the system is not

1:42:33.840,1:42:37.980
gonna be able to use the non-linearity

1:42:35.039,1:42:40.650
in the sigmoid hyperbole tangent let's

1:42:37.980,1:42:43.889
say if you make it too small is gonna

1:42:40.650,1:42:47.249
saturate so what's the right setup no

1:42:43.889,1:42:48.780
clear well you you don't care as long

1:42:47.249,1:42:50.369
it's the same variance all over the

1:42:48.780,1:42:52.079
network if you just let me variance all

1:42:50.369,1:42:54.480
over the network then you know what

1:42:52.079,1:42:55.889
you're gonna get issues some layers are

1:42:54.480,1:42:57.059
gonna learn faster than others some are

1:42:55.889,1:42:59.579
going to diverge when others are

1:42:57.059,1:43:01.110
converging so you want the variances to

1:42:59.579,1:43:06.090
be roughly the same all over the network

1:43:01.110,1:43:08.880
and you know that's what things like

1:43:06.090,1:43:12.059
betcha norm do for you we haven't talked

1:43:08.880,1:43:14.219
about batch alone yet but okay

1:43:12.059,1:43:16.550
that's it for today thank you see you

1:43:14.219,1:43:16.550
next week


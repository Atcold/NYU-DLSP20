0:00:00.939,0:00:03.029
Actually first point we have a website now

0:00:03.030,0:00:04.020
You can find a website

0:00:04.020,0:00:09.149
if you go to the repository on my github that you can click there on the link and you get

0:00:09.340,0:00:15.059
Redirected to the website where you have the summaries of the previous class and the previous lab, right?

0:00:15.059,0:00:17.489
So it's your duty to go over those

0:00:18.580,0:00:20.470
summaries before class

0:00:20.470,0:00:23.010
otherwise if I take the first 15 minutes

0:00:23.920,0:00:28.139
Revising what we have seen last time then we have 15 minutes less to see new stuff, right?

0:00:28.630,0:00:31.890
It's very hard when we have just 50 minutes on Tuesday

0:00:33.190,0:00:39.419
Nevertheless we start with a question. So let's say I'd like to do classification between images of dogs and cats

0:00:40.230,0:00:44.489
If this is my cat image, where will be my dog image?

0:00:46.420,0:00:50.399
Near to this point, right? So, how can we tell them apart?

0:00:51.280,0:00:54.960
first of all, I had to take it if this is the zero I will have to

0:00:57.190,0:01:02.850
Talk to suppose the talk. What am I doing here in my translation? How do you translate stuff?

0:01:06.460,0:01:08.460
Matrix multiplication

0:01:10.540,0:01:12.929
Okay, what does matrix multiplication do

0:01:15.729,0:01:17.729
Rotation reflection

0:01:18.880,0:01:23.519
Scaling and shear right shear what it is pronounced actually scaling

0:01:25.750,0:01:28.880
How can you do scaling? also
why are scalars called scalars?

0:01:31.450,0:01:33.450
do because they

0:01:34.479,0:01:36.479
Scale yeah good

0:01:37.210,0:01:42.540
Right, right, so you can always think about a matrix. You can just normalize it such that you have like unitary

0:01:43.270,0:01:46.380
determinant and then you actually have a scalar which is changing the

0:01:46.750,0:01:52.139
You know the size right 
so you can have I usually think about matrices as rotation

0:01:52.140,0:01:57.269
And so I usually just say we rotate stuff in whatever dimensional space

0:01:57.850,0:02:02.460
And then we are going to be doing another operation with neural nets which is gonna be squashing

0:02:02.560,0:02:05.970
So you're going to be me repeating these very many times

0:02:06.520,0:02:12.450
Neural nets are simply rotation and squashing rotation and squashing was coming after

0:02:13.930,0:02:15.850
rotation and then

0:02:15.850,0:02:21.989
Fantastic. All right. So let's get starting. I started then, you know just because I like advertising

0:02:22.560,0:02:24.560
I just put there my handle

0:02:25.810,0:02:29.910
And again, if you need to say anything just call me ALF have no idea what's going on

0:02:29.910,0:02:36.779
Please repeat and if you don't know who that guy is you may have a look about TV series from the nineties

0:02:37.510,0:02:40.170
anyhow, Oh a double n

0:02:41.050,0:02:42.700
What is it?

0:02:42.700,0:02:45.869
Artificial neural networks, I guess. Yeah 
supervised learning

0:02:46.660,0:02:50.100
Classification so there's gonna be kind of a revised of stuff

0:02:50.100,0:02:57.239
He has already seen before but you know in a very much prettier way because I spent the whole day making this stuff for you

0:02:57.610,0:02:58.320
All right

0:02:58.320,0:03:05.399
so let's go on and we have this guy right we have seen this last time and let's say those are

0:03:05.739,0:03:07.739
simply three branches of a spiral

0:03:08.110,0:03:12.959
So my case in this case, what is where is gonna be my data living?

0:03:13.900,0:03:18.500
On so, where is the data here if I show you this stuff here?
[Audience: branches]

0:03:18.940,0:03:23.160
Those branches are made of points and these points live on what space

0:03:24.400,0:03:29.910
R2 right is the plane. So all those points are moving around that plane. Why did I show colors?

0:03:31.720,0:03:35.669
Those are the labels, right so 3 different classes, 3 different labels

0:03:36.640,0:03:42.089
You can make this drawing with matplotlib and Python and numpy. Okay

0:03:43.209,0:03:43.989
on the other side

0:03:43.989,0:03:50.249
oh, yeah, so we have like a T goes from 0 to 1 and then C is gonna be the class from 1 to

0:03:51.250,0:03:53.250
capital C in this case

0:03:53.380,0:03:56.459
Let's make things a little bit more spicy. So let's add some

0:03:57.880,0:04:00.509
Crap there such that we have more

0:04:01.239,0:04:04.229
Crappy looking data, that is like actually more really sorry

0:04:05.200,0:04:06.859
Okay

0:04:06.859,0:04:08.859
What does classification?

0:04:08.989,0:04:16.298
Mean if you'd like to do craft classification, let's use the you know, whatever thing you want what's called?

0:04:18.470,0:04:24.820
The thing that no regression logistic regression, so what does logistic regression do here in this case

0:04:25.789,0:04:31.989
So it's going to do something like this right some linear planes for separating the data. What's the main issue here?

0:04:35.360,0:04:37.100
Second
[Audience]

0:04:37.250,0:04:42.970
It's not linearly separable, so but what is the main issue here? How what how would you define was the issue here?

0:04:42.970,0:04:45.639
Like yes, they are not linearly separable therefore

0:04:46.340,0:04:48.340
you have
[Audience]

0:04:49.759,0:04:54.459
Yeah, but what doesn't what don't you like from that drawing

0:04:54.700,0:05:02.400
[Audience]

0:05:02.690,0:05:07.779
Right, so points in one region you have multiple classes, right?

0:05:07.789,0:05:13.239
This means that those branches are crossing my decision boundaries, which are linear

0:05:13.880,0:05:15.880
Huh? So, how can we fix that?

0:05:16.000,0:05:21.000
[Audience]

0:05:21.409,0:05:23.409
Okay, because it will be seen my

0:05:24.470,0:05:28.749
Video from from last week. So usually how people saying oh just you know

0:05:29.300,0:05:33.760
Like make those decision boundary non linear, right? That's what use or the other professors do

0:05:33.760,0:05:38.409
I do only cool the things that are more cool. So I just get the data to be linearly separable, right?

0:05:38.409,0:05:40.929
That was just different perspective to see the same stuff

0:05:41.180,0:05:47.680
Anyhow, the main issue here is that we have intersection between these decision boundaries and data and therefore

0:05:48.919,0:05:55.179
I'm gonna try to do this right and you already seen the video from last time. So, okay. This is no news or

0:05:55.849,0:05:57.849
the thing that you may be more useful

0:05:58.220,0:06:04.089
See how the decision boundary is over training will try to adapt. What is the you know?

0:06:05.509,0:06:10.929
Like the the distribution of this data here. We are watching things from the bottom up

0:06:10.930,0:06:16.810
so if you have your network where the first layer is on the bottom and the last layer is on the top and if you

0:06:16.810,0:06:22.679
Draw networks in the other direction you get one grade less. So the input is gonna be on the

0:06:24.610,0:06:26.610
Bottom why is that?

0:06:28.509,0:06:33.179
Why do we have input on the bottom does anyone can anyone guess?

0:06:36.039,0:06:38.800
Yeah, well their neural network
[Audience]

0:06:39.069,0:06:40.469
Aha, that's correct, right

0:06:40.469,0:06:43.979
So we have low level features at the lower part of the network

0:06:43.979,0:06:49.949
And as you climb up in the hierarchy, you basically want to draw this network in this the way it is made, right?

0:06:49.949,0:06:52.859
So you have high in the hierarchy in the upper part

0:06:52.860,0:06:58.319
So if you put some classifier you will put a classifier on top, right?

0:06:58.319,0:07:01.139
so if you the first time I was reading a paper now from

0:07:01.719,0:07:06.419
Geoffrey he's blah blah. I put a classifier on top of

0:07:07.090,0:07:13.560
of the network I'm thinking. What's the top of a network? I have no idea, right?

0:07:13.560,0:07:17.969
So networks are drawn from bottom to top with the first layer on the bottom where you have the input coming in

0:07:18.279,0:07:25.559
Lower level features is you climb up you have the top and therefore if you have multiple outputs, it's called a multi-headed network

0:07:25.560,0:07:28.619
Right like a hydra, whatever. It's called

0:07:29.440,0:07:35.940
Anyhow, so we're gonna figure out now in this lesson how we can do these things. Do you know how to do this stuff?

0:07:37.690,0:07:42.209
No, yes, okay you should be because you should have taken machine learning before

0:07:43.000,0:07:49.049
But perhaps is not linearly separable, right? So we just add one more layer and things start working. Anyhow

0:07:49.839,0:07:52.319
Training data. Okay. So yes yesterday

0:07:52.319,0:07:58.468
Well last week we have seen that a neural network when you just initialize it, it makes some kind of transformation

0:07:58.900,0:08:05.250
So we are feeding that kind of cloud the walls. It was which was sample from a Gaussian distribution with a

0:08:06.310,0:08:11.399
Identity matrix as covariance matrix and mean zero. So what was the roughly?

0:08:12.669,0:08:14.908
average radius of that cloud of points

0:08:14.908,0:08:16.000
[Audience: 3]

0:08:16.100,0:08:18.270
3 you actually remember very good

0:08:20.169,0:08:27.269
The radius was 3 right so things were within a radius of 3 and those kind of circular shape

0:08:27.800,0:08:30.199
It was feed inside the network and the network was

0:08:30.780,0:08:34.729
Getting you any kind of arbitrary transformation, which was super pretty right?

0:08:35.789,0:08:38.088
Yes, it was very pretty. Yeah good

0:08:39.120,0:08:41.120
but then the transformation wasn't

0:08:41.550,0:08:47.450
Instrumental to do anything right? So today we're gonna see how by using data we can enforce

0:08:47.970,0:08:53.839
Some kind of meaning over that kind of transformation that a network does by itself

0:08:54.240,0:08:58.820
And so data is going to be the most important part. So here we're gonna have

0:08:59.519,0:09:01.519
That should be in pink

0:09:01.950,0:09:06.769
Yes, it's too bright in here. So whatever. So the X is going to be my input data

0:09:06.769,0:09:13.099
It's bold because represents a vector and this guy lives in our end. Okay, how much is n in our case?

0:09:15.089,0:09:17.089
Two because
[Audience]

0:09:17.490,0:09:20.029
Because points live on that space right? Hope the spires

0:09:20.640,0:09:28.459
Okay, fantastic, and then it's gonna be my ice sample. I have several samples, right and this takes quite long to draw

0:09:29.519,0:09:31.849
you have several samples and they're like

0:09:32.190,0:09:37.219
Row vectors and I put them I stack them one on top of each other and I have M of them

0:09:37.220,0:09:40.040
Right, so that matrix. What what is the size of this matrix?

0:09:42.270,0:09:48.619
Shout louder n by m fantastic, so I have "n" columns and "m" in the height

0:09:49.140,0:09:54.140
if I would use these metrics for doing some operations, what is the dimension where I'm shooting to

0:09:57.990,0:09:59.130
Try again

0:09:59.130,0:10:00.209
"m" And okay

0:10:00.209,0:10:05.749
because the height of the matrix is gonna be the dimension where you shoot to and then wait for the domain the metrics in a

0:10:05.750,0:10:12.000
Bigger dimension where you shoot from right because you multiply you know row times columns.  right?

0:10:12.500,0:10:13.009
All right, cool

0:10:14.520,0:10:18.499
Sorry, we have Ci which is going to be my different classes for each of those

0:10:19.470,0:10:21.060
points in the 2d plane

0:10:21.060,0:10:27.409
And so here we're gonna have those Ci 's gonna be equal to 1 to capital K before it was capital C

0:10:27.410,0:10:29.410
I see how to fix that. I know

0:10:30.120,0:10:32.659
So how many how much is capital K here?

0:10:33.750,0:10:35.750
3 because we

0:10:36.660,0:10:43.180
Have seen 3 colors. Yeah, fantastic. All right. So if I stack all the Cᵢ how many Cᵢ do I have?

0:10:45.650,0:10:49.809
Second, M.
So you should stack M of these guys you get a

0:10:53.420,0:10:55.420
Likely a column vector here C

0:10:55.880,0:10:58.479
Alright, finally, this is a height of M

0:10:58.910,0:11:03.100
But the problem with this kind of notation like 1 2 3, whatever

0:11:03.560,0:11:10.780
Basically is that they introduced some kind of ordering right? So class 1 comes before class 2 which comes before just 3 and

0:11:11.630,0:11:15.580
It doesn't make any sense, right? They are colors. So it's a categorical distribution

0:11:15.580,0:11:20.439
I don't want to have something that also has order therefore. I'm gonna be using this alternative

0:11:21.020,0:11:28.419
representation which I'm gonna be basically converting those T's into vectors of the size of my capital K

0:11:28.420,0:11:35.020
so the number of classes and then I'm gonna have a 1 in correspondence of the class, which is indexed by the

0:11:36.680,0:11:38.680
See the specific Ci, ok?

0:11:38.810,0:11:44.440
So let's say Ci is gonna be equal 1 that you have basically the first guy here

0:11:44.440,0:11:48.640
Ok, and since we are talking about mathematics I can count from 1

0:11:48.640,0:11:51.489
right 1 2 3 if you are talking about Python and

0:11:51.950,0:11:58.210
C++ whatever so you switch gear you have different hot., you can count from 0, Math you count from 1

0:11:59.870,0:12:05.049
So if you stack all those Ci's converted into that representation what you get

0:12:08.960,0:12:10.960
We don't have numbers just use letters

0:12:12.860,0:12:14.919
You have M by K matrix, right

0:12:14.920,0:12:21.400
So this is going to be my capital Y matrix and then you have my capital K number of columns and M

0:12:22.070,0:12:26.499
Number of rows and each of these guys here is going to be a vector

0:12:27.440,0:12:31.419
which is {0,1}ᴷ where only one

0:12:31.640,0:12:37.330
item is set to one and so you can say that the zero norm is equal to one

0:12:38.540,0:12:44.680
Moreover you can also think about this notation has having sound probability math

0:12:45.470,0:12:47.390
Which is completely

0:12:47.390,0:12:54.489
Concentrated in one specific spot right? So you have three possible spots. You have three possible classes you put all your

0:12:55.310,0:12:58.060
100% bet over that specific category

0:12:59.000,0:13:06.849
The network will try to approximate this I won't be able to but that's how we train a network with these kind of hard labels

0:13:08.240,0:13:10.240
questions so far

0:13:10.610,0:13:15.099
Sorry that was exercise questions so far am I too slow?

0:13:16.550,0:13:19.000
Yes a little bit. No, okay

0:13:19.760,0:13:23.289
Do you like the font the colors? Okay. Thank you

0:13:23.959,0:13:25.790
It takes forever

0:13:25.790,0:13:27.790
All right latex

0:13:28.250,0:13:30.140
That's why we moved to markdown, right?

0:13:30.140,0:13:35.169
All right. Anyhow, so this will be basically the first exercise you have something similar in your first homework

0:13:35.240,0:13:37.690
So we skip this because it's gonna be due for two weeks

0:13:37.690,0:13:41.979
And basically if this would have been a tutorial it would have been typing stuff now

0:13:42.110,0:13:42.730
All right

0:13:42.730,0:13:43.310
so

0:13:43.310,0:13:50.950
Let's see how a fully networked fully connected network works and look like so at the bottom Y is at the bottom

0:13:53.089,0:13:55.089
Say again

0:13:55.910,0:13:57.910
The input is at the bottom Y

0:13:58.760,0:14:01.089
Lower-level feature fantastic. What's the color of the X?

0:14:01.880,0:14:04.929
Pink, I mean, yes, that's correct. But yeah

0:14:05.870,0:14:11.440
Then we get a fine transformation is shown there by the arrow and then we get into that

0:14:11.959,0:14:15.039
Green F where F is gonna be a non-linearity

0:14:16.160,0:14:22.060
The output of the F is going to be called H H which is representing my hidden layer, okay

0:14:22.060,0:14:27.130
So H is something that is inside a network and I can't see from outside. And so it's called hidden

0:14:27.860,0:14:29.860
It's bold because it's a vector

0:14:31.640,0:14:33.759
Moreover then I have another affine transformation

0:14:33.760,0:14:39.760
You only see the matrix there which maps to G which is another nonlinear transformation

0:14:39.760,0:14:42.849
And now you have the final output which is ŷ

0:14:42.850,0:14:49.269
And so the output color is it don't say white, but what's the bubble output bubble color?

0:14:50.060,0:14:56.500
Blue right and then the hidden is gonna be green. It's gonna be always constant. All right, so these are the only questions

0:14:57.170,0:14:59.170
Basically, you're gonna see in in this course

0:14:59.779,0:15:05.229
You have the hidden layer H vector is gonna be it is nonlinear

0:15:05.850,0:15:13.370
Function point-wise nonlinear function which is so element-wise nonlinear function of a fine transformation of the input

0:15:13.370,0:15:15.089
Which is the X?

0:15:15.089,0:15:16.350
here

0:15:16.350,0:15:20.509
Plus, you know the bias, right? So this is a linear operator plus the bias

0:15:20.509,0:15:23.899
This is a fine transformation. And then the F is gonna be your nonlinear mapping

0:15:24.990,0:15:29.689
Again, then you have your ŷ which is gonna be my output of the network

0:15:30.689,0:15:32.989
My hypothesis is gonna be a nonlinear function

0:15:33.060,0:15:39.019
I'll apply to each element of this vector here and this vector is basically a affine

0:15:39.269,0:15:43.818
Transformation of the hidden layer. Okay, that's all you get in a neural network

0:15:44.850,0:15:47.480
Affine transformation. I usually call them rotations

0:15:47.750,0:15:54.889
I called nonlinear function as squashing and so you just repeat rotation squashing rotation squashing rotation squashing

0:15:57.300,0:16:05.240
Fantastic, thank you. All right, so that's it right easy, right so far know you have a question Mike interphase ask

0:16:07.259,0:16:09.259
What's up?

0:16:09.259,0:16:12.300
[Audience]

0:16:12.689,0:16:17.689
Yeah, both F and G are arbitrary nonlinear functions you can use anything you like

0:16:17.689,0:16:19.689
[Audience]

0:16:20.309,0:16:26.299
This is only one hidden layer my output layer is gonna be my blue guy. Let's add the output

0:16:26.730,0:16:28.399
You can see ŷ on top

0:16:28.399,0:16:31.698
So if output and the X is going to be your input on the bottom

0:16:31.709,0:16:37.849
So I call this one 3 layer neural network Yann looks like he's calling this 2 layer network

0:16:38.100,0:16:42.709
I call it free day on your network because there is a input neuron at the bottom

0:16:42.709,0:16:50.239
There is a hidden neuron at the center and it is an output. So 1 2 3, but he counts from zero like

0:16:52.540,0:17:00.149
Programmers so 2 layer, but no see how many affine transformation does a 3 layer neural network have?

0:17:00.149,0:17:01.570
[Audience: 2]

0:17:01.570,0:17:03.809
Fantastic how many layers of neurons you have?

0:17:05.920,0:17:08.580
3 okay, cool. Yeah question

0:17:08.580,0:17:12.200
[Audience]

0:17:12.460,0:17:17.159
Aha, yeah, well fine transformation right so there is also translation

0:17:17.159,0:17:20.100
[Audience]

0:17:21.130,0:17:28.380
Because I liked usually to extract the scaling and scaler from the matrix and then I have my unitary

0:17:29.200,0:17:31.619
determinant matrix, which is basically

0:17:32.200,0:17:34.469
Rotating stuff and then you had the other one which is scaling

0:17:34.540,0:17:38.639
They also have flipping right for if you have the determinant, which is negative, right?

0:17:39.370,0:17:43.770
Usually matrix is just rotating stuff. It's a bit hard to think about in high dimensions

0:17:43.770,0:17:50.790
I just say matrices rotate stuff because they apply the same kind of movement of everything right so it's kind of

0:17:51.310,0:17:53.310
global operation

0:17:53.800,0:17:55.800
Other questions

0:17:56.530,0:17:58.530
All right, so

0:17:58.570,0:18:02.909
Example of nonlinear functions are here a few. So the first one is positive part

0:18:03.220,0:18:10.530
basically, you get the positive as it is if it's negative you set it to 0 other people call it ReLU, rectified linear unit, or

0:18:11.140,0:18:13.140
Other stuff I don't know

0:18:14.140,0:18:25.479
Yeah, whatever. I like positive part, which is math ish. Then there is a sigmoid which is 1/(1+ e⁻ˣ) whatever x.

0:18:25.810,0:18:28.859
Hyperbolic tangent which is just a rescale version of this sigmoid

0:18:28.860,0:18:31.950
We saw that last time then there is the soft arg max

0:18:32.320,0:18:33.730
You're going to call it this way

0:18:33.730,0:18:38.490
Because it's going to be just a softer version of a arg max, an arg max is going to give you all zeros

0:18:38.490,0:18:41.760
but an index equal to 1 in correspondence of the

0:18:42.610,0:18:45.120
correct highest value its soft max

0:18:45.120,0:18:52.530
It's gonna give you something like that where it's gonna be almost 1 on the highest value kind of 0 ish everywhere else

0:18:52.530,0:18:54.839
Right, but if you have 2 guys at the same height

0:18:54.840,0:18:58.800
You're gonna get half and half and the rest is going to be kind of 0. Okay. Yeah

0:18:58.800,0:19:04.800
[Audience]

0:19:05.250,0:19:06.550
AAAAH

0:19:06.550,0:19:08.550
Guess this is a nice derivative

0:19:09.730,0:19:15.150
This guy is easy to I think used for training I think you could use

0:19:15.910,0:19:17.910
That kind of normalization span

0:19:18.550,0:19:21.329
So the question was like why don't we use a like?

0:19:22.000,0:19:24.000
Resizing like why don't you?

0:19:25.660,0:19:27.010
Automatically

0:19:27.010,0:19:29.099
you know set the output to be within a

0:19:29.950,0:19:30.780
0 to 1 range

0:19:30.780,0:19:35.550
I guess because it's gonna be dependent on the output right, you change the output you have to change all the time

0:19:35.550,0:19:41.550
With scaling. This one is just one scaling all the ways the same I guess. Yeah, that could be the answer

0:19:42.700,0:19:44.700
all right, so

0:19:44.740,0:19:51.449
Uh-huh. Okay, and this took five hours drawing. Okay. We have our X on the left-hand side with five elements

0:19:52.720,0:19:55.140
Then here you have for example your first hidden layer

0:19:55.300,0:20:02.339
I may have a second hidden layer a third hidden layer, and then finally my output layer. So how many layers does this network have?

0:20:02.339,0:20:10.300
[Audience]

0:20:10.780,0:20:13.379
How many how many columns can you count here?

0:20:14.230,0:20:16.230
5. Okay. Fantastic. How many?

0:20:16.840,0:20:20.129
Gaps between columns. Can you count?
 4 only

0:20:21.010,0:20:25.080
Rotations you have. Okay, cool. All right, so we go from the first

0:20:25.630,0:20:29.640
Layer, which is also called a⁽¹⁾   we start the activation at the layer 1

0:20:30.250,0:20:34.170
We go to the activation at layer 2 and so on a⁽³⁾, a⁽⁴⁾

0:20:34.840,0:20:37.649
until the a capital L [a⁽ᴸ⁾] the last one so

0:20:38.320,0:20:43.830
We got from activation at layer 1 to activation layer 2 the W⁽¹⁾ matrix

0:20:44.830,0:20:46.270
basically

0:20:46.270,0:20:53.879
You go is lie with W⁽²⁾, W⁽³⁾  and so on right? So, how do you get that first? Neuron? Can you see anything?

0:20:54.760,0:20:57.989
So, okay. Let me see if I can make not too dark

0:21:00.340,0:21:02.340
Is the any better

0:21:02.920,0:21:10.560
Kind of okay, I whatever I go like this boom. Okay, anyone taking notes on paper?

0:21:10.560,0:21:12.130
[Audience: yes]

0:21:12.130,0:21:13.930
Sorry, okay

0:21:13.930,0:21:15.550
Yeah, whatever

0:21:15.550,0:21:19.619
After I turn on the light, okay. So how do you get the values for this guy here?

0:21:19.620,0:21:24.030
So this guy is gonna be the J's neuron on my what layer?

0:21:26.140,0:21:31.710
Second layer, right so aⱼ⁽²⁾ so this is gonna be my element-wise. Sorry, my non-linear function f

0:21:32.680,0:21:39.239
Where I have this W⁽ʲ⁾ which is the j's role of W(1) matrix, which is multiplied by X

0:21:39.670,0:21:42.180
So I have a row times a vector you get a scalar

0:21:45.460,0:21:48.210
Thank you and then plus bⱼ. What is bⱼ?

0:21:50.410,0:21:52.410
It's a scalar. Yes, that's correct.

0:21:52.780,0:21:57.899
Also called bias, right? Okay. What a choice of words. Sorry

0:21:58.570,0:22:02.129
and this is gonna basically be like the sum of the

0:22:03.370,0:22:04.870
scalar

0:22:04.870,0:22:12.180
Multiplications right? I mean the sum of the multiplications, and so how do you get these on? you get these guys, you multiply them by

0:22:12.790,0:22:15.389
the weights and then you get the first guy right and

0:22:17.500,0:22:19.390
Then for the second one

0:22:19.390,0:22:23.500
you try copy and paste doesn't work. So you to draw all those lines again!

0:22:23.600,0:22:24.890
[Audience: laugh]

0:22:24.890,0:22:28.900
And then you start realizing that you made a very bad decision

0:22:31.280,0:22:37.420
When drawing everything else. Okay. So where are these weights toward?

0:22:39.320,0:22:43.090
In w⁽¹⁾. Okay fantastic not gonna be fast forward you

0:22:43.910,0:22:45.910
know fast forward

0:22:45.920,0:22:49.389
Yeah, how pretty no this is PowerPoint ninja skills

0:22:50.360,0:22:53.860
Okay, I should be doing advertisements for and this should pay me

0:22:54.500,0:22:55.640
Microsoft

0:22:55.640,0:22:59.320
All right. So this is on your network, right? How pretty is it?

0:23:00.470,0:23:02.470
Thank you
[Audience: laugh]

0:23:04.070,0:23:06.399
Okay, I give you back the light here I

0:23:08.300,0:23:14.379
Should just turn on right everything I guess. I had let me know if we can turn off the first line of light

0:23:14.810,0:23:17.950
Okay, we had to figure that next time I guess. All right, so

0:23:18.950,0:23:25.539
We just use this representation where each of those layers you were observing before I'll just condense in one

0:23:25.910,0:23:29.440
Bold here. So in this case h is a vector, right?

0:23:29.440,0:23:33.610
And so a vector of whatever number of elements is just represented there in on the screen

0:23:34.550,0:23:41.080
And the same for the y. So here you had those matrices which are shooting the right directions the right right dimensionalities

0:23:41.840,0:23:44.590
And then here you had the non-linear functions

0:23:45.380,0:23:47.380
so you can think about my

0:23:47.720,0:23:49.720
ŷ here, This guy is

0:23:50.210,0:23:54.939
Some kind of ŷ function of my current input X, right?

0:23:54.940,0:24:01.269
So the X the pink guy it's feed into the network and then a network will give me a some kind of you know

0:24:01.270,0:24:03.879
I expect you to have this kind of output prediction

0:24:04.400,0:24:11.560
So this can be thought as a function that map's ℝⁿ to our C, where C is number of classes. That was capital K

0:24:11.560,0:24:13.560
But I guess I had to fix this right

0:24:14.330,0:24:16.720
so you can see these as mapping inputs to

0:24:17.600,0:24:19.600
final predictions

0:24:20.060,0:24:22.330
Usually it's better to think in a different way. So

0:24:22.850,0:24:26.469
What's happening is actually you met these ℝⁿ to some intermediate

0:24:27.020,0:24:32.440
Representation ℝᵈ  and then you go finally to the final classification dimensions

0:24:32.540,0:24:36.789
where d the of the internal layer it's much much larger than the

0:24:37.520,0:24:42.999
Input and output dimensions. Why is that? because whenever you go in a very high dimensional space

0:24:44.179,0:24:46.179
Everything is far

0:24:46.400,0:24:48.729
Like really really really really really far apart

0:24:49.789,0:24:52.149
and so if things are very far

0:24:52.190,0:24:53.750
It's very easy to rotate

0:24:53.750,0:25:00.579
Stuff and get things to move a little bit then right if you go everything a very tiny little crumb space you try to move

0:25:00.580,0:25:01.460
things

0:25:01.460,0:25:07.449
Everything moves together right? But then if you go into this intermediate space where everything is so far apart

0:25:07.450,0:25:12.220
You can just kick things around. Okay, and it's much easier. So going to a higher

0:25:13.250,0:25:15.169
intermediate representation

0:25:15.169,0:25:17.649
dimensional intermediate representation, it's really really really

0:25:19.460,0:25:21.460
Helpful so

0:25:22.730,0:25:29.439
Potentially you can have a very fat Network right? You have an input very fat hidden layer. Just an output the cool part is that

0:25:29.990,0:25:32.799
if I have hundred neurons in my hidden layer, I

0:25:33.409,0:25:36.309
Can simply use 2 hidden layers of

0:25:37.039,0:25:43.089
10 neurons. There's going to be performing roughly the same. So instead of having this very very fat intermediate layer

0:25:43.090,0:25:50.260
I can just decide to stack a few hidden layers and the number of combination of those neurons, you know will

0:25:50.809,0:25:52.059
grow in exponentially

0:25:52.059,0:25:58.658
So it's, if you want a 1,000 layer want 1,000 neuron hidden layer you can just have 3

0:25:59.030,0:26:01.030
hidden layers of 10, right?

0:26:01.030,0:26:06.900
[Audience]

0:26:07.039,0:26:11.529
So in the second case where you have a cascade of things you will have data

0:26:11.750,0:26:14.079
Dependencies and you will have to wait for the guys

0:26:14.630,0:26:20.530
Down to have finish before starting the next operation. So by definition the more layers you stack the slower you get

0:26:21.140,0:26:27.640
on the other case you need to have so many many many more units in the other case in order to be able to

0:26:28.669,0:26:33.459
Go to actually be as good as you are by stacking just a few of those layers

0:26:33.679,0:26:38.259
So in other cases you will have to do many computations as well if you get parallel software

0:26:38.260,0:26:42.369
I guess like power parallel hardware, then you can prefer maybe those large

0:26:43.010,0:26:45.280
Versions, but I wouldn't go for those

0:26:45.280,0:26:50.900
[Audience]

0:26:51.140,0:26:56.559
Yeah, that's correct is bad so it takes also much more space in memory, all right so much time

0:26:57.260,0:26:59.260
We make noise

0:26:59.750,0:27:01.750
Okay, I can't watch my watch

0:27:02.179,0:27:04.130
Okay, never mind

0:27:04.130,0:27:06.070
All right, so, okay

0:27:06.070,0:27:08.559
So in this case my input lives in a 2 dimensional space

0:27:08.900,0:27:12.939
My hidden layer is going to be living a hundred dimensional space in my final class

0:27:13.130,0:27:16.780
my final output is going to be living in a 3 dimensional space and

0:27:18.950,0:27:24.160
This would be the second part where you start doing stuff with your computer, but we are in class we have no time

0:27:24.160,0:27:26.160
So a neural network training

0:27:26.510,0:27:30.699
Huh? How does it work? So we use this guy here the soft Arg max?

0:27:30.700,0:27:33.730
Which is a software version of the max of the art max, right?

0:27:35.179,0:27:40.899
Which is simply the fraction of the exponential divided by the sum of all the exponential of the other items, right?

0:27:40.900,0:27:45.489
We already seen this yesterday. Why do I write these stuff between zero and one?

0:27:48.799,0:27:51.879
Why it doesn't why didn't I use the square brackets?

0:27:51.879,0:27:55.900
[Audience]

0:27:56.270,0:27:59.919
So the answer was it's very unlikely I would say it's impossible why it's impossible?

0:28:01.610,0:28:03.939
Because the exponential function of the numerator is

0:28:05.840,0:28:08.350
Strictly positive, why doesn't reach one?

0:28:08.350,0:28:12.600
[Audience]

0:28:12.710,0:28:17.949
Because the exponential is strictly positive. That was the answer, correct

0:28:19.400,0:28:23.229
Yeah, the bottom doesn't is always going to be slightly larger than the numerator, right?

0:28:25.460,0:28:26.110
All right

0:28:26.110,0:28:32.229
So the input to be soft Arg max layer is called logit and the logits are the output the linear output of the network

0:28:35.120,0:28:37.070
Here we're gonna have our

0:28:37.070,0:28:39.580
Total loss the loss for the specific

0:28:40.130,0:28:47.079
for the old data set we have which is function of the Ŷ the prediction of the network for the whole set of

0:28:47.240,0:28:52.059
inputs and the vector C which is this vector of labels and

0:28:52.340,0:29:00.160
Which is gonna be simply here the average of this lower case and this curly, which is basically the per sample loss

0:29:00.740,0:29:03.160
Yann [Yann LeCun] uses capital L for that thing

0:29:04.100,0:29:11.290
so in this case, if I do if I want to do classification, my per sample loss is gonna be basically  - log of

0:29:11.870,0:29:13.190
the

0:29:13.190,0:29:15.190
output of the

0:29:15.260,0:29:18.760
Soft Arg max at the correct class C. Okay, so

0:29:19.520,0:29:23.440
C is gonna be my correct class, which is that index one hot

0:29:24.410,0:29:28.479
Actually see was the the Cardinal or Deena no Cardinal number, right?

0:29:29.480,0:29:34.570
The blue y was the one hot encoding and my ŷ is gonna be the output of the network?

0:29:34.760,0:29:41.349
Which is the output of the soft arg max, so my per simple also gonna be the - log of that

0:29:41.870,0:29:42.900
Soft arg max are the correct

0:29:44.630,0:29:46.839
Case class right? So

0:29:47.510,0:29:53.379
Do you understand what I said? Yes, so just to test your understanding you're gonna be telling me what's right next

0:29:53.600,0:29:58.810
So let's say yeah, this loss is also called cross entropy or negative log-likelihood

0:29:59.750,0:30:02.560
So let's say my pink X which looks white

0:30:03.830,0:30:11.199
Is there and then I have my class the orange class equal to 1, therefore what's gonna be my blue y?

0:30:12.080,0:30:14.259
The one hot and coding of the class C

0:30:17.660,0:30:19.660
1 0 0 okay. Fantastic

0:30:19.730,0:30:23.560
alright, so let's say I feed these X to my

0:30:24.230,0:30:30.790
Neural net. So I'm going to compute these Y or case missing a white. Sorry, my bad. So here is a Y, right?

0:30:32.270,0:30:34.359
ŷ of this item here

0:30:35.750,0:30:39.520
So why do I write almost? No, sorry. Sorry. Sorry, my bad

0:30:39.620,0:30:46.209
So I put these X here inside this ŷ and the output of the network is gonna be this almost 1 almost 0 almost

0:30:46.210,0:30:48.210
0 what is almost 1?

0:30:48.210,0:30:51.300
[Audience]

0:30:51.500,0:30:53.530
Is it 1⁺ or 1⁻?

0:30:53.530,0:30:55.900
[Audience]

0:30:56.420,0:30:58.420
What is almost 0?

0:30:59.330,0:31:02.319
0 plus very good and what about the last one?

0:31:04.040,0:31:07.749
0 plus good. Alright, so if I have this guy here

0:31:08.540,0:31:12.159
What's going to be my percent per loss so my percent per loss is going to be

0:31:13.070,0:31:20.080
These output almost 1 almost 0 almost 0 and then class number 1. Ok. So what do I get here?

0:31:22.850,0:31:27.969
So here you're going to be computing the what is ŷ of C

0:31:29.840,0:31:31.840
ŷ of C is gonna be

0:31:33.890,0:31:39.670
Hold on yes. So ŷ of C is gonna be almost 1 which is 1⁻

0:31:41.270,0:31:43.869
okay, then you have log of 1-

0:31:43.869,0:31:49.900
[Audience]

0:31:50.150,0:31:52.150
So it's gonna be

0:31:53.840,0:31:55.870
0⁻ then it is a minus in front

0:31:57.800,0:32:02.649
You get 0⁺, this correct very good. How about so basically if you

0:32:03.170,0:32:09.459
input these X in my network the class I expect to have for that input is 1 and my network says

0:32:10.309,0:32:12.309
1 0 0

0:32:12.320,0:32:18.490
the loss is gonna be which basically is the penalty for saying, you know say bullshit is gonna be

0:32:18.490,0:32:20.490
oh, no penalty you're

0:32:20.990,0:32:27.670
Doing well. Ok. So let's see what's happening instead if my network says oh no. No, this simple is 0 1 0?

0:32:29.630,0:32:36.609
So my percent per loss of 0 1 0 & 1 is going to be what so what is ŷ of C?

0:32:38.390,0:32:42.969
Almost 0, what kind of Almost 0?
0+, what is log of 0+?

0:32:45.290,0:32:48.009
Negative infinity what is with a minus in front?

0:32:49.100,0:32:52.780
So this guy approaches +∞, right?

0:32:52.780,0:32:57.399
So that's why there is a plus there if you just write ∞ that's wrong for me. Okay?

0:32:57.920,0:33:01.119
+∞, -∞, ∞ are three different

0:33:01.640,0:33:03.640
items, different anymore

0:33:04.000,0:33:05.630
Okay makes sense

0:33:05.630,0:33:12.549
So if your networks say bullshit, you're gonna say +∞, very bad Network if the network says uh

0:33:13.190,0:33:18.010
Kind of right answer. You just say. All right, you're doing well. Yeah question

0:33:18.010,0:33:24.900
[Audience]

0:33:25.040,0:33:26.000
Second sorry

0:33:26.000,0:33:32.100
[Audience]

0:33:32.210,0:33:34.160
So ŷ

0:33:34.160,0:33:40.150
the question you cannot see maybe here in if you squint a little bit in gray you have ŷ is going to be a

0:33:40.340,0:33:42.340
function of your input X

0:33:42.860,0:33:50.500
Through two rotations and two squashings one squashing being the positive part and the other squashing being the soft arg max

0:33:50.750,0:33:57.429
Right. And so if you use these per sample loss on the on top of your soft Arg max then you get

0:33:57.980,0:34:04.059
This kind of cross entropy term and then if you compute what's written here you get basically this

0:34:04.430,0:34:09.940
If you input an x and your network is correct. You say good network good boy

0:34:10.460,0:34:16.119
If you stab your network say it's bullshit. Then you're gonna say bad, you know bad network order

0:34:16.790,0:34:18.790
makes sense

0:34:18.790,0:34:23.500
[Audience]

0:34:23.600,0:34:26.110
So ŷ is a vector and

0:34:27.140,0:34:29.140
I choose the seized items

0:34:29.690,0:34:33.039
So that you can see you can think about this one is ŷ?

0:34:34.670,0:34:36.670
Subscript C

0:34:36.710,0:34:41.019
But C is actually the yeah C is gonna be 1, 2 or 3, right?

0:34:41.150,0:34:48.910
So it's gonna be basically ŷ(1) or ŷ(2), ŷ(3). So one of the 3 items elements of ŷ

0:34:49.580,0:34:52.120
Makes sense. No. Yes, you can say no

0:34:54.230,0:34:56.530
Did I answer your question? no

0:34:57.860,0:35:02.920
So ŷ it's a vector of 3 items, 3 elements

0:35:02.920,0:35:06.900
[Audience]

0:35:07.029,0:35:11.789
Yeah here right ŷ if you squint a little bit it's gonna be the output of the network

0:35:13.450,0:35:15.130
Okay K classes yeah

0:35:15.130,0:35:16.829
And the output guy

0:35:16.829,0:35:20.519
So this inside this guy here inside is gonna be the logit to the linear output

0:35:20.710,0:35:23.609
And then you apply this G which is gonna be this soft arg max

0:35:24.549,0:35:31.679
Okay, which is kind of normalizing the outputs to be within 0 and 1 such that the sum of all the items of the output

0:35:31.680,0:35:33.680
Is going to be up to 1

0:35:33.680,0:35:46.900
[Audience]

0:35:47.049,0:35:48.579
So

0:35:48.579,0:35:50.500
Why don't you use it?

0:35:50.500,0:35:54.299
Mean square error, I think young yesterday addressed. Is this question?

0:35:54.549,0:36:01.589
He he actually wrote and wrote and written here's and written on the on the notebook. Right? So but the point is that this is

0:36:02.109,0:36:06.929
The loss function that we use for classification which gives you something that actually works

0:36:07.990,0:36:10.649
Other losses may not be optimal

0:36:10.650,0:36:17.520
So when you like to do classification we use this cross entropy, which also has other statistical property

0:36:17.520,0:36:19.510
I'm not going to be talking about here

0:36:19.510,0:36:22.709
all right, so since I'd like also to go over the

0:36:23.500,0:36:26.039
Notebooks and stuff. I will try to speed up a little bit

0:36:29.079,0:36:31.169
Alright so training how does training work?

0:36:31.420,0:36:38.970
So I just get all the parameters all the matrices like weight matrices biases and whatever I get a set of those

0:36:39.279,0:36:45.449
Weights and I called them Θ is gonna be in the collection of the set of all my trainable parameters

0:36:46.750,0:36:49.469
Such that I can write now my J of Θ

0:36:50.380,0:36:52.380
which is defined as the

0:36:52.569,0:36:53.770
loss

0:36:53.770,0:36:56.729
So it's the same stuff. Why did I change notation?

0:37:00.520,0:37:05.639
So what can you notice here what's inside this function this function is function of the

0:37:06.970,0:37:10.740
Parameters where is this function here. The loss is usually function of the

0:37:12.130,0:37:12.630
output of the network, right?

0:37:12.990,0:37:17.699
This ŷ is the output of the network. in this case since its capital is for the whole batch

0:37:19.119,0:37:25.078
Right. So this is the one the way we use the right when we can be right like the loss the type of loss is

0:37:25.079,0:37:30.448
You simply say or J is going to be my objective function for a optimization problem. We're gonna see it right now

0:37:30.789,0:37:36.599
So how does it work we can think about J being this purple guy here which looks like that

0:37:37.239,0:37:39.239
There I use like a lowercase

0:37:39.789,0:37:47.248
A lowercase θ which is basically a scalar you just think about having one parameter. And so if I have J on the

0:37:48.309,0:37:53.758
Y-axis, I'm gonna have θ on the x-axis, right? So how do you train these networks?

0:37:53.759,0:37:58.109
Usually you start with you know, a randomly initialized Network, which means you know

0:37:58.109,0:38:02.998
you pick just an initial θ₀ value that specific point you can see that the

0:38:03.849,0:38:07.408
The J which is gonna be called training loss

0:38:08.079,0:38:12.568
Has a specific value which is going to be my J at point θ₀.

0:38:13.509,0:38:18.269
There you can compute the derivative which you can't even see. It's a green line. They're

0:38:19.329,0:38:21.329
Parallel to the yeah, you can't see I know

0:38:24.489,0:38:26.489
Sorry again

0:38:26.859,0:38:31.948
Right how pretty so you had the green slope there which is showing you the derivative at that point

0:38:31.949,0:38:39.779
and then that is basically the derivative for my J function with respect to the parameter theta computed at the point θ₀.

0:38:40.659,0:38:43.138
Now the only thing you have to do is going to be taking a step

0:38:44.949,0:38:48.239
Towards left, so is that variable to be positive or negative?

0:38:53.770,0:38:56.249
So do you agree that the relative is

0:38:57.369,0:39:00.689
positive fantastic, but I'm taking a step towards the left and

0:39:01.390,0:39:02.650
so

0:39:02.650,0:39:04.650
how do you do that?

0:39:05.320,0:39:07.320
is actually you just put on -

0:39:07.750,0:39:09.750
Okay. Fantastic

0:39:10.089,0:39:12.779
So this is called how it's called these stuff here? okay

0:39:15.550,0:39:20.580
So this is gradient descent, right, how do you train on your own network?

0:39:22.869,0:39:24.849
In this

0:39:24.849,0:39:31.679
Okay votes. I heard another word here,  who say back propagation here today?

0:39:33.070,0:39:40.259
No, I mean did I okay I did if I didn't mention back propagation yet, right. So, how do we train a network?

0:39:42.790,0:39:45.209
Gradient method right? Okay, cool

0:39:46.660,0:39:52.619
How to publish gradients how do you compute these gradients? So what is this the J in the W(y)

0:39:54.700,0:39:57.510
So this guy is gonna be my J in the Y

0:39:57.510,0:40:02.280
which is the partial with respect to the output times the Y in ∂W right and

0:40:02.500,0:40:06.540
similarly how this what is the partial derivative of the what is the

0:40:07.119,0:40:11.999
Jacobian of my target my objective function with respect to the Wₕ is gonna be my

0:40:12.250,0:40:18.810
partial derivative of the output of the cost with respect to the output of the network then times the Jacobian of the

0:40:20.560,0:40:22.679
The output of the network so with respect to the hidden layer

0:40:22.680,0:40:28.169
And then finally the partial of the hidden with respect to ∂W right? How is called this?

0:40:32.109,0:40:36.539
Back propagation. Okay. All right. So what is back propagation?

0:40:39.070,0:40:42.330
There are two computations, right, how do we train the network?

0:40:44.320,0:40:46.919
Okay, if you get it wrong on the midterm when I fail you

0:40:49.290,0:40:51.679
All right, yes

0:40:51.679,0:40:58.300
[Audience]

0:40:58.410,0:41:06.170
This the left is an exercise why these are plus here, okay. So in the last how many minutes I have

0:41:08.160,0:41:10.160
Five minutes really

0:41:10.470,0:41:12.649
Nine minutes, so that's a lot of time

0:41:13.710,0:41:15.710
So we're gonna go through

0:41:15.960,0:41:17.960
two notebooks

0:41:18.450,0:41:20.450
Maybe, terminal

0:41:23.130,0:41:27.440
cd work github pytorch

0:41:30.450,0:41:32.450
Conda activate

0:41:33.810,0:41:36.889
Mini course. Yeah works

0:41:38.220,0:41:40.220
Jupiter notebook

0:41:44.130,0:41:50.839
Okay, how do I share my screen system preferences yeah, but I cannot see

0:41:54.390,0:41:56.390
Arrangement mirror, okay

0:41:59.670,0:42:01.670
You can see no

0:42:03.450,0:42:07.250
So we're gonna be going through the spider classification right now

0:42:08.970,0:42:10.680
You

0:42:10.680,0:42:11.910
[Unclear]

0:42:11.910,0:42:13.470
thank you

0:42:13.470,0:42:17.030
So since he's gonna be oh you can see I stuff. I don't have to turn off the light

0:42:17.210,0:42:19.210
Okay, so here I'm gonna do basically

0:42:19.530,0:42:21.679
import random shit random stuff

0:42:22.560,0:42:30.169
Torch Torch and an optim, math, ipython such that you can see something I use my awesome default

0:42:30.839,0:42:33.559
Configuration. We have a device. What's the device used for?

0:42:36.780,0:42:43.399
For whatever the device you want to run the stuff only want so if this is taking care

0:42:44.220,0:42:46.099
Here, I just put the same number

0:42:46.099,0:42:50.539
So you see have seen before you should be able to understand this stuff and do it yourself

0:42:51.020,0:42:55.219
That's part of I think kind of the homework next year. We had to do this as homework remember

0:42:58.410,0:43:03.920
All right, so I just visualize the data you're ready seen this stuff, okay, so oh there is no surprise

0:43:04.560,0:43:07.909
So this is a starting point points are having two coordinates

0:43:07.910,0:43:13.160
Like each point has X and Y and then you have a color for the different class. I'm not running it too fast, right?

0:43:13.160,0:43:15.920
I'm not support. You're not supposed to read the code now

0:43:15.920,0:43:19.520
You read the code later and play with a notebook at least one hour right now

0:43:19.520,0:43:24.469
We just go through the notebook just to see that actually runs because you never know. It's an open source project

0:43:26.520,0:43:33.080
Right so linear model I'm gonna be training this guy over here, so I create a sequential which is a container

0:43:33.080,0:43:38.959
I don't have to use it, but it's easier and I put there to linear. What is linear? It's wrong, What's a linear?

0:43:39.810,0:43:43.310
It's on affine transformation. What's an affine transformation? five things

0:43:44.130,0:43:47.359
are gonna be on the midterm so, you know just realize

0:43:48.330,0:43:54.860
I going from D to H, where D is actually the input space H is gonna be the hidden and then from hidden on the output

0:43:55.980,0:44:00.080
Just linear layers, right? So, how are the decision boundaries here?

0:44:02.520,0:44:10.159
Crappy yes, correct linear, so I start this guy trains in a blast and then I show you the output

0:44:11.910,0:44:15.410
Nothing right, bad network

0:44:17.340,0:44:23.539
Its best right. So why are these decision boundaries put in this configuration? why they are not

0:44:24.840,0:44:26.840
rotated?

0:44:30.150,0:44:35.900
Why do you have the yellow area on the left-hand side and not on the other side

0:44:38.610,0:44:46.249
It tries to do it best right, so my accuracy it's 0.5 you can see.
 so what is 0.5?

0:44:48.060,0:44:50.060
Ok who said random?

0:44:53.549,0:44:55.549
Okay, very good

0:44:56.099,0:44:59.149
All right. Otherwise you want to refresh your probability

0:45:00.630,0:45:02.689
Hmm. Okay 1/3, right?

0:45:07.349,0:45:10.548
Okay, you have no time but I wanted to add one more thing

0:45:12.150,0:45:16.549
Yeah, we don't have training losses here
what is the first value of the training loss?

0:45:17.519,0:45:23.419
The first this is the latest value I get right. So this is my final loss all

0:45:24.209,0:45:26.899
0.86, What is the first value of my loss?

0:45:36.450,0:45:38.450
Any idea

0:45:39.210,0:45:42.230
You see my screen here damn I cannot even okay

0:45:43.019,0:45:45.019
Ok figure out how much?

0:45:45.240,0:45:48.049
The first number you get there is no

0:45:49.920,0:45:52.490
why, you figure out if you don't figure out the next week

0:45:52.490,0:45:56.029
I tell you but you should figure it out right so you should try to use your brain, too

0:45:56.759,0:45:57.900
sometimes

0:45:57.900,0:46:02.869
Alright, so let's do something here. I'm gonna be adding this positive part in the center

0:46:03.390,0:46:06.799
Ok, so I just add one more little tiny

0:46:08.009,0:46:13.339
Zero the negative numbers, right? Everything else is the same. I don't change anything, but I

0:46:13.890,0:46:19.009
delete negative number spike with the Zook the thing either. I don't know the English name

0:46:22.380,0:46:27.259
Accuracy Oh No, okay

0:46:30.930,0:46:38.809
See are you excited are you waiting it for it kind of you are sleeping, I guess nice. Oh

0:46:43.049,0:46:45.049
Okay, cool, so

0:46:46.079,0:46:53.449
Yeah, two minutes left try changing the number of hidden layers, right so the try changing the sorry try changing the

0:46:54.000,0:46:55.589
dimension of the hidden layer

0:46:55.589,0:46:57.569
What does it happen?

0:46:57.569,0:46:59.569
if you use

0:46:59.640,0:47:02.809
2 hidden, 2 neurons in the hidden layer of five neurons

0:47:03.539,0:47:05.899
Sorry, try playing with this right and figure out what's happening

0:47:08.549,0:47:11.959
You can comment on Piazza you can start talking you can have like, you know

0:47:13.680,0:47:17.059
Conversations - all right, we really highly encourage you to

0:47:17.789,0:47:23.149
Play with these notebooks and figure out things in the last 30 seconds we go through another notebook

0:47:27.480,0:47:29.480
Which is gonna be exactly the same

0:47:32.609,0:47:35.598
Okay, but in this case my point look like this

0:47:36.960,0:47:38.819
You see anything?

0:47:38.819,0:47:42.469
kind of right so you see like this kind of banana, which is not a banana is like a

0:47:43.410,0:47:50.379
You know double like see Nike. Nike symbol. Alright, so I'm gonna be training here my linear network.

0:47:52.640,0:47:54.640
I train everything

0:47:55.099,0:48:02.379
Okay, I explained now don't worry, alright, so I trained my linear network, what is my linear network the same stuff as before.

0:48:03.079,0:48:05.619
I have sequential a container where I put my tool

0:48:06.950,0:48:08.950
What are those?

0:48:09.319,0:48:13.239
affine transformations. Yes, correct. I sent to the device. Oh

0:48:14.329,0:48:16.689
Okay, how do we train a network right?

0:48:16.690,0:48:22.000
So to train a network you have your input X. These are all my points

0:48:22.000,0:48:24.399
I feed them to the network inside my mother

0:48:24.559,0:48:31.149
And my mother will give me my y_pred right, and the definition of the model. It's this one is this container here? Okay, so

0:48:32.660,0:48:35.980
We have the model we feed we call the whole X you get y_pred

0:48:36.289,0:48:39.489
Then I compute the loss which is gonna be my criterion

0:48:39.980,0:48:47.169
to which is computed over my predictions and wise, the wise are the classes right and the y prediction are those

0:48:47.240,0:48:48.710
output of the network

0:48:48.710,0:48:52.599
In this case the criterion it's written here is going to be our MSELoss()

0:48:53.089,0:48:55.719
So we change before we were using actually a cross entropy

0:48:56.960,0:48:59.169
- log of the softmax, soft arg max

0:48:59.240,0:49:03.849
Now we are going to using a MSE because we are using a regression in a very no regression problem

0:49:03.980,0:49:09.730
So we compute the loss which is the quadratic distance between your output and the target, right?

0:49:10.160,0:49:16.930
So if you talk about the regression we talked about target if we talk about classification, we're gonna be talking about classes in labels

0:49:17.269,0:49:18.829
so labels

0:49:18.829,0:49:20.119
classification

0:49:20.119,0:49:22.119
regression we talk about

0:49:24.049,0:49:25.990
Target, okay. All right

0:49:25.990,0:49:33.669
So we compute the loss here and then I had to clean up whatever happened before so I say to my optimizer

0:49:34.430,0:49:39.639
Clean all the previous left over from the previous operation, which is this zero grad pass

0:49:40.190,0:49:43.659
Then I perform backward. What is backward what is back propagation?

0:49:47.540,0:49:50.680
Computation of the partial derivative of the

0:49:54.620,0:49:56.620
Second

0:49:57.770,0:49:59.860
Partial derivative of

0:50:00.950,0:50:02.950
loss with respect to the

0:50:03.260,0:50:09.910
Parameter, so lost not backwards is simply chain rule and computes all the partial derivative of your final loss which is gonna be in this

0:50:09.910,0:50:16.809
Case the mean squared error with respect to each and every parameter of your model, finally you do a step which is gonna be stepping

0:50:17.990,0:50:21.969
If you that's the gradient you step backwards in the direction of the gradient, okay?

0:50:22.640,0:50:28.299
All right, so we train this network, which didn't have a non linearity and it did something

0:50:28.300,0:50:31.120
I don't know how to interpret this loss and this

0:50:31.970,0:50:34.960
Is gonna be my output of the network? What is it?

0:50:36.440,0:50:38.440
linear regression

0:50:39.920,0:50:46.749
okay boring. Okay. So now I'm gonna do deep networks super exciting things. What do I change?

0:50:48.520,0:50:49.310
AAA

0:50:49.310,0:50:52.749
Just put, I just delete negative values, okay

0:50:53.390,0:50:59.259
How cool is that? All right, so I remove the negative values sometimes here real

0:50:59.260,0:51:04.989
Oh all sometimes I just use a hyperbolic tangent. I just split in two times so you can see the comparison

0:51:05.900,0:51:07.900
I train this stuff

0:51:08.480,0:51:10.370
and

0:51:10.370,0:51:17.200
See, let's see the prediction before actually training. So before training you're gonna get this kind of predictions

0:51:20.630,0:51:21.680
Right

0:51:21.680,0:51:25.930
Kind of shooting towards zero. I know for example line and the

0:51:26.480,0:51:28.810
Green Line is maybe representing the variance

0:51:29.480,0:51:35.650
Here where my cursor is is 0 so the variance here is like 0.2 roughly between all those predictions

0:51:36.320,0:51:40.870
Let's go down and let's watch how the network changed their final

0:51:41.510,0:51:45.459
output given that we have used the loss right and we have

0:51:46.070,0:51:51.940
We've moved against so we are in the mountain there is fog you can't see where the valley is

0:51:51.940,0:51:54.219
it's just walk towards the

0:51:54.860,0:51:57.789
slippery, you know what that thing that goes down right towards the

0:51:58.540,0:52:01.959
down to the body so after doing this procedure several times

0:52:03.020,0:52:05.650
Are you excited to see how this network perform?

0:52:07.430,0:52:11.290
I don't hear you. Okay. Thank you. Alright, so

0:52:12.740,0:52:14.120
Here

0:52:14.120,0:52:24.400
So guess which one? Okay guess which one  is using the positive part and which one is using the hyperbolic tangent?

0:52:24.410,0:52:26.410
Okay, because you're solid Piper's damn

0:52:28.040,0:52:35.410
Okay, so left inside you can see how my network approximates my input is a what is that thing called

0:52:38.660,0:52:43.719
Can you see anything maybe you can see how does this stuff look like?

0:52:46.640,0:52:49.569
This looked like a straight line, right so my network

0:52:50.720,0:52:52.640
it's simply a

0:52:52.640,0:52:54.640
Output is simply a piecewise

0:52:55.220,0:52:56.450
linear

0:52:56.450,0:53:01.240
You know approximation of your input. This is because I just deleted the negative things

0:53:02.570,0:53:04.570
instead if you use the

0:53:07.130,0:53:08.750
What is it?

0:53:08.750,0:53:10.750
Like parabolic tangent. Yes

0:53:11.480,0:53:14.439
You get this guy here how smooth right?

0:53:15.110,0:53:19.929
Okay, why did I do this stuff? This is nicer right? I think

0:53:21.530,0:53:24.550
Okay, first of all, you can see the yellow thing which is the standard deviation

0:53:25.340,0:53:33.069
Which looks like fucked up here and less fucked up on the left hand side right see with the standard deviation is really spiky

0:53:33.440,0:53:37.990
so if you train an ensemble of network, these networks will kind of not agree as

0:53:39.440,0:53:46.690
Consistently as those other dudes on the left hand side moreover. Let me change one line. Let's put here number four

0:53:47.420,0:53:49.420
so I'm gonna be now looking at

0:53:50.030,0:53:54.820
Outside my training region data. Okay, I'm gonna be looking what's happening on the left and right

0:53:55.220,0:53:59.770
What do you expect to see? What do you expect these networks to do when you actually

0:54:00.290,0:54:02.290
Test the network

0:54:02.300,0:54:04.300
outside the training region

0:54:04.300,0:54:10.600
[Audience]

0:54:10.809,0:54:12.809
Okay, good intuition something similar

0:54:12.819,0:54:19.019
so this network will not work, right because network will only be able to

0:54:20.619,0:54:26.939
Generalize over data that is in a similar range if you ask your network that has been trained on this data

0:54:28.270,0:54:30.099
how to

0:54:30.099,0:54:32.099
interpret things that are over here?

0:54:32.740,0:54:35.309
they will say I don't know and

0:54:35.920,0:54:42.510
Unfortunately, the main issue is these are regression networks, so they will not tell you how confident

0:54:43.059,0:54:45.059
They are right, they are absolutely

0:54:45.609,0:54:47.609
I shouldn't say that word

0:54:48.430,0:54:52.740
Alright, whatever I said they won't tell you how confident they are, right. We are just telling you

0:54:53.349,0:54:56.548
Whatever that number but let's see. What's happening now

0:55:01.059,0:55:05.129
So what's going on here now, I just show you a little bit more right, on the side

0:55:06.460,0:55:08.460
The yellow one is going to be the standard deviation

0:55:08.500,0:55:12.750
and the green one is the variance you can see on the left hand side is the

0:55:13.960,0:55:14.710
ReLU

0:55:14.710,0:55:18.299
the rec linearly, the positive part function

0:55:18.670,0:55:23.250
Keeps having the final branches keeping they keep the same slope

0:55:24.130,0:55:27.029
whereas the one train with the hyperbolic tangent

0:55:28.240,0:55:31.409
It will saturate eventually. Okay, and so

0:55:32.980,0:55:38.730
You have to know this network will have side effects the choice of nonlinear function will have a side effect

0:55:39.339,0:55:42.089
Especially if you go outside the training domain region

0:55:42.670,0:55:44.670
Luckily you can use this technique

0:55:45.069,0:55:49.889
the n symbol variance prediction or estimation in order to estimate somehow the

0:55:50.500,0:55:52.109
uncertainty with which the prediction is made

0:55:52.109,0:55:56.909
This is really really really really important in terms of research, right? Sure

0:55:56.910,0:56:02.670
if you have your network that does regression you have no whatsoever clue about its own confidence if you train a

0:56:02.770,0:56:06.989
Bunch of networks that have different initial values you train them all with the same procedure

0:56:06.990,0:56:09.779
you can compute the variance in order to estimate the

0:56:10.839,0:56:12.989
Uncertainty with which a given prediction is made

0:56:14.170,0:56:15.549
with this

0:56:15.549,0:56:19.289
That was it for today. Thank you for listening and I see you now week, bye-bye

0:56:24.339,0:56:26.079
Questions

0:56:26.079,0:56:32.429
Okay other questions, hold on. Hold on. Hold on. Wait, wait, wait a sec. First of all, okay the sky bursts and whatever

0:56:33.670,0:56:35.879
They have to pay attention to what he writes

0:56:36.849,0:56:41.308
We have always the notes coming up on the website by Sunday

0:56:42.160,0:56:46.920
Such that you can revise the content before class if you come to class without revising content

0:56:47.710,0:56:50.220
You may not be fully, you know

0:56:51.099,0:56:53.099
Perceptive for whatever we talk about

0:56:56.260,0:56:58.619
That was it I think, all right. Thank you

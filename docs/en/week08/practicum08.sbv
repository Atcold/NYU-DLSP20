0:00:00.030,0:00:10.010
okay Oh 97 yes almost 100 come on three more please I should invite my mom so

0:00:10.010,0:00:16.109
she hacked this morning conversation it was funny how the heck she managed to

0:00:16.109,0:00:22.140
hack assume only God knows yeah don't join with the two devices

0:00:22.140,0:00:30.359
just increase the number yeah 100 101 ok it's like the dogs all right so

0:00:30.359,0:00:36.180
let's get back to the auto-encoders that we have started auto-encoders

0:00:36.180,0:00:42.750
generative models right and so let's restart by having a quick review about

0:00:42.750,0:00:48.210
the auto-encoders so again we have an input at the bottom in pink as now you

0:00:48.210,0:00:52.079
can see the colors then you have the rotation the affine transformation and

0:00:52.079,0:00:56.969
then you get the hidden layer again another rotation and then you get the

0:00:56.969,0:01:02.309
final output which we are going to be trying to enforce to be close to be

0:01:02.309,0:01:08.670
similar to the input again you have a parallel kind of diagram where each

0:01:08.670,0:01:13.920
transformation is represented with with a box right so in this case people call

0:01:13.920,0:01:17.900
this network is a two layer neural net because there are two transformations

0:01:17.900,0:01:22.740
but what I actually you know advocated is that this is a three layer neural net

0:01:22.740,0:01:27.630
because for me the layers are the activations that's kind of what is

0:01:27.630,0:01:34.200
usually the definition and then Yann uses that new kind of symbols that look

0:01:34.200,0:01:39.600
like a box with a round top okay all right so we have two different diagrams

0:01:39.600,0:01:43.140
here because we can switch back and forth between the representations

0:01:43.140,0:01:47.820
sometimes is easier to use the left one when we want to talk about the single

0:01:47.820,0:01:52.009
neurons but then sometimes we prefer to use the other one which you can also

0:01:52.009,0:01:58.110
like you know account for multiple layers so each like block here or like

0:01:58.110,0:02:03.229
the encoder and the decoder can be several layers as well so again these

0:02:03.229,0:02:09.989
two macro modules I guess and so the input get goes inside an encoder

0:02:09.989,0:02:13.560
which gives us a code so h which was before

0:02:13.560,0:02:18.510
hidden representation of a neural net when we talk about you know auto-encoders

0:02:18.510,0:02:23.640
h is called code and therefore we have an encoder which is encoding the input

0:02:23.640,0:02:28.709
into this code and then we have a decoder which is decoding the code into

0:02:28.709,0:02:33.630
whatever representation in this case is is a similar as the same representation

0:02:33.630,0:02:36.470
as the input okay so on the right hand side you have

0:02:44.829,0:02:49.930
an auto-encoder, on the left hand side you're gonna be seen what is a variational

0:02:49.930,0:02:53.890
auto-encoder, alright so there you go

0:02:53.890,0:02:57.879
a variational auto-encoder okay it looks the same so what's the difference

0:02:57.879,0:03:02.200
nothing is missing something so the first difference is here that instead of

0:03:02.200,0:03:08.439
having the hidden layer h now we have the code is actually made of two things

0:03:08.439,0:03:16.000
it's made of one thing that is this kind of capital E of z and V of z and they're

0:03:16.000,0:03:21.189
gonna be representing soon the mean and the variance of this latent variable

0:03:21.189,0:03:27.969
z then we are going to be sampling from this distribution that has been

0:03:27.969,0:03:33.219
parameterize by the encoder and we get to z, it's my latent variable

0:03:33.219,0:03:38.889
my latent representation and then this latent representation goes inside the

0:03:38.889,0:03:45.459
decoder so the parameters that I sample from like I have a normal distribution

0:03:45.459,0:03:50.769
which have some parameters E and V. E and V are deterministically determined by

0:03:50.769,0:03:55.659
the input x but then z is not deterministic z is a random variable

0:03:55.659,0:04:00.159
which which gets sampled from a distribution which is parameterized by

0:04:00.159,0:04:12.189
the encoder okay so let's say h was of size d now the code here

0:04:12.189,0:04:16.419
on the left hand side is going to be of size two times d right because we had to

0:04:16.419,0:04:21.310
represent the all the means and then all the variances in this case we assume

0:04:21.310,0:04:28.060
that we have you know d means and d variance so each of those components re

0:04:28.060,0:04:38.050
are independent okay alright so we can also think about the classic auto-encoder

0:04:38.050,0:04:42.970
as just encoding the means and so if you encode the mean and you have basically

0:04:42.970,0:04:49.210
zero variance you're going to get a again a deterministic auto-encoder so h

0:04:49.210,0:04:55.090
might be in this case d and therefore on the left-hand side E and V will be total

0:04:55.090,0:04:59.140
2d. since we have d means so does that mean

0:04:59.140,0:05:05.170
we're sampling the distributions it's gonna be one multivariate Gaussian

0:05:05.170,0:05:10.150
that is orthogonal and so if you have all those components that are

0:05:10.150,0:05:14.770
independent from each other and therefore z is going to be a d

0:05:14.770,0:05:21.880
dimensional vector but then to sample a d dimensional vector from a Gaussian you

0:05:21.880,0:05:27.700
will need d means and then in this case the variances because we assuming that

0:05:27.700,0:05:31.600
all the other components in the covariance matrix are all zeros

0:05:31.600,0:05:37.060
you have only have the diagonal where you have all the variances okay so here

0:05:37.060,0:05:41.020
just to make a recap you have the encoder that is mapping this kind of

0:05:41.020,0:05:47.290
input distribution into like the input set of samples into this ℝ²ᵈ and so we

0:05:47.290,0:05:52.360
can think in this case that we met from x to the hidden representation and then

0:05:52.360,0:05:58.480
the decoder instead maps the z space into ℝⁿ which is back to the original

0:05:58.480,0:06:05.160
space of the X and therefore we go from lower case z into x̂. Someone asked

0:06:05.160,0:06:12.460
E of z and the V of z is that the output of the encoder yeah E of z and V of z are

0:06:12.460,0:06:17.140
just parameters that are deterministically output by the encoder

0:06:17.140,0:06:21.550
so the encoder is a deterministic you know it's just the classical rotation

0:06:21.550,0:06:27.910
and squashing and then another fine transformation so it's just a piece of a

0:06:27.910,0:06:31.810
neural network which is outputting some parameters ok so this is the encoder

0:06:31.810,0:06:38.620
which is giving me these parameters E and V given my input x right so this is

0:06:38.620,0:06:43.390
deterministic part then given that we had these parameters these parameters

0:06:43.390,0:06:50.410
are you know giving me a Gaussian distribution with specific means and

0:06:50.410,0:06:54.160
specific variances and from these variance from this Gaussian distribution

0:06:54.160,0:07:02.470
we sample one one sample set okay and then we decode which means we're gonna

0:07:02.470,0:07:07.570
see what means this in a in a second but basically you're going to be encoding

0:07:07.570,0:07:10.560
the they mean and then you're going to be

0:07:10.560,0:07:17.220
adding some additional some noise okay to that encoding in the in the denoising

0:07:17.220,0:07:21.540
auto-encoder we were getting our input you were adding noise to the input and

0:07:21.540,0:07:25.740
then you were trying to reconstruct the input without noise in here the only

0:07:25.740,0:07:30.570
thing that is changed is the fact that the noise is added to the inner

0:07:30.570,0:07:36.990
representation rather rather than to the input does it make sense yeah that makes

0:07:36.990,0:07:41.370
a lot more sense thank you so I noticed that the notation itself kind of looks

0:07:41.370,0:07:47.730
like expected value are we generating just a normal mean from z or we actually

0:07:47.730,0:07:54.000
computing my kind of a weighted average no no there is no okay so my x instead

0:07:54.000,0:07:59.970
of outputting the is outputting let's say d is gonna be 10 that is the hidden

0:07:59.970,0:08:04.830
representation now instead of having 10 values representing the mean we're gonna

0:08:04.830,0:08:09.030
have 20 values 10 values are representing the mean and 10 values are

0:08:09.030,0:08:16.530
representing the variances okay so we just output a vector h here given my x

0:08:16.530,0:08:21.330
the first half of the vector represents the means of a standard deviation of a

0:08:21.330,0:08:26.250
Gaussian distribution and the other half of the vector represents the variances

0:08:26.250,0:08:31.230
for the same Gaussian distribution okay so the component h the first component

0:08:31.230,0:08:36.720
h1 is going to be the the mean of the first Gaussian and then the component

0:08:36.720,0:08:42.900
h let's say okay let's call it h2 in this case is gonna be the variance then

0:08:42.900,0:08:48.120
you have h3 it's gonna be another mean h4 it's gonna be another variance

0:08:48.120,0:08:54.900
and so on okay so does that make with that make z like a 10 dimensional vector

0:08:54.900,0:09:00.720
that's sampled from yeah yeah yeah so z is that here it's gonna be half of these

0:09:00.720,0:09:05.520
sides here right so the encoder gives me twice the dimension of z and then

0:09:05.520,0:09:12.450
because you get half of the dimensions like one set of these are for the memes

0:09:12.450,0:09:17.490
and one set of these are the variances then we sample from a Gaussian that has

0:09:17.490,0:09:22.470
these values so the network simply gives me not just the means as

0:09:22.470,0:09:30.480
for the classical auto-encoder but also gives me some what is the range that I

0:09:30.480,0:09:35.220
can pick things from right before when we were using the classical auto-encoder

0:09:35.220,0:09:42.390
here we only have the means and then you simply decode the means in this case you

0:09:42.390,0:09:47.510
not only have the means but also you can have some variants some variations

0:09:47.510,0:09:55.800
across those means okay so auto-encoder normal auto-encoder is deterministic the

0:09:55.800,0:10:00.180
output is deterministic input a function of the input a variational auto-encoder

0:10:00.180,0:10:06.870
the output is not longer a deterministic it's no longer a deterministic function

0:10:06.870,0:10:13.050
of the input is going to be a distribution given the input right so is

0:10:13.050,0:10:19.560
a conditional distribution given the input so in this case we did see that we

0:10:19.560,0:10:23.550
saw similar a similar diagram last time where we were going from a specific

0:10:23.550,0:10:29.430
point on the left hand side to the right hand side in this case we start here

0:10:29.430,0:10:35.100
like a point and then we get through the encoder you're gonna get some position

0:10:35.100,0:10:39.840
here but then there is a addition of noise right if you only have the mean

0:10:39.840,0:10:44.730
you would get just one z but then given that there is some additional

0:10:44.730,0:10:49.860
noise that is due to the fact that we don't have a zero variance that final

0:10:49.860,0:10:53.880
point that final z is not going to be just one point it's gonna be like a

0:10:53.880,0:10:59.340
fuzzy point okay so instead of having one point now one one X is going to be

0:10:59.340,0:11:02.850
mapped into one region of points okay so is going to be actually taking some

0:11:02.850,0:11:07.890
space and then we how do we train the system, we train the system by

0:11:07.890,0:11:14.880
sending this latent variable z back to the the decoder in order to get these x

0:11:14.880,0:11:20.100
a hat and of course it's not going to be getting it exactly to the original point

0:11:20.100,0:11:26.400
because perhaps we haven't yet trained so we have to reconstruct the original

0:11:26.400,0:11:30.360
input and to do that we're going to be trying to minimize what is the square

0:11:30.360,0:11:36.030
distance between the reconstruction and the original input and then we had the

0:11:36.030,0:11:40.020
problem before like to go to the latent to go

0:11:40.020,0:11:44.130
from the latent to the input space we need to know or to learn the

0:11:44.130,0:11:48.210
distribution or to enforce some distribution last time we were seeing

0:11:48.210,0:11:53.610
that we're doing something similar when we are using the the classical the

0:11:53.610,0:12:00.330
standard auto-encoder but we were going from one point x to one point z and then

0:12:00.330,0:12:05.790
back to x right right now instead we are going to be enforcing a distribution

0:12:05.790,0:12:09.900
over these points in the latent space before we were going through one point

0:12:09.900,0:12:13.800
one point at one point and then you don't know what's happening if you move

0:12:13.800,0:12:18.570
around in the latent space remember so if you have on the left hand side at any

0:12:18.570,0:12:22.470
samples you're gonna have automatically on the other side ten latent variables

0:12:22.470,0:12:27.720
but then you don't know how to go between this input between these you

0:12:27.720,0:12:33.570
don't know how to travel in this latent space because we don't know how this

0:12:33.570,0:12:39.390
space behave okay variational auto-encoders enforce some structure and they

0:12:39.390,0:12:46.320
do this by adding a penalty of being different or far from a normal

0:12:46.320,0:12:54.060
distribution so if you have a latent distribution which is not really

0:12:54.060,0:12:59.040
resembling a Gaussian then this term here will be very strong very high and

0:12:59.040,0:13:02.940
when we train a variational auto-encoder we're going to be training it by

0:13:02.940,0:13:09.120
minimizing both this term over here and this term over here so the term on

0:13:09.120,0:13:14.340
the left hand side makes sure that we can get back to the original position a

0:13:14.340,0:13:19.830
term on the right hand side enforce some structure in the latent space because

0:13:19.830,0:13:24.810
otherwise we wouldn't be able to you know sample from there when we'd like to

0:13:24.810,0:13:31.470
use this decoder as a generative model okay this is maybe not too clear but let

0:13:31.470,0:13:37.800
me give you a little bit more things to think about so how do we actually create

0:13:37.800,0:13:45.860
this latent variable z so my z is simply going to be my mean E of z plus some

0:13:45.860,0:13:50.750
you know some noise epsilon which is a sample

0:13:50.750,0:13:56.210
from a normal distribution which is like a normal multivariate Gaussian

0:13:56.210,0:14:01.250
distribution with zero mean an identity matrix as the covariance matrix which

0:14:01.250,0:14:08.510
has each components multiplied by the standard deviation right so you should be

0:14:08.510,0:14:13.400
familiar with this equation here on the top right this is how you rescale a

0:14:13.400,0:14:19.760
random variable epsilon which again is a normal you have to use this kind of

0:14:19.760,0:14:23.720
repair metallization in order to get a Gaussian that has you know a specific

0:14:23.720,0:14:30.260
mean in a specific variant ok, the noise in the latent variable z just in

0:14:30.260,0:14:36.140
coded version of the noise introduced in the input so there is no noise in the

0:14:36.140,0:14:41.210
input you put the input inside the encoder and then the encoder gives you

0:14:41.210,0:14:47.690
two parameters E and variance when you sample from this distribution you

0:14:47.690,0:14:54.200
basically get z and what you get here it's simply you can write the sampling

0:14:54.200,0:14:58.970
part as this one so the problem with sampling is that we don't know how to

0:14:58.970,0:15:02.930
perform back propagation through a sampling module actually there is no way

0:15:02.930,0:15:07.910
to perform back propagation through same thing because this one is just

0:15:07.910,0:15:13.010
generating a new z so how do we get gradients through this module in order

0:15:13.010,0:15:18.230
to train the encoder and so this can be done if you use this trick which is

0:15:18.230,0:15:22.550
called the reparameterization trick the reparameterization trick allows

0:15:22.550,0:15:29.330
you to express your sampling in terms of you know additions and multiplication

0:15:29.330,0:15:35.060
which we can differentiate thrown right the epsilon is simply a additional input

0:15:35.060,0:15:41.360
that is you know coming from whatever wherever well we don't have any need to

0:15:41.360,0:15:45.620
send gradients through this input the gradients will be coming going through

0:15:45.620,0:15:48.370
the multiplication and through the addition okay

0:15:48.370,0:15:53.420
so whenever you have gradients for training this system the gradient comes

0:15:53.420,0:16:00.230
down and then here we can replace the sampling module with a addition between

0:16:00.230,0:16:03.410
E Plus the epsilon multiplied by the square

0:16:03.410,0:16:07.910
root of the variance okay such that now you have you know addition you know how

0:16:07.910,0:16:12.050
to back prop through an addition therefore you get gradients for the

0:16:12.050,0:16:17.000
encoder here a output gradient and then you can compute the partial derivatives

0:16:17.000,0:16:21.040
of the you know final costs with respect to the parameters in this module

0:16:21.040,0:16:29.900
okay so in just in a you know intuition part this KL here allows me to enforce a

0:16:29.900,0:16:35.780
structure in the latent space okay that's what we think about like that's

0:16:35.780,0:16:40.010
how I'd like you to think about this KL term and so let's actually figure

0:16:40.010,0:16:48.410
out how this stuff works okay so we have two terms in my per ample loss, we have

0:16:48.410,0:16:51.620
the first one which is the reconstruction loss and then there is

0:16:51.620,0:16:56.570
the second term which is going to be these KL this relative entropy term okay

0:16:56.570,0:17:03.800
so we have some z in this case which are spheres bubbles okay in this case why

0:17:03.800,0:17:09.680
they are bubbles because if you we add some additional noise right we had the

0:17:09.680,0:17:14.720
means and the means are basically the center of these points right so you have

0:17:14.720,0:17:23.350
one mean here one mean over here one mean over here one mean over here and then

0:17:23.350,0:17:28.610
what the reconstruction term is going to be doing is the following so if this

0:17:28.610,0:17:34.550
means if these bubbles overlap what does it happen so if you have one mean here and

0:17:34.550,0:17:38.930
another mean like one bubble here another bubble it is overlapping and

0:17:38.930,0:17:42.800
there is a region where there is you know intersection

0:17:42.800,0:17:49.610
how can you reconstruct these two points later on right you can't right you're

0:17:49.610,0:17:53.900
following so far if you have a bubble here and then you have another bubble

0:17:53.900,0:17:58.820
here all points on this bubble here will be reconstructed to the original input

0:17:58.820,0:18:04.130
here so you start from an original point you go to the latent space over here and

0:18:04.130,0:18:08.930
then you have some noise you actually have a volume here then you take another

0:18:08.930,0:18:13.870
point and this other point it gets reconstructed here

0:18:13.870,0:18:20.620
right now if these two guys overlap how can you reconstruct the points over here

0:18:20.620,0:18:23.799
so if the points are in this bubble I'd like to go back to the original point

0:18:23.799,0:18:28.289
here if the points are in this bubble I'd like to go to the other point

0:18:28.289,0:18:33.070
but if points are overlapped sorry if the bubbles are overlapped then you

0:18:33.070,0:18:38.770
can't really figure out where to go back right so then the reconstruction term

0:18:38.770,0:18:43.240
we'll just do this the reconstruction term will try to get all those bubbles

0:18:43.240,0:18:48.159
as far as possible such that they don't overlap because if they overlap then the

0:18:48.159,0:18:56.080
reconstruction is not going to be good and so now we have to fix this so there

0:18:56.080,0:19:02.470
are a few ways to fix this right how can you tell me right now how can we fix

0:19:02.470,0:19:08.409
this overlapping issue right why didn't we have this overlapping issue with the

0:19:08.409,0:19:13.750
normal auto-encoder because there is no variance AHA and so what does it mean

0:19:13.750,0:19:20.140
okay can you translate what not being not having a variance mean the spheres

0:19:20.140,0:19:23.860
are not spheres but they're points correct right so if you have just points

0:19:23.860,0:19:30.190
points will never overlap right well they have to be the exact same point but

0:19:30.190,0:19:35.169
you have the exact same point only if the encoder is dead right or you have

0:19:35.169,0:19:41.080
the same input I think well it's unlikely the two points overlap if now

0:19:41.080,0:19:46.240
instead of having points you have actually volumes well you know volume

0:19:46.240,0:19:50.320
can overlap because they are many infinite points right in that volume

0:19:50.320,0:19:56.649
okay so one option is going to be kill the variance and so you have points and

0:19:56.649,0:20:01.299
now this defeats the whole variational thing right

0:20:01.299,0:20:06.730
without this spacey thing by killing the variance now you don't know anymore

0:20:06.730,0:20:11.649
what's happening between the points right because if you have space like if

0:20:11.649,0:20:17.200
they takes volume you can walk around in the latent space you can always figure

0:20:17.200,0:20:22.990
out where to go back if these are points as soon as you leave this position here

0:20:22.990,0:20:27.430
you have no whatsoever whatsoever idea how

0:20:27.430,0:20:34.480
where to go okay anyhow first point we can kill the variance other option well

0:20:34.480,0:20:39.790
the one I show you here I the other option is gonna get these bubbles as far

0:20:39.790,0:20:46.680
as possible right so if they are as far as possible what's gonna happen in your

0:20:46.680,0:20:55.720
Python script so if these these means right they go very very far then

0:20:55.720,0:21:01.810
they will increase a lot a lot a lot right and then the problem is that

0:21:01.810,0:21:05.470
you're going to get infinite right this stuff is gonna explode because all these

0:21:05.470,0:21:10.660
values are try to go as far as possible such that they don't overlap and then

0:21:10.660,0:21:15.670
that's not good okay all right so let's figure out how the variational variational

0:21:15.670,0:21:20.950
auto-encoder fixes this problem. Could you just clarify what you mean by pushing

0:21:20.950,0:21:24.730
the points apart like are you putting them in a higher dimensional space? No no

0:21:24.730,0:21:29.650
no so as they are here so each if you don't have the variance all those

0:21:29.650,0:21:34.300
circles here all those bubbles here are just points even that we have some

0:21:34.300,0:21:41.320
variance they will take some space now if this space taken by two bubbles

0:21:41.320,0:21:45.280
overlaps with another bubble the reconstruction error will increase

0:21:45.280,0:21:49.360
because you have no idea how to go back to the original point that generated

0:21:49.360,0:21:54.810
that sphere and so the network the encoder has two options in order to

0:21:54.810,0:21:59.650
reduce this reconstruction error one option is going to be to kill the

0:21:59.650,0:22:03.250
variance such that you get points the other option is going to be to send all

0:22:03.250,0:22:08.470
those points in any direction such that they don't overlap okay okay yeah that

0:22:08.470,0:22:12.040
makes sense, okay good so a reconstruction error and gets these

0:22:12.040,0:22:19.060
stuff to fly around but then let's introduce the second term so I would

0:22:19.060,0:22:24.370
really recommend you to compute these relative entropy between the a Gaussian

0:22:24.370,0:22:30.580
and a normal distribution such that you can practice maybe for next week but

0:22:30.580,0:22:34.240
then if you compute that relative entropy you get these stuff right you

0:22:34.240,0:22:39.790
get several, four terms basically and everyone should understand how these

0:22:39.790,0:22:42.980
look no okay I'm just joking I'm gonna be

0:22:42.980,0:22:46.580
actually explaining it okay so we have this expression let's try to analyze a

0:22:46.580,0:22:51.050
little bit in more detail what they these terms represent so the first term

0:22:51.050,0:22:56.390
you'd have V(z) – log[V(z)] – 1. So if we graph it it looks like

0:22:56.390,0:23:03.560
this you have a linear function right on the after you know after 2 on the x-axis

0:23:03.560,0:23:08.930
and then on the other terms you have subtracted you subtract a logarithm

0:23:08.930,0:23:13.070
which goes to +∞ … If you sum a minus logarithm it goes to +∞

0:23:13.070,0:23:19.670
at zero and then otherwise it's gonna be just you know decaying so if

0:23:19.670,0:23:23.960
you sum the two and subtract one you get this kind of cute function and if you

0:23:23.960,0:23:30.380
minimize this function you get just one and therefore these shows you how these

0:23:30.380,0:23:37.750
term and forces those spheres here to have a radius of one in each direction

0:23:37.750,0:23:43.070
because if it tries to be smaller than one this stuff you know goes up as crazy

0:23:43.070,0:23:48.830
and if the increases here doesn't go as up as crazy so they are it slightly you

0:23:48.830,0:23:54.770
know roughly always at least one or you know half but it won't be that much

0:23:54.770,0:24:01.780
smaller because this stuff you know increases a lot so in this case we have

0:24:01.780,0:24:07.550
enforced the network not to collapse these bubbles in order to make it grow

0:24:07.550,0:24:14.420
them too much right because otherwise they still get penalized here so then we

0:24:14.420,0:24:19.910
have another term here these E(z)² and that's classical

0:24:19.910,0:24:25.130
problem which has a minimum over there and so this term here basically says

0:24:25.130,0:24:30.800
that all it means should be condensed towards zero and so basically you get

0:24:30.800,0:24:37.070
like this additional force here by this purple side and now you get that all

0:24:37.070,0:24:43.760
those bubbles get squashed together in this bigger bubble so here you get the

0:24:43.760,0:24:48.880
bubbles of bubbles representation of a variational auto-encoder

0:24:49.720,0:25:00.120
how cute is this very cute how can you pack more bubbles so what is

0:25:00.120,0:25:06.100
the only parameter here which is telling you the strength of your variational

0:25:06.100,0:25:09.600
auto-encoder it's gonna be simply the dimension d

0:25:09.600,0:25:15.129
because you know given a dimension you always know how many bubbles you can

0:25:15.129,0:25:18.639
pack in a larger bubble right so it's just a function of the dimension you

0:25:18.639,0:25:23.980
pick and you choose for your hidden layer, though is the reconstruction last

0:25:23.980,0:25:29.409
the first term the yellow term is that the one that actually pushes the bubbles

0:25:29.409,0:25:35.019
further apart and not gonna the rest of it is what kind of keeps them from doing

0:25:35.019,0:25:42.610
that right so the reconstruction would push things around because we have these

0:25:42.610,0:25:49.090
additional taking volumes thing right so if we wouldn't be taking volumes the

0:25:49.090,0:25:53.440
reconstruction term wouldn't be pushing anything away because it don't overlap

0:25:53.440,0:25:58.659
given that we actually have some variance the variance will have these

0:25:58.659,0:26:03.399
points actually taking some volume and therefore this reconstruction will try

0:26:03.399,0:26:09.129
to get those points away so if you check again those few animations I'll show you

0:26:09.129,0:26:15.070
so we had at the beginning those were the points with additional noise now you

0:26:15.070,0:26:20.169
get the reconstruction that is you know pushing everything away then you get the

0:26:20.169,0:26:26.080
variance it is assuring you that those little bubbles don't collapse and then

0:26:26.080,0:26:30.610
you had the final term which is the spring term because it's the quadratic

0:26:30.610,0:26:36.940
term in the loss which is basically adding these additional pressure such

0:26:36.940,0:26:41.590
that all the little guys get you know packed towards zero but they don't overlap

0:26:41.590,0:26:46.350
because there is the reconstruction term so no overlap due to the reconstruction

0:26:46.350,0:26:53.519
size not going to smaller than one because of the first part of the

0:26:53.519,0:26:58.240
relative entropy and then all these guys are parked again for the quadratic part

0:26:58.240,0:27:05.040
which is the spring force it's a term something that needs to be

0:27:05.040,0:27:11.490
tuned like hyper parameter kind of thing so the beta is the actual in the

0:27:11.490,0:27:16.260
original version of this variation auto-encoder there was no beta and then there

0:27:16.260,0:27:22.170
is a paper which is the beta variation of auto-encoder just to say that you can

0:27:22.170,0:27:28.620
use a hyper parameter to change how much these two terms you know contribute for

0:27:28.620,0:27:35.580
the final loss. This loss the second loss term with the beta that's the KL

0:27:35.580,0:27:40.080
divergence? yeah then the normal distribution rate yeah between the

0:27:40.080,0:27:46.710
z which is coming from a Gaussian of mean E and variance V and then the

0:27:46.710,0:27:52.500
second term is going to be this normal distribution and so this term tries to

0:27:52.500,0:28:00.210
get z to be as close as possible to a normal distribution in the space V

0:28:00.210,0:28:11.910
dimensional space okay and this formula that you that's a generic this is so I

0:28:11.910,0:28:18.090
would recommend you to take a paper and pen and then then try to write the

0:28:18.090,0:28:22.590
relative entropy between a Gaussian and a normal distribution and you should get

0:28:22.590,0:28:31.580
all these terms the relative entropy so yeah this lKL is the relative entropy

0:28:31.670,0:28:37.650
yeah so just look up the formula for the relative entropy which is telling you

0:28:37.650,0:28:43.680
basically how far to distributions are and the first distribution is going to

0:28:43.680,0:28:48.120
be a multivariate Gaussian and the second one is going to be a normal

0:28:48.120,0:28:53.820
distribution right yeah normal distribution is not the same thing the

0:28:53.820,0:29:01.050
Gaussian has a mean vector and the covariance matrix the normal has 0 mean

0:29:01.050,0:29:06.240
and identity matrix for the covariance matrix we said earlier though that the z

0:29:06.240,0:29:10.800
should not have covariance it should be diagonal right yeah yeah so all it's

0:29:10.800,0:29:13.779
gonna be diagonal but the values on the diagonal

0:29:13.779,0:29:19.869
those V variances okay it's an off-center big normal versus a centered in normal

0:29:19.869,0:29:30.489
small normal so it's off center and then each direction is scaled by the by the

0:29:30.489,0:29:35.679
standard deviation right of that dimension so if you have a large

0:29:35.679,0:29:38.799
standard deviation in one dimension it means that in that direction is very

0:29:38.799,0:29:46.960
very spread, make sense? but that there is both a line it's a line on the d axis

0:29:46.960,0:29:51.940
right because again all the components are independent yeah is the

0:29:51.940,0:29:57.279
reconstruction loss the pixel wise distance between the final out and the

0:29:57.279,0:30:03.279
original image? The reconstruction loss we we saw that last week and we have two

0:30:03.279,0:30:10.450
options for the reconstruction was one was the binary for binary data and we

0:30:10.450,0:30:14.399
have the binary cross-entropy and the other one is going to be instead the

0:30:14.399,0:30:22.389
real value of one so such that you can use the half or one DMS MSC right so

0:30:22.389,0:30:27.429
these are the reconstruction losses we can use for example you talk more with

0:30:27.429,0:30:31.389
me than with Yann good now well not good you should talk as well with Yann

0:30:31.389,0:30:39.429
but we should be going over the the notebook such that we can see how to

0:30:39.429,0:30:44.440
code the stuff and also play with the distributions because before again the

0:30:44.440,0:30:49.029
main point was that before we were mapping points to points and back to

0:30:49.029,0:30:54.429
points right right now instead you're gonna map points to space and then space

0:30:54.429,0:31:01.029
to points but then also all the space now is going to be all cover by these

0:31:01.029,0:31:06.099
bubbles because of several factors right if you have some space between these

0:31:06.099,0:31:11.229
bubbles then you have no idea how to go from this region here back to the input

0:31:11.229,0:31:15.580
space right instead of variation auto-encoder gets you to this very

0:31:15.580,0:31:22.400
well-behaved coverage you know these nice covers of the latent space okay

0:31:22.400,0:31:32.770
good I can't see you you miss you guys okay so good, are there any questions so far? I

0:31:32.770,0:31:42.500
hope you can see stuff I just give feedback, can you see stuff? yeah yes yep so
	
0:31:42.500,0:31:54.640
Work/GitHub/pDL conda activate pDL jn. Boom okay so I'm gonna

0:32:02.870,0:32:09.260
be covering now the VAE and so now I'm gonna just execute everything such that this

0:32:09.260,0:32:14.870
stuffs are training and then I'm gonna be explaining things alright so at the

0:32:14.870,0:32:19.070
beginning I'm gonna be just import all random [ __ ] as usual then I have a

0:32:19.070,0:32:25.210
display routine we don't care don't add it to the notes I have some default

0:32:25.210,0:32:30.320
values for the random seed such that you're gonna get the same numbers I get

0:32:30.320,0:32:37.370
then here I just use the MNIST dataset we modify MNIST from Yann device

0:32:37.370,0:32:42.279
I set the CPU or GPU in theory I could have used GP one because my Mac here

0:32:42.279,0:32:47.750
actually has a GPU and then I have my variational auto-encoder okay so my

0:32:47.750,0:32:54.230
variational auto-encoder has two parts as a encoder here let me turn on the line

0:32:54.230,0:33:04.309
numbers so my encoder goes from 784 which is the sides size of the input to

0:33:04.309,0:33:13.100
this square for example d in this case is 20 so 400 and then from d square I go

0:33:13.100,0:33:19.330
to 2 times d which is gonna be half of my means and half is gonna be for my

0:33:19.330,0:33:25.490
Sigma squares for my variances the other case the other the decoder instead picks

0:33:25.490,0:33:31.340
only d right you can see right d here we go from d to d square and then from d

0:33:31.340,0:33:35.960
square to 794 such that we match the input dimensionality and then finally I

0:33:35.960,0:33:39.500
have a sigmoid why do we have a sigmoid? because my input

0:33:39.500,0:33:46.190
is going to be limited from zero to one there are images from zero to one then

0:33:46.190,0:33:51.080
there is a module here which is called reparameterise and if we are

0:33:51.080,0:33:57.050
training we use the reparameterisation part, could you just say again

0:33:57.050,0:34:03.170
why you use the sigmoid in the? yeah because my data it's leaving between

0:34:03.170,0:34:08.510
zero and one so I have those digits from the MNIST and they are they are

0:34:08.510,0:34:14.480
values like the values of the digits are going to be from 0 to 1 so I'd like to

0:34:14.480,0:34:19.909
have my network this module here outputs things that goes from -∞ to

0:34:19.909,0:34:24.139
+∞ if I send it through a sigmoid this stuff sends things through

0:34:24.139,0:34:31.520
like 0 to 1 okay when you say the values of the digits you mean the deactivations

0:34:31.520,0:34:37.460
right? so I use the MNIST dataset and this

0:34:37.460,0:34:42.230
is going to be both my input and also my targets right and the images and the

0:34:42.230,0:34:49.550
values of these images will be ranging between 0 to 1 like is a real value each

0:34:49.550,0:34:54.740
pixel can be between 0 and 1 yeah I think actually the the inputs are binary

0:34:54.740,0:35:00.590
so the inputs are all 0 or 1 but my network will be outputting a real range

0:35:00.590,0:35:08.320
between 0 & 1 so reparameterization we have the we

0:35:08.320,0:35:15.490
what do we do here so reparameterization given a mu and a log variance

0:35:15.490,0:35:21.620
explain later why we use log variance if you are in a you know in training we

0:35:21.620,0:35:26.000
compute the standard deviation is going to be log variance multiplied by 1/2 and

0:35:26.000,0:35:29.960
then I take the exponential and so I get the standard deviation from the log

0:35:29.960,0:35:34.850
variance and then I get my epsilon which is simply sampled from a normal

0:35:34.850,0:35:40.640
distribution which with whatever size I have here right so standard deviation I

0:35:40.640,0:35:46.570
get the size I create a new tensor and I fill it with a normal distribution data

0:35:46.570,0:35:52.130
then I return the epsilon multiply by the standard deviation and I add the mu which

0:35:52.130,0:35:57.830
is what I show you before if I am NOT training I don't have to add noise right

0:35:57.830,0:36:04.070
so I can simply return my mu so I use this network in a deterministic way the

0:36:04.070,0:36:09.710
forward mode is the following so here we have that the encoder get the

0:36:09.710,0:36:15.800
input which is going to be reshaped into you know these things such that

0:36:15.800,0:36:21.230
basically I enroll the images into a vector then the encoder is going to be a

0:36:21.230,0:36:25.070
output output in something and I will shape that one such that I have batch

0:36:25.070,0:36:30.530
size two and then d where d is the dimension of the mean and the dimension

0:36:30.530,0:36:37.610
of the variances then I have mu which is the mean simply the first part right of

0:36:37.610,0:36:41.720
these guys of this d and then the log variance is going to be the other guy

0:36:41.720,0:36:46.700
and then I have my z which is going to be my latent variable it's going to be

0:36:46.700,0:36:53.330
these reparameterisation given my mu and the logvar why do I use a logvar

0:36:53.330,0:36:57.910
you tell me why do I use a logvar right right right so given that

0:37:02.900,0:37:08.090
variances are only positive if I compute the log allows you to output the full

0:37:08.090,0:37:15.620
real range for the encoder right so you can use the whole real range and then I

0:37:15.620,0:37:20.420
define my model as this VAE and I send it to the device here I define the

0:37:20.420,0:37:26.240
optimization optimizer and then I define my loss function which

0:37:26.240,0:37:31.570
is the sum of two parts the binary cross entropy between the input and the

0:37:31.570,0:37:40.070
reconstruction which is here so I have the x_hat and then the X and then I

0:37:40.070,0:37:48.500
try I sum all of them and then the KL divergence so we have the var which is

0:37:48.500,0:37:52.580
the you know linear then you have the minus logvar which is the logarithmic

0:37:52.580,0:37:58.430
flip down and then minus one and then we have the mu and then we try to minimize

0:37:58.430,0:38:02.780
this stuff right all right so training scripts it's very

0:38:02.780,0:38:05.990
simple right so you have the model which is

0:38:05.990,0:38:10.970
outputting the prediction x_hat let me let's see here right forward outputs the

0:38:10.970,0:38:16.730
output of the decoder the mu and the log var so here you get the model

0:38:16.730,0:38:22.160
you feed the input you get x_hat mu logvar you can compute the loss using the

0:38:22.160,0:38:30.470
x̂, x, mu and logvar, x being the input but also the target and then we you know

0:38:30.470,0:38:36.260
yeah we add the item for the loss we clean up the gradient from the previous

0:38:36.260,0:38:42.580
steps perform computational compute the partial derivatives and then you step

0:38:42.580,0:38:47.570
and then here I just do the testing and do some caching for later on so we

0:38:47.570,0:38:54.680
started with initial error of 500 roughly 514 this is pre before training

0:38:54.680,0:39:02.360
and any goes immediately immediately down to 200 and then goes down to 100

0:39:02.360,0:39:08.420
okay and so now I'm gonna be showing you a few of the results this is the input I

0:39:08.420,0:39:14.030
feed to the network and the untrained network reconstructions of course look

0:39:14.030,0:39:20.240
like [ __ ] right but okay that's fine so we can keep going and there's gonna be

0:39:20.240,0:39:25.850
the first epoch right cool second epoch third fourth and so on

0:39:25.850,0:39:31.580
right and it looked better and better of course so what can we do right now a

0:39:31.580,0:39:40.460
few things we can do for example now we can simply sample z from normal

0:39:40.460,0:39:46.220
distribution and then I decode this random stuff right so this doesn't come

0:39:46.220,0:39:50.900
from our encoder and I show you now what the decoder does whenever you sample

0:39:50.900,0:39:56.540
from the distribution that the latent variable should have been following and

0:39:56.540,0:40:03.700
so these are a few examples of how sampling from the latent distribution

0:40:03.700,0:40:09.830
you know gets decoded into something we got a nine here we go to zero we got

0:40:09.830,0:40:15.410
some five so some of the regions are very well defined nine two but then other

0:40:15.410,0:40:21.040
regions like this thing here or this thing here or the number 14 here

0:40:21.040,0:40:27.820
they don't really look like well like digits this is because why what's the

0:40:27.820,0:40:32.680
problem here we haven't really covered the whole space I just trained for one

0:40:32.680,0:40:34.960
minute if I trained for 10 minutes it's going

0:40:34.960,0:40:39.550
to be just working perfectly okay so here those bubbles don't yet fill the

0:40:39.550,0:40:44.590
whole space right and it's the same problem which you would have with a

0:40:44.590,0:40:48.970
normal out encoder without this variational thing right with very normal

0:40:48.970,0:40:54.310
auto-encoders you don't have you know any kind of structure any kind of defined

0:40:54.310,0:40:58.180
behavior in the regions between different points with the variational

0:40:58.180,0:41:01.750
auto-encoder we actually take the space and enforce that the

0:41:01.750,0:41:09.490
reconstruction of the all these region actually are to make sense again so

0:41:09.490,0:41:19.140
let's do some cute stuff and then I am done here I just show you a few digits

0:41:19.590,0:41:25.810
and so let's pick two of them for example let's pick three and eight which

0:41:25.810,0:41:32.190
is going to be let me show you here so we'd like to find a interpolation now

0:41:32.190,0:41:38.500
between a five and a four okay and this is my five reconstructed and our four

0:41:38.500,0:41:44.260
reconstructed so if I perform a linear interpolation in the latent space and I

0:41:44.260,0:41:50.470
then send it to the decoder we get this one so the five gets morphed into a four

0:41:50.470,0:41:55.900
you can see slowly but it looks like crap let's try to get something that

0:41:55.900,0:42:02.800
stays on a manifold so let's get for example these three so it's going to be

0:42:02.800,0:42:16.710
number one number one and then let's say maybe these 14 here

0:42:16.780,0:42:23.540
so I do interpolation of these guys here you can see my auto-encoder actually

0:42:23.540,0:42:31.550
fixed those kind of issues here and then you can see now how the three gets those

0:42:31.550,0:42:36.020
little edges closed to look like an eight right and so all of them look like

0:42:36.020,0:42:39.920
kind of legit no it's just kind of a three kind of a three a three day became

0:42:39.920,0:42:44.840
an eight right and so you can see how now by walking in the latent space we

0:42:44.840,0:42:50.420
get to reconstruct things that look legit in the input space right this

0:42:50.420,0:42:56.810
would have never worked with a normal auto-encoder finally I'm gonna show you a

0:42:56.810,0:43:04.640
few nice representation of the embeddings of the means for this train

0:43:04.640,0:43:10.190
auo-encoder so here I'll just show you a collection of the embeddings of you know

0:43:10.190,0:43:15.620
the test data set and then I perform a like a dimensionality reduction and then

0:43:15.620,0:43:22.250
I show you how the encoder clusters all the means in different regions in the

0:43:22.250,0:43:28.280
latent space and so here is what you get when you train this variational

0:43:28.280,0:43:32.120
auto-encoder so this is the the beginning when the network is not trained

0:43:32.120,0:43:37.250
you can still see you know clusters of digits but then as you keep training

0:43:37.250,0:43:42.010
well at least you know after five epochs you get these groups to be you know

0:43:42.010,0:43:48.890
separated and then I think if you keep training more you should have like more

0:43:48.890,0:43:57.980
separation okay so here I'm basically doing the testing part I get all the

0:43:57.980,0:44:04.640
means so my model outputs x̂, mu and logvar right and so my means I append

0:44:04.640,0:44:10.820
them I append all my mus into this mean list I append all the logvars in this

0:44:10.820,0:44:17.690
logvars list and I append all the y's to these labels list during the testing

0:44:17.690,0:44:23.750
part right so this is testing and so I have like a list here of codes which is

0:44:23.750,0:44:29.990
a I have the mu, logvar and then y's right so here later on I

0:44:29.990,0:44:38.750
put those lists inside my a dictionary and then later here below I compute a

0:44:38.750,0:44:48.349
dimensionality reduction for epoch zero epoch five and epoch ten so I use this

0:44:48.349,0:44:53.680
TSNE which is a technique for reducing the dimensions of the codes which are

0:44:53.680,0:45:01.580
twenty right now the dimension height is 20 so I fit I get my X is gonna be let's

0:45:01.580,0:45:08.000
say the first thousand component first thousand samples of the means and then I

0:45:08.000,0:45:15.140
get these Es which are basically a 2d projection somehow of these twenty

0:45:15.140,0:45:22.130
dimensional mus okay and then I show you in this chart here how these 2d

0:45:22.130,0:45:27.830
projections they look at epoch 0 before training the network because this one is

0:45:27.830,0:45:34.400
before the first training epoch and then at epoch five you can see how the

0:45:34.400,0:45:42.560
network gets all this mess here to be you know kind of more nicely put here I

0:45:42.560,0:45:47.990
didn't visualize the variances I'm thinking whether I can if I'm able to do

0:45:47.990,0:45:52.700
that as well not sure so each of these points represent the location of the

0:45:52.700,0:45:55.880
mean after training the variational auto-encoder

0:45:55.880,0:46:02.839
I haven't represented the area that these means are actually taking okay are

0:46:02.839,0:46:13.010
that means supposed to be random at epoch 0 the randomness is in the encoder right

0:46:13.010,0:46:19.160
but then you still feed to the encoder and those inputs digits so the input

0:46:19.160,0:46:24.530
digits all the ones are kind of similar right so if you perform a random

0:46:24.530,0:46:29.510
transformation of those similarly looking initial vectors

0:46:29.510,0:46:35.599
you're going to have similarly looking transformed versions but then they are

0:46:35.599,0:46:42.080
not necessarily grouped all together like most of them are for it for example

0:46:42.080,0:46:46.490
let's say these are 1s let me turn on the color bar so we can see what this

0:46:46.490,0:46:50.869
stuff is so let's say these are the zeros these

0:46:50.869,0:46:57.020
over here so all zeros look like they all look similar therefore even a random

0:46:57.020,0:47:02.510
projection of those zeros will all be kind of together what you can see

0:47:02.510,0:47:06.650
instead is gonna be this purple is all spread around right so means the force

0:47:06.650,0:47:11.900
there yeah there are very many ways of drawing four you know someone right is

0:47:11.900,0:47:16.400
closest the top someone doesn't so if you see on the right hand side inside

0:47:16.400,0:47:21.079
all the fours are almost all here right there is just a little cluster here next

0:47:21.079,0:47:25.820
to the nine because you can you can think about if you if you write a four

0:47:25.820,0:47:32.240
like that it's very similar to write a nine right and so you had these fours

0:47:32.240,0:47:36.800
here that are very close to the nines just because of how people drew this

0:47:36.800,0:47:42.349
specific fours okay nevertheless they are still clustered over here you get

0:47:42.349,0:47:47.180
all these things are spread around and so this is very bad

0:47:47.180,0:47:51.170
nevertheless they tell you this that this diagram here shows you that there

0:47:51.170,0:47:56.930
is very little variance across the drawing of a zero okay so it shows you

0:47:56.930,0:48:02.210
like somehow there is a specific mode it is very concentrated here but it's

0:48:02.210,0:48:08.270
really not concentrated for these guys so I'm just curious like what are some

0:48:08.270,0:48:16.700
other some other like like motivations or usages of variational auto-encoder

0:48:16.700,0:48:27.310
like so the main point was the whenever I show you in class the two weeks ago a

0:48:27.310,0:48:31.609
generative model you cannot have a generative model with a classical

0:48:31.609,0:48:37.400
auto-encoder in this case here again I train I didn't train this stuff a lot if you

0:48:37.400,0:48:42.220
train it longer you can have better performance here and the point is that

0:48:42.220,0:48:49.040
my input z comes from just this random distribution okay and then by

0:48:49.040,0:48:53.540
sending this random number here a random a number coming from a normal

0:48:53.540,0:48:55.700
distribution you send an inside this decoder

0:48:55.700,0:48:59.390
if this this is a coder is actually a powerful

0:48:59.390,0:49:06.260
decoder then this stuff will actually draw very nice shapes or numbers like

0:49:06.260,0:49:11.420
for example those two images I show you of the two faces in the first part of

0:49:11.420,0:49:16.460
the class last time those are simply you take a number from my random

0:49:16.460,0:49:20.570
distribution you feed it to a decoder and the decoder is going to be drawing

0:49:20.570,0:49:25.760
you this very beautiful picture of whatever you trained this decoder on okay

0:49:25.760,0:49:30.950
and you cannot use a standard auto-encoder to get these kinds of properties

0:49:30.950,0:49:36.800
because again here we enforce the decoder to reconstruct meaningful or

0:49:36.800,0:49:41.800
good-looking reconstruction when they are sample from this normal distribution

0:49:41.800,0:49:47.240
therefore later on we can sample from this normal distribution feed things to

0:49:47.240,0:49:50.810
the decoder and the decoder will generate stuff that looks like legit

0:49:50.810,0:49:57.200
right if you didn't train the decoder in order to perform a good reconstruction

0:49:57.200,0:50:01.430
when you sample from this normal distribution you wouldn't be able to

0:50:01.430,0:50:07.340
actually get anything meaningful okay that's the big takeaway here next time

0:50:07.340,0:50:10.850
we're going to be seen said generative adversarial network

0:50:10.850,0:50:18.200
and how they how they are very similar to these stuff we have seen today on hi

0:50:18.200,0:50:22.940
Alfredo I have a question for the yellow bubble yellow yeah

0:50:22.940,0:50:30.200
each yellow bubble comes from one input example yeah so if we have 1000 I don't

0:50:30.200,0:50:34.520
know what images or 1000 inputs that means we have 1000 exactly yellow

0:50:34.520,0:50:42.410
bubbles yeah and each yellow bubble it comes from the the E of z, V of z

0:50:42.410,0:50:48.770
distribution together with the noise added to latent variable so the bubble

0:50:48.770,0:50:53.750
come from here let me show you should I show you this one is okay or should I

0:50:53.750,0:50:59.410
say okay so here you get these X and these X goes inside the model right

0:50:59.410,0:51:04.580
whenever you send these X through the model it goes inside forward so X goes

0:51:04.580,0:51:11.390
inside here and then it goes inside the encoder right okay and then from the

0:51:11.390,0:51:17.359
gives me this mu logvar from which I just extract the mu and logvar okay so

0:51:17.359,0:51:21.980
so far is everything it's like a normal auto-encoder okay

0:51:21.980,0:51:30.170
the bubble comes here so my z now comes out from these self-reparameterise and

0:51:30.170,0:51:36.740
this self-reparameterise is gonna be working in a different way if we are in

0:51:36.740,0:51:41.420
the training loop or we are not in the training loop so if we are not in the

0:51:41.420,0:51:46.760
training loop I just returned the min so there is no bubble when I use the

0:51:46.760,0:51:52.339
testing part ok so I get the best value the encoder can give me if I am

0:51:52.339,0:51:58.220
training instead what this is what happens so my I compute the standard

0:51:58.220,0:52:03.799
deviation from this logvar so I get the logvar I divid it by two and then I take

0:52:03.799,0:52:10.519
the exponential right so I have exp(0.5 * logvar) such that you get you

0:52:10.519,0:52:17.089
know the standard deviation and then the epsilon it can be simply an a d

0:52:17.089,0:52:23.329
dimensional vector sample from a normal distribution and so this one is one

0:52:23.329,0:52:27.769
sample coming from this normal distribution and the normal distribution

0:52:27.769,0:52:34.250
it's you know like a sphere in d dimensions right a sphere with the

0:52:34.250,0:52:38.539
radius which is going to be square root of d but then so here at the end

0:52:38.539,0:52:43.490
you simply resize that thing the point is that every time you go you called is

0:52:43.490,0:52:47.779
reparameterization the reparameterize function you can I get a different

0:52:47.779,0:52:52.339
epsilon because epsilon it's sampled from a normal distribution right so

0:52:52.339,0:52:59.900
given a mu and given a logvar you're gonna be getting every time different

0:52:59.900,0:53:04.880
Epsilon's and therefore these stuff here if you call it a hundred times it's

0:53:04.880,0:53:12.829
gonna give you 100 different points all of them cluster in mu with the radius of

0:53:12.829,0:53:18.410
you know roughly standard deviation and so this is the line which returns you

0:53:18.410,0:53:22.910
every time just one sample but if you call this in a for loop you're gonna get

0:53:22.910,0:53:27.080
you know a cloud of points all them centered mu which has a specific

0:53:27.080,0:53:33.870
radius okay and so this is where we get these bubbles come from the sampling and

0:53:33.870,0:53:43.590
of these thing right I have to run it 100 times if you want 100 samples you

0:53:43.590,0:53:49.560
get 100 times you had to run into 100 times this reparameterization gives

0:53:49.560,0:53:54.800
you every time a different point which is you know parameterize by this

0:53:54.800,0:54:01.620
location and this kind of you know up you know volume yeah and this comes from

0:54:01.620,0:54:06.960
the mu and log variance comes from one sample one input example yeah

0:54:06.960,0:54:12.960
yeah yeah so my one input x here gives me one mu and gives me one logvar and

0:54:12.960,0:54:20.220
this one mu and well one logvar gives me z which is one sample from the whole

0:54:20.220,0:54:26.370
distribution if you run this function here 1,000 times you're gonna get 1,000

0:54:26.370,0:54:34.410
z which all of them will take this volume right off okay got it got it

0:54:34.410,0:54:39.480
thank you of course I have a question about auto-encoders,

0:54:39.480,0:54:42.630
sorry, encoders and decoders in general yeah it looks like in this

0:54:42.630,0:54:45.060
implementation it's like fairly straightforward in

0:54:45.060,0:54:49.020
terms of like it just has like a couple linear layers with a relu and a

0:54:49.020,0:54:54.870
sigmoid are most like I like I previously they seemed encoders where

0:54:54.870,0:54:59.100
there's like attention all this stuff like is is this is something kind

0:54:59.100,0:55:04.410
of as basic as this it seems like it's pretty satisfactory like is that I mean

0:55:04.410,0:55:10.730
are are they usually this basic or more complex okay okay that does I think

0:55:10.730,0:55:18.240
softball for me so everything we see in class is things that I've tried it works

0:55:18.240,0:55:24.360
and it's fairly a representative of what is sufficient to get this stuff to run

0:55:24.360,0:55:30.300
so you know I'm running on my laptop on the MNIST data set you can run several of

0:55:30.300,0:55:37.020
this kind of test and play and so today we have seen how you can encode how can

0:55:37.020,0:55:41.119
you code up a auto-encoder, all you need is like three

0:55:41.119,0:55:45.289
lines four lines of code which are like like what are the differences between

0:55:45.289,0:55:49.429
the playing an auto-encoder and so the difference is that you have the

0:55:49.429,0:55:53.259
reparameterization reparameterization, reparameterize module

0:55:53.259,0:55:59.900
method here and then you know just these three lines over here right so you have

0:55:59.900,0:56:06.769
like six lines plus the relative entropy the architecture that's completely

0:56:06.769,0:56:10.910
different so it's completely orthogonal right one thing is gonna be at the

0:56:10.910,0:56:13.729
architecture which is based on the current input you can use a

0:56:13.729,0:56:16.339
convolutional net you can use a recurrent net you can use anything you

0:56:16.339,0:56:21.380
want and the other thing is the fact that you convert some deterministic

0:56:21.380,0:56:27.529
Network into a network that allows you to sample and then generate samples from

0:56:27.529,0:56:33.589
a distribution okay so we never had to talk about distributions before we

0:56:33.589,0:56:36.949
didn't know how to generate distributions now with generative model

0:56:36.949,0:56:45.319
you can actually generate data which are basically a you know you say like a

0:56:45.319,0:56:51.769
bending a rotation or a transformation of whatever with the original Gaussian

0:56:51.769,0:56:56.029
right so we had this multivariate gaussian and then the decoder takes this

0:56:56.029,0:57:00.739
ball and then it shapes it to make it look like the input the input maybe like

0:57:00.739,0:57:05.449
something curved you have this bubble here this big bubble of bubbles and then

0:57:05.449,0:57:11.799
you the decoder gets it back to whatever looks like how the input looks like so

0:57:11.799,0:57:18.289
all you need depends on the specific data you are using for MNIST this is

0:57:18.289,0:57:22.849
sufficient if you're using a convolutional version is maybe worth

0:57:22.849,0:57:27.650
working much better the point is that this class was about variational auto-encoder

0:57:27.650,0:57:31.789
know how to get crazy stuff all the crazy stuff is simply you know adding

0:57:31.789,0:57:36.979
several of these things I've been teaching you so far but the bit about

0:57:36.979,0:57:44.440
variational auto-encoder I think it was covered mostly here okay okay thanks

0:57:44.440,0:57:52.839
other questions? no okay okay that was it okay thank you so much for joining us

0:57:52.839,0:58:04.530
okay everyone almost left 70% see you next week bye all right okay

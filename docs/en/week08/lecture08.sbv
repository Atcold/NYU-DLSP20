0:00:00.399,0:00:04.720
all right you guys see the slides i

0:00:02.320,0:00:04.720
assume

0:00:05.120,0:00:08.559
alfredo i can't see you i can't see

0:00:06.399,0:00:09.920
anyone else so yes all right

0:00:08.559,0:00:12.320
yeah it's all good you can also make

0:00:09.920,0:00:15.599
signs i can see you

0:00:12.320,0:00:17.359
okay um so um

0:00:15.599,0:00:19.600
we're going to talk about still talk

0:00:17.359,0:00:21.520
about energy-based models and mostly in

0:00:19.600,0:00:23.119
the context of

0:00:21.520,0:00:25.119
cell supervision or unsupervised

0:00:23.119,0:00:27.599
learning continuing

0:00:25.119,0:00:31.119
where we left off last time so let me

0:00:27.599,0:00:34.079
start with a little bit of

0:00:31.119,0:00:35.840
reminder of where we where we left last

0:00:34.079,0:00:38.000
time

0:00:35.840,0:00:40.879
we talked about supervised learning as

0:00:38.000,0:00:40.879
the idea of

0:00:41.040,0:00:45.680
basically trying to predict everything

0:00:42.480,0:00:47.600
from everything else

0:00:45.680,0:00:49.280
pretending that a part of the input is

0:00:47.600,0:00:51.280
not visible to the system and another

0:00:49.280,0:00:52.559
part is visible and we train the system

0:00:51.280,0:00:56.719
to predict the

0:00:52.559,0:00:56.719
non-visible part from the visible part

0:00:57.039,0:01:00.079
and of course it could be it could be

0:00:58.559,0:01:00.640
anything could be part of a video or it

0:01:00.079,0:01:02.559
could be

0:01:00.640,0:01:04.000
something else there is a special case

0:01:02.559,0:01:05.680
where uh

0:01:04.000,0:01:08.000
we don't assume that anything is visible

0:01:05.680,0:01:09.280
at any time and so we're just asking the

0:01:08.000,0:01:13.439
system to just predict

0:01:09.280,0:01:16.880
uh out of the blue without any input

0:01:13.439,0:01:18.479
so we talked about this the approach of

0:01:16.880,0:01:22.320
energy-based models

0:01:18.479,0:01:23.840
which consist in essentially

0:01:22.320,0:01:25.520
basically having an implicit function

0:01:23.840,0:01:26.000
that captures the dependency between x

0:01:25.520,0:01:27.840
and y

0:01:26.000,0:01:29.920
or in the case where you don't have an x

0:01:27.840,0:01:32.000
is the dependency between the various

0:01:29.920,0:01:34.079
components of y

0:01:32.000,0:01:35.040
and the reason why we need a implicit

0:01:34.079,0:01:37.680
function is that

0:01:35.040,0:01:40.400
for a particular value of x there could

0:01:37.680,0:01:43.200
be multiple values of y that are

0:01:40.400,0:01:43.200
that are possible

0:01:43.439,0:01:46.880
and so if we had a direct prediction

0:01:45.200,0:01:48.240
from x to y we could only make one

0:01:46.880,0:01:51.200
prediction and using an

0:01:48.240,0:01:53.119
implicit function we can make multiple

0:01:51.200,0:01:55.119
predictions implicitly by

0:01:53.119,0:01:56.880
basically having a function that gives

0:01:55.119,0:01:58.159
lower energy for multiple values of y

0:01:56.880,0:02:00.000
for a given value of x

0:01:58.159,0:02:02.399
and that's a little bit what's uh

0:02:00.000,0:02:04.399
represented on the on the left with

0:02:02.399,0:02:06.399
uh essentially you can think of this as

0:02:04.399,0:02:08.879
some sort of

0:02:06.399,0:02:10.000
landscape mountainous landscape where

0:02:08.879,0:02:12.640
the data points

0:02:10.000,0:02:14.720
are in the valleys and everything else

0:02:12.640,0:02:18.000
at size the manifold of data is

0:02:14.720,0:02:18.000
uh has higher energy

0:02:18.319,0:02:22.400
so inference in this context proceeds by

0:02:21.840,0:02:24.640
basically

0:02:22.400,0:02:28.160
finding a y or a set of y's that

0:02:24.640,0:02:30.080
minimize f of x y for a given x

0:02:28.160,0:02:32.239
so this is not learning yet learning

0:02:30.080,0:02:33.599
consists in shaping f

0:02:32.239,0:02:35.519
but this we're just talking about

0:02:33.599,0:02:37.280
inference so it's very important to

0:02:35.519,0:02:38.720
be able to make the difference between

0:02:37.280,0:02:40.800
the inference process

0:02:38.720,0:02:41.760
of minimizing the energy function to

0:02:40.800,0:02:43.200
find y

0:02:41.760,0:02:45.040
and then the learning process which is

0:02:43.200,0:02:46.160
minimizing a loss function not the

0:02:45.040,0:02:47.599
energy function

0:02:46.160,0:02:49.519
with respect to the parameters of the

0:02:47.599,0:02:51.840
energy function okay the two different

0:02:49.519,0:02:51.840
things

0:02:52.160,0:02:56.239
and in the case of

0:02:56.400,0:03:00.560
the unconditional case you don't have an

0:02:58.400,0:03:05.440
x and so you're only capturing the

0:03:00.560,0:03:07.360
mutual dependencies between y's

0:03:05.440,0:03:09.040
we talked about latent variable models

0:03:07.360,0:03:10.319
and the reason why we talk about latent

0:03:09.040,0:03:12.400
variable models is that it's a

0:03:10.319,0:03:14.319
particular way of representing

0:03:12.400,0:03:15.680
uh or building the architecture of the

0:03:14.319,0:03:17.599
energy function

0:03:15.680,0:03:20.480
in such a way that it can have a

0:03:17.599,0:03:22.720
multiple y for a given x

0:03:20.480,0:03:23.760
so essentially a latent variable is an

0:03:22.720,0:03:26.319
extra variable z

0:03:23.760,0:03:27.760
that nobody gives you the value of but

0:03:26.319,0:03:29.040
the first thing you do when you see a z

0:03:27.760,0:03:30.640
is that you minimize your energy

0:03:29.040,0:03:33.040
function with respect to that z

0:03:30.640,0:03:34.239
and that gives you now an energy

0:03:33.040,0:03:35.200
function that does not depend on z

0:03:34.239,0:03:38.239
anymore

0:03:35.200,0:03:40.959
okay or if you want to do

0:03:38.239,0:03:42.480
uh inference with a model with latent

0:03:40.959,0:03:44.000
variable i give you an x

0:03:42.480,0:03:45.440
and you find the combination of y and z

0:03:44.000,0:03:46.640
that minimize the energy and then you

0:03:45.440,0:03:49.040
give me y

0:03:46.640,0:03:50.400
that's the that's the inference process

0:03:49.040,0:03:51.840
these two ways to do

0:03:50.400,0:03:54.159
inference with respect to a variable

0:03:51.840,0:03:56.400
that you don't observe uh one is to just

0:03:54.159,0:03:58.640
minimize over it as i just indicated

0:03:56.400,0:04:00.959
and the other one is to marginalize over

0:03:58.640,0:04:03.680
it if you are

0:04:00.959,0:04:05.760
a probabilist but even in other cases

0:04:03.680,0:04:08.239
and there is a simple formula to kind of

0:04:05.760,0:04:11.040
go from one to the other

0:04:08.239,0:04:12.799
which is basically a log sum exponential

0:04:11.040,0:04:13.280
over all possible values of z and this

0:04:12.799,0:04:14.799
may be

0:04:13.280,0:04:17.120
intractable so we don't do this very

0:04:14.799,0:04:17.120
often

0:04:17.840,0:04:22.960
okay so training an energy based model

0:04:20.880,0:04:24.800
consists in parameterizing the energy

0:04:22.960,0:04:26.560
function

0:04:24.800,0:04:28.639
and collecting of course a bunch of

0:04:26.560,0:04:30.720
training samples a bunch of x and y's

0:04:28.639,0:04:31.840
in the conditional case or just a bunch

0:04:30.720,0:04:35.120
of y's in a

0:04:31.840,0:04:38.000
unconditional case and then it consists

0:04:35.120,0:04:40.000
in shaping the energy function so that

0:04:38.000,0:04:42.000
you give low energy to good combinations

0:04:40.000,0:04:44.240
of x and y's and higher energy to buy

0:04:42.000,0:04:46.400
combinations of x and y so for a given

0:04:44.240,0:04:49.520
observed x

0:04:46.400,0:04:51.440
you try to make f of y for

0:04:49.520,0:04:52.960
the corresponding y that corresponds to

0:04:51.440,0:04:54.240
x as low as possible

0:04:52.960,0:04:56.080
but then you also need to make the

0:04:54.240,0:04:58.560
energy f of x and y

0:04:56.080,0:05:01.039
larger for all other values of y all

0:04:58.560,0:05:02.560
other possible values of y

0:05:01.039,0:05:04.080
and it's probably a good idea to keep

0:05:02.560,0:05:05.520
this energy function smooth if you are

0:05:04.080,0:05:06.960
in a continuous space if y is a

0:05:05.520,0:05:09.199
continuous variable

0:05:06.960,0:05:10.720
because i'll make inference easier

0:05:09.199,0:05:11.440
subsequently you'll be able to use

0:05:10.720,0:05:13.680
gradient descent

0:05:11.440,0:05:15.120
based methods to do inference on maybe

0:05:13.680,0:05:17.759
other methods

0:05:15.120,0:05:20.960
so there's two classes of learning

0:05:17.759,0:05:24.160
algorithms as we talked about last time

0:05:20.960,0:05:26.400
the first class is contrastive methods

0:05:24.160,0:05:27.280
which consist in basically pushing down

0:05:26.400,0:05:30.080
on

0:05:27.280,0:05:31.280
the energy of training samples so you

0:05:30.080,0:05:34.479
get a training sample

0:05:31.280,0:05:35.039
x i y i uh you plug it into your energy

0:05:34.479,0:05:36.800
function

0:05:35.039,0:05:38.400
and then you tune the parameters of the

0:05:36.800,0:05:38.880
energy function so that that energy goes

0:05:38.400,0:05:42.080
down

0:05:38.880,0:05:43.039
and you can do this uh with uh you know

0:05:42.080,0:05:46.080
with backdrop if

0:05:43.039,0:05:48.960
if your energy function is some sort of

0:05:46.080,0:05:49.520
neural net with other things in it as

0:05:48.960,0:05:50.960
long as

0:05:49.520,0:05:53.840
as it's a differentiable function you

0:05:50.960,0:05:56.560
can you can do that

0:05:53.840,0:05:57.120
but then uh what you have to do as well

0:05:56.560,0:05:58.400
is pick

0:05:57.120,0:06:00.400
other points that are outside the

0:05:58.400,0:06:01.680
manifold of data and then push their

0:06:00.400,0:06:04.000
energy up

0:06:01.680,0:06:04.880
so that the energy gets takes the right

0:06:04.000,0:06:06.800
shape okay

0:06:04.880,0:06:08.319
so those are contrastive methods and

0:06:06.800,0:06:09.840
then there's architectural methods and

0:06:08.319,0:06:12.319
the architectural methods

0:06:09.840,0:06:13.919
basically consist in building f of x y

0:06:12.319,0:06:17.440
in such a way that

0:06:13.919,0:06:19.440
the volume of space that can take low

0:06:17.440,0:06:22.720
energy is limited

0:06:19.440,0:06:24.479
perhaps minimized in some way

0:06:22.720,0:06:26.319
and so if you push down on the energy of

0:06:24.479,0:06:27.199
certain points automatically the rest

0:06:26.319,0:06:29.600
will

0:06:27.199,0:06:31.199
be up because we go up because the

0:06:29.600,0:06:31.680
volume of stuff that can take low energy

0:06:31.199,0:06:35.840
is

0:06:31.680,0:06:35.840
is limited

0:06:36.160,0:06:41.199
and i've made a list here uh so this is

0:06:39.280,0:06:44.800
an important slide

0:06:41.199,0:06:46.960
which you you saw last time

0:06:44.800,0:06:48.560
and there's a list of various methods

0:06:46.960,0:06:49.919
that you may have heard of

0:06:48.560,0:06:52.080
some of which are contrastive some of

0:06:49.919,0:06:53.840
which are architectural i must say that

0:06:52.080,0:06:55.840
those two classes of methods are not

0:06:53.840,0:06:57.520
necessarily incompatible with each other

0:06:55.840,0:06:59.039
you can very well use a combination of

0:06:57.520,0:07:02.160
the two

0:06:59.039,0:07:04.560
but most methods only use one so

0:07:02.160,0:07:06.000
things like maximum likelihood if your

0:07:04.560,0:07:07.360
parabolas consist in

0:07:06.000,0:07:08.479
pushing down on the energy of data

0:07:07.360,0:07:09.520
points and then pushing up every

0:07:08.479,0:07:11.919
everywhere else

0:07:09.520,0:07:14.479
for every other value of y in proportion

0:07:11.919,0:07:17.199
to how low the energy is so you push up

0:07:14.479,0:07:18.720
harder if the energy is lower so that in

0:07:17.199,0:07:22.400
the end you get kind of the

0:07:18.720,0:07:23.759
the right shape maximum likelihood

0:07:22.400,0:07:25.120
incidentally only cares about

0:07:23.759,0:07:27.199
differences of energy it doesn't

0:07:25.120,0:07:28.560
doesn't care about absolute values of

0:07:27.199,0:07:31.759
energies

0:07:28.560,0:07:33.199
um which is an important point and then

0:07:31.759,0:07:34.479
there are other methods like contrastive

0:07:33.199,0:07:37.280
divergence

0:07:34.479,0:07:38.720
uh metric learning ratio matching noise

0:07:37.280,0:07:39.440
contrastive estimation minimum

0:07:38.720,0:07:42.400
probability flow

0:07:39.440,0:07:43.840
things like that which and uh generative

0:07:42.400,0:07:45.759
average several networks

0:07:43.840,0:07:47.599
that also are based on the idea of

0:07:45.759,0:07:52.560
pushing up on the energy of

0:07:47.599,0:07:52.560
data points outside the data manifold

0:07:54.720,0:08:00.639
then there are similar methods uh

0:07:58.160,0:08:02.240
um dinosaur autoencoders that we will

0:08:00.639,0:08:04.400
talk about in just a minute and

0:08:02.240,0:08:06.160
uh as we saw last time they've been

0:08:04.400,0:08:08.479
extremely successful in the context of

0:08:06.160,0:08:09.759
natural language processing uh systems

0:08:08.479,0:08:12.400
like bird for example

0:08:09.759,0:08:13.759
basically arduino using automatically of

0:08:12.400,0:08:15.199
a particular kind

0:08:13.759,0:08:16.319
and then there are architectural methods

0:08:15.199,0:08:18.400
and last time we talked a little bit

0:08:16.319,0:08:21.360
about pca and k-means

0:08:18.400,0:08:22.720
um but they we're going to talk about a

0:08:21.360,0:08:25.840
few more

0:08:22.720,0:08:28.800
today particularly sparse coding and

0:08:25.840,0:08:28.800
something called lista

0:08:29.360,0:08:32.959
and i'm not going to talk about the

0:08:30.639,0:08:35.919
remaining ones

0:08:32.959,0:08:37.039
so this is a rehash again of something

0:08:35.919,0:08:39.519
that we talked about last

0:08:37.039,0:08:40.959
last week which is a very simple latent

0:08:39.519,0:08:42.959
variable model for

0:08:40.959,0:08:45.440
unsupervised learning k-means which i'm

0:08:42.959,0:08:47.120
sure you all you've all heard about

0:08:45.440,0:08:49.360
there the energy function is simply the

0:08:47.120,0:08:50.080
square reconstruction error between the

0:08:49.360,0:08:53.600
data vector

0:08:50.080,0:08:54.240
and the product of a prototype matrix

0:08:53.600,0:08:56.720
times

0:08:54.240,0:08:58.480
a latent variable vector and that latent

0:08:56.720,0:09:00.959
variable vector is constrained to be

0:08:58.480,0:09:01.680
a one-hot vector so in other words it

0:09:00.959,0:09:03.120
selects

0:09:01.680,0:09:04.959
one of the columns of w when you

0:09:03.120,0:09:08.399
multiply w by

0:09:04.959,0:09:11.519
by it and so what you get in the end

0:09:08.399,0:09:11.920
is the uh the square distance between

0:09:11.519,0:09:15.920
the

0:09:11.920,0:09:17.600
data vector and the uh column of w that

0:09:15.920,0:09:19.200
is closest to it

0:09:17.600,0:09:20.480
uh once you do the minimization with

0:09:19.200,0:09:22.160
respect to z which means you know

0:09:20.480,0:09:24.160
looking for which column w

0:09:22.160,0:09:25.920
is closest to y so that's the energy

0:09:24.160,0:09:28.800
function that's the inference algorithm

0:09:25.920,0:09:30.160
looking for the closest prototype and uh

0:09:28.800,0:09:32.640
the energy function of course

0:09:30.160,0:09:34.720
is zero wherever there is a prototype

0:09:32.640,0:09:36.000
and grows quadratically as you move away

0:09:34.720,0:09:37.360
from the prototype

0:09:36.000,0:09:39.440
until you get closer to another

0:09:37.360,0:09:41.519
prototype in which case the energy

0:09:39.440,0:09:42.640
again goes down as you get closer to the

0:09:41.519,0:09:46.320
second prototype

0:09:42.640,0:09:48.399
so if you train k-means on a data set

0:09:46.320,0:09:49.760
where the training samples are generated

0:09:48.399,0:09:52.160
by uh

0:09:49.760,0:09:53.839
peeking around this little uh spiral

0:09:52.160,0:09:57.360
here shown at the bottom

0:09:53.839,0:09:58.560
um what you'll you you with kegel 20 in

0:09:57.360,0:10:01.440
that case you get those

0:09:58.560,0:10:02.399
uh little dark areas which indicate the

0:10:01.440,0:10:03.760
the minima

0:10:02.399,0:10:06.480
of the energy function and there is a

0:10:03.760,0:10:08.640
ridge in the middle um

0:10:06.480,0:10:10.320
where you know the sort of energy uh

0:10:08.640,0:10:11.600
it's like um

0:10:10.320,0:10:15.040
you know the energy kind of goes down on

0:10:11.600,0:10:18.800
both sides it's like a ridge

0:10:15.040,0:10:21.279
um but here is uh a method that's become

0:10:18.800,0:10:22.800
very popular over the last few months

0:10:21.279,0:10:24.880
and it's very recent

0:10:22.800,0:10:26.240
the the first papers on this actually go

0:10:24.880,0:10:28.560
back a long time there's some of my

0:10:26.240,0:10:31.519
papers from the early 90s and from the

0:10:28.560,0:10:33.440
the mid-2000s um and they were called

0:10:31.519,0:10:34.560
siamese networks or metric learning at

0:10:33.440,0:10:38.560
the time

0:10:34.560,0:10:40.720
and the idea is to build a uh a uh

0:10:38.560,0:10:41.839
sort of energy-based model if you want

0:10:40.720,0:10:44.000
by uh

0:10:41.839,0:10:45.360
having two copies of the same network or

0:10:44.000,0:10:47.600
two different networks but very often

0:10:45.360,0:10:51.040
these two copies of the same network

0:10:47.600,0:10:52.959
and you feed uh x to the first network

0:10:51.040,0:10:54.240
and y to the second network

0:10:52.959,0:10:56.320
you have them compute some feature

0:10:54.240,0:10:57.760
vector on the output h and h prime

0:10:56.320,0:10:59.519
and then you compare those two feature

0:10:57.760,0:11:01.440
vectors with some

0:10:59.519,0:11:03.279
some methods some some way of computing

0:11:01.440,0:11:04.880
a similarity or dissimilarity

0:11:03.279,0:11:06.399
between vectors it could be a dot

0:11:04.880,0:11:08.800
product could be a cosine

0:11:06.399,0:11:10.320
a similarity it could be something of

0:11:08.800,0:11:12.640
that type

0:11:10.320,0:11:14.480
and what you do is uh to train the

0:11:12.640,0:11:17.120
system is that

0:11:14.480,0:11:18.320
you you train it with a data point

0:11:17.120,0:11:21.600
basically is a pair

0:11:18.320,0:11:23.440
of x and y um so

0:11:21.600,0:11:25.440
you indicate the location of the the

0:11:23.440,0:11:26.399
data manifold to the system by basically

0:11:25.440,0:11:29.040
telling it

0:11:26.399,0:11:30.480
here is a here is a sample tell me is a

0:11:29.040,0:11:33.519
sample x

0:11:30.480,0:11:33.839
give me another sample that basically

0:11:33.519,0:11:37.040
has

0:11:33.839,0:11:38.880
the same content as x but it's different

0:11:37.040,0:11:40.240
and of course you're never going to ask

0:11:38.880,0:11:41.440
the system to give you that sample

0:11:40.240,0:11:43.519
you're going to generate

0:11:41.440,0:11:44.800
those samples and train the system with

0:11:43.519,0:11:46.720
it so

0:11:44.800,0:11:48.880
there are there are two pairs uh

0:11:46.720,0:11:50.160
positive pairs so pairs are compatible

0:11:48.880,0:11:51.600
with each other which is a whole idea of

0:11:50.160,0:11:54.000
energy based models

0:11:51.600,0:11:55.360
and a compatible pair is or a positive

0:11:54.000,0:11:58.959
pair if you want

0:11:55.360,0:12:00.480
is consist of x being an image and y

0:11:58.959,0:12:02.560
being a transformation of this image

0:12:00.480,0:12:05.360
that basically does not change its

0:12:02.560,0:12:06.720
content so it's still the same content

0:12:05.360,0:12:10.480
of the image if you want

0:12:06.720,0:12:11.920
so you want basically uh representations

0:12:10.480,0:12:12.880
extracted by those two networks to be

0:12:11.920,0:12:14.800
very similar

0:12:12.880,0:12:16.000
because those images are similar and

0:12:14.800,0:12:17.519
that's exactly what you're gonna do

0:12:16.000,0:12:19.360
you're gonna feed those two images to

0:12:17.519,0:12:21.120
those two those two networks

0:12:19.360,0:12:23.519
and you're gonna have a loss function

0:12:21.120,0:12:25.200
that that says minimize the energy which

0:12:23.519,0:12:27.200
means minimize the distance or

0:12:25.200,0:12:30.240
similarity measure between h and h prime

0:12:27.200,0:12:32.399
between the outputs of the two networks

0:12:30.240,0:12:35.200
um so that's the positive part so that's

0:12:32.399,0:12:38.480
a way to kind of lower the energy

0:12:35.200,0:12:40.560
for for training samples

0:12:38.480,0:12:41.600
okay and then you have to generate

0:12:40.560,0:12:44.079
random

0:12:41.600,0:12:45.760
uh negative samples and the way you

0:12:44.079,0:12:48.560
generate them is by

0:12:45.760,0:12:50.320
basically picking again a sample for x

0:12:48.560,0:12:51.920
and then picking another image that you

0:12:50.320,0:12:52.160
know is different that has nothing to do

0:12:51.920,0:12:54.000
with

0:12:52.160,0:12:56.320
x it's incompatible with it if you want

0:12:54.000,0:12:57.519
it's very different

0:12:56.320,0:12:59.600
and now what you do is you feed those

0:12:57.519,0:13:02.639
two images to those two networks

0:12:59.600,0:13:05.760
and you try to push

0:13:02.639,0:13:09.200
h and h prime away from each other

0:13:05.760,0:13:11.680
so basically you're trying to make the

0:13:09.200,0:13:12.639
the similarity matrix c of h and h prime

0:13:11.680,0:13:15.680
large

0:13:12.639,0:13:17.360
for those two samples okay

0:13:15.680,0:13:18.959
and the objective function here is going

0:13:17.360,0:13:22.160
to take into account

0:13:18.959,0:13:24.240
uh uh the the object the

0:13:22.160,0:13:26.000
the energy function for similar part

0:13:24.240,0:13:27.040
pairs and the energy function for

0:13:26.000,0:13:28.320
decimator pairs

0:13:27.040,0:13:30.240
is going to push down on the energy

0:13:28.320,0:13:33.279
function for similar pairs push up on

0:13:30.240,0:13:36.560
the energy function for dissimilar pairs

0:13:33.279,0:13:39.199
okay um

0:13:36.560,0:13:39.680
so there's been a number of uh uh so

0:13:39.199,0:13:41.279
paper

0:13:39.680,0:13:42.720
b you know people have used metric

0:13:41.279,0:13:43.360
learning for various things for a long

0:13:42.720,0:13:46.399
time for

0:13:43.360,0:13:48.560
uh image search for example uh for

0:13:46.399,0:13:50.000
uh feature recognition for for things

0:13:48.560,0:13:52.000
like that

0:13:50.000,0:13:53.120
but it's only in the last few months

0:13:52.000,0:13:55.199
that there's been

0:13:53.120,0:13:57.120
a couple of works that have shown they

0:13:55.199,0:13:58.959
can use those methods to

0:13:57.120,0:14:00.399
learn good features for object

0:13:58.959,0:14:01.839
recognition

0:14:00.399,0:14:03.760
and those are really the first papers

0:14:01.839,0:14:06.800
that produce features

0:14:03.760,0:14:08.560
in a unsupervised or supervised way

0:14:06.800,0:14:09.519
that produce features that can rival the

0:14:08.560,0:14:11.839
features that are obtained through

0:14:09.519,0:14:14.399
supervised learning

0:14:11.839,0:14:16.240
so the three papers in question are

0:14:14.399,0:14:18.880
pearl which means protecting

0:14:16.240,0:14:20.079
representation learning by isha mitra

0:14:18.880,0:14:21.519
and

0:14:20.079,0:14:23.440
lawrence van der matten at facebook in

0:14:21.519,0:14:26.000
new york another one called

0:14:23.440,0:14:27.680
moko by kamingha and his collaborators

0:14:26.000,0:14:28.880
at facebook in menlo park

0:14:27.680,0:14:30.880
and the third one which appeared more

0:14:28.880,0:14:34.240
recently is called sim clear

0:14:30.880,0:14:36.399
by a group from google janet al and the

0:14:34.240,0:14:39.120
last author being

0:14:36.399,0:14:39.120
jeffery hinton

0:14:40.880,0:14:47.360
so the um

0:14:45.519,0:14:49.199
there's been other work uh using those

0:14:47.360,0:14:50.639
kind of methods uh i think there was a

0:14:49.199,0:14:51.839
question perhaps i i

0:14:50.639,0:14:53.839
wasn't the question it was actually my

0:14:51.839,0:14:57.440
phone waking up because i said google

0:14:53.839,0:14:58.800
and oh i see okay and and through

0:14:57.440,0:15:00.240
features that's something we'll talk

0:14:58.800,0:15:04.079
we'll talk about later which is

0:15:00.240,0:15:05.680
a little a little similar

0:15:04.079,0:15:07.360
okay so these are examples of results

0:15:05.680,0:15:09.199
that are obtained with moco and

0:15:07.360,0:15:10.720
uh they essentially show that even with

0:15:09.199,0:15:12.959
a very large model you can

0:15:10.720,0:15:14.399
which is basically a version of resnet50

0:15:12.959,0:15:15.839
that you train using this code some

0:15:14.399,0:15:17.680
contrasting method

0:15:15.839,0:15:19.839
uh you get sort of decent performance

0:15:17.680,0:15:22.800
this is i believe uh

0:15:19.839,0:15:24.399
uh top five performance on uh on

0:15:22.800,0:15:26.000
imagenet

0:15:24.399,0:15:27.760
pearl actually works uh quite a bit

0:15:26.000,0:15:31.120
better than than moco this is

0:15:27.760,0:15:35.199
uh top one accuracy this time um

0:15:31.120,0:15:37.360
with uh uh networks of our size so

0:15:35.199,0:15:39.600
sizes so here the there's several

0:15:37.360,0:15:42.720
scenarios the the main scenario is

0:15:39.600,0:15:44.079
you take all of your imagenet you uh

0:15:42.720,0:15:46.560
you take a sample from you mentioned it

0:15:44.079,0:15:49.680
distort it and that gives you

0:15:46.560,0:15:52.000
a positive pair uh run it through

0:15:49.680,0:15:53.440
your two networks and train the the

0:15:52.000,0:15:56.160
network to

0:15:53.440,0:15:57.440
produce similar outputs basically the

0:15:56.160,0:15:59.279
two networks are identical

0:15:57.440,0:16:01.920
actually for both moco and parallel it's

0:15:59.279,0:16:05.120
the same is net and then

0:16:01.920,0:16:06.560
take dissimilar pairs and push the

0:16:05.120,0:16:08.720
outputs away from each other using a

0:16:06.560,0:16:11.360
particular cost function that we'll

0:16:08.720,0:16:11.360
see in a minute

0:16:11.920,0:16:17.360
and then you have to do this many many

0:16:14.959,0:16:19.519
times and you have to be smart about

0:16:17.360,0:16:20.720
how you cache the negative samples

0:16:19.519,0:16:22.720
because

0:16:20.720,0:16:24.160
most samples are already very different

0:16:22.720,0:16:26.480
by the time they they get to the output

0:16:24.160,0:16:28.000
to the network so

0:16:26.480,0:16:30.079
you basically have to be smart about how

0:16:28.000,0:16:32.399
you kind of pick the the good negatives

0:16:30.079,0:16:33.120
so the type of uh objective function

0:16:32.399,0:16:36.399
that

0:16:33.120,0:16:37.279
is used by by perl is called the noise

0:16:36.399,0:16:40.000
contrastive

0:16:37.279,0:16:41.759
estimation and that goes back to kind of

0:16:40.000,0:16:42.959
previous papers it's not it's not their

0:16:41.759,0:16:45.279
uh invention

0:16:42.959,0:16:47.519
where the the similarity metric is this

0:16:45.279,0:16:49.040
uh is the cosine similarity

0:16:47.519,0:16:51.519
measure between the outputs of the

0:16:49.040,0:16:53.920
commercial nets

0:16:51.519,0:16:57.040
and and then what you compute is this uh

0:16:53.920,0:16:59.440
basically the softmax-like

0:16:57.040,0:17:00.959
function which computes the exponential

0:16:59.440,0:17:03.040
of the similarity metric

0:17:00.959,0:17:04.000
of two outputs for similar pairs and

0:17:03.040,0:17:06.400
then divides by

0:17:04.000,0:17:08.319
the sum of the similarity metric

0:17:06.400,0:17:10.480
exponentiated for similar pairs and the

0:17:08.319,0:17:12.959
sum of dissimilar pairs so you have

0:17:10.480,0:17:14.720
a batch where you have one similar pair

0:17:12.959,0:17:16.480
and a bunch of dissimilar pairs

0:17:14.720,0:17:19.199
and you compute this kind of soft max

0:17:16.480,0:17:22.240
thing and if you minimize itself max

0:17:19.199,0:17:22.959
cost function is going to push the

0:17:22.240,0:17:25.120
similarity

0:17:22.959,0:17:28.079
similarity metric of similar pair to be

0:17:25.120,0:17:29.600
as large as possible and the

0:17:28.079,0:17:32.080
the similarity metric the cosine

0:17:29.600,0:17:34.160
similarity of decimal pair to be

0:17:32.080,0:17:35.520
basically as small as possible i had

0:17:34.160,0:17:39.360
this question that

0:17:35.520,0:17:39.840
why are we separately using an lnce

0:17:39.360,0:17:42.080
function

0:17:39.840,0:17:44.160
whereas we could have probably directly

0:17:42.080,0:17:47.840
computed laws using the

0:17:44.160,0:17:49.520
the h v i v i transformed uh

0:17:47.840,0:17:51.679
probability that we have by taking the

0:17:49.520,0:17:54.799
negative log of that probability

0:17:51.679,0:17:55.520
so like what benefit would l and z

0:17:54.799,0:17:58.640
provide

0:17:55.520,0:18:00.000
using uh like not directly

0:17:58.640,0:18:02.559
taking the negative log of the

0:18:00.000,0:18:05.120
probability that we have from h

0:18:02.559,0:18:06.799
well that's a good question um it's not

0:18:05.120,0:18:09.600
entirely clear to me why

0:18:06.799,0:18:10.640
uh i think uh what what what happened

0:18:09.600,0:18:12.080
there is that

0:18:10.640,0:18:13.600
people tried lots and lots of different

0:18:12.080,0:18:14.799
things and this is what ended up working

0:18:13.600,0:18:17.919
best

0:18:14.799,0:18:19.120
there is uh in the the hinton paper

0:18:17.919,0:18:19.840
there's kind of a similar thing where

0:18:19.120,0:18:22.720
they tried

0:18:19.840,0:18:24.160
different types of uh objective function

0:18:22.720,0:18:26.160
and found that something like nc

0:18:24.160,0:18:28.960
actually works quite well

0:18:26.160,0:18:30.799
so it's an empirical question and i i

0:18:28.960,0:18:33.039
don't have a good tuition for why

0:18:30.799,0:18:34.320
you need this uh this term uh in

0:18:33.039,0:18:38.480
addition to the just the

0:18:34.320,0:18:39.679
denominator uh in h um

0:18:38.480,0:18:42.559
i hope this answers your question

0:18:39.679,0:18:43.039
although sorry i don't have any answer

0:18:42.559,0:18:45.440
for it

0:18:43.039,0:18:47.039
why do you use cosines similarity

0:18:45.440,0:18:49.760
instead of l2 norm

0:18:47.039,0:18:50.400
instead of l2 norm or instead of okay

0:18:49.760,0:18:52.320
it's because

0:18:50.400,0:18:53.679
you it's because you want to normalize

0:18:52.320,0:18:55.120
it's very easy to make two vectors

0:18:53.679,0:18:57.360
similar by making them

0:18:55.120,0:18:59.679
very short or to make two vectors very

0:18:57.360,0:19:03.360
dissimilar but making them very long

0:18:59.679,0:19:04.799
okay so by doing cosine similarity

0:19:03.360,0:19:06.160
you're basically normalizing

0:19:04.799,0:19:08.000
right you're computing a dot product but

0:19:06.160,0:19:10.000
you're normalizing this dot product

0:19:08.000,0:19:12.160
and so you make the measure independent

0:19:10.000,0:19:15.600
of the length of the

0:19:12.160,0:19:16.960
of the vectors and so it uh it forces

0:19:15.600,0:19:18.400
the system to kind of find a good

0:19:16.960,0:19:21.600
good solution to the problem without

0:19:18.400,0:19:23.919
just without cheating by just making the

0:19:21.600,0:19:24.720
uh you know vectors either short or or

0:19:23.919,0:19:27.520
large it

0:19:24.720,0:19:29.360
it also removes a an instability that

0:19:27.520,0:19:30.799
they could be in the system

0:19:29.360,0:19:32.799
the the design of those contrasting

0:19:30.799,0:19:34.320
functions is actually quite uh quite a

0:19:32.799,0:19:35.840
bit of a black heart

0:19:34.320,0:19:38.080
okay so what they actually do in pearl

0:19:35.840,0:19:41.200
is that they um

0:19:38.080,0:19:44.480
they they don't use directly the uh

0:19:41.200,0:19:46.559
the output of the of the convnet for the

0:19:44.480,0:19:48.240
for the objective function they they

0:19:46.559,0:19:50.559
have different heads

0:19:48.240,0:19:52.160
so basically the the continent has

0:19:50.559,0:19:54.960
different set of heads f and g

0:19:52.160,0:19:55.600
which are different for the two networks

0:19:54.960,0:19:57.840
and

0:19:55.600,0:19:59.200
that's what they use in the context of

0:19:57.840,0:20:00.640
this contractive learning and then there

0:19:59.200,0:20:01.440
is you know another head that they use

0:20:00.640,0:20:03.760
for

0:20:01.440,0:20:05.440
the the ultimate task of of

0:20:03.760,0:20:07.280
classification

0:20:05.440,0:20:09.039
so those f and g functions are you can

0:20:07.280,0:20:11.840
think of as sort of extra

0:20:09.039,0:20:12.559
layers that are kind of uh on top of the

0:20:11.840,0:20:14.559
network that

0:20:12.559,0:20:16.240
are different for the two the two for

0:20:14.559,0:20:16.720
the two networks all right so these are

0:20:16.240,0:20:18.080
the

0:20:16.720,0:20:20.000
these are the results that are produced

0:20:18.080,0:20:22.640
by uh by pearl and

0:20:20.000,0:20:24.480
uh you can get so this is this

0:20:22.640,0:20:26.480
particular experiment is one in which

0:20:24.480,0:20:29.520
you pre-trained the the system using

0:20:26.480,0:20:32.960
perl on the imagenet training set

0:20:29.520,0:20:35.360
and then what you do is uh you

0:20:32.960,0:20:37.200
you you retrain you fine tune the system

0:20:35.360,0:20:38.640
using either one percent of the labeled

0:20:37.200,0:20:40.000
samples or ten percent of the label

0:20:38.640,0:20:41.600
samples and you you measure the

0:20:40.000,0:20:42.799
performance top five accuracy or top

0:20:41.600,0:20:44.799
point accuracy

0:20:42.799,0:20:45.919
so this paper appeared in uh in january

0:20:44.799,0:20:48.080
on archive

0:20:45.919,0:20:49.919
um and then just a few weeks ago uh this

0:20:48.080,0:20:53.039
paper appeal called sinclair

0:20:49.919,0:20:56.080
by channing al um which is a

0:20:53.039,0:20:59.280
team from google and they

0:20:56.080,0:21:01.360
have a very sophisticated corruption

0:20:59.280,0:21:03.039
or data augmentation method to generate

0:21:01.360,0:21:05.600
similar pairs

0:21:03.039,0:21:08.799
and they train uh for a very very long

0:21:05.600,0:21:11.039
time on a lot of tpus

0:21:08.799,0:21:12.640
and they get really interestingly good

0:21:11.039,0:21:16.559
results so

0:21:12.640,0:21:20.159
much better than either pearl or moco

0:21:16.559,0:21:20.159
using very large models

0:21:20.240,0:21:23.679
and they can reach you know more than 75

0:21:22.640,0:21:26.480
percent correct

0:21:23.679,0:21:28.559
top point on imagenet by just

0:21:26.480,0:21:30.080
pre-training

0:21:28.559,0:21:32.320
in cell supervised fashion and then kind

0:21:30.080,0:21:33.520
of fine tuning with only one percent of

0:21:32.320,0:21:37.440
the

0:21:33.520,0:21:39.919
other samples yeah so this is uh

0:21:37.440,0:21:40.960
in fact the previous slide is a

0:21:39.919,0:21:42.400
different scenario

0:21:40.960,0:21:44.559
uh where you only train a linear

0:21:42.400,0:21:46.400
classifier on top of the

0:21:44.559,0:21:48.320
on top of the network the this is the

0:21:46.400,0:21:49.919
scenario where you train with either one

0:21:48.320,0:21:51.120
percent or ten percent of uh label

0:21:49.919,0:21:54.159
samples

0:21:51.120,0:21:56.240
and uh you get uh eighty-five percent

0:21:54.159,0:21:58.080
the top five with one percent of the

0:21:56.240,0:22:00.799
labels which is uh

0:21:58.080,0:22:02.240
you know pretty pretty amazing results

0:22:00.799,0:22:03.760
to some extent i think

0:22:02.240,0:22:06.080
this shows the limits of contrasting

0:22:03.760,0:22:07.440
methods because the amount of

0:22:06.080,0:22:10.159
computation and training that is

0:22:07.440,0:22:12.080
required for this is absolutely gigantic

0:22:10.159,0:22:13.200
it's it's really enormous so here is a

0:22:12.080,0:22:15.360
scenario where you

0:22:13.200,0:22:17.440
just train a linear fast fire on top so

0:22:15.360,0:22:19.360
you you freeze the features

0:22:17.440,0:22:21.280
uh produced by the system that has been

0:22:19.360,0:22:22.880
uh pre-trained using uh

0:22:21.280,0:22:24.320
self-supervised learning and then you

0:22:22.880,0:22:25.360
just train a linear classifier on top

0:22:24.320,0:22:26.880
and you measure the performance you

0:22:25.360,0:22:29.120
total point of top five

0:22:26.880,0:22:31.039
on the full image net having trained

0:22:29.120,0:22:32.159
supervisor on the full image net

0:22:31.039,0:22:34.320
and again the numbers are really

0:22:32.159,0:22:36.559
impressive

0:22:34.320,0:22:38.080
but again i think it shows the limit of

0:22:36.559,0:22:39.760
contracting methods here is the

0:22:38.080,0:22:41.840
the main issue with contrasting methods

0:22:39.760,0:22:42.640
is that there are many many many

0:22:41.840,0:22:45.039
locations

0:22:42.640,0:22:46.559
in a high dimensional space where you

0:22:45.039,0:22:49.280
need to push up the energy

0:22:46.559,0:22:50.400
to make sure that it's actually higher

0:22:49.280,0:22:54.320
uh

0:22:50.400,0:22:56.320
everywhere than on the data manifold

0:22:54.320,0:22:58.080
and so as you increase the dimension of

0:22:56.320,0:23:00.080
the representation

0:22:58.080,0:23:02.559
you need more and more negative samples

0:23:00.080,0:23:05.520
to make sure that the energy is higher

0:23:02.559,0:23:06.720
uh where it needs to be higher okay so

0:23:05.520,0:23:09.679
let's talk about another

0:23:06.720,0:23:11.280
another crop of uh of contrasting

0:23:09.679,0:23:12.080
methods called denoising a toy coder and

0:23:11.280,0:23:14.400
that's become

0:23:12.080,0:23:15.760
really uh kind of important over the

0:23:14.400,0:23:17.440
last uh

0:23:15.760,0:23:19.440
year and a half or so for for natural

0:23:17.440,0:23:21.600
language processing

0:23:19.440,0:23:23.039
uh so the idea of denoising auto encoder

0:23:21.600,0:23:24.640
is that you take a y

0:23:23.039,0:23:26.320
and the way you generate x is by

0:23:24.640,0:23:27.760
corrupting y so this sounds a little bit

0:23:26.320,0:23:28.400
like the opposite of what we were just

0:23:27.760,0:23:31.520
doing

0:23:28.400,0:23:34.000
with contrastive methods but

0:23:31.520,0:23:35.679
basically you take a clean image why you

0:23:34.000,0:23:36.960
corrupt it in some way by removing a

0:23:35.679,0:23:38.320
piece of it for example

0:23:36.960,0:23:40.799
or you take a piece of text and you

0:23:38.320,0:23:43.919
remove some of the words or you mask

0:23:40.799,0:23:45.840
a piece of it so a special case is the

0:23:43.919,0:23:48.480
masked auto encoder where the

0:23:45.840,0:23:49.760
corruption consists in masking a subset

0:23:48.480,0:23:51.200
of the input

0:23:49.760,0:23:54.799
and then you run this through an auto

0:23:51.200,0:23:57.200
encoder which is uh

0:23:54.799,0:23:58.640
you know essentially a an encoder or

0:23:57.200,0:24:02.159
called a predictor here

0:23:58.640,0:24:03.919
a decoder and you know perhaps uh

0:24:02.159,0:24:05.840
you know funnel layers that that you

0:24:03.919,0:24:07.919
know may have uh

0:24:05.840,0:24:09.840
uh soft max in the context of text or

0:24:07.919,0:24:12.000
not if you know nothing if it's

0:24:09.840,0:24:13.440
images and then you compute you compare

0:24:12.000,0:24:16.720
the predicted output

0:24:13.440,0:24:17.600
uh y bar with the uh with the observed

0:24:16.720,0:24:19.840
data

0:24:17.600,0:24:19.840
why

0:24:21.440,0:24:26.960
and so what's the what's the principle

0:24:24.960,0:24:27.919
of this the principle of this is that is

0:24:26.960,0:24:30.080
is the following

0:24:27.919,0:24:31.360
and uh you can thank alfredo for those

0:24:30.080,0:24:33.679
beautiful pictures

0:24:31.360,0:24:34.480
basically yeah we saw this in class last

0:24:33.679,0:24:36.960
tuesday

0:24:34.480,0:24:39.039
that's right so uh this is basically

0:24:36.960,0:24:41.679
just a reminder

0:24:39.039,0:24:42.480
you take a data point you so which is

0:24:41.679,0:24:45.520
one of those

0:24:42.480,0:24:47.279
pink points right and you corrupt it so

0:24:45.520,0:24:50.320
you get one of those

0:24:47.279,0:24:54.240
brown points and then you train the

0:24:50.320,0:24:56.640
uh auto encoder to

0:24:54.240,0:24:57.520
from the brown point to produce the pink

0:24:56.640,0:25:00.240
points

0:24:57.520,0:25:01.600
the the original pink point what does

0:25:00.240,0:25:03.760
that mean that means that

0:25:01.600,0:25:05.120
now the energy function which is the

0:25:03.760,0:25:06.159
reconstruction error is going to be

0:25:05.120,0:25:07.520
equal to

0:25:06.159,0:25:09.600
the difference between the original

0:25:07.520,0:25:10.640
point the pink point the distance the

0:25:09.600,0:25:13.039
square distance

0:25:10.640,0:25:14.559
if c is the euclidean square euclidean

0:25:13.039,0:25:16.960
distance

0:25:14.559,0:25:18.559
so c yybar is going to be the if you

0:25:16.960,0:25:21.440
think it's properly trained

0:25:18.559,0:25:22.880
is going to be the the distance between

0:25:21.440,0:25:26.400
the corrupted point

0:25:22.880,0:25:29.200
uh x uh the the brand point and the

0:25:26.400,0:25:29.840
and the pinpoint you started from why

0:25:29.200,0:25:31.760
okay

0:25:29.840,0:25:33.760
so basically it basically it basically

0:25:31.760,0:25:35.840
trains the the system

0:25:33.760,0:25:38.080
to produce an energy function that grows

0:25:35.840,0:25:42.320
quadratically as you move away

0:25:38.080,0:25:44.400
from the data manifold okay

0:25:42.320,0:25:46.080
um and so it's an example of a

0:25:44.400,0:25:47.360
contrastive method because you push up

0:25:46.080,0:25:48.880
on the energy of points that are outside

0:25:47.360,0:25:49.360
the data manifold essentially you tell

0:25:48.880,0:25:50.640
them

0:25:49.360,0:25:52.480
your energy should be the square

0:25:50.640,0:25:54.159
distance to the data manifold

0:25:52.480,0:25:57.279
or at least to the point that was used

0:25:54.159,0:26:00.080
before you know to

0:25:57.279,0:26:00.080
through corruption

0:26:00.799,0:26:05.120
but the problem with it is that again in

0:26:03.600,0:26:06.720
a high dimensional continuous space

0:26:05.120,0:26:08.320
there is many many many ways you can

0:26:06.720,0:26:12.000
corrupt

0:26:08.320,0:26:13.760
a piece of data and uh it's not entirely

0:26:12.000,0:26:15.520
clear that you're going to be able to

0:26:13.760,0:26:17.600
kind of shape the energy function the

0:26:15.520,0:26:18.960
proper way by just pushing up on lots of

0:26:17.600,0:26:21.360
different locations

0:26:18.960,0:26:22.720
it works in text because text is

0:26:21.360,0:26:24.480
discrete

0:26:22.720,0:26:26.080
it doesn't work so well in images people

0:26:24.480,0:26:27.679
have used this in the context of imaging

0:26:26.080,0:26:29.120
painting for example so the corruption

0:26:27.679,0:26:31.039
consists of masking

0:26:29.120,0:26:33.039
a piece of the image and then training a

0:26:31.039,0:26:34.960
system to reconstruct it

0:26:33.039,0:26:36.880
and the reason why it doesn't work is

0:26:34.960,0:26:38.559
because uh people tend to train the

0:26:36.880,0:26:40.000
system without latent variables and

0:26:38.559,0:26:42.240
in my little diagram here there is a

0:26:40.000,0:26:45.840
little variable but uh

0:26:42.240,0:26:47.039
in fact in uh versions of this that are

0:26:45.840,0:26:49.760
used

0:26:47.039,0:26:52.320
for uh in the context of images there is

0:26:49.760,0:26:53.760
no real latent variable and it's very

0:26:52.320,0:26:54.080
difficult for the system to just dream

0:26:53.760,0:26:56.080
up

0:26:54.080,0:26:57.679
a single solution to the in painting

0:26:56.080,0:26:59.230
problem here it's a

0:26:57.679,0:27:00.799
it's a multimodal

0:26:59.230,0:27:02.400
[Music]

0:27:00.799,0:27:04.000
manifold i mean it's a it's a manifold

0:27:02.400,0:27:07.200
it's probably not

0:27:04.000,0:27:10.400
just a single point there's many

0:27:07.200,0:27:11.200
ways to complete the image here uh by

0:27:10.400,0:27:14.880
filling in the

0:27:11.200,0:27:17.440
the the the masked uh part

0:27:14.880,0:27:19.120
and so uh with that latent variable the

0:27:17.440,0:27:21.360
system produces blurry predictions and

0:27:19.120,0:27:23.679
doesn't learn particularly good features

0:27:21.360,0:27:25.360
is the multi-modal part also the reason

0:27:23.679,0:27:28.000
why we had that internal

0:27:25.360,0:27:29.600
purple area in the spiral because each

0:27:28.000,0:27:31.600
of those points have two predictions

0:27:29.600,0:27:32.559
right in between the two branches of the

0:27:31.600,0:27:34.000
spiral

0:27:32.559,0:27:36.399
right so this is the additional problem

0:27:34.000,0:27:39.039
that uh if you don't uh

0:27:36.399,0:27:40.399
if you're not careful the you know

0:27:39.039,0:27:42.960
points that are in the middle

0:27:40.399,0:27:44.320
that you know that that could be the

0:27:42.960,0:27:48.080
result of a corruption of

0:27:44.320,0:27:49.600
uh uh one uh point on one side of the

0:27:48.080,0:27:50.720
manifold or a pink point on another side

0:27:49.600,0:27:52.320
of the manifold

0:27:50.720,0:27:53.760
the those points right in the middle

0:27:52.320,0:27:55.840
don't know where to go because half the

0:27:53.760,0:27:58.159
time they're trained to go to

0:27:55.840,0:27:59.440
uh one part of the manifold the other

0:27:58.159,0:28:00.960
half of the time they're trying to go to

0:27:59.440,0:28:04.880
the other part of the manifold

0:28:00.960,0:28:07.360
and so uh that might create kind of flat

0:28:04.880,0:28:09.679
spots in the energy function that uh are

0:28:07.360,0:28:11.200
not good so there you know there are

0:28:09.679,0:28:13.360
ways to alleviate this but they're not

0:28:11.200,0:28:15.600
kind of completely

0:28:13.360,0:28:17.120
uh worked out unless you use latin

0:28:15.600,0:28:18.960
variable models

0:28:17.120,0:28:20.159
okay other contrasting methods this is

0:28:18.960,0:28:22.799
just in passing for

0:28:20.159,0:28:25.120
for your own uh interest there are

0:28:22.799,0:28:28.159
things like contrastive divergence

0:28:25.120,0:28:30.880
um and and others which i'm not going to

0:28:28.159,0:28:33.200
talk about but contrastive divergence is

0:28:30.880,0:28:35.279
is a very simple idea you pick a

0:28:33.200,0:28:36.720
training sample

0:28:35.279,0:28:39.200
you lower the energy at that point of

0:28:36.720,0:28:41.360
course and then from that sample

0:28:39.200,0:28:42.880
you using some sort of gradient-based

0:28:41.360,0:28:46.000
process you move down the

0:28:42.880,0:28:47.520
energy surface with noise so

0:28:46.000,0:28:49.840
start from the sample and figure out how

0:28:47.520,0:28:51.039
do i change my sample how do i change my

0:28:49.840,0:28:55.120
y

0:28:51.039,0:28:57.200
in such a way that my

0:28:55.120,0:28:59.039
current energy based model produces a

0:28:57.200,0:29:02.240
lower energy than the one i just

0:28:59.039,0:29:04.240
i just measured for that sample okay

0:29:02.240,0:29:05.279
so basically you're trying to find

0:29:04.240,0:29:08.000
another point in

0:29:05.279,0:29:09.279
input space that has lower energy than

0:29:08.000,0:29:11.279
the training point you just

0:29:09.279,0:29:13.360
fed it okay so you can think of this as

0:29:11.279,0:29:16.159
kind of a smart way

0:29:13.360,0:29:17.120
of corrupting a training sample smart

0:29:16.159,0:29:20.720
because

0:29:17.120,0:29:23.919
you you don't randomly uh uh

0:29:20.720,0:29:26.320
corrupt it you corrupt it by uh

0:29:23.919,0:29:27.919
basically modifying it to find a point

0:29:26.320,0:29:30.320
in space that your model

0:29:27.919,0:29:32.559
already gives low energy to so it would

0:29:30.320,0:29:33.440
be a point that you would want to push

0:29:32.559,0:29:36.399
up because

0:29:33.440,0:29:38.080
your model gives low energy to it and

0:29:36.399,0:29:40.320
you don't want it to have low energy so

0:29:38.080,0:29:43.200
you push it up and i'm gonna

0:29:40.320,0:29:45.120
uh professor have people try tried uh

0:29:43.200,0:29:48.240
contrastive methods with

0:29:45.120,0:29:50.320
the this image in painting method and

0:29:48.240,0:29:52.480
how would one do that does that really

0:29:50.320,0:29:54.559
work if they if you do that together

0:29:52.480,0:29:57.039
so impainting is a contrastive method

0:29:54.559,0:30:00.960
right you you take an image

0:29:57.039,0:30:02.240
you corrupt it by uh blocking some piece

0:30:00.960,0:30:05.600
of it

0:30:02.240,0:30:08.799
and then you train uh a neural net uh

0:30:05.600,0:30:10.000
basically an autoencoder to generate the

0:30:08.799,0:30:11.679
the full image

0:30:10.000,0:30:13.840
and then you compare this reconstruction

0:30:11.679,0:30:15.840
of the full image with the original

0:30:13.840,0:30:17.440
uncorrupted image and that's your energy

0:30:15.840,0:30:21.600
function

0:30:17.440,0:30:24.880
okay so it is it is a contrasting method

0:30:21.600,0:30:28.559
right so if we if we use like uh the

0:30:24.880,0:30:32.080
nc loss with this uh in painting loss

0:30:28.559,0:30:34.320
uh does that uh like is that useful

0:30:32.080,0:30:37.120
uh you can really use nc loss because

0:30:34.320,0:30:40.880
nce kind of relies on the fact that

0:30:37.120,0:30:44.399
you you have sort of a finite number of

0:30:40.880,0:30:46.240
negative samples okay

0:30:44.399,0:30:48.480
here you sort of artificially generate

0:30:46.240,0:30:50.640
negative samples

0:30:48.480,0:30:52.159
and so uh it's really a completely

0:30:50.640,0:30:53.200
different scenario i don't think you

0:30:52.159,0:30:55.279
would

0:30:53.200,0:30:56.720
you could use uh something similar to

0:30:55.279,0:31:00.240
nce

0:30:56.720,0:31:00.240
or at least not in a meaningful way

0:31:01.679,0:31:05.840
okay so this is why space

0:31:05.919,0:31:13.200
okay y one y two and let's say your

0:31:10.159,0:31:13.200
your data manifold

0:31:13.919,0:31:16.960
is something like this

0:31:17.039,0:31:24.640
um but let's say your energy function

0:31:22.000,0:31:24.640
currently is

0:31:25.440,0:31:33.840
something like this so here i'm drawing

0:31:27.679,0:31:33.840
the region of low energy

0:31:34.399,0:31:38.480
and i'm drawing the lines of

0:31:40.159,0:31:45.919
equal cost okay so the energy looks nice

0:31:43.840,0:31:47.039
at the bottom left right you have data

0:31:45.919,0:31:49.840
points here

0:31:47.039,0:31:51.919
that your model gives low energy to but

0:31:49.840,0:31:53.919
then your model is not good because

0:31:51.919,0:31:55.519
uh at the bottom right it gives low

0:31:53.919,0:31:56.640
energy to regions that have no data and

0:31:55.519,0:31:58.640
then at the

0:31:56.640,0:32:00.000
at the top you have data points that

0:31:58.640,0:32:02.559
that your model gives high energy

0:32:00.000,0:32:03.679
to okay so here's how contrasty

0:32:02.559,0:32:07.200
divergence would work

0:32:03.679,0:32:09.919
you take a sample a training sample

0:32:07.200,0:32:10.559
it tells this guy and by gradient

0:32:09.919,0:32:13.600
descent

0:32:10.559,0:32:17.519
you go down the energy surface

0:32:13.600,0:32:21.600
to a point that has low energy okay

0:32:17.519,0:32:23.760
now this was a training sample why

0:32:21.600,0:32:26.559
the one you obtain now is a contrastive

0:32:23.760,0:32:26.559
sample y bar

0:32:26.960,0:32:30.880
and what you do now is you change the

0:32:28.799,0:32:32.799
parameters of your energy function

0:32:30.880,0:32:35.519
so that you make the energy of y smaller

0:32:32.799,0:32:38.240
and the energy of y bar larger

0:32:35.519,0:32:38.640
okay using some kind of loss function

0:32:38.240,0:32:40.320
that

0:32:38.640,0:32:42.159
you know pushes down on one pushes up on

0:32:40.320,0:32:45.039
the other which loss function you use

0:32:42.159,0:32:47.760
is a material you just need one that

0:32:45.039,0:32:49.360
will do the right thing

0:32:47.760,0:32:51.120
okay so what i've described here is kind

0:32:49.360,0:32:52.799
of a deterministic version of

0:32:51.120,0:32:54.000
uh contrasting divergence but in fact

0:32:52.799,0:32:55.679
contrasted divergence is kind of a

0:32:54.000,0:32:58.080
probabilistic version of this

0:32:55.679,0:32:59.120
where what you do is you you do this

0:32:58.080,0:33:02.399
sort of gradient

0:32:59.120,0:33:05.440
based descent i mean this sort of

0:33:02.399,0:33:07.440
search for a low energy point but you do

0:33:05.440,0:33:09.919
it with some level of random

0:33:07.440,0:33:11.279
randomness some noise in it so one way

0:33:09.919,0:33:13.600
to do this in a continuous

0:33:11.279,0:33:15.760
space like like this one is that you

0:33:13.600,0:33:18.720
give a random kick

0:33:15.760,0:33:20.720
you you think of your data point here as

0:33:18.720,0:33:22.880
a sort of a marble

0:33:20.720,0:33:24.720
that is going to go down the energy

0:33:22.880,0:33:28.960
surface you give it a random kick in

0:33:24.720,0:33:28.960
in some random direction say this

0:33:29.039,0:33:34.240
and then you let the the system kind of

0:33:31.360,0:33:34.240
follow the gradient

0:33:34.960,0:33:38.960
and you stop when you're tired you don't

0:33:37.120,0:33:40.240
wait for it to kind of

0:33:38.960,0:33:41.760
go down all the way you just stop when

0:33:40.240,0:33:44.559
you're tired and then there is a rule to

0:33:41.760,0:33:48.559
select whether you keep the point or not

0:33:44.559,0:33:48.559
um and that's your that's your white bar

0:33:50.240,0:33:55.360
why is the kick necessary okay so the

0:33:53.440,0:33:57.760
kick is necessary so that you can go

0:33:55.360,0:33:58.480
over energy barriers that would be

0:33:57.760,0:34:01.919
between you

0:33:58.480,0:34:05.279
and the the lowest energy uh

0:34:01.919,0:34:08.399
uh areas okay that's why you need the

0:34:05.279,0:34:11.200
the kick now uh if you have

0:34:08.399,0:34:13.440
uh a space a white space that is not

0:34:11.200,0:34:15.839
continuous but is discrete

0:34:13.440,0:34:17.359
uh you can still kind of do this energy

0:34:15.839,0:34:18.000
minimization by basically doing

0:34:17.359,0:34:21.520
something called

0:34:18.000,0:34:23.280
simulated annealing so essentially uh

0:34:21.520,0:34:25.040
if y is discrete variable you kind of

0:34:23.280,0:34:26.560
perturb it randomly

0:34:25.040,0:34:28.159
if the energy you get by this

0:34:26.560,0:34:29.440
perturbation is lower then you keep it

0:34:28.159,0:34:30.480
if it's higher then you keep it with

0:34:29.440,0:34:32.320
some probability

0:34:30.480,0:34:34.079
and then you keep doing this and

0:34:32.320,0:34:36.240
eventually the energy will go down so

0:34:34.079,0:34:38.000
this is a non-gradient-based

0:34:36.240,0:34:39.599
optimization algorithm or gradient-free

0:34:38.000,0:34:42.079
optimization algorithm if you want

0:34:39.599,0:34:43.520
which you kind of have to resort to when

0:34:42.079,0:34:45.839
the space is discrete and you can't use

0:34:43.520,0:34:48.480
gradient information

0:34:45.839,0:34:50.000
this technique i just described of

0:34:48.480,0:34:51.760
kicking a

0:34:50.000,0:34:53.040
a marble and sort of simulating it

0:34:51.760,0:34:54.720
rolling down the

0:34:53.040,0:34:57.599
energy it's called hamiltonian monte

0:34:54.720,0:34:59.359
carlo hnc

0:34:57.599,0:35:01.599
and you might you might see this in

0:34:59.359,0:35:02.960
other in other contexts

0:35:01.599,0:35:05.359
so that's another way of generating

0:35:02.960,0:35:06.079
negative samples yes i'm italian monte

0:35:05.359,0:35:07.680
carlo

0:35:06.079,0:35:10.160
some people call this hybrid monte carlo

0:35:07.680,0:35:10.160
sometimes

0:35:10.240,0:35:13.280
so some of you may have heard of

0:35:12.079,0:35:14.720
something called

0:35:13.280,0:35:16.640
restricted balsa machines and receipts

0:35:14.720,0:35:18.320
residual machine is an energy based

0:35:16.640,0:35:20.880
model in which the energy

0:35:18.320,0:35:21.760
is very simple um it's written at the

0:35:20.880,0:35:24.400
bottom here

0:35:21.760,0:35:25.920
the energy of y and z so y is basically

0:35:24.400,0:35:29.040
an input data vector and z

0:35:25.920,0:35:30.240
is uh it's a related variable the energy

0:35:29.040,0:35:33.359
function is minus

0:35:30.240,0:35:35.359
z transpose w y where y where where w

0:35:33.359,0:35:36.880
is a matrix uh not necessarily square

0:35:35.359,0:35:40.400
because z and y may have different

0:35:36.880,0:35:43.200
uh uh dimensions

0:35:40.400,0:35:44.480
and uh generally z and y are both binary

0:35:43.200,0:35:47.520
variables

0:35:44.480,0:35:50.160
and so i mean the binary vectors

0:35:47.520,0:35:52.560
so the components are binary variables

0:35:50.160,0:35:56.480
and they were kind of somewhat popular

0:35:52.560,0:36:00.240
in the mid-2000s but you know i'm not

0:35:56.480,0:36:02.720
spending much time on it here because uh

0:36:00.240,0:36:03.839
um the the they've kind of fallen out of

0:36:02.720,0:36:06.160
favor a little bit they're not

0:36:03.839,0:36:07.920
they're not that popular but just so

0:36:06.160,0:36:11.119
that gives you some reference of what

0:36:07.920,0:36:12.880
uh what this means um there's

0:36:11.119,0:36:14.079
of refinements of contrast divergence

0:36:12.880,0:36:15.359
one of them is called persistent

0:36:14.079,0:36:17.680
contrast divergence

0:36:15.359,0:36:19.040
and it consists in using a bunch of

0:36:17.680,0:36:21.760
particles

0:36:19.040,0:36:23.040
um and you can remember the position so

0:36:21.760,0:36:26.640
they have sort of permanent

0:36:23.040,0:36:29.599
uh persistent positions if you want

0:36:26.640,0:36:30.079
uh so you throw a bunch of uh marbles in

0:36:29.599,0:36:33.200
your

0:36:30.079,0:36:35.839
energy landscape and you keep making

0:36:33.200,0:36:37.280
you keep making them roll down maybe

0:36:35.839,0:36:40.400
with a little bit of noise or

0:36:37.280,0:36:42.560
or kicks and then you keep their

0:36:40.400,0:36:44.320
position so you don't change the

0:36:42.560,0:36:45.920
the the position of the marble according

0:36:44.320,0:36:47.599
to new samples new training samples you

0:36:45.920,0:36:49.599
just keep the marbles where they are

0:36:47.599,0:36:51.760
and eventually they'll find low energy

0:36:49.599,0:36:54.160
places in your

0:36:51.760,0:36:55.200
in your energy surface and will cause

0:36:54.160,0:36:57.359
them to be pushed up

0:36:55.200,0:36:58.320
because because that's what happens

0:36:57.359,0:37:00.960
during

0:36:58.320,0:37:00.960
during training

0:37:02.800,0:37:06.240
but this doesn't scale very well so

0:37:04.640,0:37:09.119
things like rbms you know become very

0:37:06.240,0:37:12.400
very expensive to to train

0:37:09.119,0:37:12.400
within in high dimension

0:37:16.880,0:37:21.839
okay so now for

0:37:19.920,0:37:23.119
regular isolated viable energy based

0:37:21.839,0:37:27.760
model which

0:37:23.119,0:37:27.760
is my current favorite type of model

0:37:30.240,0:37:36.000
so we we talked about the idea of

0:37:34.320,0:37:37.920
building a predictive model by having a

0:37:36.000,0:37:39.760
latent variable right so you have

0:37:37.920,0:37:40.880
the observed variable x you run it to a

0:37:39.760,0:37:41.839
predictor it extracts some

0:37:40.880,0:37:44.720
representation

0:37:41.839,0:37:46.560
of the observed variables and then that

0:37:44.720,0:37:48.240
goes into a decoder that produces the

0:37:46.560,0:37:49.200
prediction but if you want your decoder

0:37:48.240,0:37:50.800
to

0:37:49.200,0:37:52.400
be able to make multiple predictions

0:37:50.800,0:37:54.079
then you feed it with a latent variable

0:37:52.400,0:37:55.599
and as you vary

0:37:54.079,0:37:57.599
the value of this latent variable the

0:37:55.599,0:38:00.240
prediction will vary over a set

0:37:57.599,0:38:01.359
okay over hopefully the manifold of data

0:38:00.240,0:38:03.040
of uh

0:38:01.359,0:38:05.839
in the space of y that are compatible

0:38:03.040,0:38:05.839
with x

0:38:06.079,0:38:09.520
so this architecture here is here the

0:38:08.240,0:38:10.000
the formula for the energy can be

0:38:09.520,0:38:14.160
written

0:38:10.000,0:38:16.320
as as on the left here uh c of y

0:38:14.160,0:38:18.160
and uh you know c is a cost function

0:38:16.320,0:38:20.800
that compares these two argument so you

0:38:18.160,0:38:22.720
compare y the data vector with the

0:38:20.800,0:38:24.480
result of applying the decoder to the

0:38:22.720,0:38:25.200
output of the predictor that takes into

0:38:24.480,0:38:26.960
account x

0:38:25.200,0:38:29.119
and the decoder also takes into account

0:38:26.960,0:38:29.119
z

0:38:30.880,0:38:37.760
so here is the problem with this if z

0:38:34.560,0:38:40.560
is too powerful in other words if z has

0:38:37.760,0:38:42.800
too much capacity

0:38:40.560,0:38:44.560
then they're always going to be a z that

0:38:42.800,0:38:46.000
is going to produce a y bar that's going

0:38:44.560,0:38:49.119
to be exactly equal to y

0:38:46.000,0:38:50.960
so remember the uh the inference

0:38:49.119,0:38:51.680
algorithm here is that you give an x and

0:38:50.960,0:38:54.000
a y

0:38:51.680,0:38:54.880
and then you find a z that minimizes c

0:38:54.000,0:38:58.079
of y y bar

0:38:54.880,0:39:00.240
right that's how you

0:38:58.079,0:39:03.040
do inference of the latent variable in

0:39:00.240,0:39:04.480
an energy-based model right

0:39:03.040,0:39:06.400
given an x and y find the z that

0:39:04.480,0:39:09.440
minimizes the energy

0:39:06.400,0:39:11.200
so if z for example has the same

0:39:09.440,0:39:13.359
dimension as y

0:39:11.200,0:39:16.160
and the decoder is powerful enough to

0:39:13.359,0:39:18.800
represent the identity function

0:39:16.160,0:39:20.640
then for any y there's always going to

0:39:18.800,0:39:22.079
be a z that produces y bar that's

0:39:20.640,0:39:24.079
exactly equal to y

0:39:22.079,0:39:25.200
okay and if the decoder is the identity

0:39:24.079,0:39:27.920
function

0:39:25.200,0:39:28.800
which ignores h see an entity function

0:39:27.920,0:39:32.560
from z to

0:39:28.800,0:39:37.280
to uh to y to y bar then you just set

0:39:32.560,0:39:40.079
z equal to y and the energy is zero

0:39:37.280,0:39:41.359
and that would be a terrible energy

0:39:40.079,0:39:43.119
based model because

0:39:41.359,0:39:44.480
it would not give high energy to stuff

0:39:43.119,0:39:46.480
outside the manifold of data it gives

0:39:44.480,0:39:49.920
low energy to everything

0:39:46.480,0:39:52.560
okay it gives zero energy to everything

0:39:49.920,0:39:54.640
so the way to prevent the system from

0:39:52.560,0:39:57.440
giving low energy

0:39:54.640,0:39:58.400
to points outside the manifold of data

0:39:57.440,0:40:00.400
is to

0:39:58.400,0:40:03.280
limit the information capacity of the

0:40:00.400,0:40:03.280
latent variable z

0:40:03.359,0:40:10.079
to be more precise if z can only take

0:40:06.880,0:40:13.119
let's say 10 different values

0:40:10.079,0:40:14.400
what that means is so you constrain z to

0:40:13.119,0:40:17.040
only take ten possible

0:40:14.400,0:40:19.040
different values let's say you make z a

0:40:17.040,0:40:22.480
a one hot vector of dimension ten

0:40:19.040,0:40:25.119
like in k means okay then there's only

0:40:22.480,0:40:29.119
going to be 10 points in y space

0:40:25.119,0:40:31.760
that will have zero energy because

0:40:29.119,0:40:33.200
either y is equal to one of the y bars

0:40:31.760,0:40:34.240
that is produced from one of those ten

0:40:33.200,0:40:36.800
z's

0:40:34.240,0:40:38.160
or it's not if it is then the energy is

0:40:36.800,0:40:40.480
zero if it's not the energy

0:40:38.160,0:40:41.920
is gonna have to be larger than zero in

0:40:40.480,0:40:43.359
fact it's going to grow quadratically as

0:40:41.920,0:40:45.839
you move away from

0:40:43.359,0:40:47.359
uh from that z and that's exactly the

0:40:45.839,0:40:50.480
idea of k-means

0:40:47.359,0:40:51.839
okay but what if you find

0:40:50.480,0:40:54.880
other ways to limit the information

0:40:51.839,0:40:57.200
content of z so

0:40:54.880,0:40:58.400
this seems like a kind of a small

0:40:57.200,0:41:02.160
technical sub problem

0:40:58.400,0:41:03.680
but in my opinion the

0:41:02.160,0:41:05.599
question of how you limit the

0:41:03.680,0:41:07.920
information content of a latent variable

0:41:05.599,0:41:10.640
in a model of this type

0:41:07.920,0:41:11.200
is the most important question in ai

0:41:10.640,0:41:14.640
today

0:41:11.200,0:41:17.680
okay and i'm not kidding

0:41:14.640,0:41:19.680
i think the the main

0:41:17.680,0:41:22.160
problem we're facing is is how to do

0:41:19.680,0:41:23.760
self-supervised learning uh properly

0:41:22.160,0:41:25.440
and contrastive methods have shown their

0:41:23.760,0:41:27.760
limits and so we have to find

0:41:25.440,0:41:29.839
alternatives and alternatives are

0:41:27.760,0:41:32.000
uh regularized and variable models there

0:41:29.839,0:41:33.040
might be other ideas that nobody has had

0:41:32.000,0:41:34.960
so far

0:41:33.040,0:41:36.640
but these are the only two that i know

0:41:34.960,0:41:37.760
of and then the main technical issue

0:41:36.640,0:41:39.440
that we need to solve is

0:41:37.760,0:41:41.520
how do we limit the information content

0:41:39.440,0:41:42.240
of the latent variable so that we limit

0:41:41.520,0:41:45.040
the volume

0:41:42.240,0:41:46.640
of white space that can take low energy

0:41:45.040,0:41:48.720
and therefore we automatically make the

0:41:46.640,0:41:50.800
energy outside the

0:41:48.720,0:41:52.319
the the manifold of data where we train

0:41:50.800,0:41:54.079
the system to have low energy

0:41:52.319,0:41:56.000
we automatically make the energy outside

0:41:54.079,0:41:56.880
uh higher

0:41:56.000,0:41:59.440
so i'm going to go through a few

0:41:56.880,0:42:02.240
examples of systems that actually work

0:41:59.440,0:42:02.560
that uh and things that people have done

0:42:02.240,0:42:06.240
for

0:42:02.560,0:42:10.240
you know uh 20 years in some cases

0:42:06.240,0:42:13.520
um and so so that's the idea here

0:42:10.240,0:42:14.960
you you um or one of the ideas you add a

0:42:13.520,0:42:18.000
regularizer

0:42:14.960,0:42:18.800
in the energy and this regularizer takes

0:42:18.000,0:42:21.040
low value

0:42:18.800,0:42:22.480
on a kind of small part of the space of

0:42:21.040,0:42:24.480
z

0:42:22.480,0:42:25.520
and so the system will preferentially

0:42:24.480,0:42:27.040
choose values of z

0:42:25.520,0:42:31.599
that are within this sort of restricted

0:42:27.040,0:42:34.960
set where r takes a small value

0:42:31.599,0:42:37.440
and if z

0:42:34.960,0:42:39.359
needs to go outside of that set to do a

0:42:37.440,0:42:41.760
good reconstruction

0:42:39.359,0:42:42.839
you're paying a price for it in terms of

0:42:41.760,0:42:45.599
energy

0:42:42.839,0:42:48.960
okay so

0:42:45.599,0:42:51.119
the the the volume of uh of z space

0:42:48.960,0:42:52.560
that is determined by r basically limits

0:42:51.119,0:42:55.920
the volume of

0:42:52.560,0:42:58.000
uh space of y that can take low energy

0:42:55.920,0:42:59.760
and the tradeoff is controlled by uh

0:42:58.000,0:43:00.079
basically a coefficient lambda that you

0:42:59.760,0:43:04.400
can

0:43:00.079,0:43:06.800
uh adjust to uh you know make the

0:43:04.400,0:43:09.200
the volume of uh white space that take

0:43:06.800,0:43:12.240
low energy as uh as small as possible or

0:43:09.200,0:43:15.119
or not that small

0:43:12.240,0:43:16.000
so here are a few examples of rnz of r

0:43:15.119,0:43:19.839
of

0:43:16.000,0:43:21.280
r of z and some of them are kind of

0:43:19.839,0:43:22.880
useful because they're differentiable

0:43:21.280,0:43:24.079
with respect to z and some of them are

0:43:22.880,0:43:26.720
not so useful because they're not

0:43:24.079,0:43:29.359
differentiable so you have to look for

0:43:26.720,0:43:31.200
kind of you know do a discrete search so

0:43:29.359,0:43:34.160
one is the effective dimension of

0:43:31.200,0:43:36.240
oz so what you can do is you can decide

0:43:34.160,0:43:37.520
that z a priori has

0:43:36.240,0:43:38.960
three dimension four dimension five

0:43:37.520,0:43:40.640
dimension six dimension you train your

0:43:38.960,0:43:42.160
model for various dimensions of z

0:43:40.640,0:43:43.680
and there is one set of dimensions for

0:43:42.160,0:43:44.960
which you know one dimension for which

0:43:43.680,0:43:46.160
the prediction would be good but at the

0:43:44.960,0:43:48.480
same time

0:43:46.160,0:43:50.160
uh the dimension of z would be minimized

0:43:48.480,0:43:52.720
and what you will have found

0:43:50.160,0:43:54.560
is basically the lowest uh embedding

0:43:52.720,0:43:57.440
dimension of your of your space

0:43:54.560,0:43:59.599
so imagine for example that uh your data

0:43:57.440,0:43:59.599
set

0:43:59.839,0:44:03.760
consists of lots and lots of pictures of

0:44:02.079,0:44:04.720
someone making faces in front of a

0:44:03.760,0:44:08.160
camera

0:44:04.720,0:44:09.520
we know that the effective dimension of

0:44:08.160,0:44:10.480
the manifold of all the faces of a

0:44:09.520,0:44:12.560
person

0:44:10.480,0:44:14.160
is something like 60 at least less than

0:44:12.560,0:44:16.400
100 because it's bounded above by the

0:44:14.160,0:44:19.119
number of muscles in your face

0:44:16.400,0:44:20.079
and so there has to be a a z of

0:44:19.119,0:44:23.119
dimension

0:44:20.079,0:44:25.920
50 or 60 or something like that

0:44:23.119,0:44:27.520
such that when you run it through a

0:44:25.920,0:44:31.200
convolutional net you will

0:44:27.520,0:44:34.400
generate all possible uh instances of

0:44:31.200,0:44:36.800
uh of the the face of that person

0:44:34.400,0:44:39.760
okay that's the the face manifold for

0:44:36.800,0:44:39.760
that person if you want

0:44:39.920,0:44:43.760
um so what you can do is this a really

0:44:42.560,0:44:45.760
super expensive

0:44:43.760,0:44:48.319
method of kind of trying all different

0:44:45.760,0:44:50.800
different dimensions of z

0:44:48.319,0:44:51.680
uh one way to formulate this

0:44:50.800,0:44:55.280
mathematically

0:44:51.680,0:44:58.160
is uh to minimize the l zero norm of z

0:44:55.280,0:44:59.200
so uh it's actually a slightly different

0:44:58.160,0:45:01.440
thing so

0:44:59.200,0:45:02.720
what you um what what you can do is you

0:45:01.440,0:45:03.520
choose a z that's relatively high

0:45:02.720,0:45:05.599
dimension

0:45:03.520,0:45:07.359
but for any given sample you minimize

0:45:05.599,0:45:10.720
the number of components of z

0:45:07.359,0:45:13.760
that are non-zero okay that's called the

0:45:10.720,0:45:17.119
l0 norm it's just the account of the

0:45:13.760,0:45:20.240
number of components that are non-zero

0:45:17.119,0:45:22.640
and it's very difficult to uh

0:45:20.240,0:45:24.960
minimize that uh that norm because it's

0:45:22.640,0:45:27.520
not differentiable it's very discrete

0:45:24.960,0:45:29.119
so what people use is they use a convex

0:45:27.520,0:45:31.760
relaxation of that norm called

0:45:29.119,0:45:32.240
and it's the l1 norm so the l1 norm is

0:45:31.760,0:45:34.079
the

0:45:32.240,0:45:36.880
sum of the absolute values or the

0:45:34.079,0:45:39.920
components of z

0:45:36.880,0:45:41.280
and that's what you use for r and z the

0:45:39.920,0:45:43.200
sum of the absolute values of the

0:45:41.280,0:45:46.079
components of z

0:45:43.200,0:45:48.400
when you add this to your energy

0:45:46.079,0:45:50.880
function

0:45:48.400,0:45:51.440
what the system is trying to do is find

0:45:50.880,0:45:54.720
a z

0:45:51.440,0:45:57.520
that reconstructs the a the y because it

0:45:54.720,0:45:59.440
needs to minimize c of y and y bar

0:45:57.520,0:46:01.119
but also tries to minimize the number of

0:45:59.440,0:46:02.640
its components that are non-zero because

0:46:01.119,0:46:04.079
that's the best way to minimize the l1

0:46:02.640,0:46:07.920
arm

0:46:04.079,0:46:10.319
okay and

0:46:07.920,0:46:11.520
that's called sparse coding and it works

0:46:10.319,0:46:14.800
really well and i'm going to show you

0:46:11.520,0:46:16.720
some examples of this

0:46:14.800,0:46:18.319
before i go there i just want to mention

0:46:16.720,0:46:18.720
that and we'll talk about this a little

0:46:18.319,0:46:21.760
more

0:46:18.720,0:46:23.119
and it's the idea that adding noise to z

0:46:21.760,0:46:26.319
will also limit

0:46:23.119,0:46:28.400
the information content of oz i'll come

0:46:26.319,0:46:32.079
back to this in a minute

0:46:28.400,0:46:34.560
okay so here is the uh idea of

0:46:32.079,0:46:35.760
of sparse coding so spots coding is an

0:46:34.560,0:46:37.680
unconditional version

0:46:35.760,0:46:38.800
of energy-based model so there's no x

0:46:37.680,0:46:42.000
there's only a y

0:46:38.800,0:46:44.960
and a z and the energy function is

0:46:42.000,0:46:46.640
y minus wz where w is a so-called

0:46:44.960,0:46:48.400
dictionary matrix

0:46:46.640,0:46:49.920
very similar to the prototype matrix in

0:46:48.400,0:46:52.160
k-means

0:46:49.920,0:46:53.680
z is a vector generally the dimension of

0:46:52.160,0:46:56.160
z is larger than y

0:46:53.680,0:46:58.079
and so you measure the squared distance

0:46:56.160,0:47:00.400
euclidean distance between y and w

0:46:58.079,0:47:03.200
z so basically your decoder here is

0:47:00.400,0:47:05.680
linear it's just a matrix

0:47:03.200,0:47:07.520
and then you add a term lambda times the

0:47:05.680,0:47:10.640
l1 norm of z which is represented by

0:47:07.520,0:47:12.079
those two bars

0:47:10.640,0:47:14.079
and that's the energy function for

0:47:12.079,0:47:16.240
sparse coding okay and you can think of

0:47:14.079,0:47:18.400
it as a special case of

0:47:16.240,0:47:20.000
the the system i i showed the

0:47:18.400,0:47:22.640
architecture i showed previously

0:47:20.000,0:47:25.839
uh except it's not um conditional

0:47:22.640,0:47:28.880
there's no x

0:47:25.839,0:47:31.119
now what does this do um

0:47:28.880,0:47:32.559
um so alfredo will tell you that the

0:47:31.119,0:47:34.079
picture i'm showing here on the left

0:47:32.559,0:47:35.280
uh is inappropriate because it's

0:47:34.079,0:47:38.960
actually generated with a slightly

0:47:35.280,0:47:41.040
different model um

0:47:38.960,0:47:42.319
but it's uh a good sort of pictorial

0:47:41.040,0:47:45.760
representation of what

0:47:42.319,0:47:47.680
uh what sparse coding attempts to do

0:47:45.760,0:47:48.800
which is to approximate the manifold of

0:47:47.680,0:47:51.280
data

0:47:48.800,0:47:52.559
by a piecewise linear approximation

0:47:51.280,0:47:55.760
essentially

0:47:52.559,0:47:58.160
so imagine that

0:47:55.760,0:47:59.839
you have this w matrix okay and someone

0:47:58.160,0:48:02.319
has given it to you or you've learned it

0:47:59.839,0:48:05.599
in some way

0:48:02.319,0:48:06.160
now if you decide a priori that a

0:48:05.599,0:48:09.200
certain

0:48:06.160,0:48:10.640
number of components of z are non-zero

0:48:09.200,0:48:12.000
okay most of the components of z are

0:48:10.640,0:48:13.920
zero just a small number of components

0:48:12.000,0:48:15.920
with z are nonzero

0:48:13.920,0:48:18.480
and you vary the value of those

0:48:15.920,0:48:18.480
components

0:48:18.800,0:48:22.800
you know within some range the set of

0:48:21.200,0:48:24.319
vectors that you're going to generate

0:48:22.800,0:48:25.599
the set of y bars that you're going to

0:48:24.319,0:48:27.280
generate

0:48:25.599,0:48:30.800
are going to be the y bars that are in

0:48:27.280,0:48:33.839
the linear subspace spanned by the

0:48:30.800,0:48:36.800
corresponding columns of the w matrix

0:48:33.839,0:48:38.319
okay for every value of the z

0:48:36.800,0:48:39.520
coefficients that are nonzero

0:48:38.319,0:48:41.040
you basically compute a linear

0:48:39.520,0:48:42.880
combination of the corresponding columns

0:48:41.040,0:48:45.200
of w

0:48:42.880,0:48:46.079
and so you're basically moving along a

0:48:45.200,0:48:49.839
low dimensional

0:48:46.079,0:48:52.160
linear subspace of of y space

0:48:49.839,0:48:53.599
so y bar is going to be basically along

0:48:52.160,0:48:56.960
a low dimensional space

0:48:53.599,0:48:58.720
a low dimensional linear subspace

0:48:56.960,0:49:01.520
the dimension of that space will be the

0:48:58.720,0:49:04.640
number of non-zero components of z

0:49:01.520,0:49:06.559
okay so

0:49:04.640,0:49:07.839
for one particular y when you find the z

0:49:06.559,0:49:10.240
that minimizes the

0:49:07.839,0:49:11.440
the energy a number of components are

0:49:10.240,0:49:15.040
going to be nonzero

0:49:11.440,0:49:16.000
and as you move y slowly those nonzero

0:49:15.040,0:49:18.960
components are going to change

0:49:16.000,0:49:20.319
value but you're going to stay on the

0:49:18.960,0:49:23.599
same

0:49:20.319,0:49:24.079
linear subspace until y change changes

0:49:23.599,0:49:25.839
too much

0:49:24.079,0:49:28.000
and then all of a sudden you need a

0:49:25.839,0:49:30.480
different set of non-zero z

0:49:28.000,0:49:33.119
to do a the best reconstruction and now

0:49:30.480,0:49:36.079
you're switching to a different plane

0:49:33.119,0:49:39.359
okay because a different set of z now of

0:49:36.079,0:49:42.240
z components become non-zero

0:49:39.359,0:49:44.079
um and so now you you move y again and

0:49:42.240,0:49:45.920
again the coefficients in z

0:49:44.079,0:49:47.680
keep changing values except for the ones

0:49:45.920,0:49:50.160
that are zero that stay zero

0:49:47.680,0:49:51.200
and all of a sudden it switches again it

0:49:50.160,0:49:53.760
goes to another one

0:49:51.200,0:49:55.520
so it's it's kind of well symbolized by

0:49:53.760,0:49:56.559
the the picture on the left where you

0:49:55.520,0:49:58.960
see that the

0:49:56.559,0:50:00.559
money flow of data is approximated by

0:49:58.960,0:50:03.760
basically a bunch of

0:50:00.559,0:50:05.760
uh linear subspace in in this case lines

0:50:03.760,0:50:08.160
um the reason why it's difficult to

0:50:05.760,0:50:09.920
represent the actual sparse coding

0:50:08.160,0:50:11.599
uh in 2d is because it's going to

0:50:09.920,0:50:13.200
degenerate in 2d so

0:50:11.599,0:50:14.960
so one question is how do we train a

0:50:13.200,0:50:15.760
system like this so to train a system

0:50:14.960,0:50:18.720
like this

0:50:15.760,0:50:20.079
we actually uh our loss function is just

0:50:18.720,0:50:22.480
going to be the average

0:50:20.079,0:50:23.920
uh energy that our model gives to our

0:50:22.480,0:50:25.359
training samples

0:50:23.920,0:50:27.040
so the loss function is just the average

0:50:25.359,0:50:30.720
energy basically the average f

0:50:27.040,0:50:34.000
and remember f of y is equal to

0:50:30.720,0:50:36.079
uh the minimum over z of e of y and z

0:50:34.000,0:50:38.720
okay so we're going to take the average

0:50:36.079,0:50:40.559
of f over all our training samples

0:50:38.720,0:50:42.480
and minimize that average with respect

0:50:40.559,0:50:44.720
to the parameters

0:50:42.480,0:50:46.960
of the model and those parameters are

0:50:44.720,0:50:48.960
the coefficients in the w matrix

0:50:46.960,0:50:51.359
again it's called the dictionary matrix

0:50:48.960,0:50:53.839
so how do we do this we take a sample y

0:50:51.359,0:50:55.280
we find the z that minimizes the energy

0:50:53.839,0:50:57.040
okay the sum of the two terms that you

0:50:55.280,0:50:59.520
see here

0:50:57.040,0:51:00.880
and then we we take one step of gradient

0:50:59.520,0:51:03.040
descent

0:51:00.880,0:51:04.559
in w so we compute the gradient of the

0:51:03.040,0:51:06.000
energy with respect to w which is very

0:51:04.559,0:51:07.119
simple because it's a quadratic function

0:51:06.000,0:51:10.079
of w

0:51:07.119,0:51:12.000
and we take one step of uh of stochastic

0:51:10.079,0:51:14.880
gradient basically right

0:51:12.000,0:51:17.119
and now we take the next y and do it

0:51:14.880,0:51:19.119
again minimize with respect to z

0:51:17.119,0:51:20.800
and then for that value of z compute the

0:51:19.119,0:51:22.800
gradient with respect to w and take one

0:51:20.800,0:51:24.720
step in the negative gradient

0:51:22.800,0:51:26.960
and you keep doing this now if you just

0:51:24.720,0:51:28.800
do this it doesn't work

0:51:26.960,0:51:30.640
it doesn't work because the result is

0:51:28.800,0:51:32.240
that w

0:51:30.640,0:51:34.480
keeps will keep getting bigger and

0:51:32.240,0:51:35.359
bigger and z will keep getting smaller

0:51:34.480,0:51:36.880
and smaller

0:51:35.359,0:51:38.079
but the problem will not actually the

0:51:36.880,0:51:38.800
system will not actually solve the

0:51:38.079,0:51:41.280
problem

0:51:38.800,0:51:44.400
so what you need to do is normalize the

0:51:41.280,0:51:47.040
w matrix so that it cannot grow

0:51:44.400,0:51:48.640
indefinitely and allow z to shrink

0:51:47.040,0:51:51.119
correspondingly

0:51:48.640,0:51:52.160
and the way to do this is that you

0:51:51.119,0:51:54.079
basically after

0:51:52.160,0:51:57.040
every update of the w matrix you

0:51:54.079,0:52:00.240
normalize the sum of the squares

0:51:57.040,0:52:01.200
of the the terms in a column of in each

0:52:00.240,0:52:03.280
column of w

0:52:01.200,0:52:06.079
right so normalize the columns of w

0:52:03.280,0:52:06.079
after each update

0:52:06.720,0:52:12.079
and that will prevent the terms in w

0:52:09.680,0:52:13.839
from blowing up and the terms in z from

0:52:12.079,0:52:15.359
shrinking

0:52:13.839,0:52:17.599
and it will force the system to actually

0:52:15.359,0:52:20.160
find a reasonable matrix w

0:52:17.599,0:52:21.359
and not get away with just making z

0:52:20.160,0:52:24.800
shorter

0:52:21.359,0:52:27.119
okay so that's sparse coding this was uh

0:52:24.800,0:52:28.880
the learning algorithm for this was

0:52:27.119,0:52:31.200
invented by

0:52:28.880,0:52:34.559
two computational neural scientists

0:52:31.200,0:52:36.319
bruno olson and david field in 1997

0:52:34.559,0:52:37.520
and so that goes back a long time okay

0:52:36.319,0:52:38.160
so here is the problem with sparse

0:52:37.520,0:52:39.520
coding

0:52:38.160,0:52:41.920
the inference algorithm is kind of

0:52:39.520,0:52:45.599
expensive you oops

0:52:41.920,0:52:48.800
what you have to do is um

0:52:45.599,0:52:50.319
you know for for a given y is to kind of

0:52:48.800,0:52:51.599
minimize the sum of those two terms one

0:52:50.319,0:52:52.079
of which is l two the other one is at

0:52:51.599,0:52:54.400
one

0:52:52.079,0:52:55.760
there's a very large number of papers in

0:52:54.400,0:52:58.839
applied mathematics

0:52:55.760,0:53:00.400
that uh explain how to do this

0:52:58.839,0:53:02.960
efficiently

0:53:00.400,0:53:04.880
in particular one algorithm to do to do

0:53:02.960,0:53:06.559
to do so is called esta that means

0:53:04.880,0:53:08.559
iterative shrinkage and thresholding

0:53:06.559,0:53:11.680
algorithm

0:53:08.559,0:53:14.480
and i'm going to tell you what is uh in

0:53:11.680,0:53:17.760
just a minute

0:53:14.480,0:53:20.400
but it basically consists in uh

0:53:17.760,0:53:22.319
basically alternating uh a sort of

0:53:20.400,0:53:25.839
minimization with a sp

0:53:22.319,0:53:30.240
of z with respect to z of the first term

0:53:25.839,0:53:33.280
and then the second term alternately so

0:53:30.240,0:53:35.760
here's the kind of abstract form of the

0:53:33.280,0:53:36.720
of the east algorithm there's a fast

0:53:35.760,0:53:41.200
version of it called

0:53:36.720,0:53:44.800
called fista and

0:53:41.200,0:53:46.400
here it is at the um at the bottom

0:53:44.800,0:53:47.680
um actually i'm realizing that i'm

0:53:46.400,0:53:49.040
missing the reference for the easter

0:53:47.680,0:53:50.880
algorithm this is not any of the

0:53:49.040,0:53:53.319
references i'm showing here

0:53:50.880,0:53:55.520
um first author is the bull

0:53:53.319,0:53:57.280
d-e-b-o-u-l-i-d

0:53:55.520,0:54:00.079
anyway so here is the algorithm you

0:53:57.280,0:54:02.319
start with z equals zero

0:54:00.079,0:54:03.440
and then you apply uh this iteration

0:54:02.319,0:54:07.119
here which is the

0:54:03.440,0:54:10.559
the second last formula

0:54:07.119,0:54:11.839
so in the bracket the

0:54:10.559,0:54:14.079
the thing that's in the bracket is

0:54:11.839,0:54:15.839
basically a gradient step in the squared

0:54:14.079,0:54:17.119
error the square reconstruction error

0:54:15.839,0:54:19.119
so if you compute the gradient or the

0:54:17.119,0:54:21.200
square reconstruction error

0:54:19.119,0:54:22.160
and and you do a gradient step you you

0:54:21.200,0:54:24.800
basically get this

0:54:22.160,0:54:25.520
this formula where one over l is the is

0:54:24.800,0:54:29.280
the

0:54:25.520,0:54:32.400
is the the gradient uh step size

0:54:29.280,0:54:35.040
okay um

0:54:32.400,0:54:36.079
so we basically update z with the

0:54:35.040,0:54:39.599
negative gradient

0:54:36.079,0:54:41.920
of the square reconstruction error

0:54:39.599,0:54:43.280
and then the next operation you do is a

0:54:41.920,0:54:45.040
shrinkage operation so you take

0:54:43.280,0:54:47.440
every component of the resulting z

0:54:45.040,0:54:49.119
vector

0:54:47.440,0:54:51.040
and you shrink all of them to towards

0:54:49.119,0:54:52.799
zero so you basically subtract

0:54:51.040,0:54:54.880
if the component of z is positive you

0:54:52.799,0:54:57.119
subtract a constant to it

0:54:54.880,0:54:58.640
from it and if it's negative you add a

0:54:57.119,0:55:02.079
cons the same constant

0:54:58.640,0:55:02.960
to it but but if you get too close to

0:55:02.079,0:55:06.000
zero you just

0:55:02.960,0:55:09.280
click at zero okay so basically it's

0:55:06.000,0:55:12.640
a it's a function that um you know

0:55:09.280,0:55:14.880
is flat around zero and then uh grows

0:55:12.640,0:55:15.920
like the identity function above a

0:55:14.880,0:55:20.640
certain threshold

0:55:15.920,0:55:23.839
and uh and below a certain threshold

0:55:20.640,0:55:25.839
okay it shrinks towards zero

0:55:23.839,0:55:27.040
if you keep iterating this algorithm for

0:55:25.839,0:55:31.760
proper values of l

0:55:27.040,0:55:34.160
and lambda the

0:55:31.760,0:55:35.040
z vector will converge to the solution

0:55:34.160,0:55:38.400
of the

0:55:35.040,0:55:39.200
energy minimization problem which is the

0:55:38.400,0:55:42.480
minimum of this

0:55:39.200,0:55:45.760
energy here e of y z with respect to z

0:55:42.480,0:55:48.319
okay and that suggests uh so

0:55:45.760,0:55:49.760
keep this in mind now here is here is an

0:55:48.319,0:55:52.720
issue this algorithm is

0:55:49.760,0:55:53.119
kind of expensive uh if you want to run

0:55:52.720,0:55:56.319
this

0:55:53.119,0:55:57.599
over uh an image or over you know all

0:55:56.319,0:55:58.640
patches of an image or something like

0:55:57.599,0:55:59.839
this you're not going to be able to do

0:55:58.640,0:56:03.119
this

0:55:59.839,0:56:04.880
in real time on large images

0:56:03.119,0:56:06.640
and so here is an idea and the idea is

0:56:04.880,0:56:07.280
to basically train a neural net to

0:56:06.640,0:56:08.880
predict

0:56:07.280,0:56:11.200
what the solution of the energy

0:56:08.880,0:56:13.119
minimization problem is

0:56:11.200,0:56:14.400
okay so you see the diagram here on the

0:56:13.119,0:56:17.680
right

0:56:14.400,0:56:20.720
where um we train an encoder

0:56:17.680,0:56:22.079
that takes the the y value for now you

0:56:20.720,0:56:23.839
can ignore the

0:56:22.079,0:56:25.040
the piece that depends on x right you

0:56:23.839,0:56:25.599
have x going through a predictor

0:56:25.040,0:56:27.520
predicting

0:56:25.599,0:56:29.119
h and then h feeds into the anchor of

0:56:27.520,0:56:31.599
the decoder you can ignore this part for

0:56:29.119,0:56:34.079
now in the unconditional version

0:56:31.599,0:56:35.920
you just have y that goes to an encoder

0:56:34.079,0:56:38.640
it produces a prediction for what the

0:56:35.920,0:56:41.280
optimal value of the z variable is okay

0:56:38.640,0:56:41.280
called z bar

0:56:41.599,0:56:45.200
and then the z variable itself goes into

0:56:43.599,0:56:46.640
the the decoder it goes you know it's

0:56:45.200,0:56:48.559
being regularized as well and then

0:56:46.640,0:56:51.599
produces a reconstruction y bar

0:56:48.559,0:56:52.000
and what you do here is again you find

0:56:51.599,0:56:55.359
the z

0:56:52.000,0:56:58.480
value that minimizes uh the energy

0:56:55.359,0:57:00.559
um but what we're gonna

0:56:58.480,0:57:02.559
uh but the energy now is uh is still the

0:57:00.559,0:57:04.079
sum of the two those two terms c of y y

0:57:02.559,0:57:05.200
bar and r of z

0:57:04.079,0:57:07.920
but then what we're gonna do is we're

0:57:05.200,0:57:11.760
gonna train the encoder

0:57:07.920,0:57:15.119
to predict this optimal value of z

0:57:11.760,0:57:16.160
obtained through minimization and this

0:57:15.119,0:57:17.440
encoder is going to be trained by

0:57:16.160,0:57:19.920
minimizing this uh

0:57:17.440,0:57:20.640
this term d of z and z bar so basically

0:57:19.920,0:57:24.240
it views

0:57:20.640,0:57:25.119
z as a target value and you train it by

0:57:24.240,0:57:28.000
back prop by

0:57:25.119,0:57:29.760
you know greater descent to basically

0:57:28.000,0:57:32.079
make a prediction that's as close to z

0:57:29.760,0:57:36.400
as possible

0:57:32.079,0:57:36.400
okay that's one form of this idea

0:57:37.520,0:57:41.599
another form of this idea slightly more

0:57:39.680,0:57:43.200
sophisticated

0:57:41.599,0:57:45.200
is that when you're doing the

0:57:43.200,0:57:46.480
minimization with respect to z of the

0:57:45.200,0:57:48.319
energy with respect to z

0:57:46.480,0:57:49.680
you take into account the fact that you

0:57:48.319,0:57:52.640
don't want z to get

0:57:49.680,0:57:54.720
too far away from z bar so basically

0:57:52.640,0:57:56.720
your energy function now has three terms

0:57:54.720,0:57:58.079
it has the reconstruction error it has

0:57:56.720,0:58:00.880
the regularization

0:57:58.079,0:58:01.920
but it also has the difference between

0:58:00.880,0:58:04.480
the

0:58:01.920,0:58:05.040
uh z bar the prediction from the encoder

0:58:04.480,0:58:07.200
and the

0:58:05.040,0:58:08.640
current value of the z variable so the

0:58:07.200,0:58:12.160
energy function now is

0:58:08.640,0:58:15.200
is written here e of x y z

0:58:12.160,0:58:18.000
is equal to the c function

0:58:15.200,0:58:19.200
that compares y and the output of the

0:58:18.000,0:58:20.960
decoder applied to

0:58:19.200,0:58:22.640
apply to z this is the unconditional

0:58:20.960,0:58:24.079
version here

0:58:22.640,0:58:26.240
and then you have a second term which is

0:58:24.079,0:58:27.839
the this d function that sort of

0:58:26.240,0:58:28.880
measures the distance between z and the

0:58:27.839,0:58:31.520
encoder

0:58:28.880,0:58:34.839
applied to y there shouldn't be an x and

0:58:31.520,0:58:37.359
then you also regularize z

0:58:34.839,0:58:38.880
okay so basically you're telling the

0:58:37.359,0:58:40.480
system find a value for the latent

0:58:38.880,0:58:44.000
variable that reconstructs

0:58:40.480,0:58:48.640
that is sparse if r if is a

0:58:44.000,0:58:48.640
is an error norm or doesn't isn't too uh

0:58:48.720,0:58:51.599
doesn't have too much information but

0:58:50.079,0:58:53.760
also is not too far away from whatever

0:58:51.599,0:58:56.400
it is that the encoder predicted

0:58:53.760,0:58:57.680
and a specific idea there is called

0:58:56.400,0:59:00.240
lista which means the

0:58:57.680,0:59:02.240
learning esta and is to shape the

0:59:00.240,0:59:05.680
architecture of the autoencoder so that

0:59:02.240,0:59:07.680
it looks very much like a list algorithm

0:59:05.680,0:59:08.880
so if we go back to the the list

0:59:07.680,0:59:11.920
algorithm

0:59:08.880,0:59:15.280
uh the formula uh

0:59:11.920,0:59:18.799
the second last formula here um

0:59:15.280,0:59:21.280
you know looks like uh you know some

0:59:18.799,0:59:22.079
vector update with a with some matrix so

0:59:21.280,0:59:25.200
it's like a

0:59:22.079,0:59:26.720
linear stage of a neural net if you want

0:59:25.200,0:59:28.400
and then some non-linearity that happens

0:59:26.720,0:59:30.160
to be a shrinkage which is sort of a

0:59:28.400,0:59:32.640
double value if you want it's

0:59:30.160,0:59:34.480
when were you going up uh married with

0:59:32.640,0:59:36.799
another value going down

0:59:34.480,0:59:38.079
and so if you look at the diagram of

0:59:36.799,0:59:40.480
this uh of this whole

0:59:38.079,0:59:42.799
uh east algorithm it looks like this

0:59:40.480,0:59:45.839
block diagram here that i've drawn

0:59:42.799,0:59:48.160
on top you start with y

0:59:45.839,0:59:48.960
multiplied by some matrix and then

0:59:48.160,0:59:51.520
shrink

0:59:48.960,0:59:53.440
the result then it gives you the next z

0:59:51.520,0:59:55.440
apply some other matrix to it add it to

0:59:53.440,0:59:57.520
the previous value of

0:59:55.440,0:59:58.799
z that you had shrink again and then

0:59:57.520,1:00:00.319
multiply the matrix again

0:59:58.799,1:00:02.799
add to the previous value you had shrink

1:00:00.319,1:00:05.920
again et cetera and so

1:00:02.799,1:00:08.319
you have two matrices here w e and s and

1:00:05.920,1:00:09.680
at the bottom if you define w e as one

1:00:08.319,1:00:12.079
over l w d

1:00:09.680,1:00:13.680
and if you define s as the identity

1:00:12.079,1:00:16.160
minus one over l w d

1:00:13.680,1:00:17.200
transpose wd where wd is the decoding

1:00:16.160,1:00:19.680
matrix

1:00:17.200,1:00:20.319
then uh this diagram basically

1:00:19.680,1:00:24.160
implements

1:00:20.319,1:00:25.839
uh ista okay so the idea that

1:00:24.160,1:00:28.240
one of my firmware post docs carl greger

1:00:25.839,1:00:29.680
had was to say well why don't we why

1:00:28.240,1:00:30.799
don't we treat this as a recurrent

1:00:29.680,1:00:33.040
neural net

1:00:30.799,1:00:34.160
and why don't we train those matrices w

1:00:33.040,1:00:37.280
and s

1:00:34.160,1:00:39.520
so as to give us a good approximation

1:00:37.280,1:00:40.400
of the optimus pass code as quickly as

1:00:39.520,1:00:42.720
possible

1:00:40.400,1:00:45.040
okay so we're basically going to build

1:00:42.720,1:00:47.440
our encoder network

1:00:45.040,1:00:48.480
with this architecture that is copied on

1:00:47.440,1:00:50.640
ista

1:00:48.480,1:00:52.160
and we know for a fact that there is

1:00:50.640,1:00:55.119
going to be a solution

1:00:52.160,1:00:56.000
uh where the system basically learns

1:00:55.119,1:00:59.680
this the value

1:00:56.000,1:01:03.040
of w e and s that correspond to the the

1:00:59.680,1:01:04.400
the one they should be um but in fact

1:01:03.040,1:01:07.440
the system learns something else

1:01:04.400,1:01:08.880
okay so uh this is

1:01:07.440,1:01:10.799
another representation of it here at the

1:01:08.880,1:01:11.680
bottom left we have the shrinkage

1:01:10.799,1:01:14.000
function

1:01:11.680,1:01:14.799
and then this s matrix and then you add

1:01:14.000,1:01:16.880
the

1:01:14.799,1:01:18.240
you know y multiplied by w e to the s

1:01:16.880,1:01:19.839
matrix shrink again

1:01:18.240,1:01:21.040
et cetera et cetera so that this is the

1:01:19.839,1:01:21.760
recurrent net we're going to try we're

1:01:21.040,1:01:25.200
going to train

1:01:21.760,1:01:26.319
with w e and s but the objective of this

1:01:25.200,1:01:28.000
easter is

1:01:26.319,1:01:29.520
can you repeat what is the objective i

1:01:28.000,1:01:32.240
think i missed the point

1:01:29.520,1:01:33.680
so the objective of training this uh

1:01:32.240,1:01:35.839
encoder

1:01:33.680,1:01:36.960
okay so the the encoder in this in this

1:01:35.839,1:01:38.480
diagram on the right here

1:01:36.960,1:01:40.480
the architecture of the encoder is the

1:01:38.480,1:01:43.680
one you see at the bottom left

1:01:40.480,1:01:44.880
okay and the objective that you're

1:01:43.680,1:01:49.920
training this with

1:01:44.880,1:01:54.240
is the average of d of z and z bar

1:01:49.920,1:01:55.680
okay so the procedure is uh

1:01:54.240,1:01:57.359
in the case where there is no x right

1:01:55.680,1:02:00.079
but if there is an x doesn't make much

1:01:57.359,1:02:01.839
difference so take a y

1:02:00.079,1:02:03.680
for this particular y find the value of

1:02:01.839,1:02:05.440
z that minimizes

1:02:03.680,1:02:07.520
the energy and the energy is the sum of

1:02:05.440,1:02:11.200
three terms c of y y bar

1:02:07.520,1:02:13.760
r z and d or z z bar okay so find a z

1:02:11.200,1:02:16.240
that reconstructs

1:02:13.760,1:02:17.680
um has minimal capacity but also is not

1:02:16.240,1:02:18.799
too far away from the output of the

1:02:17.680,1:02:22.079
encoder

1:02:18.799,1:02:22.720
okay once you have the z compute the

1:02:22.079,1:02:24.319
gradient

1:02:22.720,1:02:26.559
of the energy with respect to the

1:02:24.319,1:02:28.559
weights of the decoder

1:02:26.559,1:02:29.599
of the encoder and the predictor if you

1:02:28.559,1:02:33.039
have one

1:02:29.599,1:02:33.440
by backup so the interesting thing is

1:02:33.039,1:02:35.599
that

1:02:33.440,1:02:37.680
the only gradient you're going to get

1:02:35.599,1:02:40.640
for the encoder is the gradient of d

1:02:37.680,1:02:42.160
of z and z bar okay so the encoder is

1:02:40.640,1:02:42.720
just going to train itself to minimize

1:02:42.160,1:02:44.240
the

1:02:42.720,1:02:45.760
z and z bar in other words it's going to

1:02:44.240,1:02:48.640
train itself to predict z as

1:02:45.760,1:02:49.839
well as possible the optimal z that you

1:02:48.640,1:02:51.920
obtain through

1:02:49.839,1:02:52.960
minimization the decoder is going to

1:02:51.920,1:02:55.520
train itself

1:02:52.960,1:02:56.559
to uh of course reconstruct y as well as

1:02:55.520,1:02:59.200
well as it can

1:02:56.559,1:03:00.240
with the z that is being given and then

1:02:59.200,1:03:01.520
if you have a predictor you're going to

1:03:00.240,1:03:02.640
get gradient to the predictor and it's

1:03:01.520,1:03:05.280
going to try to kind of

1:03:02.640,1:03:06.480
produce an edge that you know helps as

1:03:05.280,1:03:10.640
well as possible

1:03:06.480,1:03:10.640
is that clear yeah thanks

1:03:12.640,1:03:17.200
okay so that's the that's the

1:03:15.200,1:03:21.760
architecture it's basically just a

1:03:17.200,1:03:23.200
pretty garden variety recurrent net

1:03:21.760,1:03:25.119
and this works really well in the sense

1:03:23.200,1:03:28.559
that

1:03:25.119,1:03:28.559
as you go through

1:03:28.720,1:03:32.799
you know the iterations of this uh ista

1:03:31.760,1:03:35.119
algorithm

1:03:32.799,1:03:38.319
or this or or through this train neural

1:03:35.119,1:03:38.319
net that is designed to

1:03:38.480,1:03:44.319
basically approximate this solution

1:03:42.319,1:03:45.599
what you do is you um you can train the

1:03:44.319,1:03:47.359
system for example

1:03:45.599,1:03:49.760
to produce the best possible solution

1:03:47.359,1:03:52.400
after three iterations only right

1:03:49.760,1:03:54.079
so it knows the optimal value because

1:03:52.400,1:03:55.920
it's been completed with ista

1:03:54.079,1:03:58.000
but then when you train it it it trains

1:03:55.920,1:04:00.720
itself to produce the best approximation

1:03:58.000,1:04:02.240
of that value with only three iterations

1:04:00.720,1:04:04.000
and what we see is that after three

1:04:02.240,1:04:06.319
iterations it produces a much much

1:04:04.000,1:04:06.319
better

1:04:06.480,1:04:13.839
approximation than ista we produce in

1:04:10.319,1:04:13.839
three iterations

1:04:14.880,1:04:18.319
and so what you see here is the number

1:04:16.400,1:04:21.280
as a function of number of iterations of

1:04:18.319,1:04:21.599
either ista or this list algorithm is

1:04:21.280,1:04:24.559
the

1:04:21.599,1:04:26.400
reconstruction error right so by

1:04:24.559,1:04:28.000
training an encoder to kind of predict

1:04:26.400,1:04:28.640
the result of the optimization you

1:04:28.000,1:04:30.240
actually get

1:04:28.640,1:04:31.920
better result than if you actually run

1:04:30.240,1:04:32.960
the optimization for the same number of

1:04:31.920,1:04:35.359
iterations

1:04:32.960,1:04:37.119
so the accelerates inference a lot okay

1:04:35.359,1:04:40.319
so this is what spice coding

1:04:37.119,1:04:41.280
uh gives you uh with or without an

1:04:40.319,1:04:42.880
encoder actually

1:04:41.280,1:04:44.720
uh you get pretty much the same results

1:04:42.880,1:04:48.000
here when you train on mnist so

1:04:44.720,1:04:51.359
these are um

1:04:48.000,1:04:53.119
basically it's a linear decoder the

1:04:51.359,1:04:56.400
the code space here the z vector has

1:04:53.119,1:04:58.880
size 256

1:04:56.400,1:05:00.640
and so you take this 256 vector

1:04:58.880,1:05:04.480
multiplied by matrix and

1:05:00.640,1:05:04.480
and you reconstruct a digit

1:05:04.640,1:05:08.400
and what you see here are the columns of

1:05:07.359,1:05:11.359
this matrix

1:05:08.400,1:05:12.720
represented as images okay so each

1:05:11.359,1:05:15.039
column has the same dimension as an

1:05:12.720,1:05:18.079
mnist digit right

1:05:15.039,1:05:20.480
uh each column of w and so you can

1:05:18.079,1:05:23.520
represent each of them as an image

1:05:20.480,1:05:26.160
and and these are the 256 uh

1:05:23.520,1:05:27.599
columns of w and what you see is that

1:05:26.160,1:05:30.000
they basically represent

1:05:27.599,1:05:32.160
parts of characters like little pieces

1:05:30.000,1:05:33.760
of strokes

1:05:32.160,1:05:36.160
and the reason for this is that you can

1:05:33.760,1:05:37.200
basically reconstruct any character any

1:05:36.160,1:05:38.720
in this digit

1:05:37.200,1:05:41.280
by a linear combination of a small

1:05:38.720,1:05:44.960
number of those strokes

1:05:41.280,1:05:47.280
okay and so that's kind of

1:05:44.960,1:05:48.480
beautiful because this system basically

1:05:47.280,1:05:51.839
finds

1:05:48.480,1:05:53.680
constitutive parts of objects

1:05:51.839,1:05:54.960
uh in a completely unsupervised way and

1:05:53.680,1:05:56.720
that's kind of what you want out of

1:05:54.960,1:05:58.079
unsupervised running you want sort of

1:05:56.720,1:06:00.240
you know what other what other

1:05:58.079,1:06:02.799
components or the parts they can explain

1:06:00.240,1:06:04.960
what my data looks like

1:06:02.799,1:06:06.400
um so this works really beautifully for

1:06:04.960,1:06:10.400
uh for endless

1:06:06.400,1:06:12.240
uh it works quite nicely as well for

1:06:10.400,1:06:13.520
uh natural image patches there's

1:06:12.240,1:06:14.720
supposed to be an animation here but

1:06:13.520,1:06:16.160
you're not seeing it obviously because

1:06:14.720,1:06:18.559
it's pdf

1:06:16.160,1:06:20.240
uh the result is this um so the

1:06:18.559,1:06:21.200
animation shows the learning algorithm

1:06:20.240,1:06:24.319
taking place

1:06:21.200,1:06:26.319
so here again these are the the columns

1:06:24.319,1:06:28.160
of the decoding matrix

1:06:26.319,1:06:30.319
of a sparse coding system with l1

1:06:28.160,1:06:32.000
regularization that has been trained on

1:06:30.319,1:06:33.200
natural image patches

1:06:32.000,1:06:35.440
and i said that those natural image

1:06:33.200,1:06:37.200
patches have been uh whitened which

1:06:35.440,1:06:38.000
means they've been sort of normalized in

1:06:37.200,1:06:39.839
some way you know

1:06:38.000,1:06:41.520
cancel the mean and kind of normalize

1:06:39.839,1:06:45.520
the variants

1:06:41.520,1:06:48.079
and you get a nice little

1:06:45.520,1:06:49.839
what's called gabor filters so basically

1:06:48.079,1:06:50.960
you know small edge detectors at various

1:06:49.839,1:06:54.160
orientations

1:06:50.960,1:06:56.559
locations and sizes

1:06:54.160,1:06:58.079
so the reason why this was invented by

1:06:56.559,1:07:00.400
neuroscientists is that

1:06:58.079,1:07:01.359
this looks very much like what you

1:07:00.400,1:07:03.440
observe uh

1:07:01.359,1:07:04.480
in the primary area of the visual cortex

1:07:03.440,1:07:07.440
when you

1:07:04.480,1:07:10.240
um when you poke electrodes in the

1:07:07.440,1:07:13.200
visual cortex of

1:07:10.240,1:07:15.440
most animals and you figure out what

1:07:13.200,1:07:19.680
patterns do they maximally respond to

1:07:15.440,1:07:21.039
they actually respond to oriented edges

1:07:19.680,1:07:22.960
this is also what you observe when you

1:07:21.039,1:07:25.039
train a convolutional net

1:07:22.960,1:07:27.520
on imagenet the first layer features

1:07:25.039,1:07:28.720
look very much like this as well

1:07:27.520,1:07:30.640
except they're conditional these ones

1:07:28.720,1:07:32.400
are not convolutional it's it's trained

1:07:30.640,1:07:35.359
on uh you know image patches but there

1:07:32.400,1:07:35.359
is no conditions here

1:07:35.760,1:07:38.480
so this is nice because what it tells

1:07:37.119,1:07:39.920
you is that with a very simple

1:07:38.480,1:07:41.520
unsupervised learning algorithm we get

1:07:39.920,1:07:44.079
essentially qualitatively the same

1:07:41.520,1:07:45.200
features that we would get

1:07:44.079,1:07:47.039
by you know training a large

1:07:45.200,1:07:49.119
convolutional net supervised so that

1:07:47.039,1:07:53.280
gives you a hint

1:07:49.119,1:07:54.799
so here is the convolutional version

1:07:53.280,1:07:58.319
so the convolutional version basically

1:07:54.799,1:07:58.319
says you have an image

1:07:58.640,1:08:02.079
this is more responsive by the way

1:08:02.880,1:08:11.839
what you're going to do is going to take

1:08:04.400,1:08:11.839
feature maps

1:08:13.359,1:08:19.600
okay let's say here four but

1:08:17.279,1:08:21.359
it could be more and then you're gonna

1:08:19.600,1:08:22.239
you're going to convolve each of those

1:08:21.359,1:08:25.440
feature map with

1:08:22.239,1:08:27.520
uh with a kernel

1:08:25.440,1:08:29.040
okay so a feature map is i don't know

1:08:27.520,1:08:32.159
let's call this

1:08:29.040,1:08:36.640
zk okay

1:08:32.159,1:08:38.319
and we have a kernel here um

1:08:36.640,1:08:40.560
well let's call it zi because i'm going

1:08:38.319,1:08:43.359
to use k for the kernel

1:08:40.560,1:08:46.319
k i and this is going to be a

1:08:43.359,1:08:46.319
reconstruction y

1:08:47.440,1:08:51.359
and our reconstruction is simply going

1:08:50.000,1:08:56.159
to be

1:08:51.359,1:08:56.159
the sum over i of

1:08:56.640,1:09:03.920
z i convolved

1:09:00.239,1:09:07.359
with k i okay so this is different

1:09:03.920,1:09:11.839
from the original sparse coating where

1:09:07.359,1:09:15.440
y bar was equal to the sum over

1:09:11.839,1:09:21.520
columns of a column of

1:09:15.440,1:09:21.520
a w matrix so um

1:09:23.279,1:09:26.000
and uh

1:09:28.080,1:09:31.120
multiplied by a coefficient zi which is

1:09:30.159,1:09:34.719
not a scalar

1:09:31.120,1:09:37.199
right so regular sparse coding

1:09:34.719,1:09:38.640
um you have a weighted sum of columns

1:09:37.199,1:09:40.960
where the weights are scalar

1:09:38.640,1:09:42.880
coefficients of zi's

1:09:40.960,1:09:44.480
in convolutional sparse coding it's

1:09:42.880,1:09:47.199
again a linear operation but

1:09:44.480,1:09:48.480
now the dictionary matrix is a bunch of

1:09:47.199,1:09:49.920
composition kernels

1:09:48.480,1:09:51.759
and the latent variable is a bunch of

1:09:49.920,1:09:53.440
feature maps

1:09:51.759,1:09:55.199
and you're doing a convolution of each

1:09:53.440,1:09:59.760
feature map with

1:09:55.199,1:09:59.760
the each kernel and some of the results

1:10:00.320,1:10:03.760
this is what you get so so here there

1:10:02.560,1:10:05.440
are um

1:10:03.760,1:10:07.040
it's it's one of those system that has a

1:10:05.440,1:10:08.800
decoder and an encoder the encoder is

1:10:07.040,1:10:10.719
very simple here it's basically

1:10:08.800,1:10:12.320
just a essentially a single layer

1:10:10.719,1:10:13.199
network with a non-linearity and then

1:10:12.320,1:10:14.880
there is a

1:10:13.199,1:10:16.480
simple layer after that basically a

1:10:14.880,1:10:17.280
diagonal layer after that to change the

1:10:16.480,1:10:19.440
gains

1:10:17.280,1:10:20.960
but it's very very simple and the

1:10:19.440,1:10:22.320
filters in the encoders and

1:10:20.960,1:10:24.719
the encoder and the decoder look very

1:10:22.320,1:10:26.000
similar so it's basically the encoder is

1:10:24.719,1:10:27.920
just a convolution

1:10:26.000,1:10:29.679
then some non-linearity i think it was

1:10:27.920,1:10:32.080
hyperbolic tangent in that case

1:10:29.679,1:10:33.760
and then a basically what amounts to a

1:10:32.080,1:10:36.400
diagonal

1:10:33.760,1:10:37.600
layer that just changed the scale and

1:10:36.400,1:10:39.679
then the decoder

1:10:37.600,1:10:40.640
then there is a sparsity on the

1:10:39.679,1:10:42.400
constraint on the

1:10:40.640,1:10:45.280
on the code and then the decoder is just

1:10:42.400,1:10:47.040
a convolutional linear decoder

1:10:45.280,1:10:48.320
and the reconstruction is just a square

1:10:47.040,1:10:50.480
distance so

1:10:48.320,1:10:52.800
if you impose that there is only one

1:10:50.480,1:10:54.640
filter the filter looks like

1:10:52.800,1:10:56.159
the one at the top left is just a center

1:10:54.640,1:10:57.760
surround type filter

1:10:56.159,1:10:59.760
if you allow two filters you get kind of

1:10:57.760,1:11:01.280
two weirdly shaped filters

1:10:59.760,1:11:03.199
if you let four filters which is the

1:11:01.280,1:11:05.600
third row you get uh

1:11:03.199,1:11:06.640
oriented edges horizontal and vertical

1:11:05.600,1:11:09.280
but you get two

1:11:06.640,1:11:10.640
two polarities for each of the filters

1:11:09.280,1:11:12.400
for eight filters you get

1:11:10.640,1:11:14.960
uh oriented edges at eight eight

1:11:12.400,1:11:16.800
different orientations

1:11:14.960,1:11:18.400
16 filters you get more orientations and

1:11:16.800,1:11:19.679
you also get center surround

1:11:18.400,1:11:22.400
and then as you increase the number of

1:11:19.679,1:11:26.159
filters uh you get sort of more diverse

1:11:22.400,1:11:27.840
uh filters not just uh

1:11:26.159,1:11:29.679
edge detectors but also grating

1:11:27.840,1:11:30.560
detectors at various orientations center

1:11:29.679,1:11:32.960
surround

1:11:30.560,1:11:34.400
etc and that's very interesting because

1:11:32.960,1:11:36.880
this is the kind of stuff you see

1:11:34.400,1:11:38.239
in the visual cortex so again this is an

1:11:36.880,1:11:40.159
indication that you can learn really

1:11:38.239,1:11:42.800
good features

1:11:40.159,1:11:44.080
in completely unsupervised way now here

1:11:42.800,1:11:44.640
is the side news if you take those

1:11:44.080,1:11:47.120
features

1:11:44.640,1:11:47.679
you plug them into a convolutional net

1:11:47.120,1:11:49.840
and you

1:11:47.679,1:11:50.800
you trend that on some tasks you don't

1:11:49.840,1:11:52.239
necessarily get

1:11:50.800,1:11:56.320
you know better results than you you

1:11:52.239,1:11:56.320
train on imagenet from scratch

1:11:56.480,1:12:00.960
but there are a few instances where this

1:11:58.159,1:12:02.719
has helped boost the performance

1:12:00.960,1:12:04.239
particularly in cases where the number

1:12:02.719,1:12:06.000
of label samples is not that great or

1:12:04.239,1:12:07.679
the number of categories is small

1:12:06.000,1:12:09.040
and so by training purely supervised you

1:12:07.679,1:12:12.880
get degenerate

1:12:09.040,1:12:14.800
uh uh features basically

1:12:12.880,1:12:16.320
here's another example here uh same

1:12:14.800,1:12:19.120
thing um

1:12:16.320,1:12:20.880
again it's a convolutional space coding

1:12:19.120,1:12:22.320
here the decoding kernel this is on

1:12:20.880,1:12:25.600
color images

1:12:22.320,1:12:28.320
the the decoding kernels are uh

1:12:25.600,1:12:29.520
nine by nine uh applied convolutionally

1:12:28.320,1:12:31.760
over an image

1:12:29.520,1:12:34.840
and what you see on the left here are

1:12:31.760,1:12:37.440
the the sparse codes here you have

1:12:34.840,1:12:40.560
um i don't know

1:12:37.440,1:12:42.480
64 uh feature maps

1:12:40.560,1:12:43.920
uh and and you can see that the the z

1:12:42.480,1:12:46.080
vector is extremely sparse right there's

1:12:43.920,1:12:48.400
only a few components here that are

1:12:46.080,1:12:49.360
either white or black or non-gray if you

1:12:48.400,1:12:53.199
want

1:12:49.360,1:12:53.199
and it's because of because of sparsity

1:12:54.480,1:12:57.760
okay uh in the last few minutes we'll

1:12:56.719,1:12:59.520
talk about uh

1:12:57.760,1:13:01.600
variational autoencoder and i guess

1:12:59.520,1:13:02.840
you've heard a bit of this from uh

1:13:01.600,1:13:05.199
tomorrow we're gonna be covering this

1:13:02.840,1:13:06.560
tomorrow with the bubbles and

1:13:05.199,1:13:08.960
the code and everything right so

1:13:06.560,1:13:12.480
tomorrow is going to be one hour of

1:13:08.960,1:13:15.280
just this right so um but here is

1:13:12.480,1:13:16.880
a preview for um how variational

1:13:15.280,1:13:18.400
encoders work

1:13:16.880,1:13:19.840
okay so variational routing colors are

1:13:18.400,1:13:21.920
basically the same architecture as the

1:13:19.840,1:13:24.159
one i showed previously

1:13:21.920,1:13:25.120
uh so basically uh another encoder

1:13:24.159,1:13:26.800
ignore the

1:13:25.120,1:13:29.199
the conditional part the part that's

1:13:26.800,1:13:30.719
conditioned upon x for now

1:13:29.199,1:13:32.159
uh that could be a conditional version

1:13:30.719,1:13:33.520
of the twin color but for now we're just

1:13:32.159,1:13:34.239
going to have a regular version of the

1:13:33.520,1:13:37.600
encoder

1:13:34.239,1:13:39.360
so it's an auto encoder where uh

1:13:37.600,1:13:40.880
you take you know you take the variable

1:13:39.360,1:13:42.320
y you run it to an encoder

1:13:40.880,1:13:44.320
uh it could be a multi-layer neural net

1:13:42.320,1:13:46.080
convolutional net whatever you want it

1:13:44.320,1:13:47.760
it produces a prediction for the sparse

1:13:46.080,1:13:49.199
code z bar

1:13:47.760,1:13:51.280
is a term in the energy function that

1:13:49.199,1:13:53.280
measures the square this

1:13:51.280,1:13:54.880
the euclidean square euclidean distance

1:13:53.280,1:13:57.040
between between z the latent variable

1:13:54.880,1:14:00.640
and z bar

1:13:57.040,1:14:02.560
and there's also a

1:14:00.640,1:14:03.679
this is a this another cost function

1:14:02.560,1:14:06.480
here which is the

1:14:03.679,1:14:08.000
l2 norm of z bar in fact generally it's

1:14:06.480,1:14:08.719
more the autonomous z actually that

1:14:08.000,1:14:10.560
would be more

1:14:08.719,1:14:12.320
more accurate but it doesn't make much

1:14:10.560,1:14:14.960
difference

1:14:12.320,1:14:16.560
and then z goes through a decoder which

1:14:14.960,1:14:18.640
reconstructs y

1:14:16.560,1:14:20.560
and that's your reconstruction error

1:14:18.640,1:14:22.640
okay now

1:14:20.560,1:14:24.239
the difference with previous so this

1:14:22.640,1:14:26.560
looks very very similar to the type of

1:14:24.239,1:14:29.199
autoencoder we just talked about

1:14:26.560,1:14:30.719
except there is no sparsity here and the

1:14:29.199,1:14:32.719
reason there is no sparsity is because

1:14:30.719,1:14:34.239
variational encoders use another way of

1:14:32.719,1:14:35.600
limiting the information content of the

1:14:34.239,1:14:38.480
code

1:14:35.600,1:14:38.960
uh by basically making the code noisy

1:14:38.480,1:14:43.760
okay

1:14:38.960,1:14:46.080
so um here is the idea

1:14:43.760,1:14:47.600
the way you you compute z is not by

1:14:46.080,1:14:50.239
minimizing

1:14:47.600,1:14:51.520
the energy function with respect to z

1:14:50.239,1:14:54.159
but by sampling z

1:14:51.520,1:14:55.199
randomly according to a distribution

1:14:54.159,1:14:58.480
whose logarithm

1:14:55.199,1:15:01.520
is the the

1:14:58.480,1:15:02.320
the cost that links it to z bar okay so

1:15:01.520,1:15:05.360
basically

1:15:02.320,1:15:06.880
the encoder produces a z bar

1:15:05.360,1:15:09.040
and then there is an energy function

1:15:06.880,1:15:11.440
that measures the the

1:15:09.040,1:15:13.120
distance if you want between z and z bar

1:15:11.440,1:15:16.480
you think of this as the logarithm

1:15:13.120,1:15:18.800
of a probability distribution

1:15:16.480,1:15:20.320
so if this distance is a square

1:15:18.800,1:15:20.800
euclidean distance what that means is

1:15:20.320,1:15:22.719
that the

1:15:20.800,1:15:24.239
the distribution of z is going to be a

1:15:22.719,1:15:28.719
conditional gaussian

1:15:24.239,1:15:29.840
where the mean is z bar okay

1:15:28.719,1:15:32.000
so we're going to do is we're going to

1:15:29.840,1:15:34.000
sample a random value of z

1:15:32.000,1:15:36.320
according to that distribution basically

1:15:34.000,1:15:38.719
a gaussian whose mean is z bar

1:15:36.320,1:15:40.480
okay and that just means adding a bit of

1:15:38.719,1:15:43.679
gaussian noise to z bar

1:15:40.480,1:15:44.560
that's what rz is going to be and you

1:15:43.679,1:15:48.080
run this to the

1:15:44.560,1:15:49.600
the decoder so when you train a system

1:15:48.080,1:15:53.840
like this

1:15:49.600,1:15:53.840
what the system wants to do is basically

1:15:54.320,1:16:00.880
make the z bar as large as possible make

1:15:57.600,1:16:03.280
the z bar vectors as large as possible

1:16:00.880,1:16:05.120
so that the effect of the gaussian noise

1:16:03.280,1:16:06.960
on z would be as small as possible

1:16:05.120,1:16:08.159
right relatively speaking if the

1:16:06.960,1:16:11.440
variance of the

1:16:08.159,1:16:12.480
of the noise on z is one and you make

1:16:11.440,1:16:15.520
the z bar vector

1:16:12.480,1:16:18.159
very very long like uh norm it doesn't

1:16:15.520,1:16:19.199
uh then the importance of the noise

1:16:18.159,1:16:22.080
would be 0.1

1:16:19.199,1:16:24.000
with respect to z okay so if you train

1:16:22.080,1:16:25.679
an auto encoder like this

1:16:24.000,1:16:27.679
uh ignoring the fact that you had noise

1:16:25.679,1:16:30.719
by just back propagation

1:16:27.679,1:16:32.159
uh what you'll get is uh z bar vectors

1:16:30.719,1:16:33.280
that get bigger and bigger the weights

1:16:32.159,1:16:34.239
of the encoder will get bigger and

1:16:33.280,1:16:35.840
bigger

1:16:34.239,1:16:38.159
and the z-bar vector will get bigger and

1:16:35.840,1:16:38.159
bigger

1:16:38.880,1:16:42.880
so what's the trick in variational

1:16:40.560,1:16:45.199
autoencoder hold on question

1:16:42.880,1:16:47.120
uh quick question where does this z come

1:16:45.199,1:16:48.239
from is that a latent variable never

1:16:47.120,1:16:49.840
observed

1:16:48.239,1:16:51.440
it's a latent variable that we are

1:16:49.840,1:16:52.480
sampling and we're not minimizing with

1:16:51.440,1:16:54.719
respect to it

1:16:52.480,1:16:56.960
so in previous cases we were minimizing

1:16:54.719,1:16:58.400
with respect to the z variable right

1:16:56.960,1:16:59.440
minimizing the energy with respect to

1:16:58.400,1:17:00.560
the variable finding the z that

1:16:59.440,1:17:04.000
minimizes

1:17:00.560,1:17:07.280
the sum of c uh

1:17:04.000,1:17:08.320
d and r so here we're not minimizing

1:17:07.280,1:17:10.159
we're just

1:17:08.320,1:17:11.920
we're sampling we're viewing the energy

1:17:10.159,1:17:13.120
as a distribution as a log of a

1:17:11.920,1:17:14.840
distribution

1:17:13.120,1:17:17.600
and we're sampling z from that

1:17:14.840,1:17:20.560
distribution all right so imagine our

1:17:17.600,1:17:23.199
encoder produces the following points

1:17:20.560,1:17:25.280
for training samples okay so these are

1:17:23.199,1:17:27.460
the z vector the z bar vectors produced

1:17:25.280,1:17:28.840
by the encoder

1:17:27.460,1:17:31.360
[Music]

1:17:28.840,1:17:34.560
um you know at some point

1:17:31.360,1:17:38.000
in training so

1:17:34.560,1:17:40.000
what the effect of this uh sampling of z

1:17:38.000,1:17:41.679
is going to do is basically turn every

1:17:40.000,1:17:46.159
single one of those

1:17:41.679,1:17:50.080
uh training samples into a fuzzy ball

1:17:46.159,1:17:53.040
okay because uh we take a sample

1:17:50.080,1:17:53.600
we add noise to it and so basically

1:17:53.040,1:17:55.440
we've turned

1:17:53.600,1:17:57.360
a single code vector into kind of a

1:17:55.440,1:17:59.280
fuzzy ball

1:17:57.360,1:18:00.800
now the decoder needs to be able to

1:17:59.280,1:18:04.320
reconstruct

1:18:00.800,1:18:07.520
uh the the input from

1:18:04.320,1:18:10.840
whatever code is being fed and so if two

1:18:07.520,1:18:13.679
of those fuzzy ball uh

1:18:10.840,1:18:14.800
intersect then there is some probability

1:18:13.679,1:18:16.960
for

1:18:14.800,1:18:18.880
the decoder to get basically make a

1:18:16.960,1:18:21.920
mistake and confuse the two samples

1:18:18.880,1:18:24.080
confuse one sample for the other

1:18:21.920,1:18:25.679
okay so the effect of training the the

1:18:24.080,1:18:28.080
system if you if you

1:18:25.679,1:18:29.199
add fuzzy balls you know if you make

1:18:28.080,1:18:31.280
every one of your code

1:18:29.199,1:18:34.480
a fuzzy ball is that those fuzzy balls

1:18:31.280,1:18:34.480
are going to fly away from each other

1:18:35.199,1:18:38.719
and as i said before this is the same

1:18:37.440,1:18:39.920
you know i was saying this before in a

1:18:38.719,1:18:41.520
different way it's going to make the

1:18:39.920,1:18:43.040
weights of the encoder very large so

1:18:41.520,1:18:45.440
that the code vectors get very

1:18:43.040,1:18:47.199
long and basically they get away from

1:18:45.440,1:18:49.920
from each other and the noise

1:18:47.199,1:18:50.560
of those visibles don't matter anymore

1:18:49.920,1:18:52.480
okay

1:18:50.560,1:18:53.679
so here if the fusible don't intersect

1:18:52.480,1:18:56.159
the system will be able to perfectly

1:18:53.679,1:18:57.679
reconstruct every sample you throw at it

1:18:56.159,1:18:59.280
my question it was a couple of slides

1:18:57.679,1:19:01.520
again but again on the same

1:18:59.280,1:19:03.440
topic it was a couple of slides ago so

1:19:01.520,1:19:05.520
when you what exactly do you mean by

1:19:03.440,1:19:07.360
degenerate features here when you when

1:19:05.520,1:19:09.120
you said that when you were comparing

1:19:07.360,1:19:11.120
self supervision and normal super

1:19:09.120,1:19:14.000
completely supervision

1:19:11.120,1:19:15.360
i see okay uh that's a good question the

1:19:14.000,1:19:17.120
what i was saying is

1:19:15.360,1:19:18.800
it's something i i said before in

1:19:17.120,1:19:19.440
different turns it's the fact that if

1:19:18.800,1:19:21.040
you train

1:19:19.440,1:19:22.560
uh a a classifier let's say

1:19:21.040,1:19:24.719
accomplishment

1:19:22.560,1:19:26.640
uh on a problem that has very few

1:19:24.719,1:19:29.679
categories let's say phase detection

1:19:26.640,1:19:31.600
you only have two categories the

1:19:29.679,1:19:33.520
representation of faces you get out of

1:19:31.600,1:19:35.520
the commercial net are very degenerate

1:19:33.520,1:19:38.080
in the sense that

1:19:35.520,1:19:40.320
they don't represent every image

1:19:38.080,1:19:42.560
properly right they're going to kind of

1:19:40.320,1:19:43.360
collapse a lot of different images into

1:19:42.560,1:19:46.159
sort of a

1:19:43.360,1:19:47.440
common into identical representations

1:19:46.159,1:19:49.280
because the only thing

1:19:47.440,1:19:50.800
the system needs to do is discriminate

1:19:49.280,1:19:53.360
from faces to

1:19:50.800,1:19:55.520
you know faces with non-faces and so it

1:19:53.360,1:19:57.760
doesn't need to really kind of

1:19:55.520,1:19:59.120
uh produce good representations of of

1:19:57.760,1:20:01.679
the entire space

1:19:59.120,1:20:03.120
uh uh it just needs to tell you if it's

1:20:01.679,1:20:05.040
if it's a face or another face so for

1:20:03.120,1:20:06.719
example the features you will get for

1:20:05.040,1:20:10.000
two different phases will probably be

1:20:06.719,1:20:11.520
fairly identical so

1:20:10.000,1:20:13.199
so you don't so that's what i mean by

1:20:11.520,1:20:15.280
degenerate features

1:20:13.199,1:20:17.040
what you want are features that

1:20:15.280,1:20:19.600
basically

1:20:17.040,1:20:21.120
uh feature vectors that are different

1:20:19.600,1:20:22.080
for different objects regardless of

1:20:21.120,1:20:23.840
whether you trained

1:20:22.080,1:20:25.760
them to be different or not so if you

1:20:23.840,1:20:29.120
train on imagenet for example

1:20:25.760,1:20:30.400
you have 1000 categories and

1:20:29.120,1:20:31.760
and so because you have a lot of

1:20:30.400,1:20:33.440
categories you get features that are

1:20:31.760,1:20:35.760
fairly diverse and they cover a lot of

1:20:33.440,1:20:37.280
the space of possible images

1:20:35.760,1:20:39.120
i mean they're still kind of fairly

1:20:37.280,1:20:40.639
specialized but they're not completely

1:20:39.120,1:20:42.080
degenerate because you have many

1:20:40.639,1:20:44.400
categories

1:20:42.080,1:20:46.560
and you have a lot of samples the more

1:20:44.400,1:20:49.280
samples and more categories you have

1:20:46.560,1:20:50.400
the the better your features are in fact

1:20:49.280,1:20:53.040
um

1:20:50.400,1:20:53.920
uh if you if you think about it an auto

1:20:53.040,1:20:56.639
encoder

1:20:53.920,1:20:57.280
is a neural net in which every training

1:20:56.639,1:21:00.400
sample is

1:20:57.280,1:21:01.840
is its own category right

1:21:00.400,1:21:03.600
because you're basically telling the

1:21:01.840,1:21:06.239
system produce a different output for

1:21:03.600,1:21:10.639
every sample i'll show you

1:21:06.239,1:21:13.040
so you're basically training a system to

1:21:10.639,1:21:15.040
represent every object in a different

1:21:13.040,1:21:16.719
way

1:21:15.040,1:21:18.400
but it can be degenerated another way

1:21:16.719,1:21:19.600
because the system can learn the

1:21:18.400,1:21:22.239
identity function

1:21:19.600,1:21:23.920
and encode uh anything you want if you

1:21:22.239,1:21:25.920
think about the same is nets

1:21:23.920,1:21:29.040
the metric learning system the

1:21:25.920,1:21:33.679
contrastive methods moco pearl and

1:21:29.040,1:21:36.400
uh and seem clear i was i was

1:21:33.679,1:21:37.600
telling you about uh it's it's a little

1:21:36.400,1:21:40.639
bit of the same thing

1:21:37.600,1:21:43.120
um they they they try to

1:21:40.639,1:21:44.239
learn non-degenerate features by by

1:21:43.120,1:21:46.080
telling the system

1:21:44.239,1:21:47.440
you know here is two objects that i know

1:21:46.080,1:21:48.400
are the same here are two objects that i

1:21:47.440,1:21:50.320
know are different

1:21:48.400,1:21:52.080
and so make sure you produce different

1:21:50.320,1:21:52.480
feature vectors for objects that i know

1:21:52.080,1:21:55.199
are

1:21:52.480,1:21:56.800
are semantically different okay that's

1:21:55.199,1:21:59.679
kind of a way of

1:21:56.800,1:22:01.520
uh you know making sure you get you get

1:21:59.679,1:22:02.719
feature feature vectors representations

1:22:01.520,1:22:05.600
are different for

1:22:02.719,1:22:06.400
for things that are actually different

1:22:05.600,1:22:08.320
um

1:22:06.400,1:22:10.000
but you don't get this by training a

1:22:08.320,1:22:11.199
confident on a two-class problem or a

1:22:10.000,1:22:13.600
10-class problem

1:22:11.199,1:22:15.920
you need as many classes as you can

1:22:13.600,1:22:15.920
afford

1:22:17.600,1:22:22.560
so free training using self-supervised

1:22:20.560,1:22:24.960
learning basically

1:22:22.560,1:22:25.600
helps uh making the feature more more

1:22:24.960,1:22:29.520
generic

1:22:25.600,1:22:31.120
and less less degenerate for the problem

1:22:29.520,1:22:33.199
okay so let's get back to variational

1:22:31.120,1:22:34.639
twin colors so um so again

1:22:33.199,1:22:36.000
if you train your auto encoder with

1:22:34.639,1:22:37.280
those fuzzy balls they're gonna fly away

1:22:36.000,1:22:39.760
from each other

1:22:37.280,1:22:41.679
and what you want really is you want

1:22:39.760,1:22:42.639
those visibles to basically kind of

1:22:41.679,1:22:45.199
cluster around

1:22:42.639,1:22:46.719
some sort of data manifold right so you

1:22:45.199,1:22:48.639
want to actually keep them as close to

1:22:46.719,1:22:50.840
each other as possible

1:22:48.639,1:22:53.840
and how can you do this you can do this

1:22:50.840,1:22:53.840
by

1:22:54.800,1:23:03.520
essentially linking

1:22:58.080,1:23:07.760
all of them to the origin with a spring

1:23:03.520,1:23:09.600
okay so basically the spring wants to

1:23:07.760,1:23:11.520
bring all those points towards towards

1:23:09.600,1:23:14.639
the origin as close to each other

1:23:11.520,1:23:16.400
to the origin as possible

1:23:14.639,1:23:17.760
and so in doing so what the system is

1:23:16.400,1:23:20.239
going to try to do is pack

1:23:17.760,1:23:22.400
those fuzzy spheres as close to the

1:23:20.239,1:23:25.199
origin as possible

1:23:22.400,1:23:28.080
and it's going to make them sort of

1:23:25.199,1:23:29.840
overlap interpenetrate

1:23:28.080,1:23:32.000
but of course if they interpenetrate too

1:23:29.840,1:23:33.600
much

1:23:32.000,1:23:35.520
if the two spheres for two very

1:23:33.600,1:23:38.400
different uh samples

1:23:35.520,1:23:40.159
interpenetrate too much then those two

1:23:38.400,1:23:42.880
samples are going to be confused

1:23:40.159,1:23:45.360
by the decoder and the reconstruction

1:23:42.880,1:23:47.280
energy is going to get large

1:23:45.360,1:23:48.560
and so what the system ends up doing is

1:23:47.280,1:23:50.800
only letting

1:23:48.560,1:23:53.600
two spheres overlap if the two samples

1:23:50.800,1:23:53.600
are very similar

1:23:53.920,1:23:58.000
and so basically by doing this the

1:23:55.520,1:24:00.960
system finds some sort of

1:23:58.000,1:24:02.400
you know representation of the manifold

1:24:00.960,1:24:05.679
it puts the

1:24:02.400,1:24:09.040
the those those code vectors along uh

1:24:05.679,1:24:09.040
a manifold if there is one

1:24:09.360,1:24:13.520
and that's the basic idea of uh version

1:24:11.199,1:24:17.360
of autoencoder now you can derive this

1:24:13.520,1:24:17.760
with math and it doesn't make anything

1:24:17.360,1:24:20.800
much

1:24:17.760,1:24:22.080
you know easier to understand in fact

1:24:20.800,1:24:23.840
it's much more abstract

1:24:22.080,1:24:25.280
but that's basically what it does in the

1:24:23.840,1:24:28.080
end so there's a bit

1:24:25.280,1:24:31.120
uh there's a couple more tricks there um

1:24:28.080,1:24:31.120
in that in that uh

1:24:31.280,1:24:35.360
in the variation of the twin color idea

1:24:32.719,1:24:39.199
and you'll get some details uh with uh

1:24:35.360,1:24:44.080
alfredo tomorrow um you can adapt

1:24:39.199,1:24:46.400
the uh the size of those fuzzy balls

1:24:44.080,1:24:48.400
so basically you you can have the

1:24:46.400,1:24:51.520
encoder compute the optimal size of the

1:24:48.400,1:24:51.520
bolts in each direction

1:24:51.920,1:24:55.040
and what you have to do is make sure

1:24:53.520,1:24:55.520
that the balls don't get too small and

1:24:55.040,1:24:58.719
so

1:24:55.520,1:24:59.600
you put a penalty function that tries to

1:24:58.719,1:25:03.199
make the

1:24:59.600,1:25:04.880
the variance of those balls uh the size

1:25:03.199,1:25:08.159
in each dimension if you want

1:25:04.880,1:25:09.760
as close to one as possible they can

1:25:08.159,1:25:11.679
get a bit smaller they can get larger if

1:25:09.760,1:25:13.360
they want but

1:25:11.679,1:25:16.080
there's a cost for making making them

1:25:13.360,1:25:18.719
different from from one

1:25:16.080,1:25:19.520
so now the the trick the the problem you

1:25:18.719,1:25:23.760
have with this

1:25:19.520,1:25:27.199
is to adjust the relative importance

1:25:23.760,1:25:30.000
of the of this uh spring uh

1:25:27.199,1:25:30.480
strength if the spring is too large is

1:25:30.000,1:25:32.719
the

1:25:30.480,1:25:34.480
if the spring is too strong then the

1:25:32.719,1:25:35.280
visibles are all going to collapse in

1:25:34.480,1:25:36.480
the center

1:25:35.280,1:25:38.880
and the system is not going to be able

1:25:36.480,1:25:42.400
to reconstruct properly

1:25:38.880,1:25:43.679
if it's too uh weak then the fuzzy balls

1:25:42.400,1:25:45.040
are going to fly away from each other

1:25:43.679,1:25:45.840
and the system is going to be able to

1:25:45.040,1:25:49.360
reconstruct

1:25:45.840,1:25:50.880
everything and anything and so you have

1:25:49.360,1:25:51.679
to strike a balance between the two and

1:25:50.880,1:25:52.840
that's kind of the

1:25:51.679,1:25:55.040
the difficulty with variational

1:25:52.840,1:25:56.480
autocoder if you

1:25:55.040,1:25:59.120
increase the strength of the spring a

1:25:56.480,1:25:59.120
little too much

1:25:59.360,1:26:03.040
it's it's a term called kale divergence

1:26:01.040,1:26:05.040
in the in the in the system it's a cal

1:26:03.040,1:26:08.239
divergence between

1:26:05.040,1:26:09.280
the gaussian basically of a gaussian it

1:26:08.239,1:26:12.719
uh

1:26:09.280,1:26:14.560
it collapses okay um

1:26:12.719,1:26:16.000
it you know all the visibles basically

1:26:14.560,1:26:19.280
get to the center and the system does

1:26:16.000,1:26:22.159
not actually model it to that properly

1:26:19.280,1:26:23.760
i have a question uh about one of the

1:26:22.159,1:26:27.360
previous lectures actually so

1:26:23.760,1:26:28.560
is that all right sure um yeah so when

1:26:27.360,1:26:31.440
you were talking about

1:26:28.560,1:26:32.560
uh linearizability so generally when you

1:26:31.440,1:26:34.080
were saying that stacking

1:26:32.560,1:26:36.080
linear layers one after the other

1:26:34.080,1:26:37.679
without having non-linearities

1:26:36.080,1:26:39.679
is basically redundant because we can

1:26:37.679,1:26:42.960
have one linear layer to do it

1:26:39.679,1:26:45.679
but i remember that you also mentioned

1:26:42.960,1:26:46.159
there is one particular reason why you

1:26:45.679,1:26:47.840
might

1:26:46.159,1:26:49.600
want to do this where you just stack

1:26:47.840,1:26:51.679
linear layers after this

1:26:49.600,1:26:53.280
and you said well there's one reason but

1:26:51.679,1:26:55.120
you didn't go into that reason so i was

1:26:53.280,1:26:58.000
wondering if there is anything

1:26:55.120,1:26:59.280
significant behind that uh so the

1:26:58.000,1:27:01.600
situation i was describing

1:26:59.280,1:27:04.800
is that imagine you have uh you know

1:27:01.600,1:27:07.760
some some big neural net

1:27:04.800,1:27:09.440
and it produces you know a feature

1:27:07.760,1:27:11.920
vector of a certain size

1:27:09.440,1:27:13.120
and then your output is extremely large

1:27:11.920,1:27:15.120
because

1:27:13.120,1:27:16.639
maybe you have many many categories

1:27:15.120,1:27:18.239
maybe you're doing

1:27:16.639,1:27:20.159
a phoneme classification for a speech

1:27:18.239,1:27:22.480
recognition system so

1:27:20.159,1:27:26.080
the number of categories here is uh you

1:27:22.480,1:27:26.080
know 10 000 or something like that

1:27:27.520,1:27:33.760
okay i have to draw slowly

1:27:30.719,1:27:37.920
so if your feature vector here is

1:27:33.760,1:27:40.560
itself something like like 10 000

1:27:37.920,1:27:43.920
the matrix to go from from here to here

1:27:40.560,1:27:43.920
will be 100 million right

1:27:45.600,1:27:51.920
and that's probably a bit too much so

1:27:49.440,1:27:53.280
what people do is they say we're going

1:27:51.920,1:27:56.320
to factorize

1:27:53.280,1:27:59.199
that matrix into the product of two

1:27:56.320,1:27:59.199
skinny matrices

1:27:59.440,1:28:02.560
where the the middle dimension here is

1:28:01.840,1:28:05.360
maybe

1:28:02.560,1:28:05.760
i don't know a thousand well you have

1:28:05.360,1:28:08.000
you have

1:28:05.760,1:28:08.960
10k on the input 10k on the output and

1:28:08.000,1:28:11.360
then the middle one

1:28:08.960,1:28:12.159
is 1k right so if you don't have the

1:28:11.360,1:28:13.679
middle one

1:28:12.159,1:28:15.360
then the number of parameters you have

1:28:13.679,1:28:18.560
is 10 to the 8.

1:28:15.360,1:28:19.040
if you do have the middle one is uh is 2

1:28:18.560,1:28:22.400
times

1:28:19.040,1:28:24.960
uh 10 to the you know

1:28:22.400,1:28:25.600
seven okay so you you get a factor of of

1:28:24.960,1:28:27.679
10.

1:28:25.600,1:28:30.560
if you make it 100 then it's you know 10

1:28:27.679,1:28:32.239
to the 6. so it becomes more manageable

1:28:30.560,1:28:34.000
so basically you get a low rank

1:28:32.239,1:28:37.120
factorization

1:28:34.000,1:28:38.320
so the overall matrices here that you

1:28:37.120,1:28:40.480
can call w

1:28:38.320,1:28:41.360
is not going to be the the product of

1:28:40.480,1:28:44.560
two

1:28:41.360,1:28:47.840
smaller matrices u and v

1:28:44.560,1:28:48.960
and because u and v uh the the middle

1:28:47.840,1:28:52.159
dimension if you want

1:28:48.960,1:28:52.560
of u and v is smaller say 100 then the

1:28:52.159,1:28:54.840
rank

1:28:52.560,1:28:56.400
of the corresponding matrix w would be

1:28:54.840,1:28:58.960
smaller

1:28:56.400,1:29:00.639
there are people who do this without

1:28:58.960,1:29:02.560
actually specifying the dimension

1:29:00.639,1:29:04.320
of the middle layer by doing what's

1:29:02.560,1:29:06.080
called a minimization of nuclear norm

1:29:04.320,1:29:08.560
which is equivalent

1:29:06.080,1:29:10.000
but uh i don't want to go into this but

1:29:08.560,1:29:11.360
that would be kind of a situation

1:29:10.000,1:29:14.639
where you might want to actually

1:29:11.360,1:29:17.760
decompose a matrix into a product of two

1:29:14.639,1:29:20.639
matrices to kind of save uh parameters

1:29:17.760,1:29:21.760
essentially your save computation um

1:29:20.639,1:29:24.000
there's also another

1:29:21.760,1:29:25.280
uh interesting phenomenon which is that

1:29:24.000,1:29:26.800
it turns out that

1:29:25.280,1:29:28.320
both learning and generalization

1:29:26.800,1:29:30.320
actually are better

1:29:28.320,1:29:31.679
uh when you when you do this kind of

1:29:30.320,1:29:34.800
factorization

1:29:31.679,1:29:36.320
even though the now the optimization

1:29:34.800,1:29:38.000
with respect to this matrix becomes

1:29:36.320,1:29:39.920
non-convex it

1:29:38.000,1:29:41.760
actually converges faster using

1:29:39.920,1:29:44.800
stochastic gradient

1:29:41.760,1:29:46.639
there's a paper by a series of paper by

1:29:44.800,1:29:49.679
nadav

1:29:46.639,1:29:49.679
if you're interested in this

1:29:50.239,1:29:53.840
nadav cohen

1:29:54.880,1:30:03.520
i think he's from 2018

1:29:58.320,1:30:03.520
he's his co-author with sanjeev

1:30:04.840,1:30:10.320
uh aurora

1:30:07.520,1:30:10.320
from princeton

1:30:11.600,1:30:17.120
nadav was uh a postdoc with sanjivara

1:30:14.560,1:30:20.480
and they had a series of papers on this

1:30:17.120,1:30:21.199
um that explains why uh even linear

1:30:20.480,1:30:23.840
networks

1:30:21.199,1:30:25.280
actually converge faster and they use it

1:30:23.840,1:30:27.679
also to

1:30:25.280,1:30:27.679
um

1:30:28.719,1:30:32.719
to basically study the dynamics of

1:30:30.880,1:30:34.320
learning uh in sort of non-convex

1:30:32.719,1:30:38.400
optimization as well as the

1:30:34.320,1:30:40.800
the generalization properties

1:30:38.400,1:30:42.800
how important is it to have a kind of

1:30:40.800,1:30:44.480
matching in a variational order encoder

1:30:42.800,1:30:46.840
how important is it to have a matching

1:30:44.480,1:30:48.159
kind of architecture of an encoder and a

1:30:46.840,1:30:50.400
decoder

1:30:48.159,1:30:52.000
there's no reason for the two

1:30:50.400,1:30:55.920
architectures to

1:30:52.000,1:30:59.199
match it's very often the case that

1:30:55.920,1:31:01.440
decoding is much easier than encoding

1:30:59.199,1:31:03.280
so if you take the example of sparse

1:31:01.440,1:31:07.280
coding which i talked about

1:31:03.280,1:31:08.880
um the you know in sparse coding where

1:31:07.280,1:31:10.800
the encoder is one of those sort of

1:31:08.880,1:31:12.159
listed type encoder the encoder is

1:31:10.800,1:31:14.840
actually quite complicated whereas the

1:31:12.159,1:31:17.520
decoder is linear

1:31:14.840,1:31:18.080
okay uh this is kind of a special case

1:31:17.520,1:31:20.159
because

1:31:18.080,1:31:21.120
the the code is high dimensional and

1:31:20.159,1:31:24.239
sparse

1:31:21.120,1:31:25.679
and so high dimensional sparse codes

1:31:24.239,1:31:27.600
uh you know any function of a

1:31:25.679,1:31:29.440
dimensional sparse code to any anywhere

1:31:27.600,1:31:33.600
basically can be

1:31:29.440,1:31:37.360
is quasi-linear a one way to make a

1:31:33.600,1:31:39.280
a function uh linear is to basically

1:31:37.360,1:31:40.400
represent this input variable in a in a

1:31:39.280,1:31:41.600
high dimensional space using a

1:31:40.400,1:31:44.639
non-linear transformation

1:31:41.600,1:31:45.440
we've talked about this we discussed you

1:31:44.639,1:31:48.239
know what were

1:31:45.440,1:31:49.840
good features and good features

1:31:48.239,1:31:52.239
generally consistent sort of

1:31:49.840,1:31:54.880
expanding the dimension uh of the

1:31:52.239,1:31:56.800
representation in a non-linear way

1:31:54.880,1:31:58.320
and making this representation sparse

1:31:56.800,1:32:00.560
and the reason for this is that now you

1:31:58.320,1:32:03.120
make your function linear so

1:32:00.560,1:32:04.719
you could very well have a very complex

1:32:03.120,1:32:07.440
encoder and a very simple

1:32:04.719,1:32:08.320
decoder possibly a linear one as long as

1:32:07.440,1:32:10.400
your

1:32:08.320,1:32:13.199
code is high dimensional now if you have

1:32:10.400,1:32:15.040
a low dimensional code

1:32:13.199,1:32:16.239
so another encoder with the middle layer

1:32:15.040,1:32:18.480
is very narrow

1:32:16.239,1:32:20.159
where the code layer is very narrow then

1:32:18.480,1:32:21.760
it could become very complicated to do

1:32:20.159,1:32:23.679
the decoding

1:32:21.760,1:32:25.760
it may become a very highly nonlinear

1:32:23.679,1:32:27.760
function now to do the decoding

1:32:25.760,1:32:29.360
and so now you may need multiple layers

1:32:27.760,1:32:31.280
there's no reason again

1:32:29.360,1:32:32.480
to think that the architecture of the

1:32:31.280,1:32:35.600
decoder should be similar to the

1:32:32.480,1:32:35.600
architecture of the encoder

1:32:35.760,1:32:39.040
now there is you know there might be

1:32:37.840,1:32:41.120
they might

1:32:39.040,1:32:42.080
okay that said there might actually be

1:32:41.120,1:32:45.360
good reason for it

1:32:42.080,1:32:47.280
okay uh and in fact there are

1:32:45.360,1:32:49.679
models that uh i haven't talked about

1:32:47.280,1:32:52.080
because they're not kind of proven but

1:32:49.679,1:32:54.560
um which are called uh stacked

1:32:52.080,1:32:57.760
autoencoders where where basically

1:32:54.560,1:33:00.000
you have this idea so um you essentially

1:32:57.760,1:33:00.000
have

1:33:00.719,1:33:04.080
an auto encoder where you have a

1:33:02.800,1:33:06.560
reconstruction error

1:33:04.080,1:33:07.520
i'm actually going to erase this and

1:33:06.560,1:33:10.000
make it look like

1:33:07.520,1:33:11.360
the this you know the autoencoder that

1:33:10.000,1:33:13.360
we talked about where there is sort of a

1:33:11.360,1:33:14.239
cost for making the the latent variable

1:33:13.360,1:33:17.120
here

1:33:14.239,1:33:18.480
different from the output of the of the

1:33:17.120,1:33:23.199
encoder so this is

1:33:18.480,1:33:26.400
a z bar and this is a z if you want okay

1:33:23.199,1:33:28.880
now that's another encoder drawn in a in

1:33:26.400,1:33:31.840
a funny way so this is y

1:33:28.880,1:33:31.840
and this is y bar

1:33:32.000,1:33:40.960
at the bottom now i can stack

1:33:35.760,1:33:40.960
another one of those guys on top

1:33:45.520,1:33:48.880
okay now i'm going to have to call this

1:33:47.199,1:33:51.100
z1

1:33:48.880,1:33:54.489
and i'm going to call this z2

1:33:51.100,1:33:54.489
[Music]

1:34:02.840,1:34:05.840
etc

1:34:09.040,1:34:13.600
okay this i'm going to call y bar and

1:34:11.120,1:34:18.320
this i'm going to call y

1:34:13.600,1:34:19.840
now what's interesting about

1:34:18.320,1:34:23.040
okay i'm going to call the bottom one x

1:34:19.840,1:34:23.040
now change name

1:34:24.080,1:34:29.280
now if you took if you look at the

1:34:27.440,1:34:31.280
you ignore the right part of the system

1:34:29.280,1:34:33.120
look at the left part

1:34:31.280,1:34:35.760
you go to y and that looks very much

1:34:33.120,1:34:38.639
like a like a classical

1:34:35.760,1:34:40.080
recognizer where x is the input y bar is

1:34:38.639,1:34:41.760
a prediction for

1:34:40.080,1:34:42.719
the output and y is the desired output

1:34:41.760,1:34:45.679
and there's a cost function that

1:34:42.719,1:34:49.040
measures the difference between the two

1:34:45.679,1:34:50.320
okay uh the other branch that goes from

1:34:49.040,1:34:52.639
y to x

1:34:50.320,1:34:53.840
that's kind of like a decoder where y is

1:34:52.639,1:34:55.360
the code

1:34:53.840,1:34:57.040
but then you have codes all the way in

1:34:55.360,1:34:58.639
the middle because it's kind of a

1:34:57.040,1:35:00.159
stacked autoencoder right so

1:34:58.639,1:35:02.159
every pair of layer every pair of

1:35:00.159,1:35:03.040
encoder decoder is kind of a little auto

1:35:02.159,1:35:04.400
encoder

1:35:03.040,1:35:06.320
and you kind of stack them on top of

1:35:04.400,1:35:08.159
each other and what you'd like is

1:35:06.320,1:35:09.360
find a way of training the system in

1:35:08.159,1:35:11.119
such a way that

1:35:09.360,1:35:12.639
if you don't have a label for samples

1:35:11.119,1:35:15.119
you don't you don't know why

1:35:12.639,1:35:17.920
for the for example you just train this

1:35:15.119,1:35:20.239
as an auto encoder

1:35:17.920,1:35:21.119
but if you do have a y then you you you

1:35:20.239,1:35:23.840
clamp

1:35:21.119,1:35:25.360
uh y to its desired value and then this

1:35:23.840,1:35:28.880
system becomes now

1:35:25.360,1:35:31.840
a combination of predictor a recognizer

1:35:28.880,1:35:33.760
and an autoencoder

1:35:31.840,1:35:34.880
now there is a slight problem with this

1:35:33.760,1:35:36.800
picture this

1:35:34.880,1:35:39.679
a number of different problems the first

1:35:36.800,1:35:39.679
problem is that

1:35:40.000,1:35:44.800
if again if z1 for example has enough

1:35:43.199,1:35:46.880
capacity

1:35:44.800,1:35:49.280
uh and you only train on unlabeled

1:35:46.880,1:35:51.040
samples the system is only going to

1:35:49.280,1:35:52.480
carry the information through z1 and

1:35:51.040,1:35:53.360
it's going to just completely ignore the

1:35:52.480,1:35:54.639
top layers

1:35:53.360,1:35:56.880
because there's not enough capacity in

1:35:54.639,1:36:00.000
z1 to do the perfect reconstruction

1:35:56.880,1:36:01.760
so it's going to uh you know

1:36:00.000,1:36:03.600
just put all the information to z1 and

1:36:01.760,1:36:05.440
then all the others z2 and y will be

1:36:03.600,1:36:07.280
constant

1:36:05.440,1:36:09.119
because the system won't need won't need

1:36:07.280,1:36:11.360
them so again you will need to

1:36:09.119,1:36:15.040
regularize

1:36:11.360,1:36:18.400
to regularize z to prevent it from

1:36:15.040,1:36:18.400
capturing all the information

1:36:18.560,1:36:26.000
and same for the other layers perhaps

1:36:22.719,1:36:29.199
now the other thing is uh would

1:36:26.000,1:36:31.440
you know do this thing need to be

1:36:29.199,1:36:32.719
linear non-linear and that depends on

1:36:31.440,1:36:35.679
the relative size of the

1:36:32.719,1:36:36.960
various z's so if you go from low

1:36:35.679,1:36:40.480
dimension to high dimension

1:36:36.960,1:36:40.480
you need something that's non-linear

1:36:40.639,1:36:44.400
but if you go from high dimension to low

1:36:43.600,1:36:46.960
dimension

1:36:44.400,1:36:50.000
you can probably do it with a linear

1:36:46.960,1:36:50.000
kind of like sparse coding

1:36:50.960,1:36:55.520
um and so you will see that the system

1:36:53.920,1:36:58.639
may have

1:36:55.520,1:37:02.560
uh you know an alternation

1:36:58.639,1:37:05.440
of linear and nonlinear

1:37:02.560,1:37:06.480
stages in sort of opposite phase if you

1:37:05.440,1:37:09.520
want

1:37:06.480,1:37:12.080
because you need linear to go from high

1:37:09.520,1:37:13.440
dimension to low dimension

1:37:12.080,1:37:15.440
and then nonlinear to go from low

1:37:13.440,1:37:17.520
dimension to high dimension

1:37:15.440,1:37:19.840
and then again linear to go back to low

1:37:17.520,1:37:23.119
dimension

1:37:19.840,1:37:23.119
it's the opposite the other way around

1:37:23.920,1:37:27.679
this is you know people have been

1:37:26.480,1:37:29.119
proposing things like this but not

1:37:27.679,1:37:29.679
really sort of training them on a large

1:37:29.119,1:37:31.040
scale

1:37:29.679,1:37:33.040
so there's a lot of sort of open

1:37:31.040,1:37:35.679
questions around those things

1:37:33.040,1:37:36.159
uh one if you're curious one paper that

1:37:35.679,1:37:39.440
i've

1:37:36.159,1:37:43.040
i've worked on with my former student uh

1:37:39.440,1:37:43.040
called uh jake zhao

1:37:43.119,1:37:51.119
um is a system called

1:37:47.679,1:37:54.080
stacked what where

1:37:51.119,1:37:54.080
auto encoder

1:37:55.040,1:37:58.320
and it's a system a bit of this type but

1:37:56.719,1:37:59.840
there is sort of extra variables kind of

1:37:58.320,1:38:03.040
going

1:37:59.840,1:38:04.400
this way

1:38:03.040,1:38:05.920
which are basically the position of

1:38:04.400,1:38:07.280
switches in pooling i don't want to go

1:38:05.920,1:38:09.280
into details but

1:38:07.280,1:38:10.800
if you look for a paper about stack

1:38:09.280,1:38:12.560
where auto coders you'll find two paper

1:38:10.800,1:38:14.800
one by jake

1:38:12.560,1:38:17.360
and myself and a follower paper by a

1:38:14.800,1:38:20.960
group from michigan

1:38:17.360,1:38:23.440
university of michigan that

1:38:20.960,1:38:25.040
basically uh enhanced it and sort of

1:38:23.440,1:38:26.960
trained them on imagenet and got some

1:38:25.040,1:38:29.119
decent results so those are kind of

1:38:26.960,1:38:31.679
architectures you can use to do kind of

1:38:29.119,1:38:33.760
self-supervised learning

1:38:31.679,1:38:36.239
just to clarify the parameters for the

1:38:33.760,1:38:37.119
spring is for the kl divergence term in

1:38:36.239,1:38:39.360
the loss

1:38:37.119,1:38:40.800
right yeah so the killer versions uh

1:38:39.360,1:38:43.199
term in the

1:38:40.800,1:38:44.159
in the loss we're gonna see this

1:38:43.199,1:38:45.440
tomorrow guys so

1:38:44.159,1:38:47.280
uh we're gonna be going through the

1:38:45.440,1:38:49.360
equation and all these uh

1:38:47.280,1:38:51.600
details so it's right i covered this

1:38:49.360,1:38:52.239
tomorrow so i'll see you tomorrow

1:38:51.600,1:38:55.119
hopefully

1:38:52.239,1:38:56.800
with the video as well uh if the

1:38:55.119,1:38:58.800
bandwidth supports it

1:38:56.800,1:38:59.920
i will put this like this uh the

1:38:58.800,1:39:03.040
recording of this

1:38:59.920,1:39:04.159
uh class online as soon as is actually

1:39:03.040,1:39:06.800
available to me

1:39:04.159,1:39:07.760
i will add it to the nyu streaming

1:39:06.800,1:39:11.040
platform

1:39:07.760,1:39:11.280
and then you know i will try to clean it

1:39:11.040,1:39:13.600
up

1:39:11.280,1:39:15.199
as i can and you know upload it as well

1:39:13.600,1:39:18.400
on youtube later on

1:39:15.199,1:39:21.600
all right so thank you again um stay

1:39:18.400,1:39:27.760
home stay warm and i see you tomorrow

1:39:21.600,1:39:27.760
stay safe bye stay safe


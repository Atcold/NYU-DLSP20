---
lang: ru
lang-ref: ch.01
title: Неделя 1
translation-date: 25 Sep 2020
translator: Evgeniy Pak
---


<!-- ## Lecture part A -->

## Лекция, часть A

<!-- We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex. -->

Мы обсуждаем предпосылки к появлению глубокого обучения. Начинаем с истории и вдохновляющих идей глубокого обучения. Затем обсуждаем историю распознавания образов и знакомим с градиентным спуском и его вычислением методом обратного распространения ошибки. В конце мы говорим об иерархическом представлении зрительной коры.

<!-- ## Lecture part B -->

## Лекция, часть B

<!-- We first discuss the evolution of CNNs, from Fukushima to LeCun to AlexNet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations. -->

Сперва говорим о развитии свёрточных нейронных сетей (CNNs), от Фукусимы до Лекуна и AlexNet. Затем обсуждаем некоторые приложения CNN's, такие как сегментация изображений, автономные транспортные средства и анализ медицинских снимков. Рассуждаем об иерархической природе глубоких сетей и свойствах глубоких сетей, которые делают их полезными. В заключении говорится о генерации и изучении признаков/представлений.

<!-- ## Practicum -->

## Практикум

<!-- We discuss the motivation for applying transformations to data points visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to understand the function and effects of these transformations. We walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by neural networks. -->

Мы обсуждаем предпосылки применения преобразований к точкам данных, визуализированных в пространстве. Говорим о Линейной Алгебре и применении линейных и нелинейных преобразований. Обсуждаем применение визуализации для исследования функций и влияние этих преобразований. Разбираем примеры  в Jupyter Notebook и в заключение говорим о функциях, представленных нейронными сетями.

---
lang: Serbian
lang-ref: ch.01-3
title: Problem Motivation, Linear Algebra, and Visualization
lecturer: Alfredo Canziani
authors: Derek Yen, Tony Xu, Ben Stadnick, Prasanthi Gurumurthy
date: 28 Jan 2020
translation-date: 14 Dec 2020
translator: Ema Pajić
---


## Materijali

Zapratite Alfredo Canziani [na Twitter nalogu @alfcnz](https://twitter.com/alfcnz). Videi i beleške sa relevantnim detaljima o linearnoj algebri i SVD dekompozicija se mogu naći na Alfredovom tviteru, na primer pretragom `linear algebra (from:alfcnz)`.


## [Transformacije i motivacija](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=233s)

Kao motivišući primer, posmatrajmo klasifikaciju slika. Zamislite da smo napravili fotografiju kamerom sa rezolucijom 1 megapiksel. Ta slika će imati oko 1,000 piksela vertikalno i 1,000 piksela horizontalno i svaki piksel će imati 3 dimenzije za boje (RGB, crvena, zelena, plava). Svaka slika se može posmatrati kao jedna tačka u prostoru sa 3 miliona dimenzija. Sa tako velikom dimenzionalnošću, mnogo interesantnih slika koje želimo da klasifikujemo -- na primer pas *vs.* mačka -- će u suštini biti u istom regionu prostora.

Da bismo razdvojili te slike, razmatramo načine transformacije podataka. Prisetimo se da u 2D prostoru, linearna transformacija je isto što i množenje matrica. Na primer, sledeće transformacije su linearne:

-   Rotacija (kada je matrica ortonormalna).
-   Skaliranje (kada je matrica dijagonalna).
-   Refleksija (kada je determinanta negativna).
-   Odsecanje.

Treba uzeti u obzir da sama translacija nije linearna jer se 0 neće uvek mapirati u 0, ali jeste afina transformacija. Vratimo se na primer slike - možemo da transliramo tačke tako da su centrirane oko 0 i skaliramo dijagonalnom matricom tako da "uvećamo" taj region. Na kraju, možemo da uradimo klasifikaciju traženjem linija u prostoru koje razdvajaju različite tačke u njihove klase. Drugim rečima, ideja je da koristimo linearne i nelinearne trensformacije da mapiramo tačke u prostor u kome su linearno separabilne. Ova ideja će biti konkretnije opisane u narednim sekcijama.



## [Vizualizacija podataka - razdvajanje tačaka bojom koristeći neuronsku mrežu](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=798s)

U našim vizualizacijama, imamo 5 grana spirale i svaka grana je druge boje. Tačke su u dvodimenzionalnom prostoru i mogu da se reprezentuju kao tuple. Boja predstavlja treću dimenziju - klasu kojoj tačke pripadaju. Upotrebićemo neuronsku mrežu da razdvojimo tačke po boji.

| <center><img src="{{site.baseurl}}/images/week01/01-3/Spiral1.png" width="200px"/></center> | <center><img src="{{site.baseurl}}/images/week01/01-3/Spiral2.png" width="200px"/></center> |
|             (a) Ulazne tačke, pre mreže             |            (b) Izlazne tačke, nakon mreže             |

<center> Figure 1: Spirala sa 5 boja </center>
Mreža \"rasteže\" prostor u cilju da razdvoji tačke koje pripadaju različitim potprostorima. Kada iskonvergira, mreža razdvaja svaku od boja u različit potprostor konačnog prostora. Drugim rečima, svaka od boja u novom prostoru će biti linearno separabilna koristeći "1 protiv svih" regresiju. Vektori na dijagramu mogu da se predstave 5x2 matricom. Ta matrica se može pomnožiti svakom od tačaka i vratiće rezultate za svaku od 5 boja. Svaka od tačaka onda može biti klasifikovana po boji pomoću tih rezultata. Ovde je dimenzija izlaza 5, po jedna za svaku boju, a dimenzija ulaza je 2, po jedna za x i y koordinate tačaka. Da rezimiramo, ova mreža u suštini kreće od nekog prostora i vrši transformacije njega, parametrizovano sa nekoliko matrica a zatim nelinearnostima.


### Arhitektura neuronske mreže

<center>
<img src="{{site.baseurl}}/images/week01/01-3/Network.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
Figure 2: Arhitektura neuronske mreže
</center>

Prva matrica mapira dvodimenzionalni ulaz u 100-dimenzioni pomoćni skriveni sloj. Zatim imamo nelinearan sloj, `ReLU` (Rectified Linear Unit), koja je jednostavno *pozitivan deo* $(\cdot)^+$. Dalje, da bismo prikazali sliku, imamo embedding sloj koji mapira 100-dimenzioni skriveni sloj u dvodimenzioni izlaz. Na kraju, embedding sloj se projektuje na finalni, petodimenzioni sloj mreže koji predstavlja rezultat za svaku od boja.


## [Nasumične projekcije - Jupyter Notebook](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=1693s)

Jupyter Notebook se može naći [ovde](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/02-space_stretching.ipynb). Da bi se sveska pokrenula, proveriti da li je `pDL` okruženje instalirano kao što je specificirano u [`README.md`](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/docs/sr/README-SR.md).


### PyTorch `device`

PyTorch može da pokreće kod i na CPU i GPU računara. CPU je koristan za sekvencijalne zadatke, dok je GPU koristan za paralelne zadatke. Pre pokretanja na željenom uređaju, prvo moramo da budemo sigurni da su se tenzori i modeli prebacili na memoriju uređaja. To se može uraditi sledećim linijama koda:

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X = torch.randn(n_points, 2).to(device)
```

Prva linija kreira promenljivu `device`, kojoj se dodeljuje GPU ako je dostupan, a u suprotnom CPU. U sledećoj liniji se kreira tenzor i šalje na memoriju uređaja pozivom `.to(device)`.


### Jupyter Notebook savet

Za gledanje dokumentacije za funkciju u ćeliji sveske, kliknuti `Shift + Tab.`


### Vizualizacija linearnih transformacija

Prisetimo se da se linearna transformacija može predstaviti matricom. Korišćenjem SVD dekompozicije, možemo da rastavimo tu matricu na 3 komponente matrice, tako da svaka predstavlja različitu linearnu transformaciju.


$$
W = U\begin{bmatrix}s_1 & 0 \\ 0 & s_2 \end{bmatrix} V^\top
$$

U jednačini (1), matrice $U$ i $V^\top$ su ortogonalne i predstavljaju transformacije rotacije i refleksije. Srednja matrica je dijagonalna i predstavlja transformaciju skaliranja.

Vizualizujemo linearne transformacije nekoliko nasumičnih matrica na Fig. 3. Obratiti pažnju na efekte singularnih vrednosti na rezultujuću transformaciju.

Matrice su generisane koristeći Numpy, ali takođe možemo da koristimo i PyTorch `nn.Linear` klasu sa `bias = False` opcijom da kreiramo linearne transformacije.

| ![]({{site.baseurl}}/images/week01/01-3/initial_scatter_lab1.png) | ![]({{site.baseurl}}/images/week01/01-3/matrix_multiplication_lab1.png) | ![]({{site.baseurl}}/images/week01/01-3/matrix_multiplication_lab1_2.png) |
|     (a) Originalne tačke       |   (b) $s_1$ = 1.540, $s_2$ = 0.304  |   (c) $s_1$ = 0.464, $s_2$ = 0.017    |

<center> Figure 3:  Linearne transformacije nasumičnih matrica </center>


### Nelinearne transformacije

Dalje vizualizujemo sledeću transformaciju:

$$
f(x) = \tanh\bigg(\begin{bmatrix} s & 0 \\ 0 & s \end{bmatrix} \bigg)
$$

Prisetimo se, grafik $\tanh(\cdot)$ sa Fig. 4.

<center>
<img src="{{site.baseurl}}/images/week01/01-3/tanh_lab1.png" width="250px" /><br>
Figure 4: Hiperbolički tangens
</center>

Efekat ove nelinearnosti je da ograniči tačke između $-1$ and $+1$, kreirajući kvadrat. Kako se vrednost $s$ u jednačini (2) povećava, sve više tačakau bivaju odgurnute na ivice kvadrata. To je pokazano na Fig. 5. Gurajući što više tačaka na ivice, povećava se rastojanje između njih i možemo da pokušamo da ih klasifikujemo.

| <img src="{{site.baseurl}}/images/week01/01-3/matrix_multiplication_with_nonlinearity_s=1_lab1.png" width="200px" /> | <img src="{{site.baseurl}}/images/week01/01-3/matrix_multiplication_with_nonlinearity_s=5_lab1.png" width="200px" /> |
|                 (a) Nelinearnost sa $s=1$                 |                 (b) Nelinearnost sa $s=5$                  |

<center> Figure 5:   Nelinearne transformacije </center>


### Nasumična neuronska mreža

Na kraju, vizualizujemo transformaciju dobijenu jednostavnom, neobučenom neuronskom mrežom. Ova mreža ima linearni sloj, koji izvršava afinu transformaciju, iza kog sledi hiperbolički tangens nelinearnost i na kraju još jedan linearni sloj. Proučavanjem transformacije sa Fig. 6, vidimo da je različita od linearnih i nelinearnih transformacija koje smo videli ranije. U nastavku ćemo videti kako da napravimo transformacije koje vrši neuronska mreža korisne za naš krajnji cilj klasifikacije.


<center>
<img src="{{site.baseurl}}/images/week01/01-3/untrained_nn_transformation_lab1.png" width="200px" /><br>
Figure 6:  Transformacija dobijena neobučenom neuronskom mrežom
</center>

---
lang: sr
lang-ref: ch.01-3
title: Motivacija, Linearna algebra i vizualizacija
lecturer: Alfredo Canziani
authors: Derek Yen, Tony Xu, Ben Stadnick, Prasanthi Gurumurthy
date: 28 Jan 2020
translation-date: 14 Dec 2020
translator: Ema Pajić
---

<!-- ## Resources
-->
## Materijali

<!-- Please follow Alfredo Canziani [on Twitter @alfcnz](https://twitter.com/alfcnz). Videos and textbooks with relevant details on linear algebra and singular value decomposition (SVD) can be found by searching Alfredo's Twitter, for example type `linear algebra (from:alfcnz)` in the search box.
-->
Zapratite Alfredo Canziani [na Twitter nalogu @alfcnz](https://twitter.com/alfcnz). Videi i beleške sa relevantnim detaljima o linearnoj algebri i SVD dekompozicija se mogu naći na Alfredovom tviteru, na primer pretragom `linear algebra (from:alfcnz)`.

<!--
## [Transformations and motivation](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=233s)
-->
## [Transformacije i motivacija](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=233s)

<!-- As a motivating example, let us consider image classification. Suppose we take a picture with a 1 megapixel camera. This image will have about 1,000 pixels vertically and 1,000 pixels horizontally, and each pixel will have three colour dimensions for red, green, and blue (RGB). Each particular image can then be considered as one point in a 3 million-dimensional space. With such massive dimensionality, many interesting images we might want to classify -- such as a dog *vs.* a cat -- will essentially be in the same region of the space.
-->
Kao motivišući primer, posmatrajmo klasifikaciju slika. Zamislite da smo napravili fotografiju kamerom sa rezolucijom 1 megapiksel. Ta slika će imati oko 1,000 piksela vertikalno i 1,000 piksela horizontalno i svaki piksel će imati 3 dimenzije za boje (RGB, crvena, zelena, plava). Svaka slika se može posmatrati kao jedna tačka u prostoru sa 3 miliona dimenzija. Sa tako velikom dimenzionalnošću, mnogo interesantnih slika koje želimo da klasifikujemo -- na primer pas *vs.* mačka -- će u suštini biti u istom regionu prostora.

<!-- In order to effectively separate these images, we consider ways of transforming the data in order to move the points. Recall that in 2-D space, a linear transformation is the same as matrix multiplication. For example, the following are linear transformations:
-   Rotation (when the matrix is orthonormal).
-   Scaling (when the matrix is diagonal).
-   Reflection (when the determinant is negative).
-   Shearing.
-->
Da bismo razdvojili te slike, razmatramo načine transformacije podataka. Prisetimo se da u 2D prostoru, linearna transformacija je isto što i množenje matrica. Na primer, sledeće transformacije su linearne:

-   Rotacija (kada je matrica ortonormalna).
-   Skaliranje (kada je matrica dijagonalna).
-   Refleksija (kada je determinanta negativna).
-   Odsecanje.

<!-- Note that translation alone is not linear since 0 will not always be mapped to 0, but it is an affine transformation. Returning to our image example, we can transform the data points by translating such that the points are clustered around 0 and scaling with a diagonal matrix such that we "zoom in" to that region. Finally, we can do classification by finding lines across the space which separate the different points into their respective classes. In other words, the idea is to use linear and nonlinear transformations to map the points into a space such that they are linearly separable. This idea will be made more concrete in the following sections.
--> 
Treba uzeti u obzir da sama translacija nije linearna jer se 0 neće uvek mapirati u 0, ali jeste afina transformacija. Vratimo se na primer slike - možemo da transliramo tačke tako da su centrirane oko 0 i skaliramo dijagonalnom matricom tako da "uvećamo" taj region. Na kraju, možemo da uradimo klasifikaciju traženjem linija u prostoru koje razdvajaju različite tačke u njihove klase. Drugim rečima, ideja je da koristimo linearne i nelinearne trensformacije da mapiramo tačke u prostor u kome su linearno separabilne. Ova ideja će biti konkretnije opisane u narednim sekcijama.


<!-- ## [Data visualization - separating points by colour using a network](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=798s) 
-->
## [Vizualizacija podataka - razdvajanje tačaka bojom koristeći neuronsku mrežu](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=798s)

<!-- In our visualization, we have five branches of a spiral, with each branch corresponding to a different colour. The points live in a two dimensional plane and can be represented as a tuple; the colour represents a third dimension which can be thought of as the different classes for each of the points. We then use the network to separate each of the points by colour.
-->
U našim vizualizacijama, imamo 5 grana spirale i svaka grana je druge boje. Tačke su u dvodimenzionalnom prostoru i mogu da se reprezentuju kao (x,y) parovi. Boja predstavlja treću dimenziju - klasu kojoj tačke pripadaju. Upotrebićemo neuronsku mrežu da razdvojimo tačke po boji.

<!-- | <center><img src="{{site.baseurl}}/images/week01/01-3/Spiral1.png" width="200px"/></center> | <center><img src="{{site.baseurl}}/images/week01/01-3/Spiral2.png" width="200px"/></center> |
|             (a) Input points, pre-network             |            (b) Output points, post-network 
-->
| <center><img src="{{site.baseurl}}/images/week01/01-3/Spiral1.png" width="200px"/></center> | <center><img src="{{site.baseurl}}/images/week01/01-3/Spiral2.png" width="200px"/></center> |
|             (a) Ulazne tačke, pre mreže             |            (b) Izlazne tačke, nakon mreže             |

<!-- <center> Figure 1: Five colour spiral </center>
-->
<center> Figure 1: Spirala sa 5 boja </center>
<!-- The network \"stretches\" the space fabric in order to separate each of the points into different subspaces. At convergence, the network separates each of the colours into different subspaces of the final manifold. In other words, each of the colours in this new space will be linearly separable using a one vs all regression. The vectors in the diagram can be represented by a five by two matrix; this matrix can be multiplied to each point to return scores for each of the five colours. Each of the points can then be classified by colour using their respective scores. Here, the output dimension is five, one for each of the colours, and the input dimension is two, one for the x and y coordinates of each of the points. To recap, this network basically takes the space fabric and performs a space transformation parametrised by several matrices and then by non-linearities.
-->
Mreža \"rasteže\" prostor u cilju da razdvoji tačke koje pripadaju različitim potprostorima. Kada iskonvergira, mreža razdvaja svaku od boja u različit potprostor konačnog prostora. Drugim rečima, svaka od boja u novom prostoru će biti linearno separabilna koristeći "1 protiv svih" regresiju. Vektori na dijagramu mogu da se predstave 5x2 matricom. Ta matrica se može pomnožiti svakom od tačaka i vratiće rezultate za svaku od 5 boja. Svaka od tačaka onda može biti klasifikovana po boji pomoću tih rezultata. Ovde je dimenzija izlaza 5, po jedna za svaku boju, a dimenzija ulaza je 2, po jedna za x i y koordinate tačaka. Da rezimiramo, ova mreža u suštini kreće od nekog prostora i vrši transformacije njega, parametrizovano sa nekoliko matrica a zatim nelinearnostima.

<!-- ### Network architecture
-->
### Arhitektura neuronske mreže

<!-- <center>
<img src="{{site.baseurl}}/images/week01/01-3/Network.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
Figure 2: Network Architecture
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week01/01-3/Network.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
Figure 2: Arhitektura neuronske mreže
</center>

<!-- The first matrix maps the two dimensional input to a 100 dimensional intermediate hidden layer. We then have a non-linear layer, `ReLU` or Rectified Linear Unit, which is simply *positive part* $(\cdot)^+$ function. Next, to display our image in a graphical representation, we include an embedding layer that maps the 100 dimensional hidden layer input to a two-dimensional output. Lastly, the embedding layer is projected to the final, five-dimensional layer of the network, representing a score for each colour.
-->
Prva matrica mapira dvodimenzionalni ulaz u 100-dimenzioni pomoćni skriveni sloj. Zatim imamo nelinearan sloj, `ReLU` (Rectified Linear Unit), koja je jednostavno *pozitivan deo* $(\cdot)^+$. Dalje, da bismo prikazali sliku, imamo embedding sloj koji mapira 100-dimenzioni skriveni sloj u dvodimenzioni izlaz. Na kraju, embedding sloj se projektuje na finalni, petodimenzioni sloj mreže koji predstavlja rezultat za svaku od boja.

<!-- ## [Random projections - Jupyter Notebook](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=1693s)
-->
## [Nasumične projekcije - Jupyter Notebook](https://www.youtube.com/watch?v=5_qrxVq1kvc&t=1693s)

<!-- The Jupyter Notebook can be found [here](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/02-space_stretching.ipynb). In order to run the notebook, make sure you have the `pDL` environment installed as specified in [`README.md`](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/README.md).
-->
Jupyter Notebook se može naći [ovde](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/02-space_stretching.ipynb). Da bi se sveska pokrenula, proveriti da li je `pDL` okruženje instalirano kao što je specificirano u [`README.md`](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/docs/sr/README-SR.md).


### PyTorch `device`

<!-- PyTorch can run on both the CPU and GPU of a computer. The CPU is useful for sequential tasks, while the GPU is useful for parallel tasks. Before executing on our desired device, we first have to make sure our tensors and models are transferred to the device's memory. This can be done with the following two lines of code:
-->
PyTorch može da pokreće kod i na CPU i GPU računara. CPU je koristan za sekvencijalne zadatke, dok je GPU koristan za paralelne zadatke. Pre pokretanja na željenom uređaju, prvo moramo da budemo sigurni da su se tenzori i modeli prebacili na memoriju uređaja. To se može uraditi sledećim linijama koda:

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
X = torch.randn(n_points, 2).to(device)
```

<!-- The first line creates a variable, called `device`, that is assigned to the GPU if one is available; otherwise, it defaults to the CPU. In the next line, a tensor is created and sent to the device's memory by calling `.to(device)`.
-->
Prva linija kreira promenljivu `device`, kojoj se dodeljuje GPU ako je dostupan, a u suprotnom CPU. U sledećoj liniji se kreira tenzor i šalje na memoriju uređaja pozivom `.to(device)`.

<!-- ### Jupyter Notebook tip
-->
### Jupyter Notebook savet

<!-- To see the documentation for a function in a notebook cell, use `Shift + Tab.`
-->
Za gledanje dokumentacije za funkciju u ćeliji sveske, kliknuti `Shift + Tab.`

<!-- ### Visualizing linear transformations 
-->
### Vizualizacija linearnih transformacija

<!-- Recall that a linear transformation can be represented as a matrix. Using singular value decomposition, we can decompose this matrix into three component matrices, each representing a different linear transformation.
-->
Prisetimo se da se linearna transformacija može predstaviti matricom. Korišćenjem SVD dekompozicije, možemo da rastavimo tu matricu na 3 komponente matrice, tako da svaka predstavlja različitu linearnu transformaciju.


$$
W = U\begin{bmatrix}s_1 & 0 \\ 0 & s_2 \end{bmatrix} V^\top
$$

<!-- In eq. (1), matrices $U$ and $V^\top$ are orthogonal and represent rotation and reflection transformations. The middle matrix is diagonal and represents a scaling transformation.
-->
U jednačini (1), matrice $U$ i $V^\top$ su ortogonalne i predstavljaju transformacije rotacije i refleksije. Srednja matrica je dijagonalna i predstavlja transformaciju skaliranja.

<!-- We visualize the linear transformations of several random matrices in Fig. 3. Note the effect of the singular values on the resulting transformations.
-->
Vizualizujemo linearne transformacije nekoliko nasumičnih matrica na Fig. 3. Obratiti pažnju na efekte singularnih vrednosti na rezultujuću transformaciju.

<!-- The matrices used were generated with Numpy; however, we can also use PyTorch's `nn.Linear` class with `bias = False` to create linear transformations.
-->
Matrice su generisane koristeći Numpy, ali takođe možemo da koristimo i PyTorch `nn.Linear` klasu sa `bias = False` opcijom da kreiramo linearne transformacije.

<!-- | ![]({{site.baseurl}}/images/week01/01-3/initial_scatter_lab1.png) | ![]({{site.baseurl}}/images/week01/01-3/matrix_multiplication_lab1.png) | ![]({{site.baseurl}}/images/week01/01-3/matrix_multiplication_lab1_2.png) |
|     (a) Original points       |   (b) $s_1$ = 1.540, $s_2$ = 0.304  |   (c) $s_1$ = 0.464, $s_2$ = 0.017    |
-->
| ![]({{site.baseurl}}/images/week01/01-3/initial_scatter_lab1.png) | ![]({{site.baseurl}}/images/week01/01-3/matrix_multiplication_lab1.png) | ![]({{site.baseurl}}/images/week01/01-3/matrix_multiplication_lab1_2.png) |
|     (a) Originalne tačke       |   (b) $s_1$ = 1.540, $s_2$ = 0.304  |   (c) $s_1$ = 0.464, $s_2$ = 0.017    |

<!--  <center> Figure 3:  Linear transformations from random matrices </center>
-->
<center> Figure 3:  Linearne transformacije nasumičnih matrica </center>

<!-- ### Non-linear transformations
-->
### Nelinearne transformacije

<!-- Next, we visualize the following transformation:
-->
Dalje vizualizujemo sledeću transformaciju:

$$
f(x) = \tanh\bigg(\begin{bmatrix} s & 0 \\ 0 & s \end{bmatrix} \bigg)
$$

<!-- Recall, the graph of $\tanh(\cdot)$ in Fig. 4.
-->
Prisetimo se, grafik $\tanh(\cdot)$ sa Fig. 4.

<!-- <center>
<img src="{{site.baseurl}}/images/week01/01-3/tanh_lab1.png" width="250px" /><br>
Figure 4: hyperbolic tangent non-linearity
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week01/01-3/tanh_lab1.png" width="250px" /><br>
Figure 4: Hiperbolički tangens
</center>

<!-- The effect of this non-linearity is to bound points between $-1$ and $+1$, creating a square. As the value of $s$ in eq. (2) increases, more and more points are pushed to the edge of the square. This is shown in Fig. 5. By forcing more points to the edge, we spread them out more and can then attempt to classify them.
-->
Efekat ove nelinearnosti je da ograniči tačke između $-1$ and $+1$, kreirajući kvadrat. Kako se vrednost $s$ u jednačini (2) povećava, sve više tačakau bivaju odgurnute na ivice kvadrata. To je pokazano na Fig. 5. Gurajući što više tačaka na ivice, povećava se rastojanje između njih i možemo da pokušamo da ih klasifikujemo.

<!-- | <img src="{{site.baseurl}}/images/week01/01-3/matrix_multiplication_with_nonlinearity_s=1_lab1.png" width="200px" /> | <img src="{{site.baseurl}}/images/week01/01-3/matrix_multiplication_with_nonlinearity_s=5_lab1.png" width="200px" /> |
|                 (a) Non-linearity with $s=1$                 |                 (b) Nonlinearity with $s=5$                  |
<center> Figure 5:   Non-linear Transformations </center>
-->
| <img src="{{site.baseurl}}/images/week01/01-3/matrix_multiplication_with_nonlinearity_s=1_lab1.png" width="200px" /> | <img src="{{site.baseurl}}/images/week01/01-3/matrix_multiplication_with_nonlinearity_s=5_lab1.png" width="200px" /> |
|                 (a) Nelinearnost sa $s=1$                 |                 (b) Nelinearnost sa $s=5$                  |

<center> Figure 5:   Nelinearne transformacije </center>

<!-- ### Random neural net 
-->
### Nasumična neuronska mreža

<!-- Lastly, we visualize the transformation performed by a simple, untrained neural network. The network consists of a linear layer, which performs an affine transformation, followed by a hyperbolic tangent non-linearity, and finally another linear layer. Examining the transformation in Fig. 6, we see that it is unlike the linear and non-linear transformations seen earlier. Going forward, we will see how to make these transformations performed by neural networks useful for our end goal of classification.
-->
Na kraju, vizualizujemo transformaciju dobijenu jednostavnom, neobučenom neuronskom mrežom. Ova mreža ima linearni sloj, koji izvršava afinu transformaciju, iza kog sledi hiperbolički tangens nelinearnost i na kraju još jedan linearni sloj. Proučavanjem transformacije sa Fig. 6, vidimo da je različita od linearnih i nelinearnih transformacija koje smo videli ranije. U nastavku ćemo videti kako da napravimo transformacije koje vrši neuronska mreža korisne za naš krajnji cilj klasifikacije.

<!-- <center>
<img src="{{site.baseurl}}/images/week01/01-3/untrained_nn_transformation_lab1.png" width="200px" /><br>
Figure 6:  Transformation from an untrained neural network
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week01/01-3/untrained_nn_transformation_lab1.png" width="200px" /><br>
Figure 6:  Transformacija dobijena neobučenom neuronskom mrežom
</center>

---
lang: sr
lang-ref: ch.01-1
lecturer: Yann LeCun
title: Motivacija iza dubokog učenja, istorija i inspiracija
authors: Yunya Wang, SunJoo Park, Mark Estudillo, Justin Mae
date: 27 Jan 2020
translation-date: 13 Dec 2020
translator: Ema Pajić
---


## [Plan kursa](https://www.youtube.com/watch?v=0bMe_vCZo30&t=217s)

- Osnove nadgledanog učenja, Neuronske mreže, Duboko učenje
- Propagacija unazad i komponente arhitekture
- Konvolucione neuronske mreže i njihove primene
- Više o arhitekturama dubokih neuronskih mreža
- Trikovi regularizacije / trikovi optimizacije / Razumevanje kako radi duboko učenje
- Modeli zasnovani na energiji
- Samonadgledano učenje i više


## Inspiracija za duboko učenje i njegova istorija

Na konceptualnom nivou, duboko učenje je inspirisano mozgom, ali su neki detalji zanemareni. Kao poređenje, avioni su inspirisani pticama - princip letenja je isti, ali su detalji veoma različiti.

Istorija dubokog učenja počinje sa oblašću nauke koja se trenutno zove kibernetika 1940-ih sa McCulloch-om i Pitts-om. Oni su došli na ideju da su neuroni jedinice koje mogu da budu u uključenm ili isključenom stanju i da postoji neki prag kada menjaju stanje. Moglo je da se napravi Bulovo kolo (matematički model za kombinatorna digitalna logička kola) povezivanjem neurona i da se donose neki zaključci preko njih. Mozak je predstavljen kao mašina koja donosi zaključke preko neurona koji su binarni. Neuroni računaju težinsku sumu ulaza i porede je sa svojim pragom. Uključuju se ako je suma veća od praga, a isključuju ako je manja, što je uprošćen pogled na to kako mozak radi.

1947., Donald Hebb je došao na ideju da mozak uči promenama jačine veza između neurona. To je nazvano hiper učenje: ako dva neurona ispale impuls zajedno, veza između njih se pojačava, u suprotnom slabi.

Kasnije u toku 1948., Norbert Wiener je predložio kibernetiku, što je ideja da postoje sistemi koji imaju senzore i aktuatore, povratnu spregu i samoregulišući sistem. Pravila povratnog mehanizma automobila dolaze iz ovog rada.

1957., Frank Rosenblatt je smislio perceptron, algoritam koji modifikuje težine veoma jednostavnih neuronskih mreža.

Ideja da se naprave inteligentne mašine simulacijom velikog broja neurona nastala je 1940-ih, razvijala se tokom 1950-ih, i komplentno je napuštena krajem 1960-ih. Glavni razlozi za napuštanje ove ideje 1960-ih su:

- Naučnici su koristili neurone koji su binarni. Međutim, da bi propagacija unazad radila, aktivacione funkcije su morale da budu kontinualne. U to vreme, naučnici nisu došli do ideje da upotrebe kontinualne neurone i nisu mislili da je moguće da se obučava preko gradijenata jer binarni neuroni nisu diferencijabilni.
- Kod kontinualnih neurona, aktivaciju neurona treba pomnožiti težinom da bi se dobio doprinos težinskoj sumi. Međutim, pre 1980-te, množenje dva broja, pogotovu decimalnih brojeva je bilo veoma sporo. To je bio još jedan razlog da se izbegne upotreba kontinualnih neurona.

Duboko učenje je nastavilo da se razvija 1985. sa pojavom propagacije unazad. 1995., oblast je ponovo pala u zaborav i napuštena je ideja neuronskih mreža. Početkom 2010-ih, neuronske mreže su upotrebljene u prepoznavanju govora i postigle veliko poboljšanje performansi, a zatim se upotreba veoma reširila. 2013-te, računarska vizija je počela da većinski koristi neuronske mreže. 2016-te, isto se desilo i sa obradom prirodnih jezika. Uskoro će se slične revolucije desiti i u robotici, automatskom upravljanju i mnogim drugim oblastima.

### Nadgledano učenje

$90\%$ primena dubokog učenja koriste nadgledano učenje. Nadgledano učenje je proces gde se mreži da veliki broj parova ulaza i izlaza iz kojih treba da nauči kako da za novi dati ulaz predvidi tačan izlaz. U procesu obučavanja, kada je izlaz tačan, ne radi se ništa. Ako je izlaz netačan, malo se promene parametri mreže i ispravi izlaz ka onome koji želimo. Trik je znati u kom smeru i koliko promeniti parametre - i to nas dovodi do računanja gradijenata i propagacije unazad.

Nadgledano učenje potiče od perceptrona i Adaline. Adaline mreža je bazirana na istoj arhitekturi sa težinskim ulazima - iznad praga se uključuje, a ispod isključuje. Perceptron je neuronska mreža sa 2 sloja. Drugi sloj se obučava, a prvi sloj je fiksiran. Većinu vremena, prvi sloj je određen nasumično i zove se asocijativni sloj.

## [Istorija prepoznavanja oblika i uvod u gradijentni spust](https://www.youtube.com/watch?v=0bMe_vCZo30&t=1461s)

U nastavku su opisane konceptualne osnove prepoznavanja oblika pre razvoja dubokog učenja. Standardan model za prepoznavanje oblika ima deo koji izvlači obeležja i klasifikator koji se obučava. Ulazni podaci prvo idu na izvlačenje obeležja, gde se izvlače relevantne korisne karakteristike ulaza kao na primer detektovanje oka ako je cilj prepoznati lice. Nakon toga, vektor obeležja se prosleđuje klasifikatoru koji računa težinske sume i poredi sa pragom. Klasifikator koji obučavamo može da bude perceptron ili neuronska mreža. Problem je što je odluku koja obeležja izvlačimo mora da napravi čovek. To znači da se prepoznavanje oblika / računarska vizija fokusira na izvlačenje odlika i njegov dizajn za specifičan problem, a klasifikatoru se nije posvećivalo puno vremena.

Nakon pojave i razvoja dubokog učenja, umesto ova 2 dela počela je da se koristi sekvenca modula. Svaki modul ima podesive parametre i nelinearnost. Tako naređani moduli čine više slojeva i zato se ova oblast i zove duboko učenje. Razlog za korišćenje nelinearnosti umesto linearnosti je to što dva linearna sloja mogu da budu jedan linearan sloj jer je kompozicija dva linearna sloja linearna.

Najjednostavnija višeslojna arhitektura sa podesivim parametrima i nelinearnošću može da bude: ulaz (recimo slika ili audio) je predstavljen kao vektor. Ulaz se pomnoži sa matricom težina čiji koeficijenti su podesivi. Zatim, svaka komponenta vektora rezultata se prosledi nelinearnoj funkciji kao što je ReLU. Ponavljajući taj proces, dobijamo običnu neuronsku mrežu. Razlog zašto se zove neuronska mreža je to što ova arhitektura računa težinsku sumu komponenti ulaza sa odgovarajućim redovima matrice.

Da se vratimo na poentu nadgledanog učena, poredimo izlaz koji vraća neuronska mreža sa ciljanim izlazima i optimizujemo funkciju gubitka, koja računa rastojanje / kaznu / divergenciju između dobijenog i ciljanog rezultata. Zatim, usrednjujemo ovu funkciju cene po obučavajućem skupu podataka. To je vrednost koju želimo da minimizujemo. Drugim rečima, želimo da nađemo vrednosti parametara koje minimizuju prosečnu grešku na obučavajućem skupu.

Način kako da nađemo željene parametre je računanjem gradijenata. Na primer, ako zamislimo da smo se izgubili na planini u maglovitoj noći i želimo da se spustimo do sela koje se nalazi u uvali, jedan način bi bio da se okrenemo oko sebe i pronađemo najstrmiji put dole i zakoračimo u tom smeru. Taj smer je (negativni) gradijent. Sa pretpostavkom da je uvala konveksna, mogli bismo da stignemo do sela.

Efikasniji način se zove stohastički gradijentni spust (SGD). Pošto želimo da minimizujemo prosečni gubitak na obučavajućem skupu, uzmemo jedan odbirak ili malu grupu odbiraka i izračunamo grešku, zatim primenimo gradijentni spust. Zatim uzmemo novi odbirak i dobijemo novu vrednost za grešku, zatim gradijent koji je obično u drugom smeru. Dva glavna razloga za korišćenje stohastičkog gradijentnog spusta su to što pomaže modelu da brže konvergra empirijski ako je obučavajući skup veoma veliki i omogućava bolju generalizaciju, što znači dobijanje sličnih performansi na različitim skupovima podataka.

### [Računanje gradijenata propagacijom unazad](https://www.youtube.com/watch?v=0bMe_vCZo30&t=2336s)

Računanje gradijenata propagacijom unazad je praktična primena lančanog pravila. Jednačina propagacije unazad za ulazne gradijente je:

$$
\begin{aligned}
\frac{\partial C}{\partial \boldsymbol{x}_{i - 1}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial \boldsymbol{x}_i}{\partial \boldsymbol{x}_{i - 1}} \\
\frac{\partial C}{\partial \boldsymbol{x}_{i - 1}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial f_i(\boldsymbol{x}_{i - 1}, \boldsymbol{w}_i)}{\partial \boldsymbol{x}_{i - 1}}
\end{aligned}
$$

Jednačina propagacije unazad za težinske gradijente je:

$$
\begin{aligned}
\frac{\partial C}{\partial \boldsymbol{w}_{i}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial \boldsymbol{x}_i}{\partial \boldsymbol{w}_{i}} \\
\frac{\partial C}{\partial \boldsymbol{w}_{i}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial f_i(\boldsymbol{x}_{i - 1}, \boldsymbol{w}_i)}{\partial \boldsymbol{w}_{i}}
\end{aligned}
$$

Napomena: umesto skalarnih ulaza, ulazi će biti vektori - uopštenije, multidimenzionalni ulazi. Propagacija unazad omogućava računanje izvoda razlike izlaza koji želimo i koji smo dobili (funkcije gubitka) po bilo kojoj vrednosti u mreži. Na kraju, propagacija unazad je neophodna jer se primenjuje na više slojeva.

Bitno je razmotriti kako da se interpretiraju ulazi. Na primer, slika 256$$\times$$256 bi zahtevala matricu sa 200,000 vrednosti. To bi bile ogromne matrice koje bi mreža morala da koristi i bilo bi nepraktično koristiti ih. Iz tog razloga, bitno je pretpostaviti strukturu matrice.

## Hijerarhijska reprezentacija vizuelnog korteksa

Eksperimenti koje je radio Fukušima doveli su do razumevanja kako mozak interpretira ono što oči vide. Ukratko, otkriveno je da neuroni na početku retine kompresuju ulaz (ovo je poznato kao normalizacija kontrasta) i signal putuje od naših očiju do mozga. Zatim, slika se procesira u fazama i određeni neuroni se aktiviraju za određene kategorije. Dakle, vizuelni korteks radi prepoznavanje oblika hijerarhijski.

Eksperimenti u kojima su naučnici postavljali elektrode u specifične regije vizuelnog korteksa, specifično V1 regija, doveli su do zaključka da određeni neuroni reaguju na motive koji se pojavljuju u veoma maloj površini vidnog polja i da susedni neuroni vide bliske delove vidnog polja.
Dodatno, neuroni koji reaguju na isti deo vidnog polja, reaguju na različite tipove ivica (na primer, vertikalne ili horizontalne ivice). Takođe, u nauci je većinom prihvaćena ideja da je vizuelni proces direktan, unapred propagiran proces. Dakle, donekle brzo prepoznavanje se može uraditi bez rekurentnih konekcija.


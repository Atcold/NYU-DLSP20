---
lang-ref: ch.01-1
predavač: Yann LeCun
naslov: Motivacija iza dubokog učenja, istorija i inspiracija
autori: Yunya Wang, SunJoo Park, Mark Estudillo, Justin Mae
datum: 27 Jan 2020
---


## [Plan kursa](https://www.youtube.com/watch?v=0bMe_vCZo30&t=217s)

- Osnove nadgledanog učenja, Neuronske mreže, Duboko učenje
- Propagacija unazad i komponente arhitekture
- Konvolucione neuronske mreže i njihove primene
- Više o arhitekturama dubokih neuronskih mreža
- Trikovi regularizacije / trikovi optimizacije / Razumevanje kako radi duboko učenje
- Modeli zasnovani na energiji
- Samonadgledano učenje i više


## Inspiracija za duboko učenje i njegova istorija

Na konceptualnom nivou, duboko učenje je inspirisano mozgom, ali su neki detalji zanemareni. Kao poređenje, avioni su inspirisani pticama - princip letenja je isti, ali su detalji veoma različiti.

Istorija dubokog učenja počinje sa oblašću nauke koja se trenutno zove kibernetika 1940-ih sa McCulloch-om i Pitts-om. Oni su došli na ideju da su neuroni jedinice koje mogu da budu u uključenm ili isključenom stanju i da postoji neki prag kada menjaju stanje. Moglo je da se napravi Bulovo kolo (matematički model za kombinatorna digitalna logička kola) povezivanjem neurona i da se donose neki zaključci preko njih. Mozak je predstavljen kao mašina koja donosi zaključke preko neurona koji su binarni. Neuroni računaju težinsku sumu ulaza i porede je sa svojim pragom. Uključuju se ako je suma veća od praga, a isključuju ako je manja, što je uprošćen pogled na to kako mozak radi.

1947., Donald Hebb je došao na ideju da mozak uči promenama jačine veza između neurona. To je nazvano hiper učenje: ako dva neurona ispale impuls zajedno, veza između njih se pojačava, u suprotnom slabi.

Kasnije u toku 1948., Norbert Wiener je predložio kibernetiku, što je ideja da postoje sistemi koji imaju senzore i aktuatore, povratnu spregu i samoregulišući sistem. Pravila povratnog mehanizma automobila dolaze iz ovog rada.

1957., Frank Rosenblatt je smislio perceptron, algoritam koji modifikuje težine veoma jednostavnih neuronskih mreža.

Ideja da se naprave inteligentne mašine simulacijom velikog broja neurona nastala je 1940-ih, razvijala se tokom 1950-ih, i komplentno je napuštena krajem 1960-ih. Glavni razlozi za napuštanje ove ideje 1960-ih su:

- Naučnici su koristili neurone koji su binarni. Međutim, da bi propagacija unazad radila, aktivacione funkcije su morale da budu kontinualne. U to vreme, naučnici nisu došli do ideje da upotrebe kontinualne neurone i nisu mislili da je moguće da se obučava preko gradijenata jer binarni neuroni nisu diferencijabilni.
- Kod kontinualnih neurona, aktivaciju neurona treba pomnožiti težinom da bi se dobio doprinos težinskoj sumi. Međutim, pre 1980-te, množenje dva broja, pogotovu decimalnih brojeva je bilo veoma sporo. To je bio još jedan razlog da se izbegne upotreba kontinualnih neurona.

Duboko učenje je nastavilo da se razvija 1985. sa pojavom propagacije unazad. 1995., oblast je ponovo pala u zaborav i napuštena je ideja neuronskih mreža. Početkom 2010-ih, neuronske mreže su upotrebljene u prepoznavanju govora i postigle veliko poboljšanje performansi, a zatim se upotreba veoma reširila. 2013-te, računarska vizija je počela da većinski koristi neuronske mreže. 2016-te, isto se desilo i sa obradom prirodnih jezika. Uskoro će se slične revolucije desiti i u robotici, automatskom upravljanju i mnogim drugim oblastima.

### Nadgledano učenje

$90\%$ primena dubokog učenja koriste nadgledano učenje. Nadgledano učenje je proces gde se mreži da veliki broj parova ulaza i izlaza iz kojih treba da nauči kako da za novi dati ulaz predvidi tačan izlaz. U procesu obučavanja, kada je izlaz tačan, ne radi se ništa. Ako je izlaz netačan, malo se promene parametri mreže i ispravi izlaz ka onome koji želimo. Trik je znati u kom smeru i koliko promeniti parametre - i to nas dovodi do računanja gradijenata i propagacije unazad.

Nadgledano učenje potiče od perceptrona i Adaline. Adaline mreža je bazirana na istoj arhitekturi sa težinskim ulazima - iznad praga se uključuje, a ispod isključuje. Perceptron je neuronska mreža sa 2 sloja. Drugi sloj se obučava, a prvi sloj je fiksiran. Većinu vremena, prvi sloj je određen nasumično i zove se asocijativni sloj.

## [History of Pattern Recognition and introduction to Gradient Descent](https://www.youtube.com/watch?v=0bMe_vCZo30&t=1461s)

The foregoing is the conceptual basis of pattern recognition before deep learning developed. The standard model of pattern recognition consists of feature extractor and trainable classifier. Input goes into the feature extractor, extracting relevant useful characteristics of inputs such as detecting an eye when the purpose is recognizing the face. Then, the vector of features is fed to the trainable classifier for computing weighted sum and comparing it with the threshold. Here, a trainable classifier could be a perceptron or single neural network. The problem is feature extractor should be engineered by hand. Which means, pattern recognition/computer vision focus on feature extractor considering how to design it for a particular problem, not much devoted to a trainable classifier.

After the emergence and development of deep learning, the 2-stage process changed to the sequences of modules. Each module has tunable parameters and nonlinearity. Then, stack them making multiple layers. This is why it is called “deep learning”. The reason why using nonlinearity rather than linearity is that two linear layers could be one linear layer since the composition of two linear is linear.

The simplest multi-layer architecture with tunable parameters and nonlinearity could be: the input is represented as a vector such as an image or audio. This input is multiplied by the weight matrix whose coefficient is a tunable parameter. Then, every component of the result vector is passed through a nonlinear function such as ReLU. Repeating this process, it becomes a basic neural network. The reason why it is called a neural network is that this architecture calculates the weighted sum of components of input by corresponding rows of a matrix.

Back to the point of supervised learning, we are comparing the resulting output with target output then optimize the objective function which is the loss, computing a distance/penalty/divergence between the result and target. Then, average this cost function over the training set. This is the goal we want to minimize. In other words, we want to find the value of the parameters that minimize this average.

The method of how to find it is computing gradient. For example, if we are lost in a smooth mountain at foggy night and want to go to the village in the valley. One way could be turning around and seeing which way the steepest way is to go down then take a small step down. The direction is (negative) gradient. With the assumption that the valley is convex, we could reach the valley.

The more efficient way is called Stochastic Gradient Descent (SGD). Since we want to minimize average loss over the training set, we take one sample or small group of samples and calculate the error, then use gradient descent. Then, we take a new sample and get a new value for the error, then get the gradient which is a different direction normally. Two of the main reasons for using SGD are that it helps a model to converge fast empirically if the training set is very large and it enables better generalization, which means getting similar performance on various sets of data.


### [Computing gradients by backpropagation](https://www.youtube.com/watch?v=0bMe_vCZo30&t=2336s)

Computing gradients by backpropagation is a practical application of the chain rule. The backpropagation equation for the input gradients is as follows:

$$
\begin{aligned}
\frac{\partial C}{\partial \boldsymbol{x}_{i - 1}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial \boldsymbol{x}_i}{\partial \boldsymbol{x}_{i - 1}} \\
\frac{\partial C}{\partial \boldsymbol{x}_{i - 1}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial f_i(\boldsymbol{x}_{i - 1}, \boldsymbol{w}_i)}{\partial \boldsymbol{x}_{i - 1}}
\end{aligned}
$$

The backpropagation equation for the weight gradients is as follows:

$$
\begin{aligned}
\frac{\partial C}{\partial \boldsymbol{w}_{i}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial \boldsymbol{x}_i}{\partial \boldsymbol{w}_{i}} \\
\frac{\partial C}{\partial \boldsymbol{w}_{i}} &= \frac{\partial C}{\partial \boldsymbol{x}_i}\frac{\partial f_i(\boldsymbol{x}_{i - 1}, \boldsymbol{w}_i)}{\partial \boldsymbol{w}_{i}}
\end{aligned}
$$

Note that instead of scalar inputs, they will be vector inputs. More generally, multi-dimensional inputs. Backpropagation allows you to compute the derivative of the difference of the output you want and the output you get (which is the value of the objective function) with respect to any value inside the network. Finally, backpropagation is essential as it applies to multiple layers.

It is important to consider how to interpret inputs. For example, an image of 256$$\times$$256 would require a 200,000 valued matrix. These would be huge matrices that the neural network layers will need to handle. It would be impractical to utilize such matrices. Therefore, it is important to make hypothesis of the structure of the matrix.


## Hierarchical representation of the Visual Cortex

Experiments by Fukushima gave us an understanding of how our brain interprets the input to our eyes. In summary, it was discovered that neurons in front of our retina compress the input (known as contrast normalization) and the signal travels from our eyes to our brain. After this, the image gets processed in stages and certain neurons get activated for certain categories. Hence, the visual cortex does pattern recognition in a hierarchical manner.

Experiments in which researchers poked electrodes in specific areas of the visual cortex, specifically the V1 area made researchers realize that certain neurons react to motifs that appear in a very small area in a visual field and similarly with neighbouring neurons and neighbouring areas in the visual field. Additionally, neurons that react to the same visual field, react to different types of edges in an organized manner (*e.g.* vertical or horizontal edges). It is also important to note that there's also the idea that the visual process is essentially a feed forward process. Hence, somehow fast recognition can be done without some recurrent connections.

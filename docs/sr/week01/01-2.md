---
lang: Serbian
lang-ref: ch.01-2
lecturer: Yann LeCun
title: Evolution and Uses of CNNs and Why Deep Learning?
authors: Marina Zavalina, Peeyush Jain, Adrian Pearl, Davida Kollmar
date: 27 Jan 2020
translation-date: 13 Dec 2020
translator: Ema Pajić
---


## [Evolucija konvolucionih neuronskih mreža (CNN)](https://www.youtube.com/watch?v=0bMe_vCZo30&t=2965s)

U mozgu životinja, neuroni reaguju na ivice koje su specifične orijentacije. Grupe neurona koje reaguju na istu orijentaciju nalaze se svuda po vidnom polju.

Fukušima je 1982. godine napravio neuronsku mrežu koja je radila na isti način kao i mozak, bazirano na 2 koncepta. Prvo, neuroni su postavljeni po celom vidnom polju. Drugo, postoje kompleksne ćelije koje agregiraju informacije iz jednostavnih ćelija (jedinica koje reaguju na orijentaciju). Kao rezultat, pomeraj slike će uticati na aktivacije jednostavnih ćelija, ali neće uticati na agregiranu aktivaciju komplikovane ćelije (agregiranje konvolucijom).

LeCun je 1990. godine iskoristio propagaciju unazad da obuči konvolucionu neuronsku mrežu da prepozna rukom pisane cifre. Postoji video iz 1992. gde algoritam prepoznaje cifre napisane različitim stilovima. Prepoznavanje karaktera / oblika koristeći model koji rešava problem od početka do kraja je bilo novo u to vreme. Ranije je bilo neophodno izvlačenje obeležja pre modela nadgledanog učenja.

Ovi novi CNN sistemi su mogli da prepoznaju više karaktera na slici istovremeno. To se radilo tako što je postojao mali prozor koji se pomerao po celoj slici i on je prosleđivan na ulaz modela. Ako se aktivira, to znači da je prisutan određeni karakter.

Kasnije je ova ideja primenjena na detekciju lica / ljudi i semantičku segmentaciju (klasifikaciju piksela na slici). Primeri za to su Hadsel (2009) i Farabet (2012). Vremenom je ovo postalo popularno u industriji i koristi se, na primer, za praćenje trake na putu u autonomnoj vožnji.

Specijalan hardver za obučavanje konvolucionih neuronskih mreža je bila popularna tema 1980-ih, zatim je interesovanje opalo, ali se ponovo vratilo u skorije vreme.

Revolucija dubokog učenja (doduše, ovaj termin se nije koristio u to vreme) je počela 2010.-2013. Naučnici su se fokusirali na smišljanje algoritama koji bi mogli da ubrzaju treniranje velikih konvolucionih neuronskih mreža. Križevski je 2012. smislio AlexNet, mnogo veću konvolucionu neuronsku mrežu nego što su ranije koriščene, i obučio je na ImageNet-u (skupu podataka sa oko 1.3 miliona odbiraka) koristeći GPU (Grafičku procesorsku jedinicu). Nakon obučavanja nekoliko nedelja, AlexNet je imao značajno bolje rezultate od najboljih rivalskih sistema -- 25.8% *vs.* 16.4% top-5 procenat greške.

Nakon uspeha AlexNet-a, naučnici iz oblasti računarske vizije su bili ubeđeni da konvolucione neuronske mreže rade. Dok su svi radovi iz 2011.-2012. koji su pominjali CNN bili odbijeni, nakon 2016. najveći broj objavljenih radova koristi CNN.

Vremenom se broj slojeva povećavao: LeNet -- 7, AlexNet -- 12, VGG -- 19, ResNet -- 50. Međutim, postoji kompromis između broja operacija potrebnog da se sračuna izlaz modela, veličine modela i njegove tačnosti. Iz tog razloga, trenutno popularna tema je kako kompresovati mreže da bi bile brže.



## [Duboko učenje i izvlačenje obeležja](https://www.youtube.com/watch?v=0bMe_vCZo30&t=3955s)

Višeslojne mreže su uspešne jer koriste kompozitnu strukturu podataka. U kompozitnoj hijerarhiji, kombinacije objekata na jednom sloju hijerarhije kreiraju objekte na sledećem sloju. Ako imitiramo tu hijerarhiju pomoću više slojeva i pustimo mrežu da uči odgovarajuću kombinaciju obeležja, dobijemo arhitekturu duboke neuronske mreže. Dakle, duboke neuronske mreže su prirodno hijerarhijske.

Arhitekture dubokog učenja dovele su do neverovatnog napretka u računarskoj viziji, na raznim problemima, počevši od identifikacije i generacije tačnih "maski" objekata do identifikacije prostornih odlika objekta. Mask-RCNN i RetinaNet arhitekture su većinom dovele do ovog napretka.

Mask-RCNN mreže su pronašle primenu u segmentaciji pojedinačnih objekata, na primer kreiranju maske svakog objekta na slici. Ulaz i izlaz iz mreže su oba slike. Arhitektura takođe može da se primeni na segmentaciju instanci, tj identifikaciju različitih objekata istog tipa na slici. Detectron, softverski sistem Facebook AI Research (FAIR) centra, implementira sve najbolje algoritme detekcije objekata i open source-uje ih.

Neke od primena konvolucionih neuronskih mreža su i omogućavanje autonomne vožnje i analiza medicinskih slika.

Iako je nauka i matematika iza dubokog učenja dosta dobro shvaćena, idalje postoje zanimljiva pitanja koja treba istražiti. Na primer: Zašto arhitekture sa više slojeva rade bolje, uzevši u obzir da možemo da aproksimiramo funkciju pomoću 2 sloja? Zašto konvolucione neuronske mreže rade dobro sa prirodnim podacima kao što su govor, slike i tekst? Kako uspevamo da toliko dobro optimizujemo nekonveksne funkcije? Zašto arhitekture sa previše parametara rade?

Izdvajanje odlika sastoji se od proširivanja dimenzije reprezentacije tako da proširena obeležja verovatnije budu linearno separabilna, tačke u prostoru više dimenzije su verovatnije linearno separabilne zbog povećanja broja potencijalnih separacionih ravni.

Ranije se u primenama mašinskog učenja oslanjalo na kvalitetne, ručno odabrane odlike, specifične za zadatak. Zbog napretka dubokog učenja, modeli su u mogućnosti da automatski izdvoje obeležja. Neki od pristupa korišćenih u izdvajanju odlika:

- Popločavanje prostora
- Nasumične projekcije
- Polinomijalni klasifikator (unakrsni proizvodi obeležja)
- Funkcije radijalne baze
- Kernel mašine

Zbog kompozitne prirode podataka, naučena obeležja imaju hijerarhiju reprezentacija sa rastućim nivoem apstrakcije. Na primer:

-  Slike - Na najmanjem nivou, slike su pikseli. Kombinacija piksela čini ivice, dok kombinacija ivica čini tekstone (oblike sa više ivica). Tekstoni čine motive, a motivi čine delove slike. Kombinacijom delova slike dobijamo celu sliku.
-  Tekst - Slično, postoji inherentna hijerarhija u tekstualnim podacima. Karakteri formiraju reči, reči formiraju grupe reči, zatim klauzule, a kombinacijom klauzula dobijamo rečenice. Rečenice čine priču koja je zapisana.
-  Govor - U govoru, od zamisli, preko aparata za govor, do glasova i fonema, zatim celih reči i na kraju rečenica, takođe vidimo jasnu hijerarhiju.



## [Učenje reprezentacija](https://www.youtube.com/watch?v=0bMe_vCZo30&t=4767s)

Ne žele svi da prihvate duboko učenje: ako možemo da aproksimiramo bilo koju funkciju pomoću 2 sloja, zašto koristiti više?

Na primer: SVM nalazi separacionu hiperpovrš "preko podataka", tj. predikcije su bazirane na poređenjima sa podacima iz obučavajućeg skupa. SVM je u suštini veoma jednostavna dvoslojna neuronska mreža, gde prvi sloj definiše "šablone", a drugi sloj je linearni klasifikator. Problem sa dvoslojnom mrežom je to što je kompleksnost i veličina srednjeg sloja eksponencijalna po $N$ (da bi dobro radila na teškom zadatku, potrebno je PUNO šablona). Međutim, ako proširimo broj slojeva na $\log(N)$, slojevi postaju linearni po $N$. Postoji kompromis između vremena i prostora. 

Analogija je dizajniranje kola koje računa bulovu funkciju sa ne više od dva sloja kapija -- možemo da sračunamo **bilo koju bulovu funkciju** na ovaj način! Međutim, kompleksnost i resursi prvog sloja (broj kapija) brzo postaju nepraktične za kompleksne funkcije.

Šta je "duboko"?

- SVM nije dubok jer ima samo 2 sloja
- Klasifikaciono stablo nije duboko jer svaki sloj analizira ista obeležja
- Duboka neuronska mreža ima više slojeva i koristi ih da napravi **hijerarhiju obeležja rastuće kompleksnosti**

Kako modeli uče reprezentacije (dobra obeležja)?

Manifold hipoteza: prirodni podaci su u nisko-dimenzionom prostoru. Skup mogućih slika je u suštini beskonačan, ali je skup "prirodnih" slika mali podskup. Na primer: Za sliku osobe, skup mogućih slika je reda veličine broja mišića lica koji mogu da se pomere (stepeni slobode) ~ 50. Idealan (i nerealističan) izdvajač obečežja reprezentuje sve faktore (svaki mišić, svetlost, itd.)

Pitanja i odgovori sa kraja lekcije:

- Za primer lica, da li bi neka druga tehnika redukcija dimenzija (npr. PCA) uspela da izvuče ta obeležja? 
  - Odgovor: to bi radilo samo ako je površina manifolda hiperravan, što nije.


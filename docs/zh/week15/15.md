---
lang-ref: ch.15
lang: zh
title: 第十五周
translation-date: 04 Feb 2021
translator: Yang Zhou
---

## 讲座A部分

<!--When encountering the data with multiple outputs for a single input, feed-forward networks cannot capture such implicit dependencies. Instead, latent-variable energy-based models (EBMs) come to the rescue. We developed a toy ellipse example with a fixed input and the optimal model formulation. Then, we applied latent-variable EBMs to inference the best latent variables that can learn the implicit relationships.-->

当遇到一个输入对应多个输出的数据时，前向式网络无法捕捉到这样的隐藏依赖关系。相对的，隐变量能量基础模型（EBMs）就能站出来救场了。我们设计了一个小型的椭圆数据集作为例子，它包含了固定的输入和一个最佳的模型建模。然后我们采用隐变量EBMs来推理可以学到隐藏关系的最佳隐变量。

## 讲座B部分

<!--This section starts from introducing a relaxed version of free energy by modifying the "temperature" to smooth the energy function. Then we demonstrate how to train EBMs by minimizing loss functionals with several examples. Finally we give a concrete example of self-supervised learning, where we train a EBM to learn a horn-like data manifold.-->

通过修改“温度”以平滑能量函数，这个小节从引入一种松弛版本的自由能开始。之后我们用几个例子演示了如何通过最小化损失函数训练EBMs。最后，我们提供了一个自监督学习的具体例子，在这个例子中我们训练了一个EBM来学习喇叭状的流形数据。
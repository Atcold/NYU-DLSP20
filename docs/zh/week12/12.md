---
lang-ref: ch.12
lang: zh
title: 第十二周
translation-date: 04 Sep 2020
translator: Jonathan Sum(😊🍩📙)
---

## 讲座A


在这个部份，我们讨论了在自然语言处理(NLP)中所用到的不同的架构，我们开始以卷积层CNN﹑循环神经网络RNN，和最后地也说了一个十分出色的架构变压器(transformers) 。我们之后也讨论了各种不同的模组来用这些模组来比较自然语言处理(NLP)任务。最后，我们讨论了一些技巧来令我们可以更有效地训练变压器(transformers)。


## 讲座B

在这个部份，我们介绍了集束搜索(Beam Search)，我们介绍得这个集束搜索如贪心式解码和彻底搜索这两者之间的中间来介绍。我们考虑了以生成分布( generative distribution)来取样，比如:生成文字，和我们也介绍了「前K」式取样。后来，我们介绍了有一个改装过的变压器的序列对序列模型和介绍了「还原翻译」。我们也介绍了以无监督学习方式来学习嵌入，我们也介绍了「文字转成向量」(word2vec)和GPT﹑BERT。

## 动手做

我们介绍了注意力模型，集中地说了自我式注意力模型，和它的输入输入时隐藏层的表示。之后，我们介绍了一个方式，就是钥匙和数值的方式来储存(key-value store paradigm)，而且我们也讨论了如何以查询和钥匙﹑数值来表示一个旋转了的输入(rotations of an input)。最后，我们用注意力模型来解释变压器架构，也用来解释在一个简单的变压器中进行一个「前向度过(forward pass)」，之后就是比较一下「编码和解码式」和序列式架构的分别。

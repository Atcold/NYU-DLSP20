---
lang-ref: ch.11
lang: zh
title: Week 11
translation-date: 04 September 2020
translator: Jonathan Sum(😊🍩📙)
---

## 講座A

在這個部份，我們討論了在自然語言處理(NLP)中所用到的不同的架構，
我們開始以卷積層CNN﹑循環神經網絡RNN，和最後地也說了一個十分出色的架構變壓器(transformers) 。
我們之後也討論了各種不同的模組來用這些模組來比較自然語言處理(NLP)任務。
最後，我們討論了一些技巧來令我們可以更有效地訓練變壓器(transformers)。

## 講座B

在這個部份，我們介紹了集束搜索(Beam Search)
，我們介紹得這個集束搜索如貪心式解碼和徹底搜索這兩者之間的中間來介紹。我們考慮了以生成分佈( generative distribution)來取樣，
比如:生成文字，和我們也介紹了「前K」式取樣。後來，我們介紹了有一個改裝過的變壓器的序列對序列模型和介紹了「還原翻譯」。
我們也介紹了以無監督學習方式來學習嵌入，我們也介紹了「文字轉成向量」(word2vec)和GPT﹑BERT。


## 動手做

我們介紹了注意力模型，集中地說了自我式註意力模型，和它的輸入輸入時隱藏層的表示。之後，我們介紹了一個方式，就是鑰匙和數值的方式來儲存(key-value store paradigm)，而且我們也討論瞭如何以查詢和鑰匙﹑數值來表示一個旋轉了的輸入(rotations of an input)。最後，我們用注意力模型來解釋變壓器架構，也用來解釋在一個簡單的變壓器中進行一個「前向度過(forward pass)」，之後就是比較一下「編碼和解碼式」和序列式架構的分別。

---
lang: zh
lang-ref: ch.11
title: 第十一周
translation-date: 3 Sep 2020
translator: titusjgr
---

## <!--Lecture part A-->

## 讲座A部份

<!--In this section, we discussed about the common activation functions in Pytorch. In particular, we compared activations with kink(s) versus smooth activations - the former is preferred in a deep neural network as the latter might suffer with gradient vanishing problem. We then learned about the common loss functions in Pytorch.-->

在这一部份，我们讨论了Pytorch当中常见的激活函数。我们特别的比较了有着扭结和平滑的激活函数－－在深度神经网络中偏好使用前者，因为后者可能遇到梯度消失问题。接下来我们认识了Pytorch常见的损失函数。

<!--## Lecture part B-->

## 讲座B部份

<!--In this section, we continued to learn about loss functions - in particular, margin-based losses and their applications. We then discussed how to design a good loss function for EBMs as well as examples of well-known EBM loss functions. We gave particular attention to margin-based loss function here, as well as explaining the idea of "most offending incorrect answer.-->

这部份我们继续学习损失函数，特别是基于边界的损失与他们的应用。接着，我们讨论如何为能量基础模型设计一个好的损失函数，以及能量基础模型一些著名的损失函数。我们特别注意的是基于边界的损失函数，以及解释「错误答案中最接近正确答案的答案」的概念。

<!--## Practicum-->

## 实践

<!--This practicum proposed effective policy learning for driving in dense traffic. We trained multiple policies by unrolling a learned model of the real world dynamics by optimizing different cost functions. The idea is to minimize the uncertainty in the model's prediction by introducing a cost term that represents the model's divergence from the states it is trained on.-->

这次的实践提出如何高校学习在拥塞交通中行驶的策略。我们藉由优化不同损失函数于展开真实世界的动态以训练多个策略。概念是，如果要最小化模型预测的不确定性，就引入损失项，代表模型偏离其所训练于的状态的程度。

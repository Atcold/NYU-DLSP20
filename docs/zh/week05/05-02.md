---
lang: zh
lang-ref: ch.05-2
title: 优化方法
authors: Guido Petri, Haoyue Ping, Chinmay Singhal, Divya Juneja
date: 29 February 2020
translation-date: 11 April 2020
translator: [Harry (Chao) Yang](https://www.harryyang.org) 
---

## [自适应优化算法](https://www.youtube.com/watch?v=FW5gFiJb-ig&t=5s)


具有动量的随机梯度下降法(SGD)是当前针对许多ML问题的最先进的优化方法。但是还有其他一些方法，这些方法通常统称为自适应优化方法。这些方法近年来不断创新，而且这些方法对于一些条件不足的问题（特别是SGD不适用的情况下）特别有用。

在随机梯度下降法的公式中，网络中的每个权重均使用相同的学习率（全局$ \ gamma $）进行更新。与此不同的是，对于自适应的方法，我们针对每个权重分别调整学习率。为了达到这个目的，我们使用从梯度获得的关于每个权重的信息。

在实践中经常使用的网络在其不同部分具有不同的结构。例如，CNN的前几层部分可能神经图像比较大但是通道不是很多，而在网络的后期，我们可能得到小的神经图像，但是卷积的核具有大量的通道。这两种操作非常不同，因此，对于网络的前面部分而言效果很好的学习率可能对网络的后部分效果不好。这意味着逐层自适应去调整学习率可能会很有效果。

网络最后部分的权重（下图1中的4096）直接决定了网络的输出，换言之对输出有非常大的影响。因此，我们需要为这些权重降低学习率。相反，较早层的单个权重将对输出产生较小的影响，尤其是在网络权重值是随机初始化的时候。

<center>
<img src="{{site.baseurl}}/docs/images/week05/05-2/5_2_vgg.png" style="zoom:40%"><br>
<b>Figure 1: </b>VGG16
</center>

### 均方根优化（RMSprop)
均方根优化(RMSprop)的关键思想是通过均方根对梯度进行归一化。

在下面的等式中，对梯度进行平方意味着对梯度向量的每个元素分别进行平方。

$$
\begin{aligned}
v_{t+1} &= {\alpha}v_t + (1 - \alpha) \nabla f_i(w_t)^2 \\
w_{t+1} &=  w_t - \gamma \frac {\nabla f_i(w_t)}{ \sqrt{v_{t+1}} + \epsilon}
\end{aligned}
$$

其中 $\gamma$ 是整体学习率，$\epsilon$ 是接近于电脑的 $\epsilon$ 的一个非常小值（大约介于 $10 ^{-7}$ 至 $10^{-8}$ 之间）（这是为了避免除以零而报错），$v_{t + 1}$ 是梯度的二阶矩估计。

我们通过指数移动平均值（这是计算随时间变化的数量平均值的标准方法）来更新嘈杂的$v$值。我们需要对新值提供更大的权重，因为它们会提供更多信息。这里的方法以指数形式降低旧值的权重。 计算 $v$ 的时候旧的值在每个步骤中都乘上$\alpha$来指数降低权重，该 $\alpha$ 常数在0到1之间变化。这会让旧值递减，直到它们对梯度二阶矩的指数移动平均值贡献非常低为止。

这个方法会不断计算梯度二阶矩的指数移动平均值，因为是非中心的二阶矩，因此我们不用在计算的时候减去剃度的均值。梯度的二阶矩用来对梯度进行逐个归一，这意味着梯度的每个元素都将除以二阶矩的平方根。如果梯度的期望值较小，则此过程类似于将梯度除以标准差。

在分母中使用小的$\epsilon$不会导致大的偏移，因为当$v$非常小时，意味着整个梯度也非常小。

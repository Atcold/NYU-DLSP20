---
lang-ref: ch.13-3
title: å›¾å½¢å·ç§¯ç½‘ç»œ III
lecturer: Alfredo Canziani
authors: Go Inoue, Muhammad Osama Khan, Muhammad Shujaat Mirza, Muhammad Muneeb Afzal
date: 28 Apr 2020
lang: zh
translation-date: 17 Sep 2020
translator: Jonathan Sum(ğŸ˜ŠğŸ©ğŸ“™)
---


## [å›¾å½¢å·ç§¯ç½‘ç»œ(è‹±æ–‡ç®€ç§°GCN)](https://www.youtube.com/watch?v=2aKXWqkbpWg&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=25&t=47s)

å›¾å½¢å·ç§¯ç½‘ç»œæ˜¯ä¸€ç§ä½¿ç”¨æ•°æ®ç»“æ„çš„æ¶æ„ã€‚åœ¨è¯´å¾—æ›´æ·±å…¥ä¸€ç‚¹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿåœ°å›é¡¾ä¸€ä¸‹è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¦‚å›¾å½¢å·ç§¯ç½‘ç»œå’Œè‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶éƒ½æ˜¯åŸ‹è®ºä¸Šä¸€æ ·çš„ã€‚


### å›é¡¾ä¸€ä¸‹: è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶

<!-- - In self-attention, we have a set of input $\lbrace\boldsymbol{x}\_{i}\rbrace^{t}_{i=1}$.
Unlike a sequence, it does not have an order.
- Hidden vector $\boldsymbol{h}$ is given by linear combination of the vectors in the set.
- We can express this as $\boldsymbol{X}\boldsymbol{a}$ using matrix vector multiplication, where $\boldsymbol{a}$ contains coefficients that scale the input vector $\boldsymbol{x}_{i}$. -->
åœ¨è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥é›†$\lbrace\boldsymbol{x}\_{i}\rbrace^{t}\_{i=1}$ã€‚ä¸åƒåºåˆ—é‚£æ ·ï¼Œå®ƒæ²¡æœ‰é¡ºåºçš„ã€‚
éšè—å‘é‡$\boldsymbol{h}$æ˜¯ç”±é›†ä¹‹ä¸­çš„å‘é‡çš„çº¿æ€§ç»„åˆå¾—å‡ºæ¥çš„ã€‚
æˆ‘ä»¬å¯ä»¥ç”¨çŸ©é˜µå‘é‡ä¹˜æ³•æ¥ä»¥$\boldsymbol{X}\boldsymbol{a}$å»è¡¨è¾¾è¿™ä¸ªä¸œè¥¿ï¼Œè¿™é‡Œ$\boldsymbol{a}$åŒ…å«ä¸€äº›ä¼šç¼©æ”¾å‘é‡$\boldsymbol{x}_{i}$çš„ç³»æ•°ã€‚


<!-- *For a detailed explanation, refer to the notes of [Week 12]({{site.baseurl}}/en/week12/12-3/).* -->
*å¦‚æœæƒ³è¦æ›´å¤šæ·±å…¥çš„è§£é‡Šï¼Œè¯·çœ‹[ç¬¬12ä¸ªæ˜ŸæœŸçš„ç¬”è®°]({{site.baseurl}}/en/week12/12-3/)ã€‚*


<!-- ### Notation -->
### ç¬¦å·

<center>
<img src="{{site.baseurl}}/images/week13/13-3/figure1.png" height="400px" /><br>
<b>å›¾ 1</b>: å›¾å½¢å·ç§¯ç½‘ç»œ
</center>

<!-- In Figure 1, vertex $v$ is comprised of two vectors: input $\boldsymbol{x}$ and its hidden representation $\boldsymbol{h}$.
We also have multiple vertices $v_{j}$, which is comprised of $\boldsymbol{x}\_j$ and $\boldsymbol{h}\_j$.
In this graph, vertices are connected with directed edges. -->
åœ¨å›¾1ï¼Œé¡¶ç‚¹$v$åŒ…å«ä¸¤ä¸ªå‘é‡:è¾“å…¥$\boldsymbol{x}$å’Œå®ƒçš„éšè”½è¡¨ç¤º$\boldsymbol{h}$ã€‚æˆ‘ä»¬ä¹Ÿæœ‰å¤šä¸ªé¡¶ç‚¹$v_{j}$ï¼Œå®ƒå€‘åŒ…å«$\boldsymbol{x}\_j$ å’Œ $\boldsymbol{h}\_j$ã€‚åœ¨è¿™ä¸ªå›¾å½¢ï¼Œé¡¶ç‚¹éƒ½æ˜¯ä»¥æœ‰å‘è¾¹æ¥è¿æ¥èµ·æ¥ã€‚

<!-- We represent these directed edges with adjacency vector $\boldsymbol{a}$, where each element $\alpha_{j}$ is set to $1$ if there is a directed edge from $v_{j}$ to $v$. -->
æˆ‘ä»¬ä»¥é‚»æ¥å‘é‡$\boldsymbol{a}$æ¥ä»£è¡¨è¿™äº›æœ‰å‘è¾¹ï¼Œè¿™é‡Œå¦‚æœæ˜¯ä¸€ä¸ªæœ‰å‘è¾¹ç”±$v_{j}$åˆ°$v$çš„è¯ï¼Œé‚£æ¯ä¸€ä¸ªå…ƒä»¶$\alpha_{j}$éƒ½è®¾å®šä¸º$1$ã€‚ 

$$
\alpha_{j} \stackrel{\tiny \downarrow}{=} 1 \Leftrightarrow v_{j} \rightarrow v
\tag{æ–¹ç¨‹å¼1}
$$

<!-- The degree (number of incoming edges) $d$ is defined as the norm of this adjacency vector, *i.e.* $\Vert\boldsymbol{a}\Vert_{1} $, which is the number of ones in the vector $\boldsymbol{a}$. -->
åº¦æ•°$d$ï¼Œä¹Ÿå°±æ˜¯è¿æ¥å…¥æ¥çš„è¾¹çš„æ•°é‡æ˜¯å¤šå°‘æ¥å†³å®šåº¦æ•°ï¼Œè€Œå®ƒæ˜¯ä»¥ä¸€ä¸ªé‚»æ¥å‘é‡çš„èŒƒæ•°æ¥å®šä¹‰å‡ºæ¥ï¼Œ*æ¯”å¦‚*$\Vert\boldsymbol{a}\Vert_{1} $ï¼Œä¹Ÿå°±æ˜¯åœ¨å‘é‡$\boldsymbol{a}$ä¸­æœ‰å¤šå°‘ä¸ªä¸€ã€‚

$$
    d = \Vert\boldsymbol{a}\Vert_{1}
\tag{æ–¹ç¨‹å¼2}
$$

<!-- The hidden vector $\boldsymbol{h}$ is given by the following expression: -->
è€Œéšè—å‘é‡$\boldsymbol{h}$æ˜¯ä»¥è¿™ä¸ªè¡¨è¾¾å¼æ¥è¡¨é€¹:

$$
    \boldsymbol{h}=f(\boldsymbol{U}\boldsymbol{x} + \boldsymbol{V}\boldsymbol{X}\boldsymbol{a}d^{-1})
\tag{æ–¹ç¨‹å¼3}
$$

<!-- where $f(\cdot)$ is a non-linear function such as ReLU $(\cdot)^{+}$, Sigmoid $\sigma(\cdot)$, and hyperbolic tangent $\tanh(\cdot)$. -->
è¿™é‡Œ$f(\cdot)$æ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œæ¯”å¦‚ä¿®æ­£çº¿æ€§å•å…ƒè‹±æ–‡ä¸ºReLUï¼Œç¬¦å·ä¸º$(\cdot)^{+}$ï¼Œè€ŒSå½¢å‡½æ•°çš„ç¬¦å·ä¸º$\sigma(\cdot)$ï¼Œè€ŒåŒæ›²æ­£åˆ‡çš„ç¬¦å·ä¸º$\tanh(\cdot)$ã€‚

<!-- The $\boldsymbol{U}\boldsymbol{x}$ term takes into account the vertex $v$ itself, by applying rotation $\boldsymbol{U}$ to the input $v$. -->
è¿™ä¸ª$\boldsymbol{U}\boldsymbol{x}$é¡¹æ˜¯è¦ç”¨åœ¨é¡¶ç‚¹$v$ä¸Šï¼Œå°±æ˜¯å¯¹è¾“å…¥$v$ç”¨ä¸Šè¿™ä¸ªæ—‹è½¬$\boldsymbol{U}$ã€‚

<!-- Remember that in self-attention, the hidden vector $\boldsymbol{h}$ is computed by $\boldsymbol{X}\boldsymbol{a}$, which means that the columns in $\boldsymbol{X}$ is scaled by the factors in $\boldsymbol{a}$.
In the context of GCN, this means that if we have multiple incoming edges,*i.e.* multiple ones in adjacency vector $\boldsymbol{a}$, $\boldsymbol{X}\boldsymbol{a}$ gets larger.
On the other hand, if we have only one incoming edge, this value gets smaller.
To remedy this issue of the value being proportionate to the number of incoming edges, we divide it by the number of incoming edges $d$.
We then apply rotation $\boldsymbol{V}$ to $\boldsymbol{X}\boldsymbol{a}d^{-1}$. -->
è®°å¾—åœ¨è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œéšè—å‘é‡$\boldsymbol{h}$æ˜¯ä»¥$\boldsymbol{X}\boldsymbol{a}$æ¥è®¡ç®—å‡ºæ¥ï¼Œ
ä¹Ÿå°±æ˜¯è¯´åœ¨$\boldsymbol{X}$ä¸­çš„è¡Œæ˜¯ä»¥åœ¨$\boldsymbol{a}$ä¸­çš„å› ç´ ç¼©æ”¾å‡ºæ¥çš„ã€‚è€Œå›¾å½¢å·ç§¯ç½‘ç»œçš„å†…å®¹ä¸­ï¼Œ
è¿™æ„æ€ç€è¿™æ ·ï¼Œå¦‚æœæˆ‘ä»¬æœ‰å¤šä¸ªç”±å¤–ä¼ å…¥æ¥çš„è¾¹ï¼Œæ¯”å¦‚åœ¨é‚»æ¥å‘é‡$\boldsymbol{a}$ä¸­æœ‰å¤šä¸ª1ï¼Œ$\_boldsymbol{X}\_boldsymbol{a}$å°±ä¼šå˜å¾—å¾ˆå¤§ã€‚
åœ¨å¦ä¸€é¢ï¼Œå¦‚æœæˆ‘ä»¬åªæœ‰ä¸€ä¸ªä¼ å…¥æ¥çš„è¾¹ï¼Œè¿™ä¸ªå€¼å°±ä¼šå˜å¾—å¾ˆç»†ã€‚
å»è¡¥æ•‘è¿™ä¸ªæ•°å€¼ä¸ç”±å¤–ä¼ å…¥æ¥çš„è¾¹æ˜¯æˆæ¯”ä¾‹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹å®ƒé™¤ä»¥ä¸€ä¸ªæ•°ï¼Œ
è¿™ä¸ªæ•°å°±æ˜¯ç”±å¤–ä¼ å…¥æ¥çš„è¾¹æœ‰å¤šå°‘çš„æ•°é‡ï¼Œè¿™ä¸ªæ•°å«$d$ã€‚æˆ‘ä»¬ä¹‹åå¯¹$\_boldsymbol{X}\_boldsymbol{a}d^{-1}$ä½¿ç”¨ä¸€ä¸ªæ—‹è½¬$\_boldsymbol{V}$ã€‚


<!-- We can represent this hidden representation $\boldsymbol{h}$ for the entire set of inputs $\boldsymbol{x}$ using the following matrix notation: -->
æˆ‘å€‘å¯ä»¥ä»¥é€™å€‹éš±è—è¡¨ç¤º$\boldsymbol{h}$ä¾†ä»¥ä¸‹æ–¹çš„çŸ©é™£ç¬¦è™Ÿä¾†è¡¨é”æ•´å€‹è¼¸å…¥é›†$\boldsymbol{x}$

$$
\{\boldsymbol{x}_{i}\}^{t}_{i=1}\rightsquigarrow \boldsymbol{H}=f(\boldsymbol{UX}+ \boldsymbol{VXAD}^{-1})
\tag{æ–¹ç¨‹å¼4}
$$

where $\vect{D} = \text{diag}(d_{i})$.


<!-- ## [Residual Gated GCN Theory and Code](https://www.youtube.com/watch?v=2aKXWqkbpWg&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=25&t=992s) -->
## [æ®‹å·®é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œç†è®ºå’Œä»£ç ](https://www.youtube.com/watch?v=2aKXWqkbpWg&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=25&t=992s)

<!-- Residual Gated Graph Convolutional Network is a type of GCN that can be represented as shown in Figure 2: -->
æ®‹å·®é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œæ˜¯ä¸€ç§å›¾å½¢å·ç§¯ç½‘ç»œï¼Œä¹Ÿå¯ä»¥ä»¥å›¾2æ¥è¡¨ç¤ºå‡ºæ¥:

<center>
<img src="{{site.baseurl}}/images/week13/13-3/figure2.png" height="300px" /><br>
<!-- <b>Fig. 2</b>: Residual Gated Graph Convolutional Network -->
<b>å›¾ 2</b>: æ®‹å·®é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œ
</center>

<!-- As with the standard GCN, the vertex $v$ consists of two vectors: input $\boldsymbol{x}$ and its hidden representation $\boldsymbol{h}$. However, in this case, the edges also have a feature representation, where $\boldsymbol{e_{j}^{x}}$ represents the input edge representation and $\boldsymbol{e_{j}^{h}}$ represents the hidden edge representation. -->
ä»¥ä¸€å€‹æ¨™æº–çš„åœ–å½¢å·ç©ç½‘çµ¡ï¼Œé ‚é»$v$ç”±é€™å…©å€‹å‘é‡çµ„æˆ:è¼¸å…¥$\boldsymbol{x}$å’Œå®ƒçš„éš±è—è¡¨ç¤º$\boldsymbol{h}$ã€‚ç›¸åï¼Œåœ¨é€™å€‹ä¾‹å­ï¼Œé‚Šéƒ½æœ‰ä¸€å€‹ç‰¹å¾µè¡¨ç¤ºï¼Œè€Œé€™è£¡çš„$\boldsymbol{e_{j}^{x}}$ ä»£è¡¨è‘—ä¸€å€‹è¼¸å…¥é‚Šè¡¨ç¤ºå’Œ$\boldsymbol{e_{j}^{h}}$ä»£è¡¨ä¸€å€‹éš±è—é‚Šè¡¨ç¤ºã€‚


<!-- The hidden representation $\boldsymbol{h}$ of the vertex $v$ is computed by the following equation: -->
å°±å¦‚åŸºæœ¬å‹å›¾å½¢å·ç§¯ç½‘ç»œï¼Œé¡¶ç‚¹$v$ç”±ä¸¤ä¸ªå‘é‡ç»„æˆ:è¾“å…¥xå’Œå®ƒçš„éšè—è¡¨ç¤º$\boldsymbol{h}$ ã€‚

$$
    \boldsymbol{h}=\boldsymbol{x} + \bigg(\boldsymbol{Ax} + \sum_{v_jâ†’v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}}\bigg)^{+}
\tag{Eq. 5}
$$

<!-- where $\boldsymbol{x}$ is the input representation, $\boldsymbol{Ax}$ represents a rotation applied to the input $\boldsymbol{x}$ and $\sum_{v_jâ†’v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}}$ denotes the summation of elementwise multiplication of rotated incoming features $\boldsymbol{Bx_{j}}$ and a gate $\eta(\boldsymbol{e_{j}})$. In contrast to the standard GCN above where we average the incoming representations, the gate term is critical to the implementation of Residual Gated GCN since it allows us to modulate the incoming representations based on the edge representations. -->
è¿™é‡Œ$\boldsymbol{x}$æ˜¯ä¸€ä¸ªè¾“å…¥è¡¨ç¤ºï¼Œè€Œ$\boldsymbol{Ax}$ä»£è¡¨å¯¹è¾“å…¥$\boldsymbol{x}$ç”¨ä¸Šäº†ä¸€ä¸ªæ—‹è½¬ï¼Œè€Œä¸”$\sum_{v_jâ†’v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}}$è¡¨ç¤ºä¸€ä¸ªå·²æ—‹è½¬äº†çš„ä¼ å…¥æ¥çš„ç‰¹å¾$\boldsymbol{Bx_{j}}$å’Œé—¨$\eta(\boldsymbol{e_{j}})$çš„é€å…ƒç´ ä¹˜æ³•ï¼Œç„¶åæœ€åå°±æ¥ä¸€ä¸ªæ€»ç»“ã€‚å’Œæ ‡å‡†çš„å›¾å½¢å·ç§¯ç½‘ç»œæ¯”è¾ƒï¼Œä¸Šæ–¹çš„å°±ä¼šå¯¹ä¼ å…¥æ¥çš„è¡¨ç¤ºè¿›è¡Œå¹³å‡å€¼è¿ç®—ï¼Œè€Œé—¨æ§é¡¹æ˜¯å¯¹å®ä½œæ®‹å·®é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œæ¥è¯´æ˜¯æœ€é‡è¦çš„ï¼Œå› ä¸ºå®ƒå®¹è®¸æˆ‘ä»¬å»è°ƒèŠ‚ä¼ å…¥æ¥çš„åŸºäºè¾¹çš„è¡¨ç¤ºã€‚

<!-- Note that the summation is only over vertices ${v_j}$ that have incoming edges to vertex ${v}$. The term residual (in Residual Gated GCN) comes from the fact that in order to calculate the hidden representation $\boldsymbol{h}$, we add the input representation $\boldsymbol{x}$. The gate term $\eta(\boldsymbol{e_{j}})$ is calculated as shown below: -->
æ³¨æ„åˆ°çš„æ˜¯æ€»å’Œåªæ˜¯æ€»å’Œæ‰€æœ‰é¡¶ç‚¹${v_j}$ï¼Œè€Œè¿™äº›é¡¶ç‚¹éƒ½æœ‰ä¼ å…¥æ¥çš„è¾¹è¿åˆ°é¡¶ç‚¹${v}$ã€‚è€Œåœ¨æ®‹å·®é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œä¸­çš„é¡¹:ã€Œæ®‹å·®ã€ï¼Œè¿™ä¸ªé¡¹äº‹å®ä¸Šæ¥è‡ªä¸€ä¸ªåšæ³•ï¼Œå¦‚æœè¦è®¡ç®—å‡ºéšè—$\boldsymbol{h}$ï¼Œæˆ‘ä»¬å°±è¦åŠ é‚£ä¸ªè¾“å…¥è¡¨ç¤º$\boldsymbol{x}$ã€‚è€Œé‚£ä¸ªé—¨é¡¹$\eta(\boldsymbol{e_{j}})$æ˜¯ä»¥ä¸‹æ–¹é‚£æ ·æ¥è®¡ç®—:

$$
    \eta(\boldsymbol{e_{j}})=\sigma(\boldsymbol{e_{j}})\bigg(\sum_{v_kâ†’v}\sigma(\boldsymbol{e_{k}})\bigg)^{-1}
\tag{æ–¹ç¨‹å¼6}
$$

<!-- The gate value $\eta(\boldsymbol{e_{j}})$ is a normalized sigmoid obtained by dividing the sigmoid of the incoming edge representation by the sum of sigmoids of all incoming edge representations. Note that in order to calculate the gate term, we need the representation of the edge $\boldsymbol{e_{j}}$, which can be computed using the equations below: -->
è€Œé—¨æ•°å€¼$\eta(\boldsymbol{e_{j}})$æ˜¯å½’ä¸€åŒ–åçš„Så‹å‡½æ•°ï¼Œå–å¾—å®ƒçš„æ–¹æ³•æ˜¯è¿™æ ·çš„:å…ˆæŠŠæ‰€æœ‰ä¼ å…¥æ¥çš„è¾¹çš„è¡¨ç¤ºæ¯ä¸€ä¸ªè¿›è¡ŒSå‹å‡½æ•°ï¼Œç„¶ååŠ èµ·æ¥ï¼Œæœ€åå°±æ˜¯å¯¹æ‰€æœ‰ä¼ å…¥æ¥çš„è¾¹çš„è¡¨ç¤ºè¿›è¡Œäº†Så‹å‡½æ•°åé™¤ä»¥åˆšåˆšåŠ å¥½çš„æ•°ã€‚æ³¨æ„çš„æ˜¯ï¼Œä¸ºäº†è®¡ç®—é—¨é¡¹ï¼Œæˆ‘ä»¬è¦è¾¹$\boldsymbol{e_{j}}$çš„è¡¨ç¤ºï¼Œä¹Ÿå°±å¯èƒ½ä¼šè¢«è¿™æ ·è®¡ç®—å‡ºæ¥ï¼Œä»¥ä»¥ä¸‹çš„æ–¹ç¨‹å¼:

$$
    \boldsymbol{e_{j}} = \boldsymbol{Ce_{j}^{x}} + \boldsymbol{Dx_{j}} + \boldsymbol{Ex}
\tag{æ–¹ç¨‹å¼7}
$$

$$
    \boldsymbol{e_{j}^{h}}=\boldsymbol{e_{j}^{x}}+(\boldsymbol{e_{j}})^{+}
\tag{æ–¹ç¨‹å¼8}
$$

<!-- The hidden edge representation $\boldsymbol{e_{j}^{h}}$ is obtained by the summation of the initial edge representation $\boldsymbol{e_{j}^{x}}$ and $\texttt{ReLU}(\cdot)$ applied to $\boldsymbol{e_{j}}$ where $\boldsymbol{e_{j}}$ is in turn given by the summation of a rotation applied to $\boldsymbol{e_{j}^{x}}$, a rotation applied to the input representation $\boldsymbol{x_{j}}$ of the vertex $v_{j}$ and a rotation applied to the input representation $\boldsymbol{x}$ of the vertex $v$. -->
è¿™ä¸ªéšè—çš„è¾¹è¡¨ç¤º$\boldsymbol{e_{j}^{h}}$æ˜¯ç”±æ€»å’Œåˆå§‹è¾¹ç¼˜è¡¨ç¤º$\boldsymbol{e_{j}^{x}}$å’Œ$\texttt{ReLU}(\cdot)$æ‰€å¾—åˆ°ï¼Œè¿™é‡Œçš„ä¿®æ­£çº¿æ€§å•å…ƒ$\texttt{ReLU}(\cdot)$ï¼Œå°±æ˜¯ç”¨åœ¨$\boldsymbol{e_{j}}$ã€‚è€Œ$\boldsymbol{e_{j}}$å°±æ˜¯ç”±æ€»å’Œæ‰€æœ‰è¿™äº›ä¸œè¥¿ä»¬: 1. ç”¨åœ¨$\boldsymbol{x_{j}}$ä¸Šçš„æ—‹è½¬ï¹‘2.ä¸€ä¸ªç”¨åœ¨é¡¶ç‚¹$v_{j}$çš„è¾“å…¥è¡¨ç¤ºxjçš„æ—‹è½¬ï¹‘3.ä¸€ä¸ªç”¨åœ¨é¡¶ç‚¹$v$çš„è¾“å…¥è¡¨ç¤º $\boldsymbol{x}$çš„æ—‹è½¬ã€‚

<!-- *Note: In order to calculate hidden representations downstream (*e.g.* $2^\text{nd}$ layer hidden representations), we can simply replace the input feature representations by the $1^\text{st}$ layer feature representations in the equations above.* -->
*æ³¨æ„: ä¸ºäº†è®¡ç®—å‡ºä½å±‚çš„éšè—è¡¨ç¤º(æ¯”å¦‚è¯´$2^\text{nd}$å±‚çš„éšè—è¡¨ç¤º)ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸Šæ–¹çš„æ–¹ç¨‹å¼ç®€å•åœ°æ›´æ¢è¾“å…¥ç‰¹å¾è¡¨ç¤ºä¸º$1^\text{st}$å±‚ç‰¹å¾è¡¨ç¤ºã€‚*


<!-- ### Graph Classification and Residual Gated GCN Layer -->
### å›¾å½¢åˆ†ç±»å’Œæ®‹å·®é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œçš„å±‚

<!-- In this section, we introduce the problem of graph classification and code up a Residual Gated GCN layer. In addition to the usual import statements, we add the following: -->
åœ¨è¿™ä¸ªéƒ¨ä»½ï¼Œæˆ‘ä»¬ä»‹ç»äº†å›¾å½¢åˆ†ç±»é—®é¢˜å’Œå†™æ®‹å·®é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œçš„ä»£ç çš„å±‚ã€‚ä¸å•æ­¢ä¸€èˆ¬ä¹Ÿè¦çš„æ±‡å…¥ä»¬ï¼Œæˆ‘ä»¬ä¹ŸåŠ è¿™äº›:

```python
os.environ['DGLBACKEND'] = 'pytorch'
import dgl
from dgl import DGLGraph
from dgl.data import MiniGCDataset
import networkx as nx
```

<!-- The first line tells DGL to use PyTorch as the backend. Deep Graph Library ([DGL](https://www.dgl.ai/)) provides various functionalities on graphs whereas networkx allows us to visualise the graphs. -->
ç¬¬ä¸€è¡Œä»¤DGLå»ç”¨Pytorchä½œä¸ºåç«¯ã€‚æ·±åº¦å›¾å½¢å‡½å¼åº“([DGL](https://www.dgl.ai/))åœ¨å›¾å½¢ä»¬ä¸Šæä¾›å„å¼å„æ ·çš„åŠŸèƒ½ï¼Œè€Œnetworkxå®¹è®¸æˆ‘ä»¬å»è§†è§‰åœ°çœ‹æ˜æ˜è¿™äº›å›¾å½¢ã€‚

<!-- In this notebook, the task is to classify a given graph structure into one of 8 graph types. The dataset obtained from `dgl.data.MiniGCDataset` yields some number of graphs (`num_graphs`) with nodes between `min_num_v` and `max_num_v`. Hence, all the graphs obtained do not have the same number of nodes/vertices. -->
åœ¨è¿™ä¸ªç¬”è®°æœ¬ï¼Œä»»åŠ¡æ˜¯å»åˆ†ç±»å‡ºä¸€ä¸ªç»™äºˆçš„å›¾å½¢ç»“æ„ä¸º8ç§ä¸åŒçš„å›¾å½¢çš„å…¶ä¸­ä¹‹ä¸€ã€‚è€Œåˆ©ç”¨`min_num_v`å’Œ`max_num_v`ä¹‹é—´çš„èŠ‚ç‚¹å»åœ¨`dgl.data.MiniGCDataset`å¾—åˆ°çš„æ•°æ®é›†ï¼Œå°±ä¼šç»™å‡ºä¸€äº›å›¾å½¢çš„æ•°å­—(`num_graphs`)ã€‚æ‰€ä»¥ï¼Œæ‰€æœ‰å¾—åˆ°çš„å›¾å½¢ä¸ä¼šæœ‰åŒæ ·çš„èŠ‚ç‚¹æˆ–é¡¶ç‚¹çš„æ•°å­—ã€‚


<!-- *Note: In order to familiarize yourself with the basics of `DGLGraphs`, it is recommended to go through the short tutorial [here](https://docs.dgl.ai/api/python/graph.html).* -->
*ä¸ºäº†ä½¿æ‚¨ç†Ÿæ‚‰DGLGraphsçš„åŸºç¡€çŸ¥è¯†ï¼Œå»ºè®®æ‚¨é˜…è¯»[é€™ä¸ª](https://docs.dgl.ai/api/python/graph.html)ç®€çŸ­çš„æ•™ç¨‹*

<!-- Having created the graphs, the next task is to add some signal to the domain. Features can be applied to nodes and edges of a `DGLGraph`. The features are represented as a dictionary of names (strings) and tensors (**fields**). `ndata` and `edata` are syntax sugar to access the feature data of all nodes and edges. -->
åˆ›é€ è¿™ä¸ªå›¾å½¢åï¼Œä¸‹ä¸€æ­¥å°±æ˜¯åœ¨åŸŸä¸­åŠ ä¸€äº›ä¿¡å·ã€‚ç‰¹å¾å¯ä»¥è¢«åŠ åˆ°å»èŠ‚ç‚¹å’Œ`DGLGraph`çš„è¾¹ã€‚è€Œç‰¹å¾æ˜¯ä»¥åå­—ï¼ˆå­—ç¬¦ä¸²ï¼‰å’Œå¼ é‡ï¼ˆ**å­—æ®µï¼Œè‹±æ–‡field**ï¼‰ã€‚ `ndata`å’Œ`edata`éƒ½æ˜¯è¯­æ³•ç³–æ¥è®¿é—®æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹çš„ç‰¹å¾æ•°æ®ã€‚

<!-- The following code snippet shows how the features are generated. Each node is assigned a value equal to the number of incident edges, whereas each edge is assigned the value 1. -->
è€Œæ¥ä¸‹æ¥çš„ä»£ç ç‰‡æ®µæ˜¾ç¤ºå‡ºç‰¹å¾å¦‚ä½•ç”Ÿæˆå‡ºæ¥ã€‚è€Œæ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½è®¾æœ‰ä¸€ä¸ªæ•°å€¼ï¼Œè¿™æ•°å€¼ç­‰äºæœ‰å¤šå°‘æ¡å’Œåˆ«çš„èŠ‚ç‚¹è¿èµ·æ¥çš„è¿æ¥è¾¹(incident edges)ï¼Œè¿™é‡Œæ¯ä¸€æ¡è¾¹çš„æ•°å€¼è®¾ä¸º1ã€‚
```python
def create_artificial_features(dataset):
    for (graph, _) in dataset:
        graph.ndata['feat'] = graph.in_degrees().view(-1, 1).float()
        graph.edata['feat'] = torch.ones(graph.number_of_edges(), 1)
    return dataset
```

<!-- Training and testing datasets are created and features are assigned as shown below: -->
è®­ç»ƒç”¨çš„æ•°æ®é›†å’Œæµ‹è¯•ç”¨çš„æ•°æ®é›†éƒ½è¢«åˆ›é€ å‡ºæ¥ï¼Œè€Œç‰¹å¾éƒ½è¢«è®¾ä¸ºä¸‹æ–¹é‚£æ ·:

```python
trainset = MiniGCDataset(350, 10, 20)
testset = MiniGCDataset(100, 10, 20)

trainset = create_artificial_features(trainset)
testset = create_artificial_features(testset)
```

<!-- A sample graph from the trainset has the following representation. Here, we observe that the graph has 15 nodes and 45 edges and both the nodes and edges have a feature representation of shape `(1,)` as expected. Furthermore, the `0` signifies that this graph belongs to class 0. -->
ä¸€ä¸ªæ¥çš„è®­ç»ƒé›†çš„å›¾å½¢å°±æœ‰ä»¥ä¸‹çš„è®¾å®šã€‚è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°å›¾å½¢æœ‰15ä¸ªèŠ‚ç‚¹å’Œ45æ¡è¾¹ï¼Œè€Œä¸”ä¸¤ä¸ªèŠ‚ç‚¹å’Œè¾¹éƒ½æœ‰å¦‚é¢„æœŸé‚£æ ·å¤§å°ä¸º`(1, )`çš„ç‰¹å¾è¡¨ç¤ºã€‚è€Œä¸”ï¼Œè¿™ä¸ª0è¡¨ç¤ºè¿™ä¸ªå›¾å½¢æ˜¯å±äºç¬¬`0`ç±»ã€‚
```python
(DGLGraph(num_nodes=15, num_edges=45,
         ndata_schemes={'feat': Scheme(shape=(1,), dtype=torch.float32)}
         edata_schemes={'feat': Scheme(shape=(1,), dtype=torch.float32)}), 0)
```


<!-- ### Note about DGL Message and Reduce Functions -->
### æœ‰å…³DGLä¿¡æ¯çš„ç¬”è®°å’Œé™ä½å‡½æ•°

<!-- >In DGL, the *message functions* are expressed as **Edge UDF**s (User Defined Functions). Edge UDFs take in a single argument `edges`. It has three members `src`, `dst`, and `data` for accessing source node features, destination node features, and edge features respectively.
The *reduce functions* are **Node UDF**s. Node UDFs have a single argument `nodes`, which has two members `data` and `mailbox`. `data` contains the node features and `mailbox` contains all incoming message features, stacked along the second dimension (hence the `dim=1` argument).
`update_all(message_func, reduce_func)` sends messages through all edges and updates all nodes. -->
>åœ¨DGLï¼Œ*ä¿¡æ¯å‡½æ•°*ä»¬éƒ½è¢«è¡¨è¾¾ä¸º**è¾¹UDFs** (ä½¿ç”¨è€…å®šä¹‰å‡ºæ¥çš„å‡½æ•°)ã€‚è¾¹UDFsä¼šæ¥æ”¶ä¸€ä¸ªå•ä¸€çš„å®å‚`edges`ã€‚å®ƒæœ‰3ä¸ªä¸œè¥¿:`src`å’Œ`dst`ï¹‘`data`ã€‚è¿™ä¸‰ä¸ªä¸œè¥¿éƒ½å¯¹åº”åœ°ç”¨æ¥è®¿é—®æºå¤´èŠ‚ç‚¹çš„ç‰¹å¾å’Œç›®çš„åœ°èŠ‚ç‚¹çš„ç‰¹å¾ï¹‘è¾¹çš„ç‰¹å¾ã€‚è¿™ä¸ª`é™ä½å‡½æ•°`éƒ½æ˜¯``èŠ‚ç‚¹UDFs``ã€‚èŠ‚ç‚¹UDFsæœ‰ä¸€ä¸ªå®å‚node(`èŠ‚ç‚¹`)ï¼Œä¹Ÿå°±æ˜¯æœ‰ä¸¤ä¸ªä¸œè¥¿`data`å’Œ`mailbox`ã€‚å¯¹åº”ç€ç¬¬äºŒä¸ªç»´åº¦æ¥å †èµ·æ¥(æ‰€ä»¥è¿™é‡Œæ˜¯`dim=1`)ï¼Œé‚£`data`å°±åŒ…å«çš„èŠ‚ç‚¹ç‰¹å¾å’Œ`mailbox`å°±åŒ…å«æ‰€æœ‰ç”±å¤–å…¥æ¥çš„ä¿¡æ¯ç‰¹å¾ã€‚è€Œè¿™ä¸ªå‡½æ•°`update_all(message_func, reduce_func)`ä¼šå‘é€æ‰€æœ‰ä¿¡æ¯è¿è¿‡æ‰€æœ‰è¾¹ï¼Œç„¶åæ›´æ–°æ‰€æœ‰èŠ‚ç‚¹ã€‚


<!-- ### [Gated Residual GCN Layer Implementation](https://www.youtube.com/watch?v=2aKXWqkbpWg&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=25&t=2098s) -->
### [é—¨æ§å¼æ®‹å‰©å›¾å½¢å·ç§¯ç½‘ç»œçš„å±‚çš„å®ä½œ](https://www.youtube.com/watch?v=2aKXWqkbpWg&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=25&t=2098s)

<!-- A Gated Residual GCN layer is implemented as shown in the code snippets below. -->
ä¸€ä¸ªé—¨æ§å¼æ®‹å‰©å›¾å½¢å·ç§¯ç½‘ç»œçš„å±‚çš„å®è¡Œå°±å¦‚ä»¥ä¸‹çš„ä»£ç æ‰€ç¤ºã€‚

<!-- Firstly, all the rotations of the input features $\boldsymbol{Ax}$, $\boldsymbol{Bx_{j}}$, $\boldsymbol{Ce_{j}^{x}}$, $\boldsymbol{Dx_{j}}$ and $\boldsymbol{Ex}$ are computed by defining `nn.Linear` layers inside the `__init__` function and then forward propagating the input representations `h` and `e` through the linear layers inside the `forward` function. -->
ç¬¬ä¸€ï¼Œæ‰€æœ‰çš„è¾“å…¥ç‰¹å¾çš„æ—‹è½¬$\boldsymbol{Ax}$å’Œ$\boldsymbol{Bx_{j}}$ï¹‘$\boldsymbol{Ce_{j}^{x}}$ï¹‘$\boldsymbol{Dx_{j}}$ï¹‘$\boldsymbol{Ex}$éƒ½æ˜¯ä»¥åœ¨`__init__`å®šä¹‰å‡ºæ¥çš„`nn.Linear`çš„å±‚ä»¬è®¡ç®—å‡ºæ¥çš„ï¼Œä¹‹åå°±æ˜¯å¯¹è¾“å…¥è¡¨ç¤º`h`å’Œ`e`ç”±åœ¨`forward` å‡½æ•¸ä¸­çš„çº¿æ€§å±‚(è‹±æ–‡linear layers)ä¸­è¿›è¡Œå‰å‘ä¼ æ’­ã€‚

```python
class GatedGCN_layer(nn.Module):

    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.A = nn.Linear(input_dim, output_dim)
        self.B = nn.Linear(input_dim, output_dim)
        self.C = nn.Linear(input_dim, output_dim)
        self.D = nn.Linear(input_dim, output_dim)
        self.E = nn.Linear(input_dim, output_dim)
        self.bn_node_h = nn.BatchNorm1d(output_dim)
        self.bn_node_e = nn.BatchNorm1d(output_dim)
```

<!-- Secondly, we compute the edge representations. This is done inside the `message_func` function, which iterates over all the edges and computes the edge representations. Specifically, the line `e_ij = edges.data['Ce'] + edges.src['Dh'] + edges.dst['Eh']` computes *(Eq. 7)* from above. The `message_func` function ships `Bh_j` (which is $\boldsymbol{Bx_{j}}$ from (Eq. 5)) and `e_ij` (Eq. 7) through the edge into the destination node's mailbox. -->
ç¬¬äºŒï¼Œæˆ‘ä»¬è®¡ç®—è¿™ä¸ªè¾¹è¡¨ç¤ºã€‚è¿™æ˜¯åœ¨`message_func`å‡½æ•°ä¸­å®Œæˆçš„ï¼Œä¹Ÿå°±å¯¹æ‰€æœ‰è¾¹è¿›è¡Œè¿­ä»£ï¼ŒåŒæ—¶å»è®¡ç®—è¾¹çš„ç‰¹å¾ã€‚ç‰¹åˆ«åœ°ï¼Œè¿™æ¡çº¿`e_ij = edges.data['Ce'] + edges.src['Dh'] + edges.dst['Eh']`åœ¨ä¸Šæ–¹é‚£é‡Œè®¡ç®—å‡ºæ¥*(æ–¹ç¨‹å¼7)*ã€‚è€Œè¿™ä¸ª`message_func`å‡½æ•°é€è¿™ä¸ª`Bh_j`(ä¹Ÿå°±æ˜¯æ–¹ç¨‹å¼5çš„$\boldsymbol{Bx_{j}}$ )å’Œè¿™ä¸ª`e_ij`(æ–¹ç¨‹å¼7)ï¼Œé€šè¿‡è¾¹ï¼Œé€åˆ°ç›®çš„åœ°èŠ‚ç‚¹çš„é‚®ç®±ã€‚

```python
def message_func(self, edges):
    Bh_j = edges.src['Bh']
    # e_ij = Ce_ij + Dhi + Ehj
    e_ij = edges.data['Ce'] + edges.src['Dh'] + edges.dst['Eh']
    edges.data['e'] = e_ij
    return {'Bh_j' : Bh_j, 'e_ij' : e_ij}
```

<!-- Thirdly, the `reduce_func` function collects the shipped messages by the `message_func` function. After collecting the node data `Ah` and shipped data `Bh_j` and `e_ij` from the `mailbox`, the line `h = Ah_i + torch.sum(sigma_ij * Bh_j, dim=1) / torch.sum(sigma_ij, dim=1)` computes the hidden representation of each node as indicated in (Eq. 5). Note however, that this only represents the term $(\boldsymbol{Ax} + \sum_{v_jâ†’v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}})$ without the $\texttt{ReLU}(\cdot)$ and residual connection. -->
ç¬¬3ï¼Œè¿™ä¸ª`reduce_func`å‡½æ•°ä»¥`message_func`å‡½æ•°æ¥æ”¶é›†é€å‡ºå»çš„ä¿¡æ¯ã€‚ä¹‹åæ”¶é›†èŠ‚ç‚¹æ•°æ®`Ah`ï¼Œç„¶åç”±`mailbox`é€å‡ºä¸¤ä¸ªæ•°æ®`Bh_j`å’Œ`e_ij`ã€‚è€Œè¿™æ¡çº¿`h = Ah_i + torch.sum(sigma_ij * Bh_j, dim=1) / torch.sum(sigma_ij, dim=1)`è®¡ç®—æ¯ä¸€ä¸ªèŠ‚ç‚¹çš„éšè—è¡¨ç¤ºï¼Œæ–¹ç¨‹å¼5æ˜¾ç¤ºå‡ºæ¥ã€‚æ³¨æ„ä¸€ç‚¹ï¼Œä½†æ˜¯æ˜¯è¿™æ ·çš„ï¼Œè¿™ä¸ªé¡¹$(\boldsymbol{Ax} + \sum_{v_jâ†’v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}})$æ˜¯æ²¡æœ‰$\texttt{ReLU}(\cdot)$å’Œæ®‹å‰©è¿æ¥æ¥è¡¨ç¤ºå‡ºæ¥çš„ã€‚

```python
def reduce_func(self, nodes):
    Ah_i = nodes.data['Ah']
    Bh_j = nodes.mailbox['Bh_j']
    e = nodes.mailbox['e_ij']
    # sigma_ij = sigmoid(e_ij)
    sigma_ij = torch.sigmoid(e)
    # hi = Ahi + sum_j eta_ij * Bhj
    h = Ah_i + torch.sum(sigma_ij * Bh_j, dim=1) / torch.sum(sigma_ij, dim=1)
    return {'h' : h}
```

<!-- Inside the `forward` function, having called `g.update_all`, we obtain the results of graph convolution `h` and `e`, which represent the terms $(\boldsymbol{Ax} + \sum_{v_jâ†’v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}})$ from (Eq.5) and $\boldsymbol{e_{j}}$ from (Eq. 7) respectively. Then, we normalize `h` and `e` with respect to the graph node size and graph edge size respectively. Batch normalization is then applied so that we can train the network effectively. Finally, we apply $\texttt{ReLU}(\cdot)$ and add the residual connections to obtain the hidden representations for the nodes and edges, which are then returned by the `forward` function. -->
åœ¨`forward`å‡½æ•°ä¸­ï¼Œå«`g.update_all`çš„è¯ï¼Œæˆ‘ä»¬å°±å¾—åˆ°å›¾å½¢å·ç§¯çš„ç»“æœ`h`å’Œ`e`ï¼Œä¹Ÿå°±è¿™ä¸€ä¸ªé¡¹å¯¹åº”åœ°è¡¨ç¤ºå‡ºè¿™ä¸ªæ¥è‡ªæ–¹ç¨‹å¼5çš„é¡¹$(\boldsymbol{Ax} + \sum_{v_jâ†’v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}})$å’Œæ¥è‡ªæ–¹ç¨‹å¼7çš„$\boldsymbol{e_{j}}$ã€‚ä¹‹åæˆ‘ä»¬å°±ä»¥ã€Œå¯¹ç€(respect)ã€å›¾å½¢çš„èŠ‚ç‚¹å¤§å°å’Œå›¾å½¢è¾¹çš„å¤§å°ï¼Œè¿™ä¸ªä¸¤ä¸ªåˆ†åˆ«å¯¹åº”è¿™ä¸¤ä¸ª`h`å’Œ`e`æ¥å½’ä¸€åŒ–ã€‚è€Œæ‰¹é‡æ ‡å‡†åŒ–å°±æ˜¯ä¹‹åç”¨ä¸Šçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±å¯ä»¥æœ‰æ•ˆåœ°è®­ç»ƒç½‘ç»œã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨ä¸Šä¸€ä¸ª$\texttt{ReLU}(\cdot)$å’ŒåŠ ä¸€ä¸ªæ®‹å‰©è¿æ¥æ¥ä¸ºè¿™äº›èŠ‚ç‚¹ä»¬å’Œè¾¹ä»¬å–å¾—ä¸€äº›éšè—è¡¨ç¤ºï¼Œä¹Ÿå°±ä¹‹åä»¥`forward`å‡½æ•°ä½œä¸ºå›ä¼ å€¼å›ä¼ å‡ºæ¥ã€‚

```python
def forward(self, g, h, e, snorm_n, snorm_e):

    h_in = h # æ®‹å‰©è¿æ¥
    e_in = e # æ®‹å‰©è¿æ¥

    g.ndata['h']  = h
    g.ndata['Ah'] = self.A(h)
    g.ndata['Bh'] = self.B(h)
    g.ndata['Dh'] = self.D(h)
    g.ndata['Eh'] = self.E(h)
    g.edata['e']  = e
    g.edata['Ce'] = self.C(e)

    g.update_all(self.message_func, self.reduce_func)

    h = g.ndata['h'] # å›¾å½¢å·ç§¯çš„ç»“æœ
    e = g.edata['e'] # å›¾å½¢å·ç§¯çš„ç»“æœ

    h = h * snorm_n # ä»¥ã€Œå¯¹ç€(respect)ã€æ¿€æ´»æ¥å½’ä¸€åŒ–
    e = e * snorm_e # ä»¥ã€Œå¯¹ç€(respect)ã€æ¿€æ´»æ¥å½’ä¸€åŒ–

    h = self.bn_node_h(h) # æ‰¹é‡æ ‡å‡†åŒ–
    e = self.bn_node_e(e) # æ‰¹é‡æ ‡å‡†åŒ–

    h = torch.relu(h) # éçº¿æ€§æ¿€æ´»
    e = torch.relu(e) # éçº¿æ€§æ¿€æ´»

    h = h_in + h # æ®‹å‰©è¿æ¥
    e = e_in + e # æ®‹å‰©è¿æ¥

    return h, e
```

<!-- Next, we define the `MLP_Layer` module which contains several fully connected layers (FCN). We create a list of fully connected layers and forward through the network. -->
ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å®šä¹‰`MLP_Layer`æ¨¡ç»„ä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ªå®Œå…¨æ€§è¿æ¥å±‚(è‹±æ–‡:fully connected layers (FCN))ã€‚æˆ‘ä»¬åˆ›é€ ä¸€åˆ—è¿æ¥å±‚ä»¬ï¼Œç„¶åå‘å‰å¼é€šè¿‡è¿™ä¸ªç½‘ç»œã€‚

<!-- Finally, we define our `GatedGCN` model which comprises of the previously defined classes: `GatedGCN_layer` and `MLP_layer`. The definition of our model (`GatedGCN`) is shown below. -->
æœ€åæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„é—¨æ§å¼å›¾å½¢å·ç§¯ç½‘ç»œæ¨¡å‹ï¼Œè‹±æ–‡ä»£è¡¨ä¸º`GatedGCN`ï¼Œä¹Ÿå°±åŒ…å«ä¹‹å‰å®šä¹‰å¥½çš„ç±»: `GatedGCN_layer`å’Œ`MLP_layer`ã€‚è¿™ä¸ªæˆ‘ä»¬çš„æ¨¡å‹çš„å®šä¹‰(GatedGCN)å°±åœ¨ä¸‹æ–¹æ‰€ç¤ºã€‚

 ```python
 class GatedGCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, L):
        super().__init__()
        self.embedding_h = nn.Linear(input_dim, hidden_dim)
        self.embedding_e = nn.Linear(1, hidden_dim)
        self.GatedGCN_layers = nn.ModuleList([
            GatedGCN_layer(hidden_dim, hidden_dim) for _ in range(L)
        ])
        self.MLP_layer = MLP_layer(hidden_dim, output_dim)
    def forward(self, g, h, e, snorm_n, snorm_e):
        # è¾“å…¥åµŒå…¥
        h = self.embedding_h(h)
        e = self.embedding_e(e)
        # å›¾å½¢å·ç§¯å±‚ä»¬
        for GGCN_layer in self.GatedGCN_layers:
            h, e = GGCN_layer(g, h, e, snorm_n, snorm_e)
        # MLPåˆ†ç±»å™¨
        g.ndata['h'] = h
        y = dgl.mean_nodes(g,'h')
        y = self.MLP_layer(y)
        return y
 ```

<!-- In our constructor, we define the embeddings for `e` and `h` (`self.embedding_e ` and `self.embedding_h`), `self.GatedGCN_layers` which is list (of size $L$) of our previously defined model: `GatedGCN_layer`, and finally `self.MLP_layer` which was also defined before. Next, using these initializations, we simply foward through the model and output `y`. -->
åœ¨æˆ‘å€‘çš„å»ºæ§‹å­ï¼Œæˆ‘å€‘å®šç¾©å¥½ç”¨åœ¨`e`å’Œ`h`çš„åµŒå…¥(`self.embedding_e`å’Œ`self.embedding_h`)ï¼Œè€Œ`self.GatedGCN_layers`ä¹Ÿå°±æ˜¯ä¸€åˆ—å¤§å°ç‚º$L$çš„ï¼Œè€Œä¸”æ˜¯æˆ‘å€‘ä¹‹å‰å®šç¾©å¥½çš„æ¨¡å‹: `GatedGCN_layer`ï¼Œè€Œä¸”æœ€å¾Œå°±æ˜¯é€™å€‹`self.MLP_layer`ï¼Œä¹Ÿéƒ½æ˜¯ä¹‹å‰å®šç¾©å¥½çš„ã€‚ä¸‹ä¸€æ­¥ï¼Œç”¨é€™äº›åˆå§‹åŒ–çš„è©±ï¼Œæˆ‘å€‘å°±ç°¡å–®åœ°å‘å‰å¼é€šéé€™å€‹æ¨¡å‹ï¼Œç„¶å¾Œå°±å¾—å‡ºè¼¸å‡º`y`ã€‚

<!-- To better understand the model, we initiate an object of the model and print it for better visualization: -->
ç‚ºäº†æ›´å¥½çš„æ˜ç™½æ¨¡å‹ï¼Œæˆ‘å€‘åˆå§‹ä¸€å€‹æ¨¡å‹çš„ç‰©ä½“ï¼Œæˆ‘å€‘ä¹ŸæŠŠå®ƒé¡¯ç¤ºå‡ºä¾†æ›´å¥½åœ°ç”¨è¦–è¦ºä¾†çœ‹æ¸…æ¥šé€™å€‹æ¨¡å‹:

```python
# å¯¦ä¾‹åŒ–é€™å€‹ç¶²çµ¡
model = GatedGCN(input_dim=1, hidden_dim=100, output_dim=8, L=2)
print(model)
```

<!-- The main structure of the model is shown below: -->
å¯¦ä¾‹åŒ–é€™å€‹ç¶²çµ¡

```python
GatedGCN(
  (embedding_h): Linear(in_features=1, out_features=100, bias=True)
  (embedding_e): Linear(in_features=1, out_features=100, bias=True)
  (GatedGCN_layers): ModuleList(
    (0): GatedGCN_layer(...)
    (1): GatedGCN_layer(... ))
  (MLP_layer): MLP_layer(
    (FC_layers): ModuleList(...))
```

<!-- Not surprisingly, we have two layers of `GatedGCN_layer` (since `L=2`) followed by a `MLP_layer` which finally yields an output of 8 values. -->
ä¸å‡ºæ‰€æ–™ï¼Œæˆ‘å€‘æœ‰å…©å±¤ä¾†è‡ª`GatedGCN_layer`çš„å±¤ï¼Œé‚£æ˜¯æˆ‘å€‘åœ¨`MLP_layer`å¾Œè¨­å®šäº†`L=2`ï¼Œä¹Ÿå°±æœƒè¼¸å‡º8å€‹æ•¸å€¼ã€‚

<!-- Moving on, we define our `train` and `evaluate` functions. In our `train` function, we have our generic code which takes samples from `dataloader`.  Next, `batch_graphs`, `batch_x`, `batch_e`, `batch_snorm_n` and `batch_snorm_e` are fed into our model which returns `batch_scores` (of size 8). The predicted scores are compared with the ground truth in our loss function: `loss(batch_scores, batch_labels)`. Then, we zero out gradients (`optimizer.zero_grad()`), perform backprop (`J.backward()`) and updates our weights (`optimizer.step()`). Finally, loss for epoch and training accuracy is calculated. Furthermore, we use a similar code for our `evaluate` function. -->
ç¹¼çºŒèªªä¸‹å»å§ï¼Œæˆ‘å€‘å®šç¾©å‡ºæˆ‘å€‘çš„`train`å’Œ`evaluate`å‡½æ•¸ã€‚åœ¨æˆ‘å€‘çš„`train`å‡½æ•¸ï¼Œæˆ‘å€‘æœ‰æˆ‘å€‘çš„é€šç”¨æ€§çš„ä»£ç¢¼ï¼Œä¹Ÿå°±æ˜¯ç”±`dataloader`ä¸­å–å¾—ä¸€äº›æ¨£æœ¬ã€‚ä¸‹ä¸€æ­¥ï¼Œ`batch_graphs`å’Œ `batch_x`ï¹‘ `batch_e`ï¹‘ `batch_snorm_n` ï¹‘ `batch_snorm_e`éƒ½è¢«è¼¸å…¥åˆ°æˆ‘å€‘çš„æ¨¡å‹ä¸­ï¼Œä¹Ÿå°±æ˜¯æœƒå›å‚³å‡º`batch_scores`(å¤§å°ç‚º8)ã€‚è€Œé æ¸¬åˆ†æ•¸å°±æœƒåœ¨æå¤±å‡½æ•¸ä¸­è¢«ç”¨ä¾†èˆ‡å¯¦éš›æ­£ç¢ºç­”æ¡ˆ(ground truth)ä¾†åšæ¯”è¼ƒ:`loss(batch_scores, batch_labels)`ã€‚ä¹‹å¾Œï¼Œæˆ‘å€‘æŠŠæ¢¯åº¦è¨­å›é›¶(`optimizer.zero_grad()`)ï¼Œé€²è¡Œå›å‚³(`J.backward()`)å’Œæ›´æ–°æˆ‘å€‘çš„æ¬Šé‡(`optimizer.step()`)ã€‚æœ€å¾Œï¼Œæ¯ä¸€å€‹å‘¨æœŸçš„æå¤±å’Œè¨“ç·´æº–ç¢ºåº¦å°±æœƒè¢«è¨ˆç®—å‡ºä¾†äº†ã€‚åŠ ä¸Šï¼Œæˆ‘å€‘åœ¨`evaluate`å‡½æ•¸ä¸­ä½¿ç”¨ç›¸ä¼¼çš„ä»£ç¢¼ã€‚

<!-- Finally, we are ready to train! We found that after 40 epochs of training, our model has learned to classify the graphs with a test accuracy of $87$%. -->
æœ€å¾Œäº†ï¼Œæˆ‘å€‘å°±æº–å‚™å¥½å»è¨“ç·´äº†!æˆ‘å€‘çœ‹åˆ°40å€‹å‘¨æœŸè¨“ç·´ï¼Œæˆ‘å€‘çš„æ¨¡å‹å°±å­¸åˆ°å¦‚ä½•å»åˆ†é¡å‡ºåœ–å½¢ï¼ŒåŒæ™‚æ¸¬è©¦æº–ç¢ºåº¦ç‚º$87$%ã€‚

---
lang: zh
lang-ref: ch.02
title: 第二周
translation-date: 29 Feb 2020
translator: Matt Taso
---

## 讲座 A 部分

我们从理解什么是参数化的模型开始，然后讨论什么是损失函数。之后我们会涉及基于梯度的方法以及这些方法是如何被应用到传统神经网络中的反向传播算法中。最后，我们会学习如何使用PyTorch实现一个神经网络以及讨论一种反向传播的更广义的形式。


## 讲座 B 部分

我们从一个反向传播的具体例子开始，进而讨论Jacobian矩阵的维度。然后，我们会着眼于多种基础神经网络模块并计算它们的梯度，之后对softmax和logsoftmax进行简短的讨论。最后会在这个部分学习一些反向传播的实用技巧。


## 动手做

我们给出了使用（人工）神经网络进行监督学习的简介，阐述相关问题的形成以及训练这些网络所用的经典数据。我们也讨论了如何训练一个神经网络来解决多分类问题，以及在该网络训练好之后如何使用它进行推断

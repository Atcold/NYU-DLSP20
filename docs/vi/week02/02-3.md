---
lang-ref: ch.02-1
lecturer: Yann LeCun
title: Mạng thần kinh nhân tạo (ANNs)
authors: Alfredo Canziani
date: 4 Feb 2020
typora-root-url: 02-3
translator: Huynh Nguyen
lang: vi
translator-date: 29 Jul 2021
---

<!--## [Supervised learning for classification](https://www.youtube.com/watch?v=WAn6lip5oWk&t=150s)
-->
## [Học giám sát để phân loại](https://www.youtube.com/watch?v=WAn6lip5oWk&t=150s)

<!--* Consider **Fig. 1(a)** below. The points in this graph lie on the branches of the spiral, and live in $\R^2$. Each colour represents a class label. The number of unique classes is $K = 3$. This is represented mathematically by **Eqn. 1(a)**.-->
- Xem xét **Hình 1(a)** bên dưới. Các điểm trong biểu đồ này nằm trên các nhánh của đường xoắn ốc và nằm trong $\R^2$. Mỗi màu đại diện cho một nhãn (một lớp). Số lớp duy nhất là $K = 3$. Điều này được biểu diễn bằng toán học bằng **Phương trình 1(a)**.

<!--* **Fig. 1(b)** shows a similar spiral, with an added Gaussian noise term. This is represented mathematically by **Eqn. 1(b)**.
-->
- **Hình 1(b)** cho thấy một hình xoắn ốc tương tự, với một thuật ngữ nhiễu - Gaussian được thêm vào. Điều này được biểu diễn bằng toán học bằng **Phương trình 1(b)**.

<!--In both cases, these points are not linearly separable.
-->
Trong cả hai trường hợp, những điểm này không thể phân tách một cách tuyến tính.

<!--<center>
  <table border="0">
    <td>
      <center>
    <img src="{{site.baseurl}}/images/week02/02-3/clean-spiral.png" width="350px" /><br>
       <b>Fig. 1(a)</b> "Clean" 2D spiral
       </center>
      </td>
      <td>
      <center>
      <img src="{{site.baseurl}}/images/week02/02-3/noisy-spiral.png" width="350px" /><br>
       <b>Fig. 1(b)</b> "Noisy" 2D spiral
       </center>
      </td>
  </table>
  </center>
-->
<center>
  <table border="0">
    <td>
      <center>
    <img src="{{site.baseurl}}/images/week02/02-3/clean-spiral.png" width="350px" /><br>
       <<b>Hình 1(a)</b> hình xoắn ốc 2D "sạch"
       </center>
      </td>
      <td>
      <center>
      <img src="{{site.baseurl}}/images/week02/02-3/noisy-spiral.png" width="350px" /><br>
       <b>Hình 1(b)</b> hình xoắn ốc 2D "nhiễu"
       </center>
      </td>
  </table>
  </center>
  
<!--
$$
X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1)\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1)\right]}\end{array}\right) \\
0 \leq t \leq 1, \quad k=1, ..., K
$$
-->
$$
X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1)\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1)\right]}\end{array}\right) \\
0 \leq t \leq 1, \quad k=1, ..., K
$$

<!--<center><b>Eqn. 1(a)</b> </center>
-->
  <!--<center><b>Eqn. 1(a)</b> </center>-->
  <center><b>Phương trình 1(a)</b> </center>

<!--
$$
  X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]}\end{array}\right)\\0 \leq t \leq 1, \quad k=1, ..., K
$$
-->
$$
  X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]}\end{array}\right)\\0 \leq t \leq 1, \quad k=1, ..., K
$$

  <!--<center><b>Eqn. 1(b)</b></center>-->
  <center><b>Phương trình 1(b)</b></center>
  
 <!--What does it mean to perform **classification**? Consider the case of **logistic regression**. If logistic regression for classification is applied to this data, it will create a set of **linear planes** (decision boundaries) in an attempt to separate the data into its classes. The issue with this solution is that in each region, there are points belonging to multiple classes. The branches of the spiral cross the linear decision boundaries. This is **not** a great solution!
-->
Thực hiện **phân loại** nghĩa là gì? Hãy xem xét trường hợp **hồi quy logistic**. Nếu hồi quy logistic để phân loại được áp dụng cho dữ liệu này, nó sẽ tạo ra một tập hợp các **mặt phẳng tuyến tính** (ranh giới quyết định) nhằm cố gắng tách dữ liệu thành các lớp của nó. Vấn đề của giải pháp này là ở mỗi khu vực, có các điểm thuộc nhiều lớp. Các nhánh của đường xoắn ốc vượt qua ranh giới quyết định tuyến tính. Đây **không phải** là một giải pháp tuyệt vời!

<!--**How do we fix this?** We transform the input space such that the data are forced to be linearly separable. Over the course of training a neural network to do this, the decision boundaries that it learns will try to adapt to the distribution of the training data.
-->
**Làm thế nào để chúng tôi sửa lỗi này?** Chúng tôi biến đổi không gian đầu vào sao cho dữ liệu buộc phải được phân tách tuyến tính. Trong quá trình đào tạo mạng nơ-ron để làm điều này, các ranh giới quyết định mà nó học được sẽ cố gắng thích ứng với việc phân phối dữ liệu đào tạo.

<!--**Note:** A neural network is always represented from the **bottom up**. The first layer is at the bottom, and the last at the top. This is because conceptually, the input data are low-level features for whatever task the neural network is attempting. As the data traverse **upward** through the network, each subsequent layer extracts features at a higher level.
-->
**Lưu ý:** Mạng nơ-ron luôn được biểu diễn từ **dưới lên**. Lớp đầu tiên ở dưới cùng và lớp cuối cùng ở trên cùng. Điều này là do về mặt khái niệm, dữ liệu đầu vào là các tính năng cấp thấp cho bất kỳ tác vụ nào mà mạng nơ-ron đang thực hiện. Khi dữ liệu được **truyền lên** trên mạng, mỗi lớp tiếp theo sẽ trích xuất các tính năng ở mức cao hơn.

<!--## Training data
-->
## Dữ liệu đào tạo

<!--Last week, we saw that a newly initialised neural network transforms its input in an arbitrary way. This transformation, however, isn't **(initially)** instrumental in performing the task at hand. We explore how, using data, we can force this transformation to have some meaning that is relevant to the task at hand. The following are data used as training input for a network.
-->
Tuần trước, chúng ta đã thấy rằng một mạng nơ-ron mới được khởi tạo biến đổi đầu vào của nó theo một cách tùy ý. Tuy nhiên, sự biến đổi này **(ban đầu)** không phải là công cụ để thực hiện nhiệm vụ trong tầm tay. Chúng tôi khám phá cách, bằng cách sử dụng dữ liệu, chúng tôi có thể buộc sự chuyển đổi này có một số ý nghĩa có liên quan đến nhiệm vụ hiện tại. Sau đây là dữ liệu được sử dụng làm đầu vào huấn luyện cho một mạng.

<!--* $\vect{X}$ represents the input data, a matrix of dimensions $m$ (number of training data points) x $n$ (dimensionality of each input point). In case of the data shown in Figures **1(a)** and **1(b)**, $n = 2$.
-->
- $\vect{X}$ đại diện cho dữ liệu đầu vào, một ma trận có kích thước $m$ (số điểm dữ liệu đào tạo) x $n$ (thứ nguyên của mỗi điểm đầu vào).Trong trường hợp dữ liệu được hiển thị trong Hình **1(a)** và **1(b)**, $n = 2$.

<!--<center>
<img src="{{site.baseurl}}/images/week02/02-3/training-data.png" width="600px" /><br>
<b>Fig. 2</b> Training data
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week02/02-3/training-data.png" width="600px" /><br>
<b>Hình 2</b> dữ liệu đào tạo
</center>

<!--* Vector $\vect{c}$  and matrix $\boldsymbol{Y}$ both represent class labels for each of the $m$ data points. In the example above, there are $3$ distinct classes.
-->
Vectơ $\vect{c}$ và ma trận $\boldsymbol{Y}$ đều đại diện cho các lớp (nhãn) cho mỗi điểm dữ liệu $m$. Trong ví dụ trên, có $3$ các lớp riêng biệt.

<!--* $c_i \in \lbrace 1, 2, \cdots, K \rbrace$, and $\vect{c} \in \R^m$. However, we may not use $\vect{c}$ as training data. If we use distinct numeric class labels  $c_i \in \lbrace 1, 2, \cdots, K \rbrace$, the network may infer an order within the classes that isn't representative of the data distribution.
* To bypass this issue, we use a **one-hot encoding**. For each label $c_i$, a $K$ dimensional zero-vector $\vect{y}^{(i)}$ is created, which has the $c_i$-th element set to $1$ (see **Fig. 3** below).
-->
  - $c_i \in \lbrace 1, 2, \cdots, K \rbrace$, and $\vect{c} \in \R^m$. Tuy nhiên, chúng tôi không thể sử dụng $\vect{c}$ làm dữ liệu đào tạo. Nếu chúng ta sử dụng các nhãn lớp số riêng biệt $c_i \in \lbrace 1, 2, \cdots, K \rbrace$, mạng có thể suy ra thứ tự trong các lớp không đại diện cho phân phối dữ liệu. 
  - Để vượt qua vấn đề này, chúng tôi sử dụng **one-hot encoding**. Đối với mỗi nhãn $c_i$, một vector không $K$ chiều $\vect{y}^{(i)}$ được tạo, có phần tử thứ $c_i$ được đặt thành $1$ (xem thêm **Hình 3** bên dưới).

<!--<center>
<img src="{{site.baseurl}}/images/week02/02-3/one-hot.png" width="250px" /><br>
<b>Fig. 3</b> One hot encoding
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week02/02-3/one-hot.png" width="250px" /><br>
<b>Hình 3</b> One hot encoding
</center>

<!--* Therefore, $\boldsymbol Y \in \R^{m \times K}$. This matrix can also be thought of as having some probabilistic mass, which is fully concentrated on one of the $K$ spots.
-->
- Do đó, ma trận $\boldsymbol Y \in \R^{m \times K}$ này cũng có thể được coi là có một số khối lượng xác suất, tập trung hoàn toàn vào một trong các điểm $K$.

<!--## Fully (FC) connected layers-->
## Các lớp kết nối đầy đủ (FC)

<!--We will now take a look at what a fully connected (FC) network is, and how it works.
-->
Bây giờ chúng ta sẽ xem xét mạng được kết nối đầy đủ (FC) là gì và nó hoạt động như thế nào.

<!--<center>
<img src="{{site.baseurl}}/images/week02/02-3/FC-net.png" height="250px" /><br>
<b>Fig. 4</b> Fully connected neural network
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week02/02-3/FC-net.png" height="250px" /><br>
<b>Hình 4</b> Mạng nơ-ron được kết nối đầy đủ
</center>

<!--
Consider the network shown above in **Fig. 4**. The input data, $\boldsymbol x$, is subject to an affine transformation defined by $\boldsymbol W_h$, followed by a non-linear transformation. The result of this non-linear transformation is denoted as $\boldsymbol h$, representing a **hidden** output, *i.e* one that is not **seen** from outside the network. This is followed by another affine transformation ($\boldsymbol W_y$), followed by another non-linear transformation. This produces the final output, $\boldsymbol{\hat{y}}$. This network can be represented mathematically by the equations in **Eqn. 2** below. $f$ and $g$ are both non-linearities.
-->
Hãy xem xét mạng được hiển thị ở trên trong Hình 4. Dữ liệu đầu vào, $\boldsymbol x$, phải tuân theo một phép biến đổi affine được xác định bởi $\boldsymbol W_h$, theo sau là một phép biến đổi phi tuyến tính. Kết quả của phép biến đổi phi tuyến tính này được ký hiệu là $\boldsymbol h$, đại diện cho một đầu ra **ẩn**, tức là một đầu ra **không được nhìn thấy** từ bên ngoài mạng. Tiếp theo là một phép biến đổi affine khác ($\boldsymbol W_y$), tiếp theo là một phép biến đổi phi tuyến tính khác. Điều này tạo ra kết quả cuối cùng, $\boldsymbol{\hat{y}}$. Mạng này có thể được biểu diễn toán học bằng các phương trình trong **phương trình 2 bên dưới**. $f$ và $g$ đều không tuyến tính.

<!--$$
\begin{aligned}
&\boldsymbol h=f\left(\boldsymbol{W}_{h} \boldsymbol x+ \boldsymbol b_{h}\right)\\
&\boldsymbol{\hat{y}}=g\left(\boldsymbol{W}_{y} \boldsymbol h+ \boldsymbol b_{y}\right)
\end{aligned}
$$
-->
$$
\begin{aligned}
&\boldsymbol h=f\left(\boldsymbol{W}_{h} \boldsymbol x+ \boldsymbol b_{h}\right)\\
&\boldsymbol{\hat{y}}=g\left(\boldsymbol{W}_{y} \boldsymbol h+ \boldsymbol b_{y}\right)
\end{aligned}
$$

<center><b>Phương trình 2</b> Toán học đằng sau một mạng FC</center>

<!--A basic neural network such as the one shown above is merely a set of successive pairs, with each pair being an affine transformation followed by a non-linear operation (squashing). Frequently used non-linear functions include ReLU, sigmoid, hyperbolic tangent, and softmax.
-->
Một mạng nơ-ron cơ bản như mạng được trình bày ở trên chỉ là một tập hợp các cặp liên tiếp, với mỗi cặp là một phép biến đổi affine theo sau là một phép toán phi tuyến tính (squashing). Các hàm phi tuyến tính thường được sử dụng bao gồm ReLU, sigmoid, tiếp tuyến hyperbol và softmax.

<!--The network shown above is a 3-layer network:
-->
Mạng được hiển thị ở trên là mạng 3 lớp:

<!--
1. input neuron
2. hidden neuron
3. output neuron
-->
1. No-ron đầu vào
2. Nơ-ron ẩn
3. Nơ-ron đầu ra

<!--Therefore, a $3$-layer neural network has $2$ affine transformations. This can be extended to a $n$-layer network.
-->
Do đó, mạng nơ-ron $3$ lớp có các phép biến đổi affine $2$. Điều này có thể được mở rộng cho một mạng $n$ lớp.

<!--Now let's move to a more complicated case.
-->
Bây giờ chúng ta hãy chuyển sang một trường hợp phức tạp hơn.

<!--Let's do a case of 3 hidden layers, fully connected in each layer. An illustration can be found in **Fig. 5**
-->
Hãy làm một trường hợp gồm 3 lớp ẩn, được kết nối đầy đủ trong mỗi lớp. Một minh họa có thể được tìm thấy trong Hình 5

<!--<center>
<img src="{{site.baseurl}}/images/week02/02-3/pre-inference4layers.png" /><br>
<b>Fig. 5</b> Neural net with 3 hidden layers
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week02/02-3/pre-inference4layers.png" /><br>
<b>Hình 5</b> Mạng nơ-ron với 3 lớp mạng
</center>

<!--Let's consider a neuron $j$ in the second layer. It's activation is:
-->
Hãy xem xét một nơ-ron $j$ trong lớp thứ hai. Kích hoạt (activation) của nó là:

<!--
$$
a^{(2)}_j = f(\boldsymbol w^{(j)} \boldsymbol x + b_j) = f\Big( \big(\sum_{i=1}^n w_i^{(j)} x_i\big) +b_j ) \Big)
$$
-->
$$
a^{(2)}_j = f(\boldsymbol w^{(j)} \boldsymbol x + b_j) = f\Big( \big(\sum_{i=1}^n w_i^{(j)} x_i\big) +b_j ) \Big)
$$

<!--where $\vect{w}^{(j)}$ is the $j$-th row of $\vect{W}^{(1)}$.
-->
Trong đó: $\vect{w}^{(j)}$ là hàng thứ $j$ của $\vect{W}^{(1)}$.

<!--Notice that the activation of the input layer in this case is just the identity. The hidden layers can have activations like ReLU, hyperbolic tangent, sigmoid, soft (arg)max, *etc*.
-->
Lưu ý rằng việc kích hoạt lớp đầu vào trong trường hợp này chỉ là định danh. Các lớp ẩn có thể có kích hoạt như ReLU, tang (tangent) hyperbol, sigmoid, soft (arg)max...

<!--The activation of the last layer in general would depend on your use case, as explained in [this](https://piazza.com/class/k5spqaanqk51ks?cid=36) Piazza post.
-->

Việc kích hoạt lớp cuối cùng nói chung sẽ phụ thuộc vào trường hợp sử dụng của bạn, như được giải thích trong bài đăng trên [Piazza](https://piazza.com/class/k5spqaanqk51ks?cid=36).

<!--## Neural network (inference)
-->
## Mạng nơ-ron (suy luận)

<!--Let's think about the three-layer (input, hidden, output) neural network again, as seen in **Fig. 6**
-->
Chúng ta hãy nghĩ lại về mạng nơ-ron ba lớp (đầu vào, ẩn, đầu ra), như trong Hình 6

<!--<center>
<img src="{{site.baseurl}}/images/week02/02-3/2-layer-inference.png" height="250px"/><br>
<b>Fig. 6</b> Three-layer neural network
</center>
-->
<center>
<img src="{{site.baseurl}}/images/week02/02-3/2-layer-inference.png" height="250px"/><br>
<b>Hình 6</b> Mạng nơ-ron 3 lớp
</center>

<!--What kind of functions are we looking at?
-->
Chúng ta đang xem xét loại chức năng nào?

<!--
$$
\boldsymbol {\hat{y}} = \boldsymbol{\hat{y}(x)}, \boldsymbol{\hat{y}}: \mathbb{R}^n \rightarrow \mathbb{R}^K, \boldsymbol{x} \mapsto \boldsymbol{\hat{y}}
$$
-->
$$
\boldsymbol {\hat{y}} = \boldsymbol{\hat{y}(x)}, \boldsymbol{\hat{y}}: \mathbb{R}^n \rightarrow \mathbb{R}^K, \boldsymbol{x} \mapsto \boldsymbol{\hat{y}}
$$

<!--However, it is helpful to visualize the fact that there is a hidden layer, and the mapping can be expanded as:
-->
Tuy nhiên, sẽ hữu ích khi hình dung thực tế là có một lớp ẩn và ánh xạ có thể được mở rộng như sau:

<!--
$$
\boldsymbol{\hat{y}}: \mathbb{R}^{n} \rightarrow \mathbb{R}^d \rightarrow \mathbb{R}^K, d \gg n, K
$$
-->
$$
\boldsymbol{\hat{y}}: \mathbb{R}^{n} \rightarrow \mathbb{R}^d \rightarrow \mathbb{R}^K, d \gg n, K
$$

<!--What might an example configuration for the case above look like? In this case, one has input of dimension two ($n=2$), the single hidden layer could have dimensionality of 1000 ($d = 1000$), and we have 3 classes ($C=3$). There are good practical reasons to not have so many neurons in one hidden layer, so it could make sense to split that single hidden layer into 3 with 10 neurons each ($1000 \rightarrow 10 \times 10 \times 10$).
-->
Cấu hình ví dụ cho trường hợp trên có thể trông như thế nào? Trong trường hợp này, một có đầu vào là thứ nguyên hai ($n=2$), lớp ẩn đơn có thể có chiều là 1000 ($d = 1000$), và chúng ta có 3 lớp ($C=3$). Có những lý do thực tế tốt để không có quá nhiều nơ-ron trong một lớp ẩn, vì vậy có thể hợp lý khi chia lớp ẩn duy nhất đó thành 3 với 10 nơ-ron mỗi lớp ($1000 \rightarrow 10 \times 10 \times 10$).

<!--## [Neural network (training I)](https://www.youtube.com/watch?v=WAn6lip5oWk&t=822s)
-->
## [Mạng nơ-ron (đào tạo 1)](https://www.youtube.com/watch?v=WAn6lip5oWk&t=822s)

<!--So what does typical training look like? It is helpful to formulate this into the standard terminology of losses.
-->
Vậy đào tạo điển hình trông như thế nào? Sẽ rất hữu ích nếu xây dựng điều này thành thuật ngữ tiêu chuẩn về tổn thất.

<!--First, let us re-introduce the soft (arg)max and explicitly state that it is a common activation function for the last layer, when using negative log-likelihood loss, in cases for multi-class prediction. As stated by Professor LeCun in lecture, this is because you get nicer gradients than if you were to use sigmoids and square loss. In addition, your last layer will already be normalized (the sum of all the neurons in the last layer come out to 1), in a way that is nicer for gradient methods than explicit normalization (dividing by the norm).
-->
Đầu tiên, chúng ta hãy giới thiệu lại soft (arg)max và nói rõ rằng nó là một hàm kích hoạt phổ biến cho lớp cuối cùng, khi sử dụng khả năng lỗi log-âm (log-likelihood loss), trong trường hợp dự đoán nhiều lớp. Như đã nói bởi Giáo sư LeCun trong bài giảng, điều này là do bạn nhận được các gradient đẹp hơn so với khi bạn sử dụng sigmoid và lỗi bình phương (square loss). Ngoài ra, lớp cuối cùng của bạn sẽ được chuẩn hóa (tổng tất cả các tế bào thần kinh trong lớp cuối cùng là 1), theo cách tốt hơn cho các phương pháp gradient so với chuẩn hóa rõ ràng (chia cho chuẩn).

<!--The soft (arg)max will give you logits in the last layer that look like this:
-->
Soft (arg) max sẽ cung cấp cho bạn thông tin đăng nhập trong lớp cuối cùng trông như thế này:

<!--
$$
\text{soft{(arg)}max}(\boldsymbol{l})[c] = \frac{ \exp(\boldsymbol{l}[c])}   {\sum^K_{k=1} \exp(\boldsymbol{l}[k])}  \in (0, 1)
$$
-->
$$
\text{soft{(arg)}max}(\boldsymbol{l})[c] = \frac{ \exp(\boldsymbol{l}[c])}   {\sum^K_{k=1} \exp(\boldsymbol{l}[k])}  \in (0, 1)
$$

<!--It is important to note that the set is not closed because of the strictly positive nature of the exponential function.
-->
Điều quan trọng cần lưu ý là tập hợp không được đóng vì tính chất dương hoàn toàn của hàm số mũ.

<!--Given the set of the predictions $\matr{\hat{Y}}$, the loss will be:
-->
Với tập hợp các dự đoán $\matr{\hat{Y}}$, phần mất mát sẽ là:

<!--
$$
\mathcal{L}(\boldsymbol{\hat{Y}}, \boldsymbol{c}) = \frac{1}{m} \sum_{i=1}^m \ell(\boldsymbol{\hat{y}_i}, c_i), \quad
\ell(\boldsymbol{\hat{y}}, c) = -\log(\boldsymbol{\hat{y}}[c])
$$
-->
$$
\mathcal{L}(\boldsymbol{\hat{Y}}, \boldsymbol{c}) = \frac{1}{m} \sum_{i=1}^m \ell(\boldsymbol{\hat{y}_i}, c_i), \quad
\ell(\boldsymbol{\hat{y}}, c) = -\log(\boldsymbol{\hat{y}}[c])
$$

<!--Here $c$ denotes the integer label, not the one hot encoding representation.
-->
Ở đây $c$ biểu thị nhãn số nguyên, không phải là một biểu diễn one-hot encoding.

<!--So let's do two examples, one where an example is correctly classified, and one where it is not.
-->
Vì vậy, hãy làm hai ví dụ, một ví dụ được phân loại chính xác và một ví dụ không.

<!--Let's say-->
Hãy cùng xem

<!--
$$
\boldsymbol{x}, c = 1 \Rightarrow \boldsymbol{y} =
{\footnotesize\begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}}
$$
-->
$$
\boldsymbol{x}, c = 1 \Rightarrow \boldsymbol{y} =
{\footnotesize\begin{pmatrix}
1 \\
0 \\
0
\end{pmatrix}}
$$

<!--What is the instance wise loss?
-->
Ví dụ mất mát khôn ngoan là gì?

<!--For the case of *nearly perfect prediction* ($\sim$ means *circa*):
-->
Đối với trường hợp dự đoán gần như hoàn hảo ($\sim$ có nghĩa là khoảng thời gian).

<!--
$$
\hat{\boldsymbol{y}}(\boldsymbol{x}) =
{\footnotesize\begin{pmatrix} \sim 1 \\ \sim 0 \\ \sim 0 \end{pmatrix}}
 \Rightarrow \ell \left(
{\footnotesize\begin{pmatrix} \sim 1 \\ \sim 0 \\ \sim 0 \end{pmatrix}}
, 1\right) \rightarrow 0^{+}
$$
-->
$$
\hat{\boldsymbol{y}}(\boldsymbol{x}) =
{\footnotesize\begin{pmatrix} \sim 1 \\ \sim 0 \\ \sim 0 \end{pmatrix}}
 \Rightarrow \ell \left(
{\footnotesize\begin{pmatrix} \sim 1 \\ \sim 0 \\ \sim 0 \end{pmatrix}}
, 1\right) \rightarrow 0^{+}
$$

<!--For the case of *nearly absolutely wrong*:
-->
Đối với trường hợp gần như hoàn toàn sai:

<!--
$$ \hat{\boldsymbol{y}}(\boldsymbol{x}) =
{\footnotesize\begin{pmatrix} \sim 0 \\ \sim 1 \\ \sim 0 \end{pmatrix}}
\Rightarrow \ell \left(
{\footnotesize\begin{pmatrix} \sim 0 \\ \sim 1 \\ \sim 0 \end{pmatrix}}
, 1\right) \rightarrow +\infty  $$
-->
$$ \hat{\boldsymbol{y}}(\boldsymbol{x}) =
{\footnotesize\begin{pmatrix} \sim 0 \\ \sim 1 \\ \sim 0 \end{pmatrix}}
\Rightarrow \ell \left(
{\footnotesize\begin{pmatrix} \sim 0 \\ \sim 1 \\ \sim 0 \end{pmatrix}}
, 1\right) \rightarrow +\infty  $$

<!--Note in the above examples, $\sim 0 \rightarrow 0^{+}$ and $\sim 1 \rightarrow 1^{-}$. Why is this so? Take a minute to think.
-->
Lưu ý trong các ví dụ trên, $\sim 0 \rightarrow 0^{+}$ and $\sim 1 \rightarrow 1^{-}$. Tại sao là cái nay, hãy dành chút thời gian để suy nghĩ.

<!--**Note**: It is important to know that if you use `CrossEntropyLoss`, you will get `LogSoftMax` and negative loglikelihood `NLLLoss` bundled together, so don't do it twice!
-->
**Lưu ý:** Điều quan trọng cần biết là nếu bạn sử dụng `CrossEntropyLoss`, bạn sẽ nhận được `LogSoftMax` và khả năng và khả năng loglikelihood tiêu cực `NLLLoss` được gộp lại với nhau, vì vậy đừng làm điều đó hai lần!

<!--## [Neural network (training II)](https://www.youtube.com/watch?v=WAn6lip5oWk&t=2188s)
-->

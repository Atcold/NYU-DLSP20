---
lang-ref: ch.02-1
lecturer: Yann LeCun
title: Tính toán độ dốc (Gradient) cho các mô-đun NN và các thủ thuật thực lan truyền ngược
authors: Micaela Flores, Sheetal Laad, Brina Seidel, Aishwarya Rajan
date: 3 Feb 2020
translator: Huynh Nguyen
lang: vi
translator-date: 23 Jul 2021
---

## [Một ví dụ về lan truyền ngược và giới thiệu các mô-đun mạng thần kinh cơ bản](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2989s)

### Ví dụ

Tiếp theo, chúng ta sẽ xem xét một ví dụ cụ thể về sự lan truyền ngược được hỗ trợ bởi một biểu đồ trực quan. Hàm tùy ý $G(w)$ được nhập vào hàm chi phí $C$, có thể được biểu diễn dưới dạng đồ thị. Thông qua thao tác nhân các ma trận Jacobian, chúng ta có thể biến đổi đồ thị này thành đồ thị sẽ tính toán các độ dốc ngược lại. (Lưu ý rằng: PyTorch và TensorFlow thực hiện điều này hoàn toàn tự động cho người dùng, tức là biểu đồ chuyển tiếp được tự động "đảo ngược" để tạo đồ thị phái sinh sao chép ngược gradient).

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-1.png" alt="Biểu đồ gradient" style="zoom:40%;" /></center>

Trong ví dụ này, biểu đồ màu xanh lá cây bên phải đại diện cho biểu đồ gradient. Theo biểu đồ từ nút trên cùng, nó theo sau đó là:

$$
\frac{\partial C(y,\bar{y})}{\partial w}=1 \cdot \frac{\partial C(y,\bar{y})}{\partial\bar{y}}\cdot\frac{\partial G(x,w)}{\partial w}
$$

Về kích thước, $\frac{\partial C(y,\bar{y})}{\partial w}$ là một vectơ hàng có kích thước $1\times N$, trong đó $N$ là số thành phần của $w$; $\frac{\partial C(y,\bar{y})}{\partial \bar{y}}$ là một vectơ hàng có kích thước $1\times M$, trong đó $M$ là kích thước của đầu ra; $\frac{\partial \bar{y}}{\partial w}=\frac{\partial G(x,w)}{\partial w}$ là một ma trận có kích thước $M\times N$, trong đó $M$ là số đầu ra của $G$ và $N$ là thứ nguyên của $w$.

Lưu ý rằng các phức tạp có thể phát sinh khi kiến trúc của biểu đồ không cố định mà phụ thuộc vào dữ liệu. Ví dụ, chúng ta có thể chọn mô-đun mạng thần kinh tùy thuộc vào độ dài của vectơ đầu vào. Mặc dù điều này là có thể, nhưng ngày càng khó quản lý sự thay đổi này khi số lượng vòng lặp vượt quá mức hợp lý.

### Mô-đun mạng lưới thần kinh cơ bản

Tồn tại các loại mô-đun được tạo sẵn khác nhau bên cạnh các mô-đun Linear và ReLU quen thuộc. Chúng hữu ích vì chúng được tối ưu hóa duy nhất để thực hiện các chức năng tương ứng của chúng (trái ngược với việc được xây dựng bởi sự kết hợp của các mô-đun cơ bản khác).

- Tuyến tính: $Y=W\cdot X$

$$
 \begin{aligned}
 \frac{dC}{dX} &= W^\top \cdot \frac{dC}{dY} \\
 \frac{dC}{dW} &= \frac{dC}{dY} \cdot X^\top
 \end{aligned}
 $$
 
 - ReLU: $y=(x)^+$
 
 $$
 \frac{dC}{dX} =
     \begin{cases}
       0 & x<0\\
       \frac{dC}{dY} & \text{otherwise}
     \end{cases}
 $$
 
 - Bản sao (Duplicate): $Y_1=X$, $Y_2=X$

     - Tương tự như "Y - splitter" trong đó cả hai đầu ra đều bằng đầu vào.

     - Khi backpropagating, các gradient được tổng hợp.

     - Có thể chia thành các nhánh $n$ tương tự.
     
     $$
     \frac{dC}{dX}=\frac{dC}{dY_1}+\frac{dC}{dY_2}
     $$
     
- Thêm: $Y=X_1+X_2$

     - Với hai biến được tổng hợp, khi một biến bị xáo trộn, đầu ra sẽ bị xáo trộn bởi cùng một số lượng, tức là:
     
     $$
     \frac{dC}{dX_1}=\frac{dC}{dY}\cdot1 \quad \text{and}\quad \frac{dC}{dX_2}=\frac{dC}{dY}\cdot1
     $$
     
- Max: $Y=\max(X_1,X_2)$

     - Vì hàm này cũng có thể được biểu diễn dưới dạng
     
    $$
      Y=\max(X_1,X_2)=\begin{cases}
            X_1 & X_1 > X_2 \\
            X_2 & \text{else}
        \end{cases}
      \Rightarrow
      \frac{dY}{dX_1}=\begin{cases}
            1 & X_1 > X_2 \\
            0 & \text{else}
        \end{cases}
     $$
      
     - Do đó, theo quy tắc chuỗi,
      
     $$
      \frac{dC}{dX_1}=\begin{cases}
          \frac{dC}{dY}\cdot1 & X_1 > X_2 \\
          0 & \text{else}
       \end{cases}
     $$
      
## [LogSoftMax so với SoftMax](https://www.youtube.com/watch?v=d9vdh3b787Y&t=3953s)

SoftMax cũng là một mô-đun PyTorch, là một cách thuận tiện để chuyển một nhóm số thành một nhóm các số dương trong khoảng từ $0$ đến $1$ mà tổng thành một. Những con số này có thể được hiểu là một phân phối xác suất. Kết quả là nó được sử dụng phổ biến trong các bài toán phân loại. $y_i$ trong phương trình dưới đây là vectơ xác suất cho tất cả các loại (categories).

$$
y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$

Tuy nhiên, việc sử dụng softmax khiến mạng dễ bị biến mất. Độ dốc biến mất là một vấn đề, vì nó ngăn các trọng số ở hạ lưu không bị mạng thần kinh sửa đổi, điều này có thể ngăn hoàn toàn mạng thần kinh đào tạo thêm. Hàm sigmoid logistic là hàm softmax cho một giá trị, cho thấy rằng khi $s$ là lớn, $h(s)$ = $1$, và khi s là nhỏ, $h(s)$ = $0$. Vì hàm sigmoid phẳng tại $h(s) = 0$ và $h(s) = 1$, nên độ dốc = $0$, dẫn đến  dộ dốc biến mất.

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-2.png" alt="Hàm Sigmoid để minh họa độ dốc biến mất" style="background-color:#DCDCDC;" /></center>

$$
h(s) = \frac{1}{1 + \exp(-s)}
$$

Các nhà toán học đã đưa ra ý tưởng về *logsoftmax* để giải quyết vấn đề về độ dốc biến mất do softmax tạo ra. *LogSoftMax* là một mô-đun cơ bản khác trong PyTorch. Như có thể thấy trong phương trình dưới đây, *LogSoftMax* là sự kết hợp của softmax và log.

$$
\log(y_i )= \log\left(\frac{\exp(x_i)}{\Sigma_j \exp(x_j)}\right) = x_i - \log(\Sigma_j \exp(x_j))
$$

Phương trình dưới đây thể hiện một cách khác để xem xét phương trình tương tự. Hình bên dưới cho thấy phần $\log(1 + \exp(s))$ của hàm. Khi $s$ là rất nhỏ, giá trị sẽ là $0$, và $s$ là rất lớn, thì giá trị là $s$. Kết quả là nó không bão hòa và tránh được vấn đề gradient biến mất.

$$
\log\left(\frac{\exp(s)}{\exp(s) + 1}\right)= s - \log(1 + \exp(s))
$$

<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-3.png" width='400px' alt="Đồ thị phần logarit của các hàm" /></center>

## [Các thủ thuật thực tế để lan truyền ngược](https://www.youtube.com/watch?v=d9vdh3b787Y&t=4891s)

### Sử dụng ReLU làm chức năng kích hoạt phi tuyến tính

ReLU hoạt động tốt nhất cho các mạng có nhiều lớp, điều này đã khiến các lựa chọn thay thế như hàm sigmoid và hàm $\tanh(\cdot)$ tiếp tuyến hyperbol không được ưa chuộng. Lý do ReLU hoạt động tốt nhất có thể là do đường gấp khúc duy nhất của nó khiến nó có quy mô tương đương nhau.

### Sử dụng mất mát entropy chéo làm hàm mục tiêu cho các bài toán phân loại

Log softmax, mà chúng ta đã thảo luận trước đó trong bài giảng, là một trường hợp đặc biệt của mất entropy chéo. Trong PyTorch, hãy đảm bảo cung cấp hàm mất entropy chéo với log softmax làm đầu vào (trái ngược với softmax thông thường).

### Sử dụng giảm độ dốc ngẫu nhiên trên các minibatch trong quá trình đào tạo

Như đã thảo luận trước đây, các minibatch cho phép bạn đào tạo hiệu quả hơn vì có sự dư thừa trong dữ liệu; bạn không cần phải đưa ra dự đoán và tính toán sự mất mát trên mỗi lần quan sát ở mỗi bước đơn lẻ để ước tính gradient.

### Xáo trộn thứ tự của các ví dụ đào tạo khi sử dụng giảm độ dốc ngẫu nhiên

Vấn đề đặt hàng. Nếu mô hình chỉ thấy các ví dụ từ một lớp duy nhất trong mỗi bước huấn luyện, thì nó sẽ học cách dự đoán lớp đó mà không cần học tại sao nó phải dự đoán lớp đó. Ví dụ, nếu bạn đang cố gắng để chữ số classify từ dataset MNIST và các dữ liệu được unshuffled, các thông số sai lệch trong lớp cuối cùng sẽ chỉ đơn giản là luôn dự đoán không, sau đó điều chỉnh để luôn luôn dự đoán một, sau đó hai, vv . Tốt nhất, bạn nên có các mẫu từ mọi lớp trong mỗi minibatch.

Tuy nhiên, vẫn đang có cuộc tranh luận về việc liệu bạn có cần thay đổi thứ tự của các mẫu trong mỗi lần vượt qua (kỷ nguyên) hay không.

### Chuẩn hóa các đầu vào để có phương sai đơn vị và giá trị trung bình bằng 0

Trước khi đào tạo, sẽ hữu ích khi chuẩn hóa từng tính năng đầu vào để nó có giá trị trung bình bằng 0 và độ lệch chuẩn là một. Khi sử dụng dữ liệu hình ảnh RGB, thông thường sẽ lấy giá trị trung bình và độ lệch chuẩn của từng kênh riêng lẻ và chuẩn hóa kênh hình ảnh một cách thông thường. Ví dụ: lấy giá trị trung bình $m_b$ và độ lệch chuẩn $\sigma_b$ của tất cả các giá trị màu xanh lam trong tập dữ liệu, sau đó chuẩn hóa các giá trị màu xanh lam cho từng hình ảnh riêng lẻ như:

$$
b_{[i,j]}^{'} = \frac{b_{[i,j]} - m_b}{\max(\sigma_b, \epsilon)}
$$

Trong đó: $\epsilon$ là một số nhỏ tùy ý mà chúng tôi sử dụng để tránh chia cho số không. Lặp lại tương tự cho các kênh màu xanh lá cây và màu đỏ. Điều này là cần thiết để có được một tín hiệu có ý nghĩa từ các hình ảnh được chụp ở các ánh sáng khác nhau; Ví dụ, những bức tranh chiếu sáng ban ngày có rất nhiều màu đỏ trong khi những bức tranh dưới nước hầu như không có.

### Sử dụng lịch trình để giảm tốc độ học tập

Tỷ lệ học tập sẽ giảm khi quá trình đào tạo tiếp tục. Trong thực tế, hầu hết các mô hình tiên tiến được đào tạo bằng cách sử dụng các thuật toán như Adam điều chỉnh tốc độ học thay vì SGD đơn giản với tốc độ học không đổi.

### Sử dụng điều hòa L1 và / hoặc L2 để giảm trọng lượng

Bạn có thể thêm chi phí cho các trọng lượng lớn vào hàm chi phí. Ví dụ: sử dụng quy định hóa L2, chúng tôi sẽ xác định khoản mất mát $L$ và cập nhật trọng số $w$ như sau:

$$
L(S, w) = C(S, w) + \alpha \Vert w \Vert^2\\
\frac{\partial R}{\partial w_i} = 2w_i\\
w_i = w_i - \eta\frac{\partial L}{\partial w_i} = w_i - \eta \left( \frac{\partial C}{\partial w_i} + 2 \alpha w_i \right)
$$

Để hiểu tại sao điều này được gọi là giảm trọng lượng, hãy lưu ý rằng chúng ta có thể viết lại công thức trên để cho thấy rằng chúng ta nhân $w_i$ với một hằng số nhỏ hơn một trong quá trình cập nhật.

$$
w_i = (1 - 2 \eta \alpha) w_i - \eta\frac{\partial C}{\partial w_i}
$$

Chính quy hóa L1 (Lasso) cũng tương tự, ngoại trừ việc chúng tôi sử dụng $\sum_i \vert w_i\vert$ thay vì $\Vert w \Vert^2$.

Về cơ bản, chính quy hóa cố gắng nói với hệ thống để giảm thiểu hàm chi phí với vectơ trọng số ngắn nhất có thể. Với sự chính quy hóa L1, các trọng số không hữu ích sẽ được thu hẹp xuống còn $0$.

### Khởi tạo trọng số

Các trọng số cần được khởi tạo ngẫu nhiên, tuy nhiên, chúng không được quá lớn hoặc quá nhỏ để đầu ra có cùng phương sai với đầu vào. Có nhiều thủ thuật khởi tạo trọng số khác nhau được tích hợp trong PyTorch. Một trong những thủ thuật hoạt động tốt cho các mô hình sâu là khởi tạo Kaiming trong đó phương sai của trọng số tỷ lệ nghịch với căn bậc hai của số lượng đầu vào.

### Sử dụng droupot

Dropout là một hình thức khác của chính quy. Nó có thể được coi như một lớp khác của mạng nơ-ron: nó nhận các đầu vào, đặt ngẫu nhiên $ n / 2 $ của các đầu vào thành 0 và trả về kết quả là đầu ra. Điều này buộc hệ thống phải lấy thông tin từ tất cả các đơn vị đầu vào thay vì trở nên quá phụ thuộc vào một số lượng nhỏ các đơn vị đầu vào do đó phân phối thông tin trên tất cả các đơn vị trong một lớp. Phương pháp này ban đầu được đề xuất bởi <a href="https://arxiv.org/abs/1207.0580">Hinton et al (2012)</a>.

Để biết thêm các thủ thuật, xem <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">LeCun et al 1998</a>.

Cuối cùng, hãy lưu ý rằng sự lan truyền ngược không chỉ hoạt động đối với các mô hình xếp chồng lên nhau; nó có thể hoạt động cho bất kỳ đồ thị xoay chiều có hướng nào (DAG) miễn là có một phần thứ tự trên các mô-đun.

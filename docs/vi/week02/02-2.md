---
lang-ref: ch.02-1
lecturer: Yann LeCun
title: Tính toán độ dốc (Gradient) cho các mô-đun NN và các thủ thuật thực lan truyền ngược
authors: Micaela Flores, Sheetal Laad, Brina Seidel, Aishwarya Rajan
date: 3 Feb 2020
translator: Huynh Nguyen
lang: vi
translator-date: 23 Jul 2021
---

<!--## [A concrete example of backpropagation and intro to basic neural network modules](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2989s)
-->
## [Một ví dụ về lan truyền ngược và giới thiệu các mô-đun mạng thần kinh cơ bản](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2989s)

<!--### Example
-->
### Ví dụ

<!--We next consider a concrete example of backpropagation assisted by a visual graph. The arbitrary function $G(w)$ is input into the cost function $C$, which can be represented as a graph. Through the manipulation of multiplying the Jacobian matrices, we can transform this graph into the graph that will compute the gradients going backwards. (Note that PyTorch and TensorFlow do this automatically for the user, i.e. the forward graph is automatically "reversed" to create the derivative graph that backpropagates the gradient.)
-->
Tiếp theo, chúng ta sẽ xem xét một ví dụ cụ thể về sự lan truyền ngược được hỗ trợ bởi một biểu đồ trực quan. Hàm tùy ý $G(w)$ được nhập vào hàm chi phí $C$, có thể được biểu diễn dưới dạng đồ thị. Thông qua thao tác nhân các ma trận Jacobian, chúng ta có thể biến đổi đồ thị này thành đồ thị sẽ tính toán các độ dốc ngược lại. (Lưu ý rằng: PyTorch và TensorFlow thực hiện điều này hoàn toàn tự động cho người dùng, tức là biểu đồ chuyển tiếp được tự động "đảo ngược" để tạo đồ thị phái sinh sao chép ngược gradient).

<!--<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-1.png" alt="Gradient diagram" style="zoom:40%;" /></center>
-->
<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-1.png" alt="Biểu đồ gradient" style="zoom:40%;" /></center>

<!--In this example, the green graph on the right represents the gradient graph. Following the graph from the topmost node, it follows that
-->
Trong ví dụ này, biểu đồ màu xanh lá cây bên phải đại diện cho biểu đồ gradient. Theo biểu đồ từ nút trên cùng, nó theo sau đó là:

<!--$$
\frac{\partial C(y,\bar{y})}{\partial w}=1 \cdot \frac{\partial C(y,\bar{y})}{\partial\bar{y}}\cdot\frac{\partial G(x,w)}{\partial w}
$$
-->
$$
\frac{\partial C(y,\bar{y})}{\partial w}=1 \cdot \frac{\partial C(y,\bar{y})}{\partial\bar{y}}\cdot\frac{\partial G(x,w)}{\partial w}
$$

<!--In terms of dimensions, $\frac{\partial C(y,\bar{y})}{\partial w}$ is a row vector of size $1\times N$ where $N$ is the number of components of $w$; $\frac{\partial C(y,\bar{y})}{\partial \bar{y}}$  is a row vector of size $1\times M$, where $M$ is the dimension of the output; $\frac{\partial \bar{y}}{\partial w}=\frac{\partial G(x,w)}{\partial w}$ is a matrix of size $M\times N$, where $M$ is the number of outputs of $G$ and $N$ is the dimension of $w$.
-->
Về kích thước, $\frac{\partial C(y,\bar{y})}{\partial w}$ là một vectơ hàng có kích thước $1\times N$, trong đó $N$ là số thành phần của $w$; $\frac{\partial C(y,\bar{y})}{\partial \bar{y}}$ là một vectơ hàng có kích thước $1\times M$, trong đó $M$ là kích thước của đầu ra; $\frac{\partial \bar{y}}{\partial w}=\frac{\partial G(x,w)}{\partial w}$ là một ma trận có kích thước $M\times N$, trong đó $M$ là số đầu ra của $G$ và $N$ là thứ nguyên của $w$.

<!--Note that complications might arise when the architecture of the graph is not fixed, but is data-dependent. For example, we could choose neural net module depending on the length of input vector. Though this is possible, it becomes increasingly difficult to manage this variation when the number of loops exceeds a reasonable amount.
-->
Lưu ý rằng các phức tạp có thể phát sinh khi kiến trúc của biểu đồ không cố định mà phụ thuộc vào dữ liệu. Ví dụ, chúng ta có thể chọn mô-đun mạng thần kinh tùy thuộc vào độ dài của vectơ đầu vào. Mặc dù điều này là có thể, nhưng ngày càng khó quản lý sự thay đổi này khi số lượng vòng lặp vượt quá mức hợp lý.

<!--### Basic neural net modules
-->
### Mô-đun mạng lưới thần kinh cơ bản

<!--There exist different types of pre-built modules besides the familiar Linear and ReLU modules. These are useful because they are uniquely optimized to perform their respective functions (as opposed to being built by a combination of other, elementary modules).
-->
Tồn tại các loại mô-đun được tạo sẵn khác nhau bên cạnh các mô-đun Linear và ReLU quen thuộc. Chúng hữu ích vì chúng được tối ưu hóa duy nhất để thực hiện các chức năng tương ứng của chúng (trái ngược với việc được xây dựng bởi sự kết hợp của các mô-đun cơ bản khác).

<!--- Linear: $Y=W\cdot X$
-->
- Tuyến tính: $Y=W\cdot X$

<!--$$
  \begin{aligned}
  \frac{dC}{dX} &= W^\top \cdot \frac{dC}{dY} \\
  \frac{dC}{dW} &= \frac{dC}{dY} \cdot X^\top
  \end{aligned}
  $$
-->
$$
 \begin{aligned}
 \frac{dC}{dX} &= W^\top \cdot \frac{dC}{dY} \\
 \frac{dC}{dW} &= \frac{dC}{dY} \cdot X^\top
 \end{aligned}
$$
 
 <!--ReLU: $y=(x)^+$
-->
- ReLU: $y=(x)^+$
 
 <!--$$
  \frac{dC}{dX} =
      \begin{cases}
        0 & x<0\\
        \frac{dC}{dY} & \text{otherwise}
      \end{cases}
  $$
-->
$$
 \frac{dC}{dX} =
     \begin{cases}
       0 & x<0\\
       \frac{dC}{dY} & \text{otherwise}
     \end{cases}
$$
 
 <!-- Duplicate: $Y_1=X$, $Y_2=X$
-->
- Bản sao (Duplicate): $Y_1=X$, $Y_2=X$

<!-- Akin to a "Y - splitter" where both outputs are equal to the input.
-->
    - Tương tự như "Y - splitter" trong đó cả hai đầu ra đều bằng đầu vào.
     
<!-- When backpropagating, the gradients get summed
-->
    - Khi backpropagating, các gradient được tổng hợp.

<!--Can be split into $n$ branches similarly
-->
    - Có thể chia thành các nhánh $n$ tương tự.

<!--$$
    \frac{dC}{dX}=\frac{dC}{dY_1}+\frac{dC}{dY_2}
    $$
-->
  $$\frac{dC}{dX}=\frac{dC}{dY_1}+\frac{dC}{dY_2}$$

<!-- Add: $Y=X_1+X_2$
-->
- Thêm: $Y=X_1+X_2$

<!-- With two variables being summed, when one is perturbed, the output will be perturbed by the same quantity, i.e.
-->
    - Với hai biến được tổng hợp, khi một biến bị xáo trộn, đầu ra sẽ bị xáo trộn bởi cùng một số lượng, tức là:

<!-- $$
    \frac{dC}{dX_1}=\frac{dC}{dY}\cdot1 \quad \text{and}\quad \frac{dC}{dX_2}=\frac{dC}{dY}\cdot1
    $$
-->
  $$\frac{dC}{dX_1}=\frac{dC}{dY}\cdot1 \quad \text{and}\quad \frac{dC}{dX_2}=\frac{dC}{dY}\cdot1$$

<!-- Max: $Y=\max(X_1,X_2)$
-->
- Max: $Y=\max(X_1,X_2)$

<!-- Since this function can also be represented as
-->
    - Vì hàm này cũng có thể được biểu diễn dưới dạng

<!--$$
    Y=\max(X_1,X_2)=\begin{cases}
          X_1 & X_1 > X_2 \\
          X_2 & \text{else}
       \end{cases}
    \Rightarrow
    \frac{dY}{dX_1}=\begin{cases}
          1 & X_1 > X_2 \\
          0 & \text{else}
       \end{cases}
    $$
-->
   $$
    Y=\max(X_1,X_2)=\begin{cases}
          X_1 & X_1 > X_2 \\
          X_2 & \text{else}
      \end{cases}
    \Rightarrow
    \frac{dY}{dX_1}=\begin{cases}
          1 & X_1 > X_2 \\
          0 & \text{else}
      \end{cases}
   $$
 
<!--Therefore, by the chain rule,
-->
    - Do đó, theo quy tắc chuỗi,

<!--$$
    \frac{dC}{dX_1}=\begin{cases}
          \frac{dC}{dY}\cdot1 & X_1 > X_2 \\
          0 & \text{else}
      \end{cases}
    $$
-->
  $$
    \frac{dC}{dX_1}=\begin{cases}
        \frac{dC}{dY}\cdot1 & X_1 > X_2 \\
        0 & \text{else}
     \end{cases}
   $$

<!--## [LogSoftMax *vs.* SoftMax](https://www.youtube.com/watch?v=d9vdh3b787Y&t=3953s)
-->
## [LogSoftMax so với SoftMax](https://www.youtube.com/watch?v=d9vdh3b787Y&t=3953s)

<!--*SoftMax*, which is also a PyTorch module, is a convenient way of transforming a group of numbers into a group of positive numbers between $0$ and $1$ that sum to one. These numbers can be interpreted as a probability distribution. As a result, it is commonly used in classification problems. $y_i$ in the equation below is a vector of probabilities for all the categories.
-->
SoftMax cũng là một mô-đun PyTorch, là một cách thuận tiện để chuyển một nhóm số thành một nhóm các số dương trong khoảng từ $0$ đến $1$ mà tổng thành một. Những con số này có thể được hiểu là một phân phối xác suất. Kết quả là nó được sử dụng phổ biến trong các bài toán phân loại. $y_i$ trong phương trình dưới đây là vectơ xác suất cho tất cả các loại (categories).

<!--$$
y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$
-->
$$
y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$

<!--However, the use of softmax leaves the network susceptible to vanishing gradients. Vanishing gradient is a problem, as it prevents weights downstream from being modified by the neural network, which may completely stop the neural network from further training. The logistic sigmoid function, which is the softmax function for one value, shows that when $s$ is large, $h(s)$ is $1$, and when s is small, $h(s)$ is $0$. Because the sigmoid function is flat at $h(s) = 0$ and $h(s) = 1$, the gradient is $0$, which results in a vanishing gradient.
-->
Tuy nhiên, việc sử dụng softmax khiến mạng dễ bị biến mất. Độ dốc biến mất là một vấn đề, vì nó ngăn các trọng số ở hạ lưu không bị mạng thần kinh sửa đổi, điều này có thể ngăn hoàn toàn mạng thần kinh đào tạo thêm. Hàm sigmoid logistic là hàm softmax cho một giá trị, cho thấy rằng khi $s$ là lớn, $h(s)$ = $1$, và khi s là nhỏ, $h(s)$ = $0$. Vì hàm sigmoid phẳng tại $h(s) = 0$ và $h(s) = 1$, nên độ dốc = $0$, dẫn đến  dộ dốc biến mất.

<!--<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-2.png" alt="Sigmoid function to illustrate vanishing gradient" style="background-color:#DCDCDC;" /></center>
-->
<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-2.png" alt="Hàm Sigmoid để minh họa độ dốc biến mất" style="background-color:#DCDCDC;" /></center>

<!--$$
h(s) = \frac{1}{1 + \exp(-s)}
$$
-->
$$
h(s) = \frac{1}{1 + \exp(-s)}
$$

<!--Mathematicians came up with the idea of logsoftmax in order to solve for the issue of the vanishing gradient created by softmax. *LogSoftMax* is another basic module in PyTorch. As can be seen in the equation below, *LogSoftMax* is a combination of softmax and log.
-->
Các nhà toán học đã đưa ra ý tưởng về *logsoftmax* để giải quyết vấn đề về độ dốc biến mất do softmax tạo ra. *LogSoftMax* là một mô-đun cơ bản khác trong PyTorch. Như có thể thấy trong phương trình dưới đây, *LogSoftMax* là sự kết hợp của softmax và log.

<!--$$
\log(y_i )= \log\left(\frac{\exp(x_i)}{\Sigma_j \exp(x_j)}\right) = x_i - \log(\Sigma_j \exp(x_j))
$$
-->
$$
\log(y_i )= \log\left(\frac{\exp(x_i)}{\Sigma_j \exp(x_j)}\right) = x_i - \log(\Sigma_j \exp(x_j))
$$

<!--The equation below demonstrates another way to look at the same equation. The figure below shows the $\log(1 + \exp(s))$ part of the function. When $s$ is very small, the value is $0$, and when $s$ is very large, the value is $s$. As a result it doesn’t saturate, and the vanishing gradient problem is avoided.
-->
Phương trình dưới đây thể hiện một cách khác để xem xét phương trình tương tự. Hình bên dưới cho thấy phần $\log(1 + \exp(s))$ của hàm. Khi $s$ là rất nhỏ, giá trị sẽ là $0$, và $s$ là rất lớn, thì giá trị là $s$. Kết quả là nó không bão hòa và tránh được vấn đề gradient biến mất.

<!--$$
\log\left(\frac{\exp(s)}{\exp(s) + 1}\right)= s - \log(1 + \exp(s))
$$
-->
$$
\log\left(\frac{\exp(s)}{\exp(s) + 1}\right)= s - \log(1 + \exp(s))
$$

<!--<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-3.png" width='400px' alt="Plot of logarithmic part of the functions" /></center>
-->
<center><img src="{{site.baseurl}}/images/week02/02-2/02-2-3.png" width='400px' alt="Đồ thị phần logarit của các hàm" /></center>

<!--## [Practical tricks for backpropagation](https://www.youtube.com/watch?v=d9vdh3b787Y&t=4891s)
-->
## [Các thủ thuật thực tế để lan truyền ngược](https://www.youtube.com/watch?v=d9vdh3b787Y&t=4891s)

<!--### Use ReLU as the non-linear activation function
-->
### Sử dụng ReLU làm chức năng kích hoạt phi tuyến tính

<!--ReLU works best for networks with many layers, which has caused alternatives like the sigmoid function and hyperbolic tangent $\tanh(\cdot)$ function to fall out of favour. The reason ReLU works best is likely due to its single kink which makes it scale equivariant.
-->
ReLU hoạt động tốt nhất cho các mạng có nhiều lớp, điều này đã khiến các lựa chọn thay thế như hàm sigmoid và hàm $\tanh(\cdot)$ tiếp tuyến hyperbol không được ưa chuộng. Lý do ReLU hoạt động tốt nhất có thể là do đường gấp khúc duy nhất của nó khiến nó có quy mô tương đương nhau.

<!--### Use cross-entropy loss as the objective function for classification problems
-->
### Sử dụng mất mát entropy chéo làm hàm mục tiêu cho các bài toán phân loại

<!--Log softmax, which we discussed earlier in the lecture, is a special case of cross-entropy loss. In PyTorch, be sure to provide the cross-entropy loss function with *log* softmax as input (as opposed to normal softmax).
-->
Log softmax, mà chúng ta đã thảo luận trước đó trong bài giảng, là một trường hợp đặc biệt của mất entropy chéo. Trong PyTorch, hãy đảm bảo cung cấp hàm mất entropy chéo với log softmax làm đầu vào (trái ngược với softmax thông thường).

<!--### Use stochastic gradient descent on minibatches during training
-->
### Sử dụng giảm độ dốc ngẫu nhiên trên các minibatch trong quá trình đào tạo

<!--As discussed previously, minibatches let you train more efficiently because there is redundancy in the data; you shouldn't need to make a prediction and calculate the loss on every single observation at every single step to estimate the gradient.
-->
Như đã thảo luận trước đây, các minibatch cho phép bạn đào tạo hiệu quả hơn vì có sự dư thừa trong dữ liệu; bạn không cần phải đưa ra dự đoán và tính toán sự mất mát trên mỗi lần quan sát ở mỗi bước đơn lẻ để ước tính gradient.

<!--### Shuffle the order of the training examples when using stochastic gradient descent
-->
### Xáo trộn thứ tự của các ví dụ đào tạo khi sử dụng giảm độ dốc ngẫu nhiên

<!--Order matters. If the model sees only examples from a single class during each training step, then it will learn to predict that class without learning why it ought to be predicting that class. For example, if you were trying to classify digits from the MNIST dataset and the data was unshuffled, the bias parameters in the last layer would simply always predict zero, then adapt to always predict one, then two, *etc*. Ideally, you should have samples from every class in every minibatch.
-->
Vấn đề đặt hàng. Nếu mô hình chỉ thấy các ví dụ từ một lớp duy nhất trong mỗi bước huấn luyện, thì nó sẽ học cách dự đoán lớp đó mà không cần học tại sao nó phải dự đoán lớp đó. Ví dụ, nếu bạn đang cố gắng để chữ số classify từ dataset MNIST và các dữ liệu được unshuffled, các thông số sai lệch trong lớp cuối cùng sẽ chỉ đơn giản là luôn dự đoán không, sau đó điều chỉnh để luôn luôn dự đoán một, sau đó hai, vv . Tốt nhất, bạn nên có các mẫu từ mọi lớp trong mỗi minibatch.

<!--However, there's ongoing debate over whether you need to change the order of the samples in every pass (epoch).
-->
Tuy nhiên, vẫn đang có cuộc tranh luận về việc liệu bạn có cần thay đổi thứ tự của các mẫu trong mỗi lần vượt qua (kỷ nguyên) hay không.

<!--### Normalize the inputs to have zero mean and unit variance
-->
### Chuẩn hóa các đầu vào để có phương sai đơn vị và giá trị trung bình bằng 0

<!--Before training, it's useful to normalize each input feature so that it has a mean of zero and a standard deviation of one. When using RGB image data, it is common to take mean and standard deviation of each channel individually and normalize the image channel-wise. For example, take the mean $m_b$ and standard deviation $\sigma_b$ of all the blue values in the dataset, then normalize the blue values for each individual image as
-->
Trước khi đào tạo, sẽ hữu ích khi chuẩn hóa từng tính năng đầu vào để nó có giá trị trung bình bằng 0 và độ lệch chuẩn là một. Khi sử dụng dữ liệu hình ảnh RGB, thông thường sẽ lấy giá trị trung bình và độ lệch chuẩn của từng kênh riêng lẻ và chuẩn hóa kênh hình ảnh một cách thông thường. Ví dụ: lấy giá trị trung bình $m_b$ và độ lệch chuẩn $\sigma_b$ của tất cả các giá trị màu xanh lam trong tập dữ liệu, sau đó chuẩn hóa các giá trị màu xanh lam cho từng hình ảnh riêng lẻ như:

<!--$$
b_{[i,j]}^{'} = \frac{b_{[i,j]} - m_b}{\max(\sigma_b, \epsilon)}
$$
-->
$$
b_{[i,j]}^{'} = \frac{b_{[i,j]} - m_b}{\max(\sigma_b, \epsilon)}
$$

<!--where $\epsilon$ is an arbitrarily small number that we use to avoid division by zero. Repeat the same for green and red channels. This is necessary to get a meaningful signal out of images taken in different lighting; for example, day lit pictures have a lot of red while underwater pictures have almost none.
-->
Trong đó: $\epsilon$ là một số nhỏ tùy ý mà chúng tôi sử dụng để tránh chia cho số không. Lặp lại tương tự cho các kênh màu xanh lá cây và màu đỏ. Điều này là cần thiết để có được một tín hiệu có ý nghĩa từ các hình ảnh được chụp ở các ánh sáng khác nhau; Ví dụ, những bức tranh chiếu sáng ban ngày có rất nhiều màu đỏ trong khi những bức tranh dưới nước hầu như không có.

<!--### Use a schedule to decrease the learning rate
-->
### Sử dụng lịch trình để giảm tốc độ học tập

<!--The learning rate should fall as training goes on. In practice, most advanced models are trained by using algorithms like Adam which adapt the learning rate instead of simple SGD with a constant learning rate.
-->
Tỷ lệ học tập sẽ giảm khi quá trình đào tạo tiếp tục. Trong thực tế, hầu hết các mô hình tiên tiến được đào tạo bằng cách sử dụng các thuật toán như Adam điều chỉnh tốc độ học thay vì SGD đơn giản với tốc độ học không đổi.

<!--### Use L1 and/or L2 regularization for weight decay
-->
### Sử dụng điều hòa L1 và / hoặc L2 để giảm trọng lượng

<!--You can add a cost for large weights to the cost function. For example, using L2 regularization, we would define the loss $L$ and update the weights $w$ as follows:
-->
Bạn có thể thêm chi phí cho các trọng lượng lớn vào hàm chi phí. Ví dụ: sử dụng quy định hóa L2, chúng tôi sẽ xác định khoản mất mát $L$ và cập nhật trọng số $w$ như sau:

<!--$$
L(S, w) = C(S, w) + \alpha \Vert w \Vert^2\\
\frac{\partial R}{\partial w_i} = 2w_i\\
w_i = w_i - \eta\frac{\partial L}{\partial w_i} = w_i - \eta \left( \frac{\partial C}{\partial w_i} + 2 \alpha w_i \right)
$$
-->
$$
L(S, w) = C(S, w) + \alpha \Vert w \Vert^2\\
\frac{\partial R}{\partial w_i} = 2w_i\\
w_i = w_i - \eta\frac{\partial L}{\partial w_i} = w_i - \eta \left( \frac{\partial C}{\partial w_i} + 2 \alpha w_i \right)
$$

<!--To understand why this is called weight decay, note that we can rewrite the above formula to show that we multiply $w_i$ by a constant less than one during the update.
-->
Để hiểu tại sao điều này được gọi là giảm trọng lượng, hãy lưu ý rằng chúng ta có thể viết lại công thức trên để cho thấy rằng chúng ta nhân $w_i$ với một hằng số nhỏ hơn một trong quá trình cập nhật.

<!--$$
w_i = (1 - 2 \eta \alpha) w_i - \eta\frac{\partial C}{\partial w_i}
$$
-->
$$
w_i = (1 - 2 \eta \alpha) w_i - \eta\frac{\partial C}{\partial w_i}
$$

<!--L1 regularization (Lasso) is similar, except that we use $\sum_i \vert w_i\vert$ instead of $\Vert w \Vert^2$.
-->
Chính quy hóa L1 (Lasso) cũng tương tự, ngoại trừ việc chúng tôi sử dụng $\sum_i \vert w_i\vert$ thay vì $\Vert w \Vert^2$.

<!--Essentially, regularization tries to tell the system to minimize the cost function with the shortest weight vector possible. With L1 regularization, weights that are not useful are shrunk to $0$.
-->
Về cơ bản, chính quy hóa cố gắng nói với hệ thống để giảm thiểu hàm chi phí với vectơ trọng số ngắn nhất có thể. Với sự chính quy hóa L1, các trọng số không hữu ích sẽ được thu hẹp xuống còn $0$.

<!--### Weight initialisation
-->
### Khởi tạo trọng số

<!--The weights need to be initialised at random, however, they shouldn't be too large or too small such that output is roughly of the same variance as that of input. There are various weight initialisation tricks built into PyTorch. One of the tricks that works well for deep models is Kaiming initialisation where the standard deviation of the weights is inversely proportional to square root of number of inputs.
-->
Các trọng số cần được khởi tạo ngẫu nhiên, tuy nhiên, chúng không được quá lớn hoặc quá nhỏ để đầu ra có cùng phương sai với đầu vào. Có nhiều thủ thuật khởi tạo trọng số khác nhau được tích hợp trong PyTorch. Một trong những thủ thuật hoạt động tốt cho các mô hình sâu là khởi tạo Kaiming trong đó phương sai của trọng số tỷ lệ nghịch với căn bậc hai của số lượng đầu vào.

<!--### Use dropout
-->
### Sử dụng droupot

<!--Dropout is another form of regularization. It can be thought of as another layer of the neural net: it takes inputs, randomly sets $n/2$ of the inputs to zero, and returns the result as output. This forces the system to take information from all input units rather than becoming overly reliant on a small number of input units thus distributing the information across all of the units in a layer. This method was initially proposed by <a href="https://arxiv.org/abs/1207.0580">Hinton et al (2012)</a>.
-->
Dropout là một hình thức khác của chính quy. Nó có thể được coi như một lớp khác của mạng nơ-ron: nó nhận các đầu vào, đặt ngẫu nhiên $ n / 2 $ của các đầu vào thành 0 và trả về kết quả là đầu ra. Điều này buộc hệ thống phải lấy thông tin từ tất cả các đơn vị đầu vào thay vì trở nên quá phụ thuộc vào một số lượng nhỏ các đơn vị đầu vào do đó phân phối thông tin trên tất cả các đơn vị trong một lớp. Phương pháp này ban đầu được đề xuất bởi <a href="https://arxiv.org/abs/1207.0580">Hinton et al (2012)</a>.

<!--For more tricks, see  <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">LeCun et al 1998</a>.
-->
Để biết thêm các thủ thuật, xem <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">LeCun et al 1998</a>.

<!--Finally, note that backpropagation doesn't just work for stacked models; it can work for any directed acyclic graph (DAG) as long as there is a partial order on the modules.
-->
Cuối cùng, hãy lưu ý rằng sự lan truyền ngược không chỉ hoạt động đối với các mô hình xếp chồng lên nhau; nó có thể hoạt động cho bất kỳ đồ thị xoay chiều có hướng nào (DAG) miễn là có một phần thứ tự trên các mô-đun.

---
lang-ref: ch.07
title: Week 7
lang: ko
translation-date: 25 June 2020
translator: Seok Hoan (Kevin) Choi

---

<!--
## Lecture part A

We introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions.
--> 
## 강의 파트 A
에너지 기반 모델<sup>EBD energy-based model</sup>의 컨셉과 피드 포워드 신경망 <sup>feed-forward networks</sup>과 다른 접근법의 사용 목적을 소개한다. EBM의 쉽지않은 추론<sup>inference</sup> 을 풀기 위해 잠재 변수<sup>latent variable</sup>는 보조정보 제공과 다수의 가능한 예측을 갖게 해주는데 사용된다. 마지막으로, EBM은 좀 더 유연한 스코어링 함수<sup>scoring function</sup>를 가진 확률모델로 일반화가 가능하다.
<!--
## Lecture part B
--> 
<!--
We discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map.
-->

## 강의 파트 B

자기 지도 학습<sup>self-supervised learning</sup>에 대하여 논의하고, 에너지 기반 모델을 어떻게 학습시키는지 소개하며, K-means 예를 이용해 잠재변수 에너지 기반 모델<sup>Latent Variable EBM</sup>에 대해 논의한다. 또한, 대조법, 지형도<sup>topographic map</sup>를 이용한 디노이징 오토인코더<sup>denoising autoencoder</sup>, 학습 과정, 그리고 이것이 어떻게 사용되는지 BERT의 소개와 함께 설명한다. 마지막으로, 지형도를 이용해 대조적 발산<sup>Contrastive Divergence</sup>에 대해 이야기한다. 

<!--
## Practicum
We discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder.
-->

## 실습
오토인코더의 응용과 사용하는 이유 등을 논의하고, 각각의 다른 오토인코더 구조(과소 혹은 과잉 완벽 은닉망<sup>under or over complete hidden layer</sup>)에 대해 이야기하며, 어떻게 오버피팅<sup>overfitting </sup> 이슈를 방지하며 어떠한 손실 함수<sup>loss functions</sup>가 사용되어야 하는지에 대해서 이야기한다. 마지막으로 기본적인 오토인코더와 디노이징 오토인코더를 구현한다.
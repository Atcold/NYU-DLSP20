---
lang-ref: ch.09
title: 9주차
date: 20 Mar 2020
lang: ko
translation date: Jul 2020
translator: ChoongHee
---

## 이론 part A


<!--We discussed discriminative recurrent sparse auto-encoders and group sparsity. The main idea was how to combine sparse coding with discriminative training. We went through how to structure a network with a recurrent autoencoder similar to LISTA and a decoder. Then we discussed how to use group sparsity to extract invariant features.-->

우리는 식별하는 순환 희소 오토인코더와 그룹 희소성에 대하여 논하였다. 주요 개념은 희박한 코딩과 식별하는 학습을 결합하는 방법이었다. 우리는 리스타<sup>LISTA</sup>, 디코더와 유사한 순환 오토인코더를 동반한 신경망을 구축하는 방법도 살펴보았다. 그 다음, 우리는 변하지 않는 특징들을 추출하기 위한 그룹 희소성 활용 방법을 논의하였다.


## 이론 part B


<!--In this section, we talked about the World Models for autonomous control including the neural network architecture and training schema. Then, we discussed the difference between World Models and Reinforcement Learning (RL). Finally, we studied Generative Adversarial Networks (GANs) in terms of energy-based model with the contrastive method.-->

이번 섹션에서는 신경망 아키텍처와 학습 개요를 포함한 자율 제어에 관한 세계 모델<sup>World Models</sup>에 대하여 알아보았다. 그런 다음, 세계 모델과 강화학습<sup>Reinforcement Learning(RL)</sup>의 차이를 살펴보았다. 마지막으로, 대조적 방법을 동반한 에너지 기반 모델 측면에서 생성적 적대 신경망<sup>GANs</sup>을 알아보았다.

## 실습


<!--During this week's practicum, we explored Generative Adversarial Networks (GANs) and how they can produce realistic generative models. We then compared GANs with VAEs from week 8 to highlight key differences between two networks. Next, we discussed several model limitations of GANs. Finally, we looked at the source code for the PyTorch example Deep Convolutional Generative Adversarial Networks (DCGAN).-->

이번 주 실습과정 동안, 생성적 적대 신경망과 어떻게 생성적 적대 신경망이 사실적인 생성 모델을 만들어낼 수 있는지 확인한 다음 8주차에서 다루었던 두 신경망 사이의 핵심적인 차이점들을 강조하기 위하여 생성적 적대 신경망과 변이형 오토인코더<sup>Variational Auto Encoders</sup>를 비교해보았다. 마지막으로, 파이토치 기반 예제 심층 합성곱 생성적 적대 신경망<sup>Deep Convolutional Generative Adversarial Networks(DCGAN)</sup> 소스 코드를 살펴보았다.
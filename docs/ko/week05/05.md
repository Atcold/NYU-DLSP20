---
lang-ref: ch.05
title: 5주차
lang: ko
translation-date: 03 Apr 2020
translator: Seungjae Ryan Lee
---

<!-- 
## Lecture part A

We begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on covergence.
 -->

## 이론 part A

우리는 경사하강법<sup>Gradient Descent (GD)</sup>을 소개하는 것으로 시작한다. 우리는 경사하강법의 직관적 이해에 대해 토론하고, 해결책에 도달하는 데 학습률<sup>step size</sup>이 어떤 중요한 역할을 하는지에 대해서도 이야기한다. 그 후 확률적 경사하강법<sup>Stochastic Gradient Descent (SGD)</sup>와 이것의 성능을 확인하고 풀 배치<sup>full batch</sup> GD와 비교한다. 마지막으로 모멘텀 업데이트<sup>momentum update</sup>에 대해 얘기할텐데, 특히 두 가지 업데이트 방식과, 모멘텀 뒤의 직관, 그리고 수렴에 미치는 영향에 대해 이야기한다.

<!-- 
## Lecture part B

We discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient.
 -->

## 이론 part B

우리는 RMSprop과 ADAM과 같은 SGD를 위한 적응형 학습률<sup>adaptive methods</sup>에 대해 논의한다. 또한 우리는 정규화 계층를 소개하고, 이것이 신경망 학습 과정에 미치는 영향에 대해 이야기한다. 마지막으로, 실제 사례로 산업에서 MRI 스캔을 더 빠르고 더 효율적으로 만들기 위해 신경망을 사용하는 것을 소개한다.

<!-- 
## Practicum

We briefly review the matrix-multiplications and then discuss the convolutions. Key point is we use kernels by stacking and shifting. We first understand the 1D convolution by hand, and then use PyTorch to learn the dimension of kernels and output width in 1D and 2D convolutions examples. Furthermore, we use PyTorch to learn about how automatic gradient works and custom-grads.
 -->

## 실습

우리는 행렬 곱셈을 간단히 복습한 다음, 그 합성곱에 대해 논의한다. 여기서 요즘은 우리가 커널<sup>kernel</sup>을 쌓고 옮겨서 사용한다는 것이다. 우리는 우선 손으로 계산하면서 1차원 합성곱을 이해한 다음, 파이토치를 사용하여 1차원과 2차원의 합성곱 예제로 커널의 치수와 출력 너비를 확인한다. 또한, 우리는 자동 경사 계산과 맞춤형 경사에 대해 배우기 위해 파이토치를 사용할 것이다.

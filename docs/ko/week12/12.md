---
lang-ref: ch.12
title: 12주차
lang: ko
translation-date: 18 July 2020
translator: Jieun
---



<!-- ## Lecture part A

In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively.  -->

## 강의 part A

이 섹션에서는 NLP에서 쓰이는 다양한 아키텍쳐들, CNNs과 RNNs에서 시작해 최종적으로 최첨단<sup>state-of-the-art</sup> 아키텍쳐인 트랜스포머<sup>Transformer</sup>까지 커버한다. 그 다음으로는 트랜스포머를 구성하는 다양한 모듈과 어떻게 이 모듈들이 트랜스포머를 NLP 과제들에서 이점을 갖게 했는지 논한다. 마지막에 가서는, 트랜스포머를 효과적으로 트레이닝 시킬 수 있는 트릭들에 대해 이야기 할 것이다.



<!-- ## Lecture part B

In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT. -->

## 강의 part B

여기서는 탐욕 디코딩<sup>greedy decoding</sup>과 완전 탐색<sup>exhaustive search</sup> 사이에 위치한, 빔서치<sup>beam search</sup>를 소개한다. 생성 분포<sup>generative distribution</sup>로부터 샘플링을 하고 싶은 케이스(즉, 텍스트 생성)도 고려하여 "톱-k" 샘플링을 알려줄 것이다. 이후에는 트랜스포머 변종들과 함께하는 시퀀스-투-시퀀스 모델과 역번역<sup>backtranslation</sup>도 선보인다. 다음으로는 임베딩을 학습하기 위한 비지도 학습 접근법과 워드투벡터<sup>Word2Vec</sup>, GPT, 그리고 BERT까지 논한다.



<!-- ## Practicum

We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures. -->

## 실습

셀프-어텐션과 그 입력값의 은닉층 표현 방법에 집중하여 어텐션을 소개한다. 이후 키-밸류 저장 모형<sup>key-value store paradigm</sup>을 알려주고, 쿼리<sup>queries</sup>, 키<sup>keys</sup>, 그리고 밸류들<sup>values</sup>이 입력값의 순환으로 어떻게 나타내어 지는지 논할 것이다. 끝으로, 트랜스포머 아키텍쳐 해석을 위해 어텐션을 사용하고, 기본적인 트랜스포머를 지나 쭉쭉 나아가, 인코더-디코더 모형과 시퀀셜 아키텍쳐까지 비교한다.

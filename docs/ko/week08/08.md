---
lang-ref: ch.08
title: 8주차
lang: ko
translation-date: 17 Jul 2020 
translator: Yujin
---

<!-- 
## Lecture part A -->
## 이론 part A

<!-- In this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence. -->
이번 섹션에서는 에너지 기반 모델<sup>Energy-Based Models</sup>의 대조적 방법<sup>contrastive methods</sup>을 몇가지 측면에서 소개하는데 중점을 둔다. 먼저, 자기 지도 학습<sup>self-supervised learning</sup>에 대조적 방법을 적용하는 것의 장점에 대해 논의한다. 둘째, 디노이징 오토인코더<sup>denoising autoencoders</sup>의 구조와,  이미지 재구성 작업에서 디노이징 오토인코더가 지니는 약점에 대해 알아본다. 또한 contrastive divergence<sup>대조 발산</sup>와 persistent contrastive divergence와 같은 다른 대조적 방법에 대해서도 이야기한다. 

<!-- ## Lecture part B -->
## 이론 part B

<!-- In this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved. -->
이 섹션에서는 정규화된 잠재변수 EBM의 조건부<sup>conditional</sup> 그리고 무조건부<sup>unconditional</sup> 모델 개념을 포함해 자세히 논의한다. 다음으로, ISTA, FISTA 그리고 LISTA 알고리즘에 대해 이야기하고, 희소 코딩<sup>sparse coding</sup> 예제와 합성곱 희소 인코더<sup>convolutional sparse encoders</sup>로부터 학습된 필터에 대해 알아본다. 마지막으로 Variational 오토인코더<sup>Variational Auto-Encoders</sup>와 이것과 관련된 기본 개념에 대해 이야기한다.  

<!-- ## Practicum -->
## 실습
<!-- In this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples. -->
이 섹션에서는 변이형 오토인코더<sup>Variational autoencoders</sup>라는 특정한 생성 모델<sup>generative models</sup>에 대해 논의하고 고전적인<sup>Classic</sup> 오토인코더와 기능 및 장점에 대해 비교한다. 우리는 잠재 공간<sup>latent space</sup>에서 변이형 오토인코더가 어떤 구조를 어떻게 적용하는지 이해하며 변이형 오토인코더의 목적 함수를 자세히 알아본다. 마지막으로 MNIST 데이터셋에 대해 변이형 오토인코더를 구현하고 훈련시켜서 이것이 새로운 샘플을 생성하도록 한다. 

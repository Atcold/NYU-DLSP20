---
lang-ref: ch.10
title: 10주차
lang: ko
translation-date: 25 Jul 2020
translator: Chanseok Kang
---

<!-- ## Lecture part A -->
## 이론 part A

<!-- In this section, we understand the motivation behind Self-Supervised Learning (SSL), define what it is and see some of its applications in NLP and Computer Vision. We understand how pretext tasks aid with SSL and see some example pretext tasks in images, videos and videos with sound. Finally, we try to get an intuition behind the representation learned by pretext tasks. -->

이번 섹션에서는, 자기 지도 학습 <sup>Self-Supervised Learning (SSL) </sup>에 담겨진 의도에 대해서 이해하고, 그 것이 어떤 것인지 정의하며, NLP와 컴퓨터 영상 분야에서 어떻게 적용되는지에 대해서 살펴볼 것이다. 또한 SSL을 활용해서 어떤 작업들이 선처리되는지 이해하고, 이미지와 영상, 그리고 음성이 포함된 영상


## Lecture part B

In this section, we discuss the shortcomings of pretext tasks, define characteristics that make a good pretrained feature, and how we can achieve this using Clustering and Contrastive Learning. We then learn about ClusterFit, its steps and performance. We further dive into a specific simple framework for Contrastive Learning known as PIRL. We discuss its working as well as its evaluation in different contexts.

## Practicum

During this week's practicum, we explore the [Truck Backer-Upper](http://neuro.bstu.by/ai/To-dom/My_research/Papers-2.1-done/RL-sparce-reward/9/Ref/truckbackerupper.pdf) (Nguyen & Widrow, '90).
This problem shows how to solve an non-linear control problem using neural networks.
We learn a model of a truck's kinematics, and optimize a controller through this learned model, finding that the controller is able to learn complex behaviors through purely observational data.

---
lang: ko
lang-ref: ch.10-1
title: 자기 지도 학습 - Pretext Tasks
lecturer: Ishan Misra
authors: Aniket Bhatnagar, Dhruv Goyal, Cole Smith, Nikhil Supekar
date: 6 Apr 2020
translation-date: 30 Jul 2020
translator: Chanseok Kang
---

<!-- ## [Success story of supervision: Pre-training](https://www.youtube.com/watch?v=0KeR6i1_56g&t=75s) -->
## [supervision의 성공적 사례: 선행 학습](https://www.youtube.com/watch?v=0KeR6i1_56g&t=75s)

<!-- In the past decade, one of the major success recipes for many different Computer Vision problems has been learning visual representations by performing supervised learning for ImageNet classification. And, using these learned representations, or learned model weights as initialization for other computer vision tasks, where a large quantum of labelled data might not be available. -->
지난 세월 동안, 수많은 영상 처리 문제를 해결했던 사례 중 하나는, ImageNet classification을 위해서 수행한 지도 학습으로부터 얻은 시각적인 대표성들을 학습했던 것이다. 그리고 라벨이 달린 데이터가 많지 않은 다른 영상 처리 문제에 적용하기 위해서 학습된 대표성이나 모델의 가중치를 초기 가중치로 사용되었다.

<!-- However, getting annotations for a dataset of the magnitude of ImageNet is immensely time-consuming and expensive. Example: ImageNet labelling with 14M images took roughly 22 human years. -->
하지만 ImageNet과 같은 크기의 데이터들로부터 의미를 얻는 것은 매우 시간이 많이 필요하고 어렵다. 예를 들어 1400만개의 이미지가 들어있는 ImageNet에서 사람이 라벨링을 하려면 약 22년 정도가 걸린다.


<!-- Because of this, the community started to look for alternate labelling processes, such as hashtags for social media images, GPS locations, or self-supervised approaches where the label is a property of the data sample itself. -->
이로 인해서, 개발 업계에서는 SNS에 올리는 해시태그나 GPS 위치 같은 것으로 라벨링을 대체할 수 있는 방안을 찾기 시작했고, 라벨이 데이터 샘플 자체의 특성이 될 수 있는 자기 지도적 접근 방법도 제안되었다.


<!-- But an important question that arises before looking for alternate labelling processes is: -->
일단 라벨링 작업을 대체할 수 있는 방법을 찾기 전에 던질 수 있는 중요한 질문은 다음과 같다:

<!-- **How much labelled data can we get after all?** -->
**최종적으로 우리가 얻을 수 있는 라벨링된 데이터는 얼마나 될까?**

<!-- - If we search for all images with object-level category and bounding box annotations then there are roughly 1 million images.
- Now, if the constraint for bounding box coordinates is relaxed, the number of images available jumps to 14 million (approximately).
- However, if we consider all images available on the internet, there is a jump of 5 orders in the quantity of data available.
- And, then there is data apart from images, which requires other sensory input to capture or understand. -->

- 만약 객체 단위로 구분짓고, 사각형으로 둘러싸인 모든 이미지를 찾는다면, 대략 백만여개의 이미지가 나올 것이다.
- 이제 사각형의 제약을 조금 완화시킨다면, 이미지의 수는 (대략적으로) 1400만개로 확 뛸 것이다.
- 하지만, 인터넷에서 찾을 수 있는 모든 이미지를 고려한다면, 가용 데이터의 크기는 5승으로 증가할 것이다.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img01.jpg" width="80%"/><br>
<b>그림 1:</b> 주석상에서의 가용 데이터 요소들의 범주
</center>

<!-- Hence, drawing from the fact that ImageNet specific annotation alone took 22 human years worth of time, scaling labelling to all internet photos or beyond is completely infeasible. -->

더불어, ImageNet에서 특정 라벨링 작업을 사람이 했을때 22년 정도가 걸리는 것을 감안한다면, 인터넷상의 모든 사진에 대해서 라벨링을 하거나 그 이상의 작업은 효율성이 거의 없다.

<!-- **Problem of Rare Concepts** (*Long Tail Problem*) -->
**희소 개념에 대한 문제** (*긴 꼬리 문제*)

<!-- Generally, the plot presenting distribution of the labels for internet images looks like a long tail. That is, most of the images correspond to very few labels, while there exist a large number of labels for which not many images are present. Thus, getting annotated samples for categories towards the end of the tail requires huge quantities of data to be labelled [commenting out this redundant phrase]: <> (because of the nature of the distribution of categories). -->

일반적으로, 인터넷상에 있는 사진의 라벨에 대한 분포를 그려보면 마치 긴 꼬리와 같이 보인다. 그 말은, 엄청나게 많은 라벨이 존재하기는 하나 그 사진들의 수는 많지 않은 가운데, 대부분의 사진들이 매우 적은 수의 라벨을 가지는 것을 말한다. 결국, 앞에서 이야기한 꼬리의 끝을 넘어서서 각 종류에 대한 라벨을 얻는 것은 (각 종류의 분포에 담긴 이치에 따라서) 라벨링된 엄청난 수의 데이터들이 필요하게 된다.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img02.png" width="80%"/><br>
<b>그림 2:</b> 라벨이 달린 가용 이미지의 분포에 대한 범주
</center>

<!-- **Problem of Different Domains** -->
**다른 영역에 대한 문제**

<!-- This method of ImageNet pre-training and fine-tuning on downstream task gets even murkier when the downstream task images belong to a completely different domain, such as medical imaging. And, obtaining a dataset of the quantum of ImageNet for pre-training for different domains is not possible. -->
사전에 학습된 ImageNet model을 하위 task를 위해 fine-tuning하는 것은 하위 task에 사용될 이미지가 의학 이미지와 같이 완전히 다른 영역에 속한 이미지일 경우, 그 의미가 퇴색된다. 또한 서로 다른 영역에 대해서 사전에 학습된 ImageNet dataset을 얻는 것은 불가능하다.


<!-- ## What is self-supervised Learning? -->
## 자기 지도 학습이란 무엇일까?

<!-- **Two ways to define self-supervised learning** -->
**자기 지도 학습을 정의하는 두가지 방법**

<!-- - **Basis supervised learning definition**, i.e. the network follows supervised learning where labels are obtained in a semi-automated manner, without human input. -->
**기본 지도학습의 정의**, 예를 들어 신경망이 인간의 입력없이, 반자동화된 방법을 통해 얻은 라벨을 활용해서 지도학습을 수행하는 것을 말한다.
<!-- - **Prediction problem**, where a part of the data is hidden, and rest visible. Hence, the aim is to either predict the hidden data or to predict some property of the hidden data. -->
**예측 문제**, 데이터의 일부가 감춰져 있고, 나머지 부분은 보이는 상황이다. 이 때의 목표는 감춰진 데이터를 예측하거나, 감춰진 데이터의 일부 속성을 예측하는 것이 된다.

<!-- **How self-supervised learning differs from supervised learning and unsupervised learning?** -->
**자기 지도 학습이 일반 지도 학습이나 비지도 학습이랑 다른 부분은 무엇인가?**

<!-- - Supervised learning tasks have pre-defined (and generally human-provided) labels,
- Unsupervised learning has just the data samples without any supervision, label or correct output.
- Self-supervised learning derives its labels from a co-occurring modality for the given data sample or from a co-occurring part of the data sample itself. -->
- 지도 학습에서는 사전에 정의된(그리고 일반적으로는 사람이 정의한) 라벨을 가지고 있다.
- 비지도 학습은 어떤 supervision이나 라벨, 올바른 출력없이 딱 데이터만 가지고 있다.
- 자기 지도 학습에서는 주어진 데이터로부터 동시에 발생하는 양식<sup>modality</sup>이나 데이터 자체의 반복적인 부분으로부터 라벨을 얻어낸다.

<!-- ### Self-Supervised Learning in Natural Language Processing -->
### 자연 언어 처리에서의 자기 지도 학습

#### Word2Vec

<!-- - Given an input sentence, the task involves predicting a missing word from that sentence, which is specifically omitted for the purpose of building a pretext task.
- Hence, the set of labels becomes all possible words in the vocabulary, and, the correct label is the word that was omitted from the sentence.
- Thus, the network can then be trained using regular gradient-based methods to learn word-level representations. -->
- 입력 문장이 주어졌을때, 보통은 pretext task를 만들기 위한 목적으로 특별히 생략된 단어를 문장을 통해서 예측하는 작업을 수행한다.
- 이때, 라벨들은 문맥내에서 뽑아낼 수 있는 모든 단어가 될 것이고, 여기서 올바른 라벨은 문장상에서 생략된 단어가 된다.
- 그렇기 때문에, 신경망은 단어-기준간의 대표성을 학습하기 위해서 일반적인 기울기 기반의 학습법을 통해서 학습될 수 있다.

<!-- ### Why self-supervised learning? -->
### 왜 자기 지도 학습을 사용하는가?

<!-- - Self-supervised learning enables learning representations of data by just observations of how different parts of the data interact.
- Thereby drops the requirement of huge amount of annotated data.
- Additionally, enables to leverage multiple modalities that might be associated with a single data sample. -->
- 자기 지도 학습은 데이터들의 서로 다른 영역이 어떻게 상호작용하는지를 단순히 관찰만을 통해서 데이터의 대표성을 학습할 수 있게 된어 있다.
- 그렇게 때문에 라벨링된 데이터들이 엄청나게 많을 필요가 없어지게 된다.
- 추가로 하나의 데이터와 연관되어 있을 수도 있는 여러 양식의 이점을 취할 수 있다.

<!-- ### Self-Supervised Learning in Computer Vision -->
### 영상 처리에서의 자기 지도 학습

<!-- Generally, computer vision pipelines that employ self-supervised learning involve performing two tasks, a pretext task and a real (downstream) task. -->
일반적으로, 자기 지도 학습이 반영된 영상 처리 파이프라인은 pretext task와 실제 (하위) task, 이 두 개의 task를 수행한다.
<!-- - The real (downstream) task can be anything like classification or detection task, with insufficient annotated data samples.
- The pretext task is the self-supervised learning task solved to learn visual representations, with the aim of using the learned representations or model weights obtained in the process, for the downstream task. -->
- 실제 (하위) task는 라벨링된 데이터가 충분하지 않은 경우에 수행해야 할 분류나 감지 task같은 모든 task들이 될 수 있다.
- Pretext task는 시각적인 대표성을 학습할 자기 지도 학습이 될텐데, 이때 하위 task에서 사용될 학습된 대표성이나 모델의 가중치를 구할 목적으로 사용된다.

<!-- #### Developing pretext tasks -->
#### pretext task를 개발하는 방법

- Pretext tasks for computer vision problems can be developed using either images, video, or video and sound.
- In each pretext task, there is part visible and part hidden data, while the task is to predict either the hidden data or some property of the hidden data.


#### [Example pretext tasks: Predicting relative position of image patches](https://www.youtube.com/watch?v=0KeR6i1_56g&t=759s)

- Input: 2 image patches, one is the anchor image patch while the other is the query image patch.
- Given the 2 image patches, the network needs to predict the relative position of the query image patch with respect to the anchor image patch.
- Thus, this problem can be modelled as an 8-way classification problem, since there are 8 possible locations for a query image, given an anchor.
- And, the label for this task can be automatically generated by feeding the relative position of query patch with respect to the anchor.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img03.jpg" width="70%"/><br>
<b>Figure 3:</b> Relative Position task
</center>


#### Visual representations learned by relative position prediction task

We can evaluate the effectiveness of the learned visual representations by checking nearest neighbours for a given image patch basis feature representations provided by the network. For computing nearest neighbours of a given image patch,

- Compute the CNN features for all images in the dataset, that will act as the sample pool for retrieval.
- Compute CNN features for the required image patch.
- Identify nearest neighbours for the feature vector of the required image, from the pool of feature vectors of images available.

Relative position task finds out image patches that are very similar to the input image patch, while maintains invariance to factors such as object colour. Thus, the relative position task is able to learn visual representations, where representations for image patches with similar visual appearance are closer in the representation space as well.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img04.jpg" width="100%"/><br>
<b>Figure 4:</b> Relative Position: Nearest Neighbours
</center>


#### Predicting Rotation of Images

- Predicting rotations is one of the most popular pretext task which has a simple and straightforward architecture and requires minimal sampling.
- We apply rotations of 0, 90, 180, 270 degrees to the image and send these rotated images to the network to predict what sort of rotation was applied to the image and the network simply performs a 4-way classification to predict the rotation.
- Predicting rotations does not make any semantic sense, we are just using this pretext task as a proxy to learn some features and representations to be used in a downstream task.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img05.png" width="80%"/><br>
<b>Figure 5:</b> Rotations of Image
</center>


#### Why rotation helps or why it works?

It has been proven that it works empirically. The intuition behind it is that in order to predict the rotations, model needs to understand the rough boundaries and representation of an image. For example, it will have to segregate the sky from water or sand from the water or will understand that trees grow upwards and so on.


#### Colourisation

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img06.png" width="65%"/><br>
<b>Figure 6:</b> Colourisation
</center>

In this pretext task, we predict the colours of a grey image. It can be formulated for any image, we just remove the colour and feed this greyscale image to the network to predict its colour. This task is useful in some respects like for colourising the old greyscale films [//]: <> (we can apply this pretext task). The intuition behind this task is that the network needs to understand some meaningful information like that the trees are green, the sky is blue and so on.

It is important to note that colour mapping is not deterministic, and several possible true solutions exist. So, for an object if there are several possible colours then the network will colour it as grey which is the mean of all possible solutions. There have been recent works using Variational Auto Encoders and latent variables for diverse colourisation.


#### Fill in the blanks

We hide a part of an image and predict the hidden part from the remaining surrounding part of the image. This works because the network will learn the implicit structure of the data like to represent that cars run on roads, buildings are composed of windows & doors and so on.


### Pretext Tasks for videos

Videos are composed of sequences of frames and this notion is the idea behind self-supervision, which can be leveraged for some pretext tasks like predicting the order of frames, fill in the blanks and object tracking.


#### Shuffle & Learn

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img07.png" width="70%"/><br>
<b>Figure 7:</b> Interpolation
</center>

Given a bunch of frames, we extract three frames and if they are extracted in the right order we label it as positive, else if they are shuffled, label it as negative. This now becomes a binary classification problem to predict if the frames are in the right order or not. So, given a start and end point, we check if the middle is a valid interpolation of the two.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img08.png" width="70%"/><br>
<b>Figure 8:</b> Shuffle & Learn architecture
</center>

We can use a triplet Siamese network, where the three frames are independently fed forward and then we concatenate the generated features and perform the binary classification to predict if the frames are shuffled or not.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img09.png" width="100%"/><br>
<b>Figure 9:</b> Nearest Neighbours Representation
</center>

Again, we can use the Nearest Neighbours algorithm to visualize what our networks are learning. In fig. 9 above, first we have a query frame which we feed-forward to get a feature representation and then look at the nearest neighbours in the representation space. While comparing, we can observe a stark difference between neighbours obtained from ImageNet, Shuffle & Learn and Random.

ImageNet is good at collapsing the entire semantic as it could figure out that it is a gym scene for the first input. Similarly, it could figure out that it is an outdoor scene with grass etc. for the second query. Whereas, when we observe Random we can see that it gives high importance to the background colour.

On observing Shuffle & Learn, it is not immediately clear whether it is focusing on the colour or on the semantic concept. After further inspection and observing various examples, it was observed that it is looking at the pose of the person. For example, in the first image the person is upside down and in second the feet are in a particular position similar to query frame, ignoring the scene or background colour. The reasoning behind this is that our pretext task was predicting whether the frames are in the right order or not, and to do this the network needs to focus on what is moving in the scene, in this case, the person.

It was verified quantitatively by fine-tuning this representation to the task of human key-point estimation, where given a human image we predict where certain key points like nose, left shoulder, right shoulder, left elbow, right elbow, etc. are. This method is useful for tracking and pose estimation.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img10.png" width="80%"/><br>
<b>Figure 10:</b> Key point Estimation comparison
</center>

In figure 10, we compare the results for supervised ImageNet and Self-Supervised Shuffle & Learn on FLIC and MPII datasets and we can see that Shuffle and Learn gives good results for key point estimation.


### Pretext Tasks for videos and sound

Video and Sound are multi-modal where we have two modalities or sensory inputs one for video and one for sound. Where we try to predict whether the given video clip corresponds to the audio clip or not.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img11.png" width="100%"/><br>
<b>Figure 11:</b> Video and sound sampling
</center>

Given a video with audio of a drum, sample the video frame with corresponding audio and call it a positive set. Next, take the audio of a drum and the video frame of a guitar and tag it as a negative set. Now we can train a network to solve this as a binary classification problem.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img12.png" width="70%"/><br>
<b>Figure 12:</b> Architecture
</center>

**Architecture:** Pass the video frames to the vision subnetwork and pass the audio to the audio subnetwork, which gives 128-dimensional features and embeddings, we then fuse them together and solve it as a binary classification problem predicting if they correspond with each other or not.

It can be used to predict what in the frame might be making a sound. The intuition is if it is the sound of a guitar, the network roughly needs to understand how the guitar looks and same should be true for drums.


## [Understanding what the "pretext" task learns](https://www.youtube.com/watch?v=0KeR6i1_56g&t=2426s)

* Pretext tasks should be **complementary**

  * Let's take for example the pretext tasks *Relative Position* and *Colourisation*. We can boost performance by training a model to learn both pretext tasks as shown below:

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img13.png" width="80%"/><br>
<b>Figure 13:</b> Comparison of disjoint vs combined training of Relative Position and Colourisation pretext tasks. ResNet101. (Misra)
</center>

  * A single pretext task may not be the right answer to learn SS representations

* Pretext tasks vary greatly in what they try to predict (difficultly)

  * Relative position is easy since it's a simple classification
  * Masking and fill-in is far harder --> **better representation**
  * **Contrastive methods** generate even more info than pretext tasks

* **Question:** How do we train multiple pre-training tasks?

  * The pretext output will depend on the input. The final fully-connected layer of the network can be **swapped** depending on the batch type.
  * For example: A batch of black-and-white images is fed to the network in which the model is to produce a coloured image. Then, the final layer is switched, and given a batch of patches to predict relative position.

*  **Question:** How much should we train on a pretext task?

  * Rule of thumb: Have a very difficult pretext task such that it improves the downstream task.
  * In practice, the pretext task is trained, and may not be re-trained. In development, it is trained as part of the entire pipeline.


## Scaling Self-Supervised Learning


### Jigsaw Puzzles

* Partition an image into multiple tiles and then shuffle these tiles. The model is then tasked with un-shuffling the tiles back to the original configuration. (Noorozi & Favaro, 2016)

  * Predict which permutation was applied to the input
  * This is done by creating batches of tiles such that **each tile of an image is evaluated independently**. The convolution output are then concatenated and the permutation is predicted as in figure below

<center>
<img src="{{site.baseurl}}/images/week10/10-1/img14.png" width="80%"/><br>
<b>Figure 14:</b> Siamese network architecture for a Jigsaw pretext task. Each tile is passed through independently, with encodings concatenated to predict a permutation. (Misra)
</center>

  * Considerations:
    1. Use a subset of permutations (i.e.: From 9!, use 100)
    2. The n-way ConvNet uses shared parameters
    3. The problem complexity is the size of the subset. The *amount of information you are predicting.*

* Sometimes, this method can perform better on downstream tasks than supervised methods, since the network is able to learn some concepts about the geometry of its input.

* Shortcomings: Few Shot Learning: Limited number of training examples

  * **Self-supervised representations are not as sample efficient**


### Evaluation: Fine-tuning vs Linear Classifier

  This form of evaluation is a kind of **Transfer Learning**.

* **Fine Tuning**: When applying to our downstream task, we use our entire network as an **initialization** for which to train a new one, updating all the weights.

* **Linear Classifier**: On top of our pretext network, we train a small linear classifier to perform our downstream task, leaving the rest of the network intact.

> A good representation should transfer with a **little training**.

* It is helpful to evaluate the pretext learning on a **multitude of different tasks**. We can do so by extracting the representation created by different layers in the network as **fixed features** and evaluating their usefulness across these different tasks.
  * Measurement: Mean Average Precision (mAP) --The precision averaged across all the different tasks we are considering.
  * Some examples of these tasks include: Object Detection (using fine-tuning), Surface Normal Estimation (see NYU-v2 dataset)
* What does each layer learn?
  * Generally, as the layers become deeper, the mean average precision on downstream tasks using their representations will increase.
  * However, the **final layer** will see a sharp drop in the mAP due to the layer becoming overly **specialized**.
    * This contrasts with supervised networks, in that the mAP generally always increases with depth of layer.
    * This shows that the pretext task is **not well-aligned** to the downstream task.

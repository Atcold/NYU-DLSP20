---
lang-ref: ch.02-1
title: 경사하강법과 역전파 알고리즘
authors: Amartya Prasad, Dongning Fang, Yuxin Tang, Sahana Upadhya
date: 3 February 2020
translation-date: 24 Mar 2020
translator: Chanseok Kang
---


## [경사 하강 최적화 알고리즘](https://www.youtube.com/watch?v=d9vdh3b787Y&t=29s)


### 매개변수화된 모델

$$
\bar{y} = G(x,w)
$$

<!-- Parametrised models are simply functions that depend on inputs and trainable parameters. There is no fundamental difference between the two, except that trainable parameters are shared across training samples whereas the input varies from sample to sample. In most deep learning frameworks, parameters are implicit, that is, they aren't passed when the function is called. They are 'saved inside the function', so to speak, at least in the object-oriented versions of models. -->
매개변수화된 모델은 입력과 학습가능한 매개변수에 의해서 결정되는 간단한 함수를 말합니다. 이 두 개의 변수간에 원론적인 차이점은 없지만, 한가지 다른 부분은 학습가능한 매개변수는 샘플들이 학습되는 동안에 서로간에 공유가 되지만, 입력은 샘플과 샘플마다 다르다는 점입니다. 최근에 출시된 딥러닝 프레임워크에서는 매개변수들이 함축적으로 정의되어 있는데, 이 말은, 함수가 호출될 때 포함되어 있지 않다는 것을 의미합니다. 적어도 객체-기반으로 구현된 모델을 바탕으로 굳이 말하자면, 이 변수들은 '함수 내부에 저장되어' 있습니다.

<!-- The parametrised model (function) takes in an input, has a parameter vector and produces an output. In supervised learning, this output goes into the cost function ($C(y,\bar{y}$)), which compares the true output (${y}$) with the model output ($\bar{y}$). The computation graph for this model is shown in Figure 1. -->
매개변수화된 모델 (함수)는 입력을 받고, 매개변수 벡터를 가지고 있으며, 이를 이용해서 출력을 만들어냅니다. 지도 학습에서는 이 출력값이 비용 함수 ($C(y,\bar{y}$)) 로 들어가는데, 이 함수는 실제의 출력값 (${y}$) 과 모델에서 생성된 출력값 ($\bar{y}$)을 비교해주는 역할을 합니다. 이 모델에 대한 연산 그래프는 그림 1에서 보여줍니다.


| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure1.jpg" alt="Figure1" style="zoom: 33%;" /></center> |
<!-- | <center>Figure 1: Computation Graph representation for a Parametrised Model </center>| -->
| <center>그림 1: 매개변수화된 모델에 대한 연산 그래프 </center>|

<!-- Examples of parametrised functions - -->
매개변수화된 함수의 예시
<!-- - Linear Model - Weighted Sum of Components of the Input Vector : -->
- 선형 모델 - 입력 벡터의 각 요소들에 대한 가중합:
  $$
  \bar{y} = \sum_i w_i x_i \text{  ;  } C(y,\bar{y}) = ||y - \bar{y}||^2
  $$

<!-- - Nearest Neighbor - There is an input x and a weight matrix W with each row of the matrix indexed by k. The output is the value of k that corresponds to the row of W that is closest to x : -->
- 최근접 이웃 - 입력 x와 k로 인덱싱할 수 있는 행을 가진 가중치 행렬 W가 있습니다. 이때의 출력은 k의 값 자체가 될텐데, 이 값은 x와 가장 가까운 W의 행을 나타냅니다.:
  $$
  \bar{y} = \text{argmin}_k ||x - w_{k,.} ||^2
  $$
  <!-- Parameterized models could also involve complicated functions. -->
  매개변수화된 모델은 복잡한 함수와도 관련있을 수 있습니다.


<!-- #### Block diagram notations for computation graphs -->
#### 연산 그래프에 대한 각 요소에 대한 표현

<!-- - Variables (tensor, scalar, continuous, discrete)
    - <img src="{{site.baseurl}}/images/week02/02-1/x.PNG" alt="x" style="zoom:50%;" /> is an observed input to the system
    - <img src="{{site.baseurl}}/images/week02/02-1/y.PNG" alt="y" style="zoom:50%;" /> is a computed variable which is produced by a deterministic function -->

- 변수 (텐서, 스칼라, 연속적인 값, 비연속적인 값)
    - <img src="{{site.baseurl}}/images/week02/02-1/x.PNG" alt="x" style="zoom:50%;" /> 는 시스템상에서 관찰된 입력값입니다.
    - <img src="{{site.baseurl}}/images/week02/02-1/y.PNG" alt="y" style="zoom:50%;" /> 는 결정적 함수에 의해서 생성된 계산된 값입니다.


<!-- - Deterministic functions

    <img src="{{site.baseurl}}/images/week02/02-1/deterministic_function.PNG" alt="deterministic_function" style="zoom:50%;" />

    - Takes in multiple inputs and can produce multiple outputs
    - It has an implicit parameter variable (${w}$)
    - The rounded side indicates the direction in which it is easy to compute. In the above diagram, it is easier to compute ${\bar{y}}$ from ${x}$ than the other way around -->

- 결정적 함수

    <img src="{{site.baseurl}}/images/week02/02-1/deterministic_function.PNG" alt="deterministic_function" style="zoom:50%;" />

    - 여러 개의 입력을 받고 여러 개의 출력을 생성합니다.
    - 내부적으로 함축된 매개변수 (${w}$) 를 가지고 있습니다.
    - 곡선으로 나타난 부분은 어느 쪽이 계산하기 쉬운지를 방향으로 표현한 것입니다. 위의 다이어그램에서는 ${x}$ 으로부터 ${\bar{y}}$ 를 계산하는 것이 다른 방법보다 쉽습니다.



<!-- - Scalar-valued function

  <img src="{{site.baseurl}}/images/week02/02-1/scalar-valued.PNG" alt="scalar-valued" style="zoom:50%;" />

    - Used to represent cost functions
    - Has an implicit scalar output
    - Takes multiple inputs and outputs a single value (usually the distance between the inputs) -->

- 스칼라 값을 가지는 함수

  <img src="{{site.baseurl}}/images/week02/02-1/scalar-valued.PNG" alt="scalar-valued" style="zoom:50%;" />

    - 비용 함수를 나타낼 때 사용합니다.
    - 내부적으로 함축된 출력을 가지고 있습니다.
    - 다수의 입력을 가지고, 하나의 값을 출력으로 내보냅니다. (보통 입력들간의 거리를 표현합니다.)


<!-- #### Loss functions -->
#### 손실 함수

<!-- Loss function is a function that is minimized during training. There are two types of losses: -->
손실 함수는 학습 과정 동안 최소화되어야 할 함수 입니다. 여기에는 두가지의 손실이 있습니다:

<!-- 1) Per Sample Loss -
$$
 L(x,y,w) = C(y, G(x,w))
$$
2) Average Loss -

​	For any set of Samples $$S = \{(x[p],y[p])/p=0,1...P-1 \}$$

​	Average Loss over the Set S is given by :  $$L(S,w) = \frac{1}{P} \sum_{(x,y)} L(x,y,w)$$

| <center><img src="{{site.baseurl}}/images/week02/02-1/Average_Loss.png" alt="Average_Loss" style="zoom:33%;" /></center> |
|   <center>Figure 2: Computation graph for model with Average Loss    </center>| -->

1) 샘플당 손실 -
$$
 L(x,y,w) = C(y, G(x,w))
$$
2) 평균 손실 -

​	어떤 샘플들로 구성된 집합 $$S = \{(x[p],y[p])/p=0,1...P-1 \}$$ 가 있을 때,

​	집합 S에 대한 평균 손실은 다음과 같이 주어집니다. :  $$L(S,w) = \frac{1}{P} \sum_{(x,y)} L(x,y,w)$$

| <center><img src="{{site.baseurl}}/images/week02/02-1/Average_Loss.png" alt="Average_Loss" style="zoom:33%;" /></center> |
|   <center>그림 2: 평균 손실에 기반한 모델의 연산 그래프    </center>|


<!-- In the standard Supervised Learning paradigm, the loss (per sample) is simply the output of the cost function. Machine Learning is mostly about optimizing functions (usually minimizing them). It could also involve finding Nash Equilibria between two functions like with GANs. This is done using Gradient Based Methods, though not necessarily Gradient Descent. -->
일반적인 지도 학습에서는, (샘플당) 손실은 단순하게 손실 함수의 출력으로 표현할 수 있습니다. 기계 학습에서는 대부분 최적화해야 할 함수로 표현합니다.(일반적으로는 이 함수를 최적화해야 합니다.) 또한 GAN에서처럼 두 함수간의 내시 균형<sup>Nash Equilibria</sup>을 찾는 것과 연관되어 있기도 합니다. 이런 작업은 경사 기반의 방법들을 통해서 수행할 수 있지만, 이것이 꼭 경사하강법일 필요는 없습니다.


<!-- ### Gradient descent -->
### 경사 하강

<!-- A **Gradient Based Method** is a method/algorithm that finds the minima of a function, assuming that one can easily compute the gradient of that function. It assumes that the function is continuous and differentiable almost everywhere (it need not be differentiable everywhere). -->

**경사 기반의 방법**은 함수의 최소점을 찾는 방법/알고리즘이며, 이때 해당 함수의 경사를 쉽게 구할 수 있을 것라는 전제를 둡니다. 이 전제는 함수가 거의 모든 구간에서 연속적이고, 미분가능하다는 것을 포함하고 있습니다. (꼭 모든 구간에서 미분가능할 필요는 없습니다.)

<!-- **Gradient Descent Intuition** - Imagine being in a mountain in the middle of a foggy night. Since you want to go down to the village and have only limited vision, you look around your immediate vicinity to find the direction of steepest descent and take a step in that direction. -->

**직관적인 경사 하강법** - 안개가 짙게 낀 한밤중의 산에 있다고 가정해봅니다. 당신은 마을로 내려가길 원하지만, 한정된 시야각을 가지고 있기 때문에, 가장 가파르게 경사가 하강하는 방향을 찾기 위해서 당신 부근을 살펴 볼 것이고, 그 방향으로 한 발자국 나아갈 것입니다.

<!-- **Different methods of Gradient Descent** -->
**경사 하강의 다른 방법들**

<!-- - Full (batch) gradient descent update rule :
  $$
  w \leftarrow w - \eta \frac{\partial L(S,w)}{\partial w}
  $$

- For SGD (Stochastic Gradient  Descent), the update rule becomes :
  - Pick a $p$ in $\text{0...P-1}$, then update
    $$
    w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
    $$

Where $${w}$$ represents the parameter to be optimized.

$\eta \text{ is a constant here but in more sophisticated algorithms, it could be a matrix}$. -->

- 완전 (배치) 경사 하강법에서의 업데이트 :
  $$
  w \leftarrow w - \eta \frac{\partial L(S,w)}{\partial w}
  $$

- SGD (확률적 경사 기반)에서는 업데이트가 다음과 같이 바뀝니다.:
  - $p$ in $\text{0...P-1}$를 선택하고, 다음과 같이 업데이트합니다.
    $$
    w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
    $$

여기서 $${w}$$ 는 최적화해야 할 매개변수를 나타냅니다.

$\eta \text{는 위 식에서는 상수로 정의되어 있지만, 조금더 복잡한 알고리즘에서는 행렬로 정의될 수도 있습니다.}$.


<!-- If it is a positive semi-definite matrix, we'll still move downhill but not necessarily in the direction of steepest descent. In fact the direction of steepest descent may not always be the direction we want to move in.

If the function is not differentiable, i.e, it has a hole or is staircase like or flat, where the gradient doesn't give you any information, one has to resort to other methods - called 0-th Order Methods or Gradient-Free Methods. Deep Learning is all about Gradient Based Methods.

However, RL (Reinforcement Learning) involves **Gradient Estimation** without the explicit form for the gradient. An example is a robot learning to ride a bike where the robot falls every now and then. The objective function measures how long the bike stays up without falling. Unfortunately, there is no gradient for the objective function. The robot needs to try different things.

The RL cost function is not differentiable most of the time but the network that computes the output is gradient-based. This is the main difference between supervised learning and reinforcement learning. With the latter, the cost function C is not differentiable. In fact it completely unknown. It just returns an output when inputs are fed to it, like a blackbox. This makes it highly inefficient and is one of the main drawbacks of RL - particularly when the parameter vector is high dimensional (which implies a huge solution space to search in, making it hard to find where to move).

A very popular technique in RL is Actor Critic Methods. A critic method basically consists of a second C module which is a known, trainable module. One is able to train the C module, which is differentiable, to approximate the cost function/reward function. The reward is a negative cost, more like a punishment. That’s a way of making the cost function differentiable, or at least approximating it by a differentiable function so that one can backpropagate. -->

만약 이 행렬이 양의 준정부호 행렬<sup>positive semi-definite matrix</sup>라면, 우리는 감소하는 방향으로 움직이긴 하나, 꼭 가장 가파르게 하강하는 방향으로 움직일 필요는 없습니다. 실제로 가장 가파르게 하강하는 방향이 항상 우리가 움직이기를 원하는 방향을 지향하는 것은 아닙니다. 

만약 함수가 미분 가능하지 않다면, 다시 말해, 구멍이 나 있거나 계단처럼 층이 져 있다던가 평평해서 경사 값이 아무런 정보도 주지 못하는 경우에는, 0차 방법 또는 경사를 이용하지 않는 방법<sup>Gradient-Free Methods</sup>라고 불리는 다른 방법들을 적용해야 합니다. 딥러닝에서는 모두 경사 기반의 방법들에 대해서만 다루고 있습니다.

하지만, RL (강화 학습)에서는 경사에 대한 명확한 형태 없이 **경사를 추측하는 방법**이 관련되어 있습니다. 한가지 예를 들자면, 로봇이 매번 넘어지는 것을 통해서 자전거를 타는 방법을 학습하는 것을 들 수 있습니다. 이 때 목적 함수는 자전거를 얼마나 넘어지지 않고 오래 타는지를 통해서 계산할 수 있습니다. 하지만, 여기서는 목적 함수에 대한 경사를 구할 수 없습니다. 로봇은 뭔가 다른 방법을 시도해보아야 합니다.

RL에서의 비용 학습는 대부분의 시간동안 미분가능하지 않지만, 출력을 계산하는 신경망은 경사 기반의 방법을 취하고 있습니다. 이 것이 바로 지도 학습과 강화 학습간의 핵심 차이 입니다. 후자의 경우에서, 비용 함수 C는 미분 가능하지 않습니다. 실제롤 이 함수를 전혀 알지 못합니다. 마치 블랙박스처럼 입력을 넣었을 때, 단순하게 출력만 돌려줍니다. 이러한 방법은 매우 비효율적이며, RL의 단점 중 하나인데, 구체적으로 말하자면 매개변수 벡터가 고차원을 가지는 것을 말합니다. (이 말은 우리가 탐색해야할 정답의 영역이 매우 넣고, 결국 어디로 움직여야 할지 찾는게 어렵다는 것을 의미합니다.)

RL에서 가장 유명한 방법이 Actor Critic 방법입니다. Critic 방법은 기본적으로 알고있고, 학습이 가능한 두번째 비용 함수 모듈을 포함하고 있습니다. 하나는 미분가능한 비용 함수 모듈을 학습시킬 수 있는데, 이를 이용해서 비용 함수/보상 함수를 근사하게 됩니다. 보상은 마치 벌칙을 주는 것처럼 음의 값을 가지고 있습니다. 이렇게 하는 것이 비용 함수를 미분가능하게 만드는 방법이거나, 적어도 미분가능한 함수로 근사함으로써 이를 역전파 알고리즘에 적용시킬 수 있게 해줍니다. 

<!-- ## [Advantages of SGD and backpropagation for traditional neural nets](https://www.youtube.com/watch?v=d9vdh3b787Y&t=1036s) -->

## [전통적인 신경망 구조에서 SGD와 역전파 알고리즘의 이점]](https://www.youtube.com/watch?v=d9vdh3b787Y&t=1036s)


<!-- ### Advantages of Stochastic Gradient Descent (SGD) -->
### 확률적 경사 하강법 (SGD)의 이점

<!-- In practice, we use stochastic gradient to compute the gradient of the objective function w.r.t the parameters. Instead of computing the full gradient of the objective function, which is the average of all samples, stochastic gradient just takes one sample, computes the loss, $L$, and the gradient of the loss w.r.t the parameters, and then takes one step in the negative gradient direction. -->

실제로 적용할 때, 우리는 매개변수에 관한 목적함수의 기울기를 계산할 때, 경사 하강법을 사용합니다. 전체 샘플에 대한 평균을 통해서 목적함수의 전제 경사를 계산하는 것 대신에, 확률적 경사 하강법은 딱 하나의 샘플을 취하고, 이에 대한 손실 $L$ 를 계산하고, 매개변수에 대한 손실의 기울기를 구한 후, 해당 기울기 방향의 반대로 한 걸음 나아갑니다. 

$$
w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
$$

<!-- In the formula, $w$ is approached by $w$ minus the step-size, times the gradient of the per-sample loss function w.r.t the parameters for a given sample, ($x[p]$,$y[p]$). -->

공식에서는, $w$ 가 $w$에 주어진 샘플에 대한 매개변수 ($x[p]$,$y[p]$) 를 활용하여 구한 샘플당 손실 함수의 기울기를 곱한 값을 빼주는 방향으로 접근합니다.

<!-- If we do this on a single sample, we will get a very noisy trajectory as shown in Figure 3. Instead of the loss going directly downhill, it’s stochastic. Every sample will pull the loss towards a different direction. It’s just the average that pulls us to the minimum of the average. Although it looks inefficient, it’s much faster than batch gradient descent at least in the context of machine learning when the samples have some redundancy. -->

이렇게 단일 샘플에 대해서 해당 방법을 적용한다면, 그림 3에서 보여지는 것처럼 매우 오차가 심한 경로가 나올 것입니다. 이렇게, 손실이 바로 감소하지 않고, 확률적으로 나타나게 됩니다. 매 샘플은 손실을 서로 다른 방향으로 유도하게 됩니다. 단순히 평균이 최소화되는 방향으로 이끄는 평균이 됩니다. 비록 이 방법이 비효율적으로 보일지 모르겠지만, 적어도 샘플이 약간의 반복성을 나타내는 머신 러닝 관점에서는 배치 경사 하강법보다는 빠르게 동작합니다.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure2.png" alt="Figure2" style="zoom:80%;" /></center> |
<!-- | <center>Figure 3: Stochastic Gradient Descent trajectory for per sample update </center>| -->
| <center>그림 3: 매 샘플을 업데이트 할 때의 확률적 경사 하강법의 경로 </center>|

<!-- In practice, we use batches instead of doing stochastic gradient descent on a single sample. We compute the average of the gradient over a batch of samples, not a single sample, and then take one step. The only reason for doing this is that we can make more efficient use of the existing hardware  (i.e. GPUs, multicore CPUs) if we use batches since it's easier to parallelize. Batching is the simplest way to parallelize. -->

실제로, 단일 샘플 상에서는 확률적 경사 하강법 대신에 배치를 사용합니다. 그래서 단일 샘플이 아닌, 샘플들의 배치에 대해서 기울기의 평균값을 계산하고, 다음 단계를 진행합니다. 이렇게 하는 이유는 보통 배치를 사용할 경우, 병렬화하기 쉽기 때문에 (GPU나 멀티코어 CPU와 같은) 주어진 하드웨어를 효율적으로 사용할 수 있습니다. 배치로 나누는 것은 병렬화할 수 있는 가장 간단한 방법입니다.

### Traditional neural network

Traditional Neural Nets are basically interspersed layers of linear operations and point-wise non-linear operations. For linear operations, conceptually it is just a matrix-vector multiplication. We take the (input) vector multiplied by a matrix formed by the weights. The second type of operation is to take all the components of the weighted sums vector and pass it through some simple non-linearity (i.e. $\texttt{ReLU}(\cdot)$, $\tanh(\cdot)$, …).

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure3.png" alt="Figure3" style="zoom:30%;" /></center> |
|             <center>Figure 4: Traditional Neural Network             </center>|

Figure 4 is an example of a 2-layer network, because what matters are the pairs (i.e linear+non-linear). Some people call it a 3-layer network because they count the variables. Note that if there are no non-linearities in the middle, we may as well have a single layer because the product of two linear functions is a linear function.

Figure 5 shows how the linear and non-linear functional blocks of the network stack:

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure4.png" alt="Figure4" style="zoom:30%;" /></center> |
|  <center>Figure 5: Looking inside the linear and non-linear blocks   </center>|

In the graph, $s[i]$ is the weighted sum of unit ${i}$ which is computed as:

$$
s[i]=\Sigma_{j \in UP(i)}w[i,j]\cdot z[j]
$$

where $UP(i)$ denotes the predecessors of $i$ and  $z[j]$ is the $j$th output from the previous layer.

The output $z[i]$ is computed as:

$$
z[i]=f(s[i])
$$

where $f$ is a non-linear function.


### Backpropagation through a non-linear function

The first way to do backpropagation is to backpropagate through a non linear function. We take a particular non-linear function $h$ from the network and leave everything else in the blackbox.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure5.png" alt="Figure5" style="zoom: 25%;" /></center> |
|    <center>Figure 6: Backpropagation through non-linear function     </center>|

We are going to use the chain rule to compute the gradients:

$$
g(h(s))' = g'(h(s))\cdot h'(s)
$$

where $h'(s)$ is the derivative of $z$ w.r.t $s$ represented by $\frac{\mathrm{d}z}{\mathrm{d}s}$.
To make the connection between derivatives clear, we rewrite the formula above as:

$$
\frac{\mathrm{d}C}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot \frac{\mathrm{d}z}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot h'(s)
$$

Hence if we have a chain of those functions in the network, we can backpropagate by multiplying by the derivatives of all the ${h}$ functions one after the other all the way back to the bottom.

It’s more intuitive to think of it in terms of perturbations. Perturbing $s$ by $\mathrm{d}s$ will perturb $z$ by:

$$\mathrm{d}z = \mathrm{d}s \cdot h'(s)$$

This would in turn perturb C by:

$$
\mathrm{d}C = \mathrm{d}z\cdot\frac{\mathrm{d}C}{\mathrm{d}z} = \mathrm{d}s\cdot h’(s)\cdot\frac{\mathrm{d}C}{\mathrm{d}z}
$$

Once again, we end up with the same formula as the one shown above.


### Backpropagation through a weighted sum

For a linear module, we do backpropagation through a weighted sum. Here we view the entire network as a blackbox except for 3 connections going from a ${z}$ variable to a bunch of $s$ variables.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure6.png" alt="Figure6" style="zoom: 25%;" /></center> |
|        <center>Figure 7: Backpropagation through weighted sum        </center>|


This time the perturbation is a weighted sum. Z influences several variables. Perturbing $z$ by $\mathrm{d}z$ will perturb $s[0]$, $s[1]$ and $s[2]$ by:

$$
\mathrm{d}s[0]=w[0]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[1]=w[1]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[2]=w[2]\cdot\mathrm{d}z
$$

 This will perturb C by

$$
\mathrm{d}C = \mathrm{d}s[0]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[0]}+\mathrm{d}s[1]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[1]}+\mathrm{d}s[2]\cdot\frac{\mathrm{d}C}{\mathrm{d}s[2]}
$$

Hence C is going to vary by the sum of the 3 variations:

$$
\frac{\mathrm{d}C}{\mathrm{d}z} = \frac{\mathrm{d}C}{\mathrm{d}s[0]}\cdot w[0]+\frac{\mathrm{d}C}{\mathrm{d}s[1]}\cdot w[1]+\frac{\mathrm{d}C}{\mathrm{d}s[2]}\cdot w[2]
$$


## [PyTorch implementation of neural network and a generalized backprop algorithm](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2288s)


### Block diagram of a traditional neural net

- Linear blocks $s_{k+1}=w_kz_k$
- Non-linear blocks $z_k=h(s_k)$

  <center><img src="{{site.baseurl}}/images/week02/02-1/Figure 7.png" alt="Figure 7" style="zoom: 33%;" /></center>

$w_k$: matrix $z_k$: vector $h$: application of scalar ${h}$ function to every component. This is a 3-layer neural net with pairs of linear and non-linear functions, though most modern neural nets do not have such clear linear and non-linear separations and are more complex.


### PyTorch implementation

```python
import torch
from torch import nn
image = torch.randn(3, 10, 20)
d0 = image.nelement()

class mynet(nn.Module):
    def __init__(self, d0, d1, d2, d3):
        super().__init__()
        self.m0 = nn.Linear(d0, d1)
        self.m1 = nn.Linear(d1, d2)
        self.m2 = nn.Linear(d2, d3)

    def forward(self,x):
        z0 = x.view(-1)  # flatten input tensor
        s1 = self.m0(z0)
        z1 = torch.relu(s1)
        s2 = self.m1(z1)
        z2 = torch.relu(s2)
        s3 = self.m2(z2)
        return s3
model = mynet(d0, 60, 40, 10)
out = model(image)
```

- We can implement neural nets with object oriented classes in PyTorch. First we define a class for the neural net and initialize linear layers in the constructor using predefined nn.Linear class. Linear layers have to be separate objects because each of them contains a parameter vector. The nn.Linear class also adds the bias vector implicitly. Then we define a forward function on how to compute outputs with $\text{torch.relu}$ function as the nonlinear activation. We don't have to initialize separate relu functions because they don't have parameters.

- We do not need to compute the gradient ourselves since PyTorch knows how to back propagate and calculate the gradients given the forward function.


### Backprop through a functional module

We now present a more generalized form of backpropagation.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure9.png" alt="Figure9" style="zoom:33%;" /></center> |
|    <center>Figure 8: Backpropagation through a functional module     </center>|


- Using chain rule for vector functions

  $$
   z_g : [d_g\times 1]
  $$

  $$
   z_f:[d_f\times 1]
  $$

  $$
  \frac{\partial c}{\partial{z_f}}=\frac{\partial c}{\partial{z_g}}\frac{\partial {z_g}}{\partial{z_f}}
  $$

  $$
  [1\times d_f]= [1\times d_g]\times[d_g\times d_f]
  $$

  This is the basic formula for $\frac{\partial c}{\partial{z_f}}$ using the chain rule. Note that the gradient of a scalar function with respect to a vector is a vector of the same size as the vector with respect to which you differentiate. In order to make the notations consistent, it is a row vector instead of a column vector.

- Jacobian matrix

  $$
  \left(\frac{\partial{z_g}}{\partial {z_f}}\right)_{ij}=\frac{(\partial {z_g})_i}{(\partial {z_f})_j}
  $$

  We need $\frac{\partial {z_g}}{\partial {z_f}}$ (Jacobian matrix entries) to compute the gradient of the cost function with respect to $z_f$ given gradient of the cost function with respect to $z_g$. Each entry $ij$ is equal to the partial derivative of the $i$th component of the output vector with respect to the $j$th component of the input vector.

  If we have a cascade of modules, we keep multiplying the Jacobian matrices of all the modules going down and we get the gradients w.r.t all the internal variables.


### Backprop through a multi-stage graph

Consider a stack of many modules in a neural network as shown in Figure 10.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure10.png" alt="Figure10" style="zoom:33%;" /></center> |
|         <center>Figure 9: Backprop through multi-stage graph         </center>|

For the backprop algorithm, we need two sets of gradients - one with respect to the states (each module of the network) and one with respect to the weights (all the parameters in a particular module). So we have two Jacobian matrices associated with each module. We can again use chain rule for backprop.

- Using chain rule for vector functions

  $$
  \frac{\partial c}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {z_k}}
  $$

  $$
  \frac{\partial c}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {w_k}}
  $$

- Two Jacobian matrices for the module
    - One with respect to $z[k]$
    - One with respect to $w[k]$


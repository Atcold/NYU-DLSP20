---
lang-ref: ch.02
title: 2주차
lang: ko
translation-date: 30 Mar 2020
translator: Chanseok Kang
---


## 이론 part A

<!-- We start by understanding what parametrised models are and then discuss what a loss function is. We then look at Gradient-based methods and how it's used in the backpropagation algorithm in a traditional neural network. We conclude this section by learning how to implement a neural network in PyTorch followed by a discussion on a more generalized form of backpropagation. -->

우리는 매개변수화된 모델이 무엇인지에 대해서 이해하는 것부터 시작하고 나서, 손실 함수에 대해서 다룰 것이다. 그리고 난 후에 경사-기반의 방법들을 살펴보고, 과거의 신경망안에 포함되어 있는 역전파 알고리즘 내에서 해당 방법이 어떻게 사용되는지를 알아볼  것이다. 그리고 파이토치로 신경망을 구현하는 방법을 학습하고, 역전파 알고리즘의 조금 더 일반화된 형태에 대해 다뤄보는 것으로 이번 단락을 마무리지을 것이다.

## 이론 part B

<!-- We begin with a concrete example of backpropagation and discuss the dimensions of Jacobian matrices. We then look at various basic neural net modules and compute their gradients, followed by a brief discussion on softmax and logsoftmax. The other topic of discussion in this part is Practical Tricks for backpropagation. -->

우리는 역전파 알고리즘에 대한 자세한 예제를 해보고 야코비안 행렬의 차원에 대해서 논의할 것이다. 그 후에 다양한 기본 신경망 모듈을 살펴보면서, 이에 대한 경사를 구한 후, softmax와 logsoftmax에 대해서 간단히 다뤄볼 것이다. 역전파 알고리즘을 실제로 사용할 때 적용할 수 있는 트릭에 대해서도 다뤄보겠다.

## 실습

<!-- We give a brief introduction to supervised learning using artificial neural networks. We expound on the problem formulation and conventions of data used to train these networks. We also discuss how to train a neural network for multi class classification, and how to perform inference once the network is trained. -->

우리는 인공 신경망을 사용한 지도 학습법에 대해서 간략하게 소개할 것이다. 그리고 이런 신경망을 학습하는데 있어 필요한 문제 공식화와 데이터의 형태에 대해서 설명하고자 한다. 또한 다계층 분류에 활용할 수 있는 신경망을 학습하는 방법과 신경망이 학습되었을 때 추론이 발생하는 원리에 대해서 다룰 것이다.

---
lang-ref: ch.14
title: 14 주차
lang: ko
translation-date: 07 Aug 2020
translator: Yujin
---


<!-- ## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net. -->
## 이론 part A

이 섹션에서는 구조화된 예측<sup>structured prediction</sup>을 논의한다. 먼저 에너지 기반 요소 그래프<sup>Energy-Based factor graph</sup>와 이를 위한 효율적인 추론을 소개한다. 다음으로 "얕은<sup>shallow</sup>" 요인을 가지는 간단한 에너지 기반 요소 그래프에 대한 몇 가지 예시를 살펴본다. 마지막으로 그래프 트랜스포머 넷<sup>Graph Transformer Net</sup>에 대해 이야기한다. 


<!-- ## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models. -->
## 이론 part B

강의의 두 번째 부분에서는 그래픽 모델 방법을 에너지 기반 모델에 적용하는 방법에 대해 자세히 이야기한다. 서로 다른 손실 함수를 비교하고, 비터비 알고리즘<sup>Viterbi algorithm</sup>과 포워드 알고리즘<sup>forward algorithm</sup>을 그래픽 트랜스포머 네트워크에 적용하는 것에 대해 논의한다. 이 다음 역전파의 라그랑지안 공식화<sup>Lagrangian formulation</sup>와 에너지 기반 모델에 대한 변이 추론<sup>variational inference</sup>에 대해 논의한다.


<!-- ## Practicum

When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise. -->
## 실습

심층 신경망과 같이 고도로 매개변수화된 모델을 훈련시킬 때에는 훈련 데이터에 대한 과적합 위험이 있다. 이는 커다란 일반화 오류를 초래한다. 과적합을 줄이기 위해서 훈련에 정규화를 도입하여 모델이 노이즈에 대해서도 적합해 버리는 정도를 줄이도록 특정 해법<sup>solution</sup>의 사용을 자제시킬 수 있다. 
---
lang-ref: ch.06
title: 6주차
lang: ko
translation-date: 17 Apr 2020
translator: Yujin
---

<!-- ## Lecture part A -->
## 이론 part A

<!-- We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment. -->

우리는 합성곱 신경망이 활용되는 세 가지 사례에 대해 논의했다. 먼저, 숫자 인식 및 이를 활용한 5자리 우편 번호 인식을 살펴보았다. 객체 탐지<sup>object detection</sup>와 관련해서는 어떻게 멀티 스케일<sup>multi-scale</sup> 구조를 이용해서 얼굴 인식<sup>face detection</sup>을 할 수 있는지에 대해 이야기했다. 마지막으로, 로봇 비전<sup>robot vision</sup> 시스템과 관련한 구체적인 예를 통해 의미론적 분할<sup>semantic segmentation</sup>작업에서 ConvNets가 어떻게 사용되는지 확인하고, 도시 환경 속 객체 분할<sup>object segmentation</sup>에 대해서도 살펴봤다.


<!-- ## Lecture part B -->
## 이론 part B

<!-- We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq. -->

우리는 순환 신경망(RNN)<sup>Recurrent Neural Networks</sup>과, 이것의 문제점, 또 이러한 문제를 완화하기 위해 널리 쓰이는 기술을 알아본다. 그 다음 순환 신경망(RNN) 모델의 문제를 해결하기 위해 개발된 Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), Seq2Seq 와 같은 다양한 모듈을 살펴본다. 



<!-- ## Practicum -->
## 실습
<!-- We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models. -->

우리는 Vanilla RNN 과 LSTM 모델의 구조에 대해 이야기하고 이 둘 사이의 성능을 비교해 보았다. LSTM은 RNN의 장점을 그대로 이어받아 가져오는 동시에 '메모리 셀<sup>memory cell</sup>'을 내부에 포함하여 장기간의 정보를 저장하고, 이를 통해 기존 RNN의 약점을 개선한다. LSTM 모델은 RNN 모델보다 훨씬 좋은 성능을 보인다. 
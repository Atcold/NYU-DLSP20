---
lang-ref: ch.05
title: Week 5
---

## Bölüm A

We begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on convergence.
Gradient Descent'i tanıtarak başlıyoruz. Sezgiyi tartışıyoruz ve aynı zamanda adım boyutlarının çözüme ulaşmada nasıl önemli bir rol oynadığından bahsediyoruz. Ardından, Full Batch GD'ye kıyasla SGD'ye ve performansına geçiyoruz. Son olarak Momentum Güncellemeleri, özellikle iki güncelleme kuralı, momentumun ardındaki sezgi ve yakınsama üzerindeki etkisi hakkında konuşuyoruz.

## Bölüm B

We discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient.
SGD için RMSprop ve ADAM gibi uyarlanabilir yöntemleri tartışıyoruz. Ayrıca normalizasyon katmanları ve bunların sinir ağı eğitim sürecine etkileri hakkında konuşuyoruz. Son olarak, MRI taramalarını daha hızlı ve daha verimli hale getirmek için endüstride kullanılan nöral ağların gerçek dünyadaki bir örneğini tartışıyoruz.

## Practicum

We briefly review the matrix-multiplications and then discuss the convolutions. Key point is we use kernels by stacking and shifting. We first understand the 1D convolution by hand, and then use PyTorch to learn the dimension of kernels and output width in 1D and 2D convolutions examples. Furthermore, we use PyTorch to learn about how automatic gradient works and custom-grads.
Matris çarpmalarını kısaca gözden geçirip kıvrımları tartışıyoruz. Kilit nokta, yığınları istifleyerek ve kaydırarak kullanmamızdır. Önce 1D evrişimini elle anlıyoruz ve daha sonra 1D ve 2D evrişim örneklerinde çekirdeklerin boyutunu ve çıktı genişliğini öğrenmek için PyTorch'u kullanıyoruz. Ayrıca, otomatik gradyanın ve özel gradların nasıl çalıştığını öğrenmek için PyTorch'u kullanıyoruz.

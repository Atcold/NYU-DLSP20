---
lang: tr
lang-ref: tr.12
title: Hafta 12
translation-date: 25 June 2020
translator: Murat Ekici
---


## Ders bölümü A

Bu bölümde, CNN'lerden, RNN'lerden başlayarak, en son teknoloji ürünü mimarileri, dönüştürücüleri ve NLP uygulamalarında kullanılan çeşitli mimarileri tartışacağız. Daha sonra, dönüştürücüleri (transformer) içeren çeşitli modülleri ve bu modüllerin dönüştürücüleri NLP görevleri için nasıl avantajlı hale getirdiklerini tartışacağız. Son olarak, transformatörlerin etkili bir şekilde eğitilmesini sağlayan püf noktaları tartışacağız.

<!--

## Lecture part A

In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively.

-->

## Ders bölümü B

Bu bölümde, açgözlü kod çözme (greedy decoding) ve kapsamlı arama (exhaustive search) arasında bir orta zemin olarak ışın aramasını (beam search) tanıtacağız. Üretken dağılımdan örnekleme yapmak isteyebileceğimiz durumları (örn. metin oluştururken) ve “top-k” örneklemesini tanıtacağız.Daha sonra bir dönüştürücü türü kullanaral diziden (sequence) ve dizi modellerine geçişi ve geri çeviriyi sunacağız. Daha sonra gömmeleri (embeddings) öğrenmek için gözetimsiz öğrenme yaklaşımlarını tanıtıyoruz ve word2vec, GPT ve BERT'i tartışıyoruz.

<!--
## Lecture part B

In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce “top-k” sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT.

-->

## Uygulama
Öz-ilgiye (self-attention) odaklanarak ilgi (attention) ve girdilerin gizli katman gösterimlerini tanıtacağız.Ardından, anahtar-değer deposu paradigmasını tanıtacak ve sorguların, anahtarların ve değerlerin bir girdinin dönüşümleri olarak nasıl temsil edileceğini tartışacağız. Son olarak, dönüştürücü mimarisini yorumlamak, temel bir dönüştürücüden geçişi ve kodlayıcı-kod çözücü paradigmasını sıralı mimarilerle karşılaştırmak için ilgiyi kullanacağız.

<!--
## Practicum

We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures.

-->

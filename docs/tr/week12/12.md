---
lang-ref: ch.12
title: Hafta 12
lang: tr
translation-date: 20 Jul 2020
translator: mevah
---


## Ders Kısım A
<!--
In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively. 
-->
Bu bölümde CNN *(Evrişimli Sinir Ağları)* ve RNN *(Özyineli Sinir Ağları)*'lerden başlayıp şu an kullanılan modelleri de kapsayacak şekilde (NLP *(natural language processing, doğal dil işleme)* uygulamalarında kullanılan değişik modelleri tartışacağız. Daha sonra dönüştürücüleri içine alan değişik modülleri ve onların NLP işlerinde dönüştürücüleri nasıl avantajlı kıldıklarını tartışacağız.

## Ders Kısım B
<!--
In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT.
-->
Bu bölümde açgözlü kodlama *(greedy decoding)* ve kapsamlı arama *(exhaustive search)* arasında bir yerde olan ışın araması *(beam search)*'nı tanıtacağız. Üretici bir dağılımdan örnekleme yapmak isteyeceğimiz bir durumu göz önüne alıp (örn. yazı yaratırken) "ilk-k" *(top-k)* örneklemeyi tanıtacağız.

## Uygulama
<!--
We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures.
-->
Dikkat konseptini, öz-dikkat ve girdinin bu durumdaki saklı katmanlarındaki gösterimlerine odaklanarak tanıtacağız. Daha sonra anahtar-değer *(key-value)* saklama paradigmasını tanıtıp, sorgu, anahtar ve değerleri nasıl girdinin döndürülmüş halleri olarak gösterebileceğimizi tartışacağız. Son olarak, basit bir dönüştürücüden ileri geçiş yaparak dönüştürücü modelini açıklamak için dikkat konseptini kullanacağız ve kodlayıcı-çözücü paradigmasını dizili modellerle karşılaştıracağız.
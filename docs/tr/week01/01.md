---
lang: tr
lang-ref: ch.01
title: Hafta 1
translation-date: 12 June 2020
translator: Murat Ekici
---


## Ders bölümü A

Derin öğrenmenin (Deep Learning, DL) arkasındaki motivasyonu tartışacağız. Derin öğrenmenin tarihi ve ilhamı ile başlıyoruz. Sonra, örüntü tanımanın (_Pattern Recognition_) tarihini tartışacak ve gradyan inişini (_Gradient Descent_) ve geri yayılım (_Backpropagation_)ile hesaplanmasını tanıtacağız. Son olarak, görsel korteksin hiyerarşik temsilini tartışacağız.

<!--

## Lecture part A

We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.
-->

## Ders bölümü B

İlk önce Fukushima'dan LeCun'a ve AlexNet'e kadar Evrişimsel Sinir Ağlarının (CNN) evrimini tartışacağız. Daha sonra CNN'lerin görüntü segmentasyonu, otonom araçlar ve tıbbi görüntü analizi gibi bazı uygulamalarını tartışacağız. Derin ağların hiyerarşik doğasını ve bu ağları avantajlı kılan özelliklerini tartışıyoruz. Özniteliklerin / temsillerin (_features_) üretilmesi ve öğrenilmesi üzerine bir tartışma ile sonuçlandıracağız.


<!--
## Lecture part B

We first discuss the evolution of CNNs, from Fukushima to LeCun to AlexNet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.
-->

## Uygulama

Uzayda görselleştirilmiş veri noktalarına dönüşümlerin uygulanmasına ilişkin motivasyonu tartışacağız. Lineer Cebir, doğrusal ve doğrusal olmayan dönüşümlerin uygulanması hakkında konuşacağız. Bu dönüşümlerin işlevini ve etkilerini anlamak için görselleştirmenin kullanımını tartışacağız. Bir Jupyter Notebook'ta yer alan örnekleri takip edecek ve sinir ağları tarafından temsil edilen işlevlerin bir tartışmasıyla sonuçlandıracağız.

<!--
## Practicum

We discuss the motivation for applying transformations to data points visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to understand the function and effects of these transformations. We walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by neural networks.
-->

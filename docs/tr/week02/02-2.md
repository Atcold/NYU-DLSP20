---
lang-ref: ch.02-2
lang: tr
lecturer: Yann LeCun
title: NN modüllerinde  Gradyan Hesaplama ve Geri Yayılım için Kısa Yollar
authors:  Micaela Flores, Sheetal Laad, Brina Seidel, Aishwarya Rajan
date: 3 Feb 2020
translation-date: 15 Nov 2020
translator: Melvin Selim Atay
---
## [Geriyayılımın somut örneği ve  basit yapay sinir ağlarına giriş](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2989s)

### Örnek
Görsel bir grafik yardımıyla somutlaştırılmış geri yayılım örneğini inceliyoruz. $C$ maliyet fonksiyonuna bir girdi olarak bir $G(w)$ fonksiyonu olsun, bu fonksiyon bir grafik şeklinde gösterilebilir. Jacobian matrislerinin çarpımlarını manipüle ederek bu grafiği, ters dönerek gradyan hesaplayacak olan grafiğe dönüştürebiliriz. (PyTorch ve TensorFlow bunu otomatik olarak yapabilmektedir, ileri grafik otomatik olarak ters çevirilir ve geri yayılım gradyanı olan türevlenmiş grafik ortaya çıkar.)

 <center><img src="{{site.baseurl}}/images/week02/02-2/02-2-1.png" alt="Gradient diagram" style="zoom:40%;" /></center>

 Bu örnekte, sağdaki yeşil grafik gradyan grafiğidir Yanındaki grafik de en tepedeki boğumdan bunu takip eder

 $$
 \frac{\partial C(y,\bar{y})}{\partial w}=1 \cdot \frac{\partial C(y,\bar{y})}{\partial\bar{y}}\cdot\frac{\partial G(x,w)}{\partial w}
 $$

 Boyut olarak ele alınırsa $\frac{\partial C(y,\bar{y})}{\partial w}$ satır vektörüdür ve büyüklüğü $1\times N$ where $N$ is the number of components of $w$; $\frac{\partial C(y,\bar{y})}{\partial \bar{y}}$  ise sütun vektörüdür ve büyüklüğü $1\times M$, where $M$ çıktı boyutudur; $\frac{\partial \bar{y}}{\partial w}=\frac{\partial G(x,w)}{\partial w}$ matris boyutu ıse $M\times N$, where $M$ is the number of outputs of $G$ and $N$ is the dimension of $w$ 'olur'.

 Grafiğin mimarisi sabit değil veriye bağlı ise çeşitli karışıklıklar oluşabilir. Örneğin yapay sinir ağı modlülünü girdi vektörü uzunluuna göre seçebiliriz. Bu mümkün olsa da bu değişimi yönetmek döngü sayısı arttıkça fazlasıyla zorlaşır.

### Basit yapay sinir ağı modülleri
Tanıdık Lineer ve ReLU modulleri dışında önceden inşa edilmiş pek çok farklı yapay sinir ağı modülü vardır. Bu modüller ilgili fonksiyonları yapmak için özgün bir şekilde optimizedirler ve bu yönleriyle kullanışlıdırlar. (birbirinin kombinasyonu gibi üretilmiş, orta düzey modüllere zıt olarak).

- Lineer: $Y=W\cdot X$

  $$
  \begin{aligned}
  \frac{dC}{dX} &= W^\top \cdot \frac{dC}{dY} \\
  \frac{dC}{dW} &= \frac{dC}{dY} \cdot X^\top
  \end{aligned}
  $$

- ReLU: $y=(x)^+$

  $$
  \frac{dC}{dX} =
      \begin{cases}
        0 & x<0\\
        \frac{dC}{dY} & \text{otherwise}
      \end{cases}
  $$

- Eşleme: $Y_1=X$, $Y_2=X$

  - "Y - ayırıcısına" benzer şekilde ve her çıktı birbirine eşit. .

  - Geri yayılım yaparken gradyanlar toplanır

  - Birbirine benzer şekilde $n$ tane dallanırlar

    $$
    \frac{dC}{dX}=\frac{dC}{dY_1}+\frac{dC}{dY_2}
    $$


- Toplama: $Y=X_1+X_2$

  - Ikı değişken toplandığında bir değişken perturbe ise çıktı da aynı değerde perturbedir örneğin:

    $$
    \frac{dC}{dX_1}=\frac{dC}{dY}\cdot1 \quad \text{and}\quad \frac{dC}{dX_2}=\frac{dC}{dY}\cdot1
    $$


- Maks: $Y=\max(X_1,X_2)$

  -  Bu fonksiyon şu şeklide de ifade edilir

    $$
    Y=\max(X_1,X_2)=\begin{cases}
          X_1 & X_1 > X_2 \\
          X_2 & \text{else}
       \end{cases}
    \Rightarrow
    \frac{dY}{dX_1}=\begin{cases}
          1 & X_1 > X_2 \\
          0 & \text{else}
       \end{cases}
    $$

  - Sonuç olarak zincir kuralıyla,

    $$
    \frac{dC}{dX_1}=\begin{cases}
          \frac{dC}{dY}\cdot1 & X_1 > X_2 \\
          0 & \text{else}
      \end{cases}
    $$

## [LogSoftMax *vs.* SoftMax](https://www.youtube.com/watch?v=d9vdh3b787Y&t=3953s)

Bir PyTorch modülü olan *SoftMax*, toplamı bir olan,  $0$ ve $1$ arasındaki  olan pozitif sayıları dönüştürmenin kolay bir yoludur. Bu sayılar olasılık dağılımı olarak anlaşılabilir. Sonuç olarak yaygın sınıflama sorunlarında...  

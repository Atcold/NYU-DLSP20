---
lang-ref: ch.02-3
lang: tr
lecturer: Alfredo Canziani
title: Yapay Sinir Ağları (ANNs)
authors:  
date: 4 Feb 2020
typora-root-url: 02-3
translation-date: 15 Nov 2020
translator: Melvin Selim Atay
---

## [Sınıflama için denetimli öğrenme](https://www.youtube.com/watch?v=WAn6lip5oWk&t=150s)
Aşağıdaki **Fig. 1(a)**yı değerlendirelim. Grafikteki noktalar spiraldeki kollar gibi uzanır ve $\R^2$ içinde yaşar. Her renk sınıf etiketini verir. Özgün sınıf sayısı $K = 3$. Matematiksel olarak **Eqn. 1(a)** da anlatılmıştır.

<center>
  <table border="0">
    <td>
      <center>
    <img src="{{site.baseurl}}/images/week02/02-3/clean-spiral.png" width="350px" /><br>
       <b>Fig. 1(a)</b> "Clean" 2D spiral
       </center>
      </td>
      <td>
      <center>
      <img src="{{site.baseurl}}/images/week02/02-3/noisy-spiral.png" width="350px" /><br>
       <b>Fig. 1(b)</b> "Noisy" 2D spiral
       </center>
      </td>
  </table>
  </center>


$$
X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1)\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1)\right]}\end{array}\right) \\
0 \leq t \leq 1, \quad k=1, ..., K
$$

  <center><b>Eqn. 1(a)</b> </center>

$$
  X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]}\end{array}\right)\\0 \leq t \leq 1, \quad k=1, ..., K
$$

<center><b>Eqn. 1(b)</b></center>

**Sınıflama yapmak** ne demektir? **Lojistik regresyonu** düşünelim. Sınıflama için bu veriye lojistik regresyon uygulanmış olsaydı, veriyi sınıflarına ayırmak için bir grup **doğrusal düzlem** (karar sınırları) oluşturacaktır. Bu çözümdeki sorun her bölgede her sınıftan noktalar olmasıdır. Spiralin kolları doğrusal karar verme sınırlarını aşar. Bu iyi bir çözüm **değildir!**

**Bu nasıl düzeltilir?** Girdi alanını veri doğrusal bir şekilde ayrışabiliyormuş gibi zorlayarak dönüştürürüz. Eğitim sonunda sinir ağı karar sınırlarını eğitim verisinin dağıtımına uyduracak şekilde karar sınırlarını öğrenerek yapar


## Eğitim verisi

Geçen hafta, yeni başlatılmış yapay sinir ağlarının girdilerini gelişigüzel dönüştürdüğünü gördük. Bu dönüşüm hesaplamayı elle yaptığı için aslında özünde yardımcı değildir. Girdi dönüşümlerinin el lle hesaplanan göreve göre anlamlı bir şekilde yapmaya zorlayacağız ve bunları veriyi kullanarak keşfedeceğiz.

* $\vect{X}$ girdi verisi, $m$  (eğitilen veri noktası sayıları) x $n$ (her girdi noktasının boyutu) matris boyutlarında. In case of the data shown in Figures **1(a)** and **1(b)**, $n = 2$.

<center>
<img src="{{site.baseurl}}/images/week02/02-3/training-data.png" width="600px" /><br>
<b>Fig. 2</b> Training data
</center>

* $\vect{c}$ vektörü ve $\boldsymbol{Y}$ matrisi her ikisi de $m$ veri noktalarına karşılık gelen sınıf etiketlerini ifade eder. Yukarıdaki örnekte  $3$ farklı sınıf vardır.

  * $c_i \in \lbrace 1, 2, \cdots, K \rbrace$, and $\vect{c} \in \R^m$. However, we may not use $\vect{c}$ as training data. If we use distinct numeric class labels  $c_i \in \lbrace 1, 2, \cdots, K \rbrace$, the network may infer an order within the classes that isn't representative of the data distribution.
  * To bypass this issue, we use a **one-hot encoding**. For each label $c_i$, a $K$ dimensional zero-vector $\vect{y}^{(i)}$ is created, which has the $c_i$-th element set to $1$ (see **Fig. 3** below).

<center>
<img src="{{site.baseurl}}/images/week02/02-3/one-hot.png" width="250px" /><br>
<b>Fig. 3</b> One hot encoding
</center>
* Therefore, $\boldsymbol Y \in \R^{m \times K}$. This matrix can also be thought of as having some probabilistic mass, which is fully concentrated on one of the $K$ spots.




## Tamamen bağlı katmanlar



## Sinir ağı (çıkarım)



## [Sinir ağı (eğitim I)](https://www.youtube.com/watch?v=WAn6lip5oWk&t=822s)



## [Sinir ağı (eğitim II)](https://www.youtube.com/watch?v=WAn6lip5oWk&t=2188s)



## Spiral Sınıflama - Jupyter defteri

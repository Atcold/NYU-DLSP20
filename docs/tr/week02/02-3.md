---
lang-ref: ch.02-3
lang: tr
lecturer: Alfredo Canziani
title: Yapay Sinir Ağları (ANNs)
authors:  
date: 4 Feb 2020
typora-root-url: 02-3
translation-date: 15 Nov 2020
translator: Melvin Selim Atay
---

## [Sınıflama için denetimli öğrenme](https://www.youtube.com/watch?v=WAn6lip5oWk&t=150s)
Aşağıdaki **Fig. 1(a)**yı değerlendirelim. Grafikteki noktalar spiraldeki kollar gibi uzanır ve $\R^2$ içinde yaşar. Her renk sınıf etiketini verir. Özgün sınıf sayısı $K = 3$. Matematiksel olarak **Eqn. 1(a)** da anlatılmıştır.

<center>
  <table border="0">
    <td>
      <center>
    <img src="{{site.baseurl}}/images/week02/02-3/clean-spiral.png" width="350px" /><br>
       <b>Fig. 1(a)</b> "Clean" 2D spiral
       </center>
      </td>
      <td>
      <center>
      <img src="{{site.baseurl}}/images/week02/02-3/noisy-spiral.png" width="350px" /><br>
       <b>Fig. 1(b)</b> "Noisy" 2D spiral
       </center>
      </td>
  </table>
  </center>


$$
X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1)\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1)\right]}\end{array}\right) \\
0 \leq t \leq 1, \quad k=1, ..., K
$$

  <center><b>Eqn. 1(a)</b> </center>

$$
  X_{k}(t)=t\left(\begin{array}{c}{\sin \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]} \\ {\cos \left[\frac{2 \pi}{K}(2 t+k-1 +\mathcal{N}\left(0, \sigma^{2}\right))\right]}\end{array}\right)\\0 \leq t \leq 1, \quad k=1, ..., K
$$

<center><b>Eqn. 1(b)</b></center>

**Sınıflama yapmak** ne demektir? **Lojistik regresyonu** düşünelim. Sınıflama için bu veriye lojistik regresyon uygulanmış olsaydı, veriyi sınıflarına ayırmak için bir grup **doğrusal düzlem** (karar sınırları) oluşturacaktır. Bu çözümdeki sorun her bölgede her sınıftan noktalar olmasıdır. Spiralin kolları doğrusal karar verme sınırlarını aşar. Bu iyi bir çözüm **değildir!**

**Bu nasıl düzeltilir?** Girdi alanını veri doğrusal bir şekilde ayrışabiliyormuş gibi zorlayarak dönüştürürüz. Eğitim sonunda sinir ağı karar sınırlarını eğitim verisinin dağıtımına uyduracak şekilde karar sınırlarını öğrenerek yapar


## Eğitim verisi

Geçen hafta, yeni başlatılmış yapay sinir ağlarının girdilerini gelişigüzel dönüştürdüğünü gördük. Bu dönüşüm hesaplamayı elle yaptığı için aslında özünde yardımcı değildir. Girdi dönüşümlerinin el lle hesaplanan göreve göre anlamlı bir şekilde yapmaya zorlayacağız ve bunları veriyi kullanarak keşfedeceğiz.

* $\vect{X}$ girdi verisi, $m$  (eğitilen veri noktası sayıları) x $n$ (her girdi noktasının boyutu) matris boyutlarında. Figür **1(a)**  ve **1(b)** de görünen verilere göre, $n = 2$.

<center>
<img src="{{site.baseurl}}/images/week02/02-3/training-data.png" width="600px" /><br>
<b>Fig. 2</b> Training data
</center>

* $\vect{c}$ vektörü ve $\boldsymbol{Y}$ matrisi her ikisi de $m$ veri noktalarına karşılık gelen sınıf etiketlerini ifade eder. Yukarıdaki örnekte  $3$ farklı sınıf vardır.

  * $c_i \in \lbrace 1, 2, \cdots, K \rbrace$,ve $\vect{c} \in \R^m$.  $\vect{c}$ eğitim verisi olarak kullanılamayabilir.  Eğer farklı sayısal sınıf etiketleri $c_i \in \lbrace 1, 2, \cdots, K \rbrace$, kullanılırsa, ağ veri dağılımına uygun olmayan şekilde sıralamalı tahmin edebilir.
  * Bu problemin üstesinden gelmek için **ikili karakter gizleme**. Her $c_i$ verisi için, $K$ boyutlu bir sıfır vektörü  $\vect{y}^{(i)}$ yapılır, bunun  $c_i$-ıncı elemanı $1$e eşitlenir ( **Fig. 3** ).

<center>
<img src="{{site.baseurl}}/images/week02/02-3/one-hot.png" width="250px" /><br>
<b>Fig. 3</b> One hot encoding
</center>
*  $\boldsymbol Y \in \R^{m \times K}$. Bu matris olasılıksal ağırlığa sahiptir ve bu ağırlıklar  $K$ noktalarındadır.



## Tamamen bağlı (Fully Connected-FC) katmanlar

Tamamen bağlı (FC) ağları ve nasıl çalıştıklarını inceleyeceğiz.

<center>
<img src="{{site.baseurl}}/images/week02/02-3/FC-net.png" height="250px" /><br>
<b>Fig. 4</b> Fully connected neural network
</center>

Yukarıdaki **Fig. 4**'de girdi verisi, $\boldsymbol x$, $\boldsymbol W_h$, tarafından tanımlanan afin dönüşümü yapar ve liner olmayan diğer dönüşümler bunu takip eder. Bu non-lineer dönüşümler sonucunda  $\boldsymbol h$,  **saklı** çıktıyı gösterir, *yani* ağ dışından **görünmeyen** çıktı.
Bu tekrar bir başka afin dönüşüm ($\boldsymbol W_y$), ve takip eden dönüşüme uğrar, sonundaki nihai çıktı: $\boldsymbol{\hat{y}}$. Bu ağ matematiksel olarak  **Eqn. 2** de ifade edilmiştir. $f$ ve $g$ her ikiside lineer olmayan kısımlardır.

$$
\begin{aligned}
&\boldsymbol h=f\left(\boldsymbol{W}_{h} \boldsymbol x+ \boldsymbol b_{h}\right)\\
&\boldsymbol{\hat{y}}=g\left(\boldsymbol{W}_{y} \boldsymbol h+ \boldsymbol b_{y}\right)
\end{aligned}
$$

<center><b>Eqn. 2</b> Mathematics behind a FC network</center>
Yukarıda gösterildiği gibi bir basit sinir ağı birbirini takip eden çiftlerden oluşur, her çift afin dönüşüm ve lineer olmayan operasyon (sıkıştırma) içerir. ReLU, sigmoid, hiperbolik tanjant ve softmax sıklıkla kullanılan lineer olmayan operasyonlardır.

Yukarıdaki ağ üç katmanlıdır:

1. girdi nöronu
2. saklı noron
3. çıktı nöronu

$3$ katmanlı ağ $2$ afin dönüşüm içerir. Bu $n$ katmanlı bir ağ olacak kadar genişletilebilir.

Şimdi daha karmaşık bir durumu inceleyelim.

3 saklı katmanlı, her katmanı tamamen bağlı bir ağ olsun, şekli **Fig. 5** de görülmektedir.

<center>
<img src="{{site.baseurl}}/images/week02/02-3/pre-inference4layers.png" /><br>
<b>Fig. 5</b> Neural net with 3 hidden layers
</center>
İkinci katmandaki nöron $j$'yi düşünelim ve bunun aktivasyon fonksiyonu:

$$
a^{(2)}_j = f(\boldsymbol w^{(j)} \boldsymbol x + b_j) = f\Big( \big(\sum_{i=1}^n w_i^{(j)} x_i\big) +b_j ) \Big)
$$

olur. $\vect{w}^{(j)}$, $\vect{W}^{(1)}$ vektrörünün  $j$. denklemidir.

Girdi katmanının aktivasyonu yalnızca kendisidir. Saklı katmanlarda ReLU, tanjant hiperbolik, sigmoid, soft(arg)max gibi fonksiyonlar olabilir.

[Buradaki](https://piazza.com/class/k5spqaanqk51ks?cid=36) Piazza duyurusunda açıklandığı gibi, son katmandaki aktivasyon kullanım durumuna bağlıdır.

## Sinir ağı (çıkarım)



## [Sinir ağı (eğitim I)](https://www.youtube.com/watch?v=WAn6lip5oWk&t=822s)



## [Sinir ağı (eğitim II)](https://www.youtube.com/watch?v=WAn6lip5oWk&t=2188s)



## Spiral Sınıflama - Jupyter defteri

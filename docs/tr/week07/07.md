---
lang: tr
lang-ref: ch.07
title: Hafta 7
translation-date: 12 June 2020
translator: Murat Ekici
---


## Ders bÃ¶lÃ¼mÃ¼ A

Bu bÃ¶lÃ¼mde Enerji TabanlÄ± Modelleri (Energy Based Model, EBM) ve ileri beslemeli aÄŸlar dÄ±ÅŸÄ±nda farklÄ± yaklaÅŸÄ±mlarÄ± ele alÄ±yoruz.

<!--

## Lecture part A

We introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions.
-->

## Ders bÃ¶lÃ¼mÃ¼ B

Bu kÄ±sÄ±mda ise, Ã¶z-denetimli Ã¶ÄŸrenme (self-supervised learning), EBM'lerin nasÄ±l eÄŸitileceÄŸini ve Gizli DeÄŸiÅŸken (Latent Variable) EBM'yi Ã¶zellikle bir K-ortalamalÄ± Ã¶rnekle aÃ§Ä±klÄ±yoruz.AyrÄ±ca KarÅŸÄ±tsal YÃ¶ntemleri (Contrastive Methods), topografik haritalarÄ± ve BERT'i tanÄ±tÄ±yoruz. Son olarak, bir topografik harita kullanarak da aÃ§Ä±klanan KarÅŸÄ±t Ã‡eÅŸitlilik (Contrastive Divergence) kavramÄ±nÄ± aÃ§Ä±kloyoruz.

<!--
## Lecture part B

We discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map.
-->

## Uygulama

OtokodlayÄ±cÄ±larÄ±n bazÄ± uygulamalarÄ±nÄ± ve neden kullanÄ±ldÄ±ÄŸÄ±nÄ± ele alÄ±yoruz. ArdÄ±ndan, otokodlayÄ±cÄ±larÄ±n farklÄ± mimarileri, aÅŸÄ±rÄ± Ã¶ÄŸrenme sorunlarÄ±ndan nasÄ±l kaÃ§Ä±nÄ±lacaÄŸÄ± ve kullanmamÄ±z gereken kayÄ±p fonksiyonlarÄ± hakkÄ±nda konuÅŸacaÄŸÄ±z. Son olarak, standart bir otokodlayÄ±cÄ± ve gÃ¼rÃ¼ltÃ¼ azaltÄ±cÄ± bir otokodlayÄ±cÄ± uygulamasÄ± yapacaÄŸÄ±z.

<!--
## Practicum

We discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder.
-->

ğŸ‡¹ğŸ‡· muratekici

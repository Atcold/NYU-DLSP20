---
lang-ref: ch.08
title: Hafta 8
lang: tr
translation-date: 14 Jun 2020
translator: emirceyani
---

## Ders Kısım A

<!-- ## Lecture part A -->

Bu bölümde, karşıtsal yöntemlerin enerji bazlı modellerdeki(EBM) kullanımına çeşitli açılardan odaklandık.
Önce, karşıtsal yöntemlerin özdenetimli öğrenmede kullanımının getirdiği avantaj üzerine konuşacağız.
Sonra, arıtan otokodlayıcıların mimarisini ve bu yapıların bir görüntüyü yeniden oluşturkenki zayıflıklarını ele aldık. Ayrıca, karşıtsal ıraksama(contrastive divergence) ve kalıcı karşıtsal ıraksama(persistent contrastive divergence) gibi diğer karşıtsal yöntemlerden de bahsettik.

<!-- In this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence. -->


## Ders Kısım B

<!-- ## Lecture part B -->

Bu bölümde, düzenlileştirilmiş saklı değişken EBM'le ve bu modellerin koşullu ve koşulsuz versiyonlarını detaylıca inceledik.
Daha sonra, ISTA, FISTA ve LISTA algoritmalarını tartıştık ve seyrek kodlama örnekleri ile evrişimsel seyrek otokodlayıcılardan öğrenilen filtreleri birlikte inceledik.
Son olarak, değişimsel otokodlayıcılar ve bunun altında yatan temel kavramları ele aldık.

<!-- In this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved. -->

## Uygulama

<!-- ## Practicum -->

Bu bölümde, üretici modellerin özel bir türü olan değişimsel otokodlayıcılardan bahsettik ve bu modelin özellikleri ile avantajlarını klasik otokodlayıcılar ile kıyasladık.
VAE'nin amaç fonksiyonunu, saklı uzayda nasıl bir yapı oluşturmaya zorladığını anlayarak, detaylica inceledik.
Son olarak, en baştan kodladığımız VAE modelini MNIST veri kümesinde eğittik ve yeni örnekler üretmek için kullandık.


<!-- In this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples. -->

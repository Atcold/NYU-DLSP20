---
lang-ref: ch.08-1
title: Enerjı Bazlı Modellerde Karşıtsal Yöntemler
authors: Vishwaesh Rajiv, Wenjun Qu, Xulai Jiang, Shuya Zhao
date: 23 Mar 2020
traslation-date: 14 June 2020
translator : emirceyani
---


## [Tekrar](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=5s)

Dr. LeCun dersin ilk 15 dakikasında enerji bazli modelleri tekrar etmişti.
Bu konu icin lütfen bir önceki haftayı(7. haftanin notlari), özellikle karşıtsal öğrenme yöntemlerini,  gözden geçirinizç

Önceki dersten öğrendiğimiz üzere, öğrenme yöntemleri iki ana sınıfa ayrılmaktadır:
1. Karşıtsal Yontemler that push down the energy of training data points, $F(x_i, y_i)$, while pushing up energy on everywhere else, $F(x_i, y’)$.
2. Yapisal Yontemler that build energy function F which has minimized/limited low energy regions by applying regularization.

To distinguish the characteristics of different training methods, Dr. Yann LeCun has further summarized 7 strategies of training from the two classes mention before. One of which is methods that are similar to Maximum Likelihood method, which push down the energy of data points and push up everywhere else.

Maximum Likelihood method probabilistically pushes down energies at training data points and pushes everywhere else for every other value of $y’\neq y_i$. Maximum Likelihood doesn't “care” about the absolute values of energies but only “cares” about the difference between energy. Because the probability distribution is always normalized to sum/integrate to 1, comparing the ratio between any two given data points is more useful than simply comparing absolute values.


## [Özdenetimli öğrenmede Karşıtsal Yontemler ](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=613s)

Karşıtsal yöntemlerde eğitim kümesinden gözlemiş olduğumuz örneklerin ($x_i$, $y_i$) enerjisini baskılarken, eğitim kümesinin manifoldu dışında kalan örneklerin enerjisini artırıyoruz.

Özdenetimli öğrenmedeö girdinin bir kısmını kulllanarak diğer kısımlarını tahmin etmeye çalışırız. Bu öğrenme modelinden beklentimiz ise modelimizin bilgisdayarlı görü  We hope that our model can produce good features for computer vision that rival those from supervised tasks.

Researchers have found empirically that applying contrastive _embedding_ methods to self-supervised learning models can indeed have good performances which rival that of supervised models. We will explore some of these methods and their results below.


### Karsitsal Gomulme

Consider a pair ($x$, $y$), such that $x$ is an image and $y$ is a transformation of $x$ that preserves its content (rotation, magnification, cropping, *etc.*). We call this a **positive** pair.

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig1.png" width="50%"/><br>
<b>Fig. 1</b>: Pozitif Çift
</center>

Conceptually, contrastive embedding methods take a convolutional network, and feed $x$ and $y$ through this network to obtain two feature vectors: $h$ and $h'$. Because $x$ and $y$ have the same content (*i.e.* a positive pair), we want their feature vectors to be as similar as possible. As a result, we choose a similarity metric (such as cosine similarity) and a loss function that maximizes the similarity between $h$ and $h'$. By doing this, we lower the energy for images on the training data manifold.

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig2.png" width="50%"/><br>
<b>Fig. 2</b>: Negatif Çift
</center>

However, we also have to push up on the energy of points outside this manifold. So we also generate **negative** samples ($x_{\text{neg}}$, $y_{\text{neg}}$), images with different content (different class labels, for example). We feed these to our network above, obtain feature vectors $h$ and $h'$, and now try to minimize the similarity between them.

This method allows us to push down on the energy of similar pairs while pushing up on the energy of dissimilar pairs.

Recent results (on ImageNet) have shown that this method can produce features that are good for object recognition that can rival the features learned through supervised methods.


### Ozdenetimli öğrenilmis Sonuclar (MoCo, PIRL, SimCLR)

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig3.png" height="75%" width="75%"/><br>
<b>Fig. 3</b>: ImageNet üzerinde PIRL ve MoCo
</center>

Yukaridaki sekilde de goruldugu uzere, MoCo and PIRL SOTA sonuclara ulasmistir (ozellikle az sayida parametreli dusuk kapasiteli modellerde). PIRl is starting to approach the top-1 linear accuracy of supervised baselines (~75%).

PIRL yontemini amac fonksiyonu asagidaki gibi olan NCE(Noise Contrastive Estimator) u inceleyerek daha iyi anlayabiliriz.

$$
h(v_I,v_{I^t})=\frac{\exp\big[\frac{1}{\tau}s(v_I,v_{I^t})\big]}{\exp\big[\frac{1}{\tau}s(v_I,v_{I^t})\big]+\sum_{I'\in D_{N}}\exp\big[\frac{1}{\tau}s(v_{I^t},v_{I'})\big]}
$$

$$
L_{\text{NCE}}(I,I^t)=-\log\Big[h\Big(f(v_I),g(v_{I^t})\Big)\Big]-\sum_{I'\in D_N}\log\Big[1-h\Big(g(v_{I^t}),f(v_{I'})\Big)\Big]
$$

Here we define the similarity metric between two feature maps/vectors as the cosine similarity.

What PIRL does differently is that it doesn't use the direct output of the convolutional feature extractor. It instead defines different _heads_ $f$ and $g$, which can be thought of as independent layers on top of the base convolutional feature extractor.

Putting everything together, PIRL's NCE objective function works as follows. In a mini-batch, we will have one positive (similar) pair and many negative (dissimilar) pairs. We then compute the similarity between the transformed image's feature vector ($I^t$) and the rest of the feature vectors in the minibatch (one positive, the rest negative). We then compute the score of a softmax-like function on the positive pair. Maximizing a softmax score means minimizing the rest of the scores, which is exactly what we want for an energy-based model. The final loss function, therefore, allows us to build a model that pushes the energy down on similar pairs while pushing it up on dissimilar pairs.

Dr. LeCun mentions that to make this work, it requires a large number of negative samples. In SGD, it can be difficult to consistently maintain a large number of these negative samples from mini-batches. Therefore, PIRL also uses a cached memory bank.

**Question**: Why do we use cosine similarity instead of L2 Norm?
Answer: With an L2 norm, it's very easy to make two vectors similar by making them "short" (close to centre) or make two vectors dissimilar by making them very "long" (away from the centre). This is because the L2 norm is just a sum of squared partial differences between the vectors. Thus, using cosine similarity forces the system to find a good solution without "cheating" by making vectors short or long.


### SimCLR

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig5.png" height="75%" width="75%"/><br>
<b>Fig. 4</b>: SimCLR Results on ImageNet
</center>

SimCLR diger metodlardan daha iyi sonuclar gostermektedir. Hatta, denetimli metodlarin Imagenet en iyi -1 lineer doğruluktaki performansina erismistir. 
SimCLR The technique uses a sophisticated data augmentation method to generate similar pairs, and they train for a massive amount of time (with very, very large batch sizes) on TPUs. 
Dr. LeCun, SimCLR'in bir ölçüde karşıtsal yontemlerin sinir cizgisi olduguna inanmakta. 
There are many, many regions in a high-dimensional space where you need to push up the energy to make sure it's actually higher than on the data manifold. 
Gosterimlerin boyutlari arttikca, daha fazla negatif ornege ihtiyac duyulmaktadir ki enerjinin manifold uzerinde olmayan yerlerde daha yuksek olmasi sarti saglanabilsin.  


## [Arıtan otokodlayıcı](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=1384s)

[7. haftanin uygulamasinda ](https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-3/), arıtan otokodlayıcıdan bashetmistik.
Bu yapi, veri gostermini The model tends to learn the representation of the data by reconstructing corrupted input to the original input.
Detaylica bahsedersek, we train the system to produce an energy function that grows quadratically as the corrupted data move away from the data manifold.

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig6.png" height="75%" width="75%"/><br>
<b>Fig. 5</b>: Architecture of denoising autoencoder
</center>


### Issues

Ancak arıtan otokodlayıcılar ile ilgili çeşitli sorunlar bulunmaktadır. 
Öncelikle, cok boyutlu ve sürekli bir uzayda, veriyi bozmanın sonsuz sayıda yolu vardır. Bu yüzden enerji fonksiyonunu bir sürü değişik bölgede yukarı itmenin de garantisi yokturç
Bir diger problem ise saklı degişkenlerin noksanlığından ötürü arıtan otokodlayicilar göruntulerde kötu performans sergilemektedir.
Bir resmi yeniden oluşturmanın sayısız yöntemi olduğundan, sistem çeşitli tahminler üretir ve bilhassa iyi öznitelikler öğrenemez. 
Ayrıca, corrupted points in the middle of the manifold could be reconstructed to both sides. This will create flat spots in the energy function and affect the overall performance.


## Diger Karşıtsal Yontemler 

There are other contrastive methods such as contrastive divergence, Ratio Matching, Noise Contrastive Estimation, and Minimum Probability Flow. We will briefly discuss the basic idea of contrastive divergence.


### Karşıtsal Iraksama(contrastive divergence)

Contrastive divergence is another model that learns the representation by smartly corrupting the input sample.
In a continuous space, we first pick a training sample $y$ and lower its energy. 
For that sample, we use some sort of gradient-based process to move down on the energy surface with noise. If the input space is discrete, we can instead perturb the training sample randomly to modify the energy. If the energy we get is lower, we keep it. Otherwise, we discard it with some probability. Keep doing so will eventually lower the energy of $y$. We can then update the parameter of our energy function by comparing $y$ and the contrasted sample $\bar y$ with some loss function.


### Inatci Karşıtsal Iraksama(persistent contrastive divergence)

One of the refinements of contrastive divergence is persistent contrastive divergence. The system uses a bunch of "particles" and remembers their positions. These particles are moved down on the energy surface just like what we did in the regular CD. Eventually, they will find low energy places in our energy surface and will cause them to be pushed up. However, the system does not scale well as the dimensionality increases.

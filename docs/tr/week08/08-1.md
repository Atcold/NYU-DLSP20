---
lang-ref: ch.08-1
title: Enerjı Bazlı Modellerde Karşıtsal Yöntemler
authors: Vishwaesh Rajiv, Wenjun Qu, Xulai Jiang, Shuya Zhao
date: 23 Mar 2020
traslation-date: 17 June 2020
translator : emirceyani
---


## [Tekrar](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=5s)

Dr. LeCun dersin ilk 15 dakikasında enerji bazli modelleri tekrar etmişti.
Bu konu icin lütfen bir önceki haftayı, özellikle karşıtsal öğrenme yöntemlerini, gözden geçiriniz.

Önceki dersten öğrendiğimiz üzere, öğrenme yöntemleri iki ana sınıfa ayrılmaktadır:
1. Eğitim kümesindeki örneklerin enerjisini F(x_i, y_i)$ aşağı çekerken geri kalan her yerdeki enerjiyi yukarı iten $F(x_i, y’)$ Karşıtsal Yontemler.
2. Düzenlileştirme yardımı ile sınırlı sayıda düşük enerji bölgelerine sahip bir enerji fonksiyonu F modelleyen Yapısal Yöntemler.

Farklı eğitim yöntemlerinin özelllklerini ayırt edebilmek için, Dr. Yann LeCun has further summarized 7 strategies of training from the two classes mention before. Bu yöntemlerden biri veri noktalarının enerjisini aşağı çeken ve geri kalan tüm noktaların enerjisini iten En Büyük Olabilirlik(Maximum Likelihood) yöntemine benzer yöntemlerdir.

Maximum Likelihood yöntemi olasılıksal bir şekilde veri noktalarının enerjisini aşağı çeken ve başka her yere iten En Büyük Olabilirlik(Maximum Likelihood) yöntemine benzer yöntemlerdir. method probabilistically pushes down energies at training data points and pushes everywhere else for every other value of $y’\neq y_i$. Maximum Likelihood yöntemi enerjilerin  "mutlak değerleri"nden ziyade "aralarındaki farkı" dikkate alır. Bunun sebebi ise olasılık dağılımlarının toplamının/integralinin her zaman 1 etmesi için normalize edilmesidir    Because the probability distribution is always normalized to sum/integrate to 1, comparing the ratio between any two given data points is more useful than simply comparing absolute values.


## [Özdenetimli öğrenmede Karşıtsal Yontemler](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=613s)

Karşıtsal yöntemlerde eğitim kümesinden gözlemiş olduğumuz örneklerin ($x_i$, $y_i$) enerjisini aşağı iterken, eğitim kümesinin manifoldu dışında kalan örneklerin enerjisini yukarı itiyoruz.

Özdenetimli öğrenmede girdinin bir kısmını kulllanarak diğer kısımlarını tahmin etmeye çalışırız. Bu öğrenme modelinden beklentimiz modelimizin bilgisayarlı görü için denetimli görevlerle rekabet edebilecek seviyede iyi öznitelikler üretebilmesidir.

Araştırmacılar, özdenetimli öğrenme modellerinde karşıtsal gömülme(contrastive embedding) yöntemlerini uygulamanın denetimli modellere rakip olabilecek performanslar elde edilebildiğini deneysel olarak gözlemlemişlerdir. Bu yöntemlerden bazılarını ve sonuçlarını aşağıda inceleyeceğiz.


### Karşıtsal Gömulme(Contrastive Embedding)

Bir çift ($x$, $y$) düşünelim, böylece $x$ is an görüntü ve $y$'de $x$ 'in içeriği koruyan bir dönüşüm i(rotation, magnification, cropping, *etc.*) olsun.  Bu çifti **pozitif** çift olarak adlandıralım.

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig1.png" width="50%"/><br>
<b>Fig. 1</b>: Pozitif Çift
</center>

Konsept olarak karşıtsal gömulme yöntemleri, pozitif çifti evrişimsel bir ağın girdisi olarak kullanır ve  iki öznitelik vektörü $h$ and $h'$ öğrenir. Pozitif çift $x$ and $y$ aynı içeriğe sahip olduğu için, öznitelik vektörlerinin de olabildiğince benzer olmalalırnı istiyoruz. Bu amacı gerçekleştirmek için, benzerlik metriği(kosinüs benzerliği gibi) ve $h$ ile $h'$ arasındaki benzerliği maksimize eden bir kayıp fonksiyonu tanımlamamız gerekiyor. Bunu yaparak, eğitim verisinni manifoldunda bulunan görüntülerin enerjilerini düşürüyoruz.

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig2.png" width="50%"/><br>
<b>Fig. 2</b>: Negatif Çift
</center>

Ancak, manifoldun dışında kalan noktaların enerjisini yukarı etmek zorundayız. Bü yüzden **negatif** örnekler ,yani farklı içeriğe sahip resimler(örneğin farklı sınıf etiketi),  ($x_{\text{neg}}$, $y_{\text{neg}}$),  üretiyoruz . Bu örnekleri ağımıza girdi olarak verip, öznitelik vektörleri $h$ ve $h'$'ı elde ediyor, ve bu iki vektör arasındaki benzerliği minimize etmeye çalışıyoruz.

Bu yöntem, benzer çiftlerin enerjisini aşağı çekerken benzer olmayan çiftlerin enerjilerini de yukarı itmemizi sağlar.

ImageNet'teki yeni sonuçları bize bu yöntemin denetimli yöntemlerle öğrenilen özelliklere rakip olabilecek nesne tanıma için iyi özellikler üretebileceğini göstermiştir.

### Özdenetimli Öğrenilmiş Sonuçlar (MoCo, PIRL, SimCLR)

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig3.png" height="75%" width="75%"/><br>
<b>Fig. 3</b>: ImageNet üzerinde PIRL ve MoCo
</center>

Yukarıdaki şekilde de görüldüğü üzere, MoCo ve PIRL SOTA sonuclara ulaşmıştır (özellikle az sayıda parametreli ve düşük kapasiteli modellerde). PIRl is starting to approach the top-1 linear accuracy of supervised baselines (~75%).

PIRL yönteminin amaç fonksiyonu aşağıdaki gibi olan NCE(Noise Contrastive Estimator) u inceleyerek daha iyi anlayabiliriz.

$$
h(v_I,v_{I^t})=\frac{\exp\big[\frac{1}{\tau}s(v_I,v_{I^t})\big]}{\exp\big[\frac{1}{\tau}s(v_I,v_{I^t})\big]+\sum_{I'\in D_{N}}\exp\big[\frac{1}{\tau}s(v_{I^t},v_{I'})\big]}
$$

$$
L_{\text{NCE}}(I,I^t)=-\log\Big[h\Big(f(v_I),g(v_{I^t})\Big)\Big]-\sum_{I'\in D_N}\log\Big[1-h\Big(g(v_{I^t}),f(v_{I'})\Big)\Big]
$$

Burada iki özniitelik haritası/vektörü arasındaki benzerlik metriği kosinüs benzerliği olarak tanımlanmıştır.

PIRL'ın farkı evrişimsel öznitelik üreticisinin çıktısını doğrudan kullanmamasıdır. Aksine, evrişimsel öznitelik üreticisinden bağımsız katmanlar $f$ and $g$'yi kullanır. 

Her şeyi bir araya getirirsek, PIRL'in NCE amaç fonksiyosunu şu şekilde çalışır. Bir miniyığında(mini-batch), bir pozitif(benzer) çift ve çok sayıda negatif(benzer olmayan) çiftlere sahip olalım. We then compute the similarity between the transformed image's feature vector ($I^t$) and the rest of the feature vectors in the minibatch (one positive, the rest negative). We then compute the score of a softmax-like function on the positive pair. Softmax skorunu maksimize etmemiz geri kalan tüm skorları minimize etmek demektir ki bu bizim enerji bazlı modellerde istediğimiz bir durumdur. Böylelikle, NCE kayıp fonksiyonu, benzer çiftlerin enerjisini aşağı iterken benzer olmayan çiftlerin enerjisini yukarı itmemize olanak veren bir model tasarlamamızı sağlar.

Dr. LeCun, PIRL'ın çalışması için oldukça fazla sayıda negatif örneğe ihtiyaç uyulduğunu belirtmektedir. SGD'de mini-batchlerde negatif örneklerin çok sayıda parçasını sürekli olarak tutabilmek zor olabilir. Bü yüzden, PIRL  özel bir ön bellek yapısı kullanmaktadır.

**Soru**: L2 normu yerine neden kosinüs benzerliği kullanıyoruz?
Answer:Çünkü, oldukça kısa(merkeze yakın) ya da oldukça uzun(merkezden uzak) iki vektör seçersek bu iki vektör L2 normuna göre birbirlerine benzer diyebiliriz. Bunun sebebi, L2 normunun vektörlerin elemanları arasındaki farklarının kareleri toplamıdır. Dolayısıyla, kosinüs benzerliği kullanarak sistemimizi "hile yapmamaya" yani vektörleri uzaltıp kısaltmadan iyi bir sonuç bulmaya zorluyoruz. 


### SimCLR

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig5.png" height="75%" width="75%"/><br>
<b>Fig. 4</b>: SimCLR'ın  ImageNet Üzerindeki Sonuçları
</center>

SimCLR diger metodlardan daha iyi sonuclar gostermektedir. Hatta, denetimli metodlarin Imagenet en iyi -1 lineer doğruluktaki performansina erismistir. 
benzer çiftler oluşturmak için karmaşık bir veri büyütme yöntemi kullanır ve TPU'larda büyük miktarda (çok, çok büyük parti yığınlarla) eğitim yapar.

Dr. LeCun, SimCLR'in bir ölçüde karşıtsal yontemlerin sinir cizgisi olduguna inanmakta. 
Yüksek boyutlu bir uzayda, veri manifoldundan gerçekten daha yüksek olduğundan emin olmak için enerjiyi yükseltmeniz gereken birçok bölge var. Gösterimlerin boyutları arttıkça, daha fazla negatif örneğe ihtiyaç duyulmaktadir ki enerjinin manifold üzerinde olmayan bölgelerinde daha yüksek olması şartı sağlanabilsin.  

## [Arıtan Otokodlayıcılar](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=1384s)

[7. haftanin uygulamasinda](https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-3/), arıtan otokodlayıcıdan bahsetmiştik.
Bu yapı, veri gösterimini bozuk girdiyi orjinal veriye öğrenir. The model tends to learn the representation of the data by reconstructing corrupted input to the original input.
Detaylıca bahsedersek, sistemi , bozulmuş veriler veri manifoldundan uzaklaştıkça karesel bir oranda büyüyen bir enerji fonksiyonu üretebilmesi  için eğitiyoruz.

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig6.png" height="75%" width="75%"/><br>
<b>Fig. 5</b>:Arıtan Otokodlayıcı Mimarisi
</center>

### Issues

Ancak arıtan otokodlayıcılar ile ilgili çeşitli sorunlar bulunmaktadır. 
Öncelikle, cok boyutlu ve sürekli bir uzayda, veriyi bozmanın sonsuz sayıda yolu vardır. Bu yüzden enerji fonksiyonunu bir sürü değişik bölgede yukarı itmenin de garantisi yokturç
Bir diger problem ise saklı degişkenlerin noksanlığından ötürü arıtan otokodlayicilar göruntulerde kötu performans sergilemektedir.
Bir resmi yeniden oluşturmanın sayısız yöntemi olduğundan, sistem çeşitli tahminler üretir ve bilhassa iyi öznitelikler öğrenemez. 
Ayrıca, manifoldun ortasındak kalan bozunmuş noktalar iki taraftan da onarmamız mümkün. Bu durum enerji fonksiyonumuzda düz bölgeler oluşmasına neden olur ve genel performansı etkiler.

## Diger Karşıtsal Yontemler 

Karşıtsal ıraksama(Contrastive divergence), Oran Eşleştirme(Ratio Matching) , Gürültüye Karşıt Kestirim(Noise Contrastive Estimation), ve Minimum OLasılık Akışı(Minimum Probability Flow) gibi çeşitli karşıtsal yöntemler lıteratürde bulunmaktadır. Bu bölümde, karşıtsal ıraksamanın arkasında yatan temel fikirden bahsedeceğiz. 

### Karşıtsal Iraksama(Contrastive Divergence)

Karşıtsal Iraksama(CD) girdileri akkılıca bozarak veri ggösterimi öğrenmeyi hedefler. 
Sürekli bir uzayda, önce eğitim kümemizden bir örnek $y$ seçip  sonra onun enerjisinin düşürüyoruz. Seçtiğimiz örnek için, gradyan bazlı bir süreç ile gürültü kullanarak enerji yüzeyinde hareket ediyoruz. Eğer girdilerimiz ayrık bir uzaydan alınıyorsa, örneği rastgele bozmamız o örneğin enerjisini değiştirmek için yeterli olucaktır. Bozunum sonucu eğer enerji düşüyor ise o örneği tutuyoruz. Aksi takdirde, belli bir olaslılık ile o örneği atıyoruz. Bu sürecin devam ettirdikçe $y$'nin enerjisi de en sonunda düşecektir. Daha sonra, enerji fonksiyonumuzun parametresini $y$ ve zıtlaştırılmış örneğimiz $\bar y$'i kayıp fonksiyonu kullanarak kıyaslayıp güncelleyebilmiş oluruz.


### Kalıcı Karşıtsal Iraksama(Persistent Contrastive Divergence)

Karşıtsal Iraksama'nın özel bir versiyonu ise Kalıcı Karşıtsal Iraksama yöntemidir. Önceki yönteme kıyasla sistem, konumunu hatırlayabilen bir grup  "parçacık" kullanır. Bu parçacıklar tıpkı CD'daki gibi enerji yüzeyinde hareket ettirilir. Sonunda, bu parçaçıklar enerji yüzeyimizdeki düşük enerjili yerleri bulucak ve yukarı itilmelerni sağlayacaktır. Ancak, boyut arttıkça sistem performansı kötğ etkilenmektedir.

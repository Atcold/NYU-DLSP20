---
lang-ref: ch.14
lang: tr
title: Hafta 14
translation-date: 22 Aug 2020
translator: Murat Ekici
---


## Ders Kısım A

Bu kısımda, yapılandırılmış kestirimden bahsedeceğiz. İlk önce enerji-bazlı faktör çizgelerini tanıtacak ve bu yapılar için verimli çıkarımın
nasıl gerçekleştirildiğini öğreneceğiz. Sonra,"sığ" faktörlü basit enerji-bazlı faktör çizelgelerinden bazı örnekler vereceğiz. Son olarak, Çizge Dönüştürücü Ağ *(Graph Transformer Net)*
üzerine konuşacağız.


<!--
## Lecture part A
In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. T
hen we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net.
-->

## Ders Kısım B

Dersin ikinci ayağında, grafik modellerinin enerji bazlı modellerine uygulanmasını ele alacağız. Değişik kayıp fonksiyonlarını kıyasladıktan sonra, Viterbi algoritmasının ve ileri algoritmanın çizgesel dönüştürücü ağına uygulanmasını tartışacağız.
Sonra, geri yayılımın Lagrange formülasyonunu ve enerji bazlı modeller için değişimsel çıkarımı ele alacağız.

<!--
## Lecture part B
The second leg of the lecture further discusses the application of graphical model methods to energy-based models. 
After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. 
We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.
-->

## Pratik
Derin sinir ağları gibi fazla parametreli modelleri eğitirken modelin eğitim verisini aşırı öğrenme riski vardır. Bu durum yüksek genelleme hatasına sebep verir. Aşırı öğrenmeyi azaltmak için eğitim sırasında düzenlileştirme kullanabilir ve modelin bazı çözümleri öğrenmekten caydırarak gürültüye olan direncini güçlendirebiliriz.
<!--
## Practicum
When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. 
This leads to greater generalization error. 
To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.
-->

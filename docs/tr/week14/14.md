---
lang-ref: ch.14
lang: tr
title: Week 14
translation-date: 22 Aug 2020
translator: emirceyani
---


## Ders Kısım A

Bu kısımda, yapılandırılmış kestirimden bahsedeceğiz. İlk önce enerji-bazlı faktör çizgelerini tanıtacak ve bu yapılar için verimli çıkarımın
nasıl gerçekleştirildiğini öğreneceğiz. Sonra, enerji-bazlı modellerden bazı örnekleri ele alacağız. Son olarak, Çizge Dönüştürücü Ağ *(Graph Transformer Net)*
üzerine konuşacağız.


<!--
## Lecture part A
In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. T
hen we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net.
-->

## Ders Kısım B

Dersin ikinci ayağında, grafik modellerinin enerji bazlı modelleerdki uygulamasını ele alacağız. değişik kayıp fonksiyonlarını kıyasladıktan sonra, Viterbi ile ileri algorştmalarının çizgesel dönüştürücü ağında uygulanmasına bakacağız.
Sonra, geri yayılımın Lagrange formülasyonunu ve enerji bazlı modellerde değişimsel çıkarımı ele alacağız.

<!--
## Lecture part B
The second leg of the lecture further discusses the application of graphical model methods to energy-based models. 
After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. 
We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.
-->

## Pratik
Derin sinir ağı gibi fazla parametreli modelleri eğitirken  modelin eğitim verisini ezberleme riski her zaman vardır. Bu durum yüksek genelleme hatasına sebep verir. Bu durumu çözmek için, eğitim sürecini düzenlileştirebilir ve modelin bazı çözümleri öğrenmekten caydırarak gürültüye olan direncini güçlendirebiliriz.
<!--
## Practicum
When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. 
This leads to greater generalization error. 
To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.
-->

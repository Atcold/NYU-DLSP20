---
lang: tr
lang-ref: tr.14
title: Hafta 14
translation-date: 25 June 2020
translator: Murat Ekici
---


## Ders bölümü A

Bu bölümde yapılandırılmış tahmini tartıştık. İlk olarak Enerji Tabanlı faktör grafiğini ve bunun için etkin çıkarımları sunduk. Daha sonra “sığ” faktörlere sahip basit Enerji Tabanlı faktör grafikleri için bazı örnekler verdik. Son olarak Graph Transformer Net'i tartıştık.
<!--

## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net.

-->

## Ders bölümü B

Dersin ikinci ayağında, grafik model yöntemlerin enerji tabanlı modellere uygulanışını tartıştık. Farklı kayıp fonksiyonlarını karşılaştırdıktan sonra, Viterbi algoritmasının ve ileri algoritmanın grafiksel dönüştürücü ağlarına uygulanmasını tartıştık. Daha sonra, geri yayılımın Lagrange formülasyonunu ve enerji tabanlı modeller için çeşitlilik çıkarımını tartıştık.

<!--
## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.

-->

## Uygulama

Derin sinir ağları gibi yüksek parametrelendirilmiş modelleri eğitirken, eğitim verilerine aşırı uyum (overfitting) riski vardır. Bu daha aşırı genelleme hatasına yol açar. Aşırı uyumu azaltmaya yardımcı olmak için eğitimimizde düzenlileştirme kullanabilir ve modellerimizin gürültüye tepki verme düzeyini düşürebiliriz.

<!--
## Practicum

When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.

-->


lang: tr
lang-ref: ch.14
title: Hafta 14
translation-date: 12 Jun 2020
translator: Murat Ekici
---


## Ders bölümü A

Bu bölümde, yapılandırılmış kestirimi tartıştık. İlk olarak enerji tabanlı faktör çizgelerini ve bu yapılar için verimli çıkarımın nasıl yapılacağını tanıttık. Daha sonra, basit yapıda enerji tabanlı faktör çizgelerine örnekler verdik. Son olarak, Çizge Dönüştürücü Ağı (Graph Transformer Net) tartıştık.

<!--

## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net.
-->

## Ders bölümü B

Dersin ikinci ayağında, grafik model yöntemlerinin enerji tabanlı modellere uygulanmasını daha ayrıntılı olarak ele aldık. Farklı kayıp fonksiyonlarını karşılaştırdıktan sonra, Viterbi algoritmasının ve ileri yayılım algoritmansının çizge dönüştürücü ağlara uygulanmasını tartıştık. Daha sonra Lagrangian geri yayılım formülasyonunu ve ardından enerji tabanlı modeller için varyasyonel çıkarımı üstünde durduk.

<!--
## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.
-->

## Uygulama

Derin sinir ağları gibi yüksek düzeyde parametreleştirilmiş modelleri eğitirken, modelin eğitim verilerini aşırı öğrenme riski vardır. Bu durum yüksek genelleme hatasına yol açar. Aşırı öğrenmeyi azaltmak için düzenlileştirme kullanabilir ve modelin belirli çözümleri öğrenmesini engelleyerek gürültüye olan direncini arttırabiliriz.

<!--
## Practicum

When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.
-->

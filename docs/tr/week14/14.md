
lang: tr
lang-ref: ch.14
title: Hafta 14
translation-date: 12 Jun 2020
translator: Murat Ekici
---


## Ders bÃ¶lÃ¼mÃ¼ A

Bu bÃ¶lÃ¼mde, yapÄ±landÄ±rÄ±lmÄ±ÅŸ kestirimi tartÄ±ÅŸtÄ±k. Ä°lk olarak enerji tabanlÄ± faktÃ¶r Ã§izgelerini ve bu yapÄ±lar iÃ§in verimli Ã§Ä±karÄ±mÄ±n nasÄ±l yapÄ±lacaÄŸÄ±nÄ± tanÄ±ttÄ±k. Daha sonra, basit yapÄ±da enerji tabanlÄ± faktÃ¶r Ã§izgelerine Ã¶rnekler verdik. Son olarak, Ã‡izge DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ AÄŸÄ± (Graph Transformer Net) tartÄ±ÅŸtÄ±k.

<!--

## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with â€œshallowâ€ factors. Finally, we discussed the Graph Transformer Net.
-->

## Ders bÃ¶lÃ¼mÃ¼ B

Dersin ikinci ayaÄŸÄ±nda, grafik model yÃ¶ntemlerinin enerji tabanlÄ± modellere uygulanmasÄ±nÄ± daha ayrÄ±ntÄ±lÄ± olarak ele aldÄ±k. FarklÄ± kayÄ±p fonksiyonlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rdÄ±ktan sonra, Viterbi algoritmasÄ±nÄ±n ve ileri yayÄ±lÄ±m algoritmansÄ±nÄ±n Ã§izge dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ aÄŸlara uygulanmasÄ±nÄ± tartÄ±ÅŸtÄ±k. Daha sonra Lagrangian geri yayÄ±lÄ±m formÃ¼lasyonunu ve ardÄ±ndan enerji tabanlÄ± modeller iÃ§in varyasyonel Ã§Ä±karÄ±mÄ± Ã¼stÃ¼nde durduk.

<!--
## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.
-->

## Uygulama

Derin sinir aÄŸlarÄ± gibi yÃ¼ksek dÃ¼zeyde parametreleÅŸtirilmiÅŸ modelleri eÄŸitirken, modelin eÄŸitim verilerini aÅŸÄ±rÄ± Ã¶ÄŸrenme riski vardÄ±r. Bu durum yÃ¼ksek genelleme hatasÄ±na yol aÃ§ar. AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi azaltmak iÃ§in dÃ¼zenlileÅŸtirme kullanabilir ve modelin belirli Ã§Ã¶zÃ¼mleri Ã¶ÄŸrenmesini engelleyerek gÃ¼rÃ¼ltÃ¼ye olan direncini arttÄ±rabiliriz.

<!--
## Practicum

When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.
-->


ğŸ‡¹ğŸ‡· muratekici

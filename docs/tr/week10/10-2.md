---
lang-ref: ch.10-2
lang: tr
title: Özdenetimli Öğrenme - ClusterFit ve PIRL
lecturer: Ishan Misra
authors: Zhonghui Hu, Yuqing Wang, Alfred Ajay Aureate Rajakumar, Param Shah
date: 6 Apr 2020
translator: Yunus Emre Özköse
translation-date: 19 July 2020
---


## Motivasyon:


### "Pretext" görevlerde eksik olan nedir?


#### Genelleştirme umudu

Pretext görevi genellikle denetimli ön eğitim adımlarından oluşur ve daha sonra genellikle sınıflandırma veya saptama olabilen transfer görevlerimiz vardır. Ön eğitim görevinin ve transfer görevlerinin "hizalandığını", yani pretext görevini çözmenin aktarım görevlerini çok iyi çözmesine yardımcı olacağını *umuyoruz*. Bu nedenle, çok sayıda araştırmacı bir pretext görev tasarlamakta ve bunları gerçekten iyi uygulamaya çalışmaktadır.

Ancak, semantik olmayan bir görevin uygulanmasının neden iyi öznitelikler üretmesi gerektiği çok açık değildir. Örneğin, yapboz gibi bir şeyi çözerken neden “anlamsallık” hakkında bilgi edinmeyi beklemeliyiz ki? Ya da neden görüntülerden “hashtag'leri tahmin etmenin” transfer görevleri hakkında bir sınıflandırıcı öğrenmesine yardımcı olması bekleniyor? Bu nedenle, elimizde bir sorumuz kalıyor. Transfer görevleriyle iyi hizalanmış iyi ön eğitim görevlerini nasıl tasarlamalıyız?

Bu problemi değerlendirmenin bir yolu da her bir katmandaki (bkz. Fig. 1) gösterimlere bakmak. Eğer son katmandaki gösterimler transfer görevi ile iyi hizanlamamış ise ön eğitim görevleri çözüm için doğru olmayabilir.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig01.png" height="75%" width="75%"/><br>
<b>Fig. 1</b>: Feature representations at each layer
</center>

Fig. 2'de her bir katmandaki yapboz ön eğitimi kullanılan VOC07 üzerindeki doğrusal sınıflandırıcılar için Mean Average Precision gösterilmiştir. Çok açık ki son katman yapboz görevi için çok özelleşmiştir.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig02.png" height="70%" width="80%"/><br>
<b>Fig. 2</b>: Performance of Jigsaw based on each layer
</center>


### Ön eğitim özniteliklerinden ne istiyoruz?

* Bir görüntünün bir diğeriyle nasıl ilişkili olduğunu

  * ClusterFit: Görsel Gösterimlerin Genelleştirilmesini Geliştirmek

* "Sıkıntılı faktörlere" karşı gürbüz olmak --  değişmezlik

  Örn. nesnelerin tam yeri, ışık, tam renk.

  * PIRL: Sabit pretext gösterimlerinin öz denetimli öğrenmesi

Yukarıdaki özelliklerin başarılması için iki yol: **Kümeleme** ve **Karşıtsal Öğrenme**. Şimdiye kadar tasarlanan pretext görevlerinden çok daha iyi performans göstermeye başladılar. Kümelemeye ait bir yöntem ise **ClusterFit** ve değişmezliğin yer aldığı **PIRL**'dir.


## ClusterFit: Görsel Gösterimlerin Genelleştirilmesini Geliştirmek

Öznitelik uzayını kümeleme, resimlerin bir diğeri ile nasıl ilişkili olduğunu görmenin bir yoludur.


### Yöntem

ClusterFit 2 adımdan oluşur: Birincisi kümeleme adımı, ikincisi öngörme adımı.


#### Küme: Öznitelik Kümeleme

Ön eğitimli bir ağı alırız ve ağı bi grup resimden bir grup öznitelik çıkarımı yapmak için kullanırız. Ağ her türlü ön eğitimli ağ olabilir. Sonrasında bu özniteliklere K-means kümeleme uygulanır, yani her bir resim ait olduğu kümeye yani onun etiketine atanır. 


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig03.png"
height="75%" width="75%" /><br>
<b>Fig. 3</b>: Cluster step
</center>


#### Oturtma: Küme Atamasını Öngörme

Bu adım için, resimlerin yalancı etiketini öngörmek adına sıfırdan bir ağ eğitiriz. Bu yalancı etiketler birinci adımda kümeleme yoluyla elde ettiğimiz kümelerdir.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig04.png"
height="75%" width="75%"/><br>
<b>Fig. 4</b>: Predict step
</center>

Standart bir ön eğitim ve transfer görevi ilk önce bir ağa ön eğitim yapar ve sonrasında ağı bir downstream görevinde değerlendirir, tıpkı Fig. 5'in ilk satırında gösterildiği gibi. ClusterFit ön eğitimli bir ağ $N_{pre}$ elde etmek için bir veri kümesi $D_{cf}$ üzerinde ön eğitim gerçekleştirir. Ön eğitimli ağ $N_{pre}$ kümeler üretmek için veri kümesi $D_{cf}$ üzerine uygulanır. Sonrasında bu veri kümesi üzerinde sıfırdan yeni bir ağ $N_{cf}$ öğreniriz. Son olarak $N_{cf}$ ağını bürün downstream görevler için kullanırız. 


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig05.png"
height="75%" width="75%"/><br>
<b>Fig. 5</b>: "Standard" pretrain + transfer vs. "Standard" pretrain + ClusterFit
</center>


### ClusterFit Neden Çalışıyor?

Bunu sebebi kümeleme adımında sadece ana bilgi yakalanıyor, ve yapılar atılarak ikinci ağın biraz daha genel bir şey öğrenilmesi sağlanıyor.

Bu noktayı anlamak için, uygun basit bir deney uyguladık. ImageNet-1K veri kümesine etiket gürültüsü ekledik, ve bu veri kümesi temel alınarak bir ağ eğittik. Sonrasında bu ağdaki öznitelik gösterimlerini ImageNet-9K veri kümesi ile bir downstream görevi üzerinde değerlendirdik. Fig. 6'da görüldüğü gibi, ImageNet-1K veri kümesine farklı büyüklüklerde etiket gürültüsü ekledik, ve ImageNet-9K üzerinde farklı yöntemlerin transfer performansını değerlendirdik.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig06.png"
height="50%" width="50%"/>
<br><b>Fig. 6</b>: Control Experiment
</center>


Pembe çizgi ön eğitimli ağın performansını gösteriyor ki etiket gürültüsünün büyüklüğü attıkça azalıyor. Mavi çizgi model saflaştırmasını gösteriyor, başlangıç ağı alınıp etiketler üretmek için kullanılıyor. Saflaştırma genel olarak ön eğitimli ağdan daha iyi performans gösteriyor. Yeşil çizgi, ClusterFit, devamlı olarak bu ağlardan daha iyi. Bu sonuç hipotezimizi doğruluyor.


* Soru: Karşılaştırma için neden saflaştırma kullandık? Saflaştırma ve ClusterFit arasındaki fark nedir?

Model saflaştırmada ön eğitimli ağı alırız ve ağın öngördüğü etiketleri resimlerimizin etiketlerini üretmek için daha yumuşak bir şekilde (softer fashion) kullanırız. Örneğin, bütün sınıflar üzerinde bir dağılım elde edip bu dağılımı ikinci bir ağ eğitmek için kullanırız. Daga yumuşak bir dağılım sahip olduğumuz başlangıç sınıflarını artırır. ClusterFit'de etiket uzayını önemsemeyiz. 


### Performans

Bu yöntemi öz denetimli öğrenmeye uygularız. Burada yapboz ClusterFit'de ön eğitimli ağı $N_{pre}$ elde etmek için uygulanır. Fig 7.'de farklı veri kümeleri üzerindeki transfer performansının diğer öz denetimli yöntemlerle karşılaştırıldığında şaşırtıcı derecede kazanç gösterdiğini görebiliriz.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig07.png"
height="70%" width="70%"/><br><b>Fig. 7</b>: Transfer performance on different datasets
</center>

ClusterFit herhangi bir ön eğitimli ağ için çalışır. Ekstra veri olmadan kazanç, etiketler veya mimarideki değişimler Fig. 8'de görülebilir. Dolayısıyla, bir şekilde, ClusterFit'i temsil kalitesini artıran, öz denetimli ince ayar adımı olarak düşünebiliriz.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig08.png"
height="70%" width="70%"/><br><b>Fig. 8</b>: Gains without extra data, labels or changes in architecture
</center>


## Sabit Pretext Gösterimlerinin Öz Denetimli Öğrenmesi (Pretext Invariant Representations (PIRL))


### Karşıtsal Öğrenme

Karşıtsal Öğrenme basit olarak ilişkili noktaları birleştiren ve ilişkili olmayan noktaları ayıran bir öznitelik uzayını öğrenmeye çalışan genel bir çatıdır. 


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig09.png"
height="20%" width="20%"/><br><b>Fig. 9</b>: Groups of Related and Unrelated Images
</center>

Bu durumda, mavi kutuları birbiri ile, yeşil kutuları birbiri ile ve morların birbiri ile ilişkili olduğunu düşünün. 


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig10.png"
height="100%" width="100%"/><br><b>Fig. 10</b>: Contrastive Learning and Loss Function
</center>

Bu veri noktalarının her biri için öznitelikler bu veri noktalarının herbiri içinn bir grup resim özniteliğini elde etmek için Siamese Ağı adında ortak bir ağ yoluyla çıkarım yapılabilir. Sonra bir karşıtsal yitim fonksiyonu mavi noktalar arasındaki uzaklığı, mesela, yeşil noktalar arasındaki uzaklıklara göre küçültmeye çalışmak için uygulanır. Veya temel olarak mavi noktalar arasındaki mesafe, mavi nokta ile yeşil nokta veya mavi nokta ile mor nokta arasındaki mesafeden daha az olmalıdır. 

Dolayısıyla, ilgili örneklerdeki gömülme uzayı, ilgisiz örneklerdeki gömülme uzayından çok daha yakın olmalıdır. Bu, karşıtsal öğrenmenin ne olduğu hakkında genel bir fikirdir ve elbette Yann, bu yöntemi öneren ilk öğretmenlerden biriydi. Dolayısıyla, karşıtsal öğrenme şimdi öz denetimli öğrenmede bir yeniden dirilme yapıyor, hemen hemen öz denetimli güncel yöntemlerinin çoğu gerçekten karşıtsal öğrenme tabanlıdır.


### İlgili veya ilgili olmayanı nasıl tanımlarız?

Ve ana soru ilgili veya ilgili olmayanı nasıl tanımlamaktır. Öz denetimli öğrenmenin bu durumlarında, köpek resimlerinin ilgili resimler olduğu ve başka herhangi bir resmin de basit olarak ilgili resim olmadığı açıktır. Fakat bu öz denetimli öğrenme durumunda ilintililiği ve ilintili olmamayı tanımlamak çok açık değil. Pretext görevi gibi şeylerla olan bir diğer ana fark karşıtsal öğrenmenin bir anda çok fazla veriden sonuç çıkarmasıdır. Eğer yitim fonksiyonuna bakarsanız, fonksiyon her zaman birden fazla resmi kapsamaktadır. Birinci satırda fonksiyon basit olarak mavi resimleri ve yeşil resimleri kapsamakta, ikinci satırda mavi resimler ile mor resimleri kapsamaktadır. Fakat, mesela yapboz veya rotasyon görevi gibi görevlere bakarsanız, her zaman bağımsız olarak tek bir resim hakkında sonuç çıkarabilirsiniz. Dolayısıyla, bu karşıtsal öğrenmenin bir diğer farkıdır: karşıtsal öğrenme birden fazla veri noktasında aynı anda sonuç çıkarabilir. 

Daha önce tartışılan benzerlik teknikleri kullanılabilir: videoların görüntüleri veya verinin ardışık doğası. Videodaki yakın görüntüler ilişkilidir ve mesela başka bir videodaki veya başka zzaman dilimindeki görüntüler ilişkili değildir. Ve bu alanda öz denetimli birçok öğrenme yönteminin temelini oluşturmuştur. Bir sinyalin ardışık doğasına bağlı olan bu yöntem CPC (contrastive predictive coding - karşıtsal öngörü kodlaması) olarak isimlendirilir, ve temel olarak yakın olan örneklemler, mesela zaman uzayında, ilişkilidir ve zaman uzayında ayrılmış örneklemler ilişkili değildir. Uygun büyüklükteki iş basit olarak şunlardan yararlanır: konuşma alanından, videodan, yazıdan veya belirli resimlerden olabilir. Ve son dönemlerde, video ve sesler üzerinde de çalışıyoruz, yani, temel olarak bir videonun ve ona karşılık gelen sesin ilişkili bir örneklem olduğunu ve farklı video ile ses örneklemlerinin ilişkili olmadığını söylüyoruz. 


### Nesneleri İzleme

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig11.png"
height="80%" width="80%"/><br><b>Fig. 11</b>: Tracking the Objects
</center>

Bazı erken dönem çalışmaları, öz denetimli öğrenme gibi, aynı zamanda bu karşıtsal öğrenme yöntemini de kullanır ve gerçekten ilgili örnekleri şaşırtıcı derecede uyumlu tanımlar. Bir video üzerinde bir izlenen nesne izleyicisini koşturursun, ve bu sana bir hareket parçası verir, ve sense izleyici tarafından izlenen herhangi bir parçanın orjinal parça ili ilişkili olduğunu söylersin. Oysaki farklı bir videodaki herhangi bir parça ilişkili bir parça değildir. Dolayısıyla bu temel olarak bu ilişkili ve ilişkili olmayan örneklem gruplarını verir. Figür 11(c)'de bunun gibi bir şeye sahibiz, uzaklık notasyonu gibi. Bu ağın öğrenmeye çalıştığı şey temel olarak aynı videodan gelen parçaların ilişkili olduğu ve farklı videolardan gelen parçaların ilişkili olmadığıdır. Bu yolla otomatik olarak bir nesnenin farklı konumlarını öğrenmiş oluruz. Bir köpeğin farklı bakış açılarından veya farklı pozlarından bakıldığında bir döngüyü gruplandırmaya çalışırız.


### Bir resmin yakın parçaları vs. uzak parçaları

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig12.png"
height="50%" width="50%"/><br><b>Fig. 12</b>: Nearby patches vs. distant patches of an Image
</center>

Genel olarak, resimlerden konuşacak olursak, birçok çalışma resimlerin yakın parçalarına karşı uzak parçalarına bakarak yapılmıştır, dolayısıyla CPC v1 and CPC v2 yöntemlerinin çoğu gerçekten resimlerin bu özelliğinden yararlanır. Dolayısıyla yakın olan resim parçaları pozitif olarak ve uzak olan resim parçaları negatif olarak isimlendirilir, ve amaç bu pozitif ve negatif tanımının kullanarak karşıtsal yitimi küçültmektir. 


### Bir resmin parçaları vs başka bir resmin parçaları

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig13.png"
height="60%" width="60%"/><br><b>Fig. 13</b>: Patches of an Image vs. Patches of Other Images
</center>

Bunu yapmanın daha popüler veya performanslı yolu bir resimden gelen parçaya bakmak ve farklı bir resimden gelen parça ile karşılaştırmaktır. Bu örnek ayrımcılığı (instance discrimination), MoCo, PIRL, SimCLR gibi birçok popüler yöntemin temelini inşa eder. Fikir temel olarak resimde ne göründüğüdür. Detaya inecek olursak, bu yöntemler bir resimden tamamıyla rastgele parçalar çıkarımı yapar. Bu parçalar üst üste gelebilir, bir başka resmin içinde olabilir veya tamamıyla farklı bir parça olabilir, ve sonrasında bazı veri artırımları uygulanır. Bu durumda, mesela bir renk birleştirilmesinde veya rengin kaldırlmasında gibi gibi.  Ve sonrasında bu iki parça pozitif örnekler olarak tanımlanır. Birbaşka parça ise farklı bir resimden çıkarım yapılır. Ve bu yine rastgele bir parça olur, ve bu parçalar temel olarak negatif olarak adlandırılır. Ve bu yöntemlerin birçoğu birçok negatif örnek parça çıkarımı yapar ve sonrasında temel olarak karşıtsal öğrenme uygularlar. Dolayısıyla ilişkili olan iki pozitif örnek vardır fakat karşıtsal öğrenmeye karşı yapılan birçok negatif örnek vardır. 


### Pretext Görevlerinin Esas Prensipleri

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig14.png"
height="50%" width="50%"/><br><b>Fig. 14</b>: Pretext Image Transform and Standard Pretext Learning
</center>

Şimdi biraz PIRL'ye bakalım, ve pretext görevler ile olan ana farkını ve karşıtsal öğrenin pretext görevlerden nasıl çok farklı olduğunu anlamaya çalışalım. Tekrar söylemek gerekirse, pretex görevler her zaman aynı anda tek bir resim hakkında sonuç çıkarırlar. So the idea is that given an image your and prior transform to that image, in this case a Jigsaw transform, and then inputting this transformed image into a convnet and trying to predict the property of the transform that you applied to, the permutation that you applied or the rotation that you applied or the kind of color that you removed and so on. Dolayısıyla pretext görevler her zaman tek bir resim hakkında sonuç çıkarırlar. Ve, ikinci olarak, bu durumda uyguladığın görev gerçekten dönüşümün bazı özelliklerini yakalamak zorundadır. Uygulanan rotasyon türünü veya uygulanan tam permütasyonu yakalamaya ihtiyaç duyar, yani son katman gösterimleri aslında değişiklikleri dönüştürürken PIRL'a çok fazla gider ve bu da tasarım gereğidir, çünkü bu pretext görevlerini gerçekten çözmeye çalışıyorsunuzdur. Fakat maalesef bu, son katman gösterimlerinin sinyalin çok düşük seviyeli özelliklerini yakalaması demektir. Son katman gösterimleri rotasyon veya bunun gibi şeyleri yakalarlar. Oysaki tasarlanan şey veya bu gösterimlerden ne beklenilen şey onların bir kediyi tanıyabilmeye, dik durup urmamasına veya mesela 90 derece dönüp dönmemesine olan değişmezliğidir. Oysaki bu özel pretext görevini çözerken tam karşıt şeyleri uygulamaya koyuyorsun. Bu resmin yukarıya bakıyor oluşunu veya bu resmin temel olarak yan taraflarına dönmüş olmasını tanımabilmemiz gerektiğinden bahsediyoruz. 

Bu düşük seviyeli gösterimlerin kovaryant olmasını istediğiniz birçok istisna vardır ve bunların çoğu gerçekleştirdiğiniz görevlerle gerçekten ilgilidir ve 3 boyuttaki birkaç görev gerçekten öngörülebilir olmak ister. Böylece, kameranızın hangi dönüşümleri dönüştürdüğünüzü tahmin etmek istersiniz: aynı nesnenin iki görünümüne bakıyorsunuzdur. Ancak, birçok anlamsal görev için bu tür özel bir uygulamanız yoksa, bu girdiyi kullanmak için kullanılan dönüşümlere gerçekten aşina olmak istersiniz.


### Değişmezlik ne kadar önemli?

Değişmezlik öznitelik öğrenme için temel bir ders olmuştur. Buraya eklediğimiz, oldukça popüler bir el yapımı öznitelik olan SIFT gibi şeyler değişmezliğe transfer edilir. Ve denetimli ağlar, örneğin denetimli Alexnet, değişmez veri artırımı için eğitilirler. Bu ağın, giriş için uygulanan dönüşümün tam olarak ne olduğunu öngörmesini istemek yerine, bu görüntünün farklı kesitlerini veya farklı döndürmelerini bir ağaç olarak sınıflandırmasını istersin.


### PIRL

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig15.png"
height="70%" width="70%"/><br><b>Fig. 15</b>: PIRL
</center>

Bu PIRL'in ilham aldığı şeydir. Dolayısıyla PIRL sabit pretext gösterim öğrenimini temsil etmektedir, ki fikir 
This is what inspired PIRL. So PIRL stands for pretext invariant representation learning, where the idea is that you want the representation to be invariant or capture as little information as possible of the input transform. So you have the image, you have the transformed version of the image, you feed-forward both of these images through a convnet, you get a representation and then you basically encourage these representations to be similar. In terms of the notation referred earlier, the image $I$ and any pretext transformed version of this image $I^t$ are related samples and any other image is underrated samples. So in this way when you frame this network, representation hopefully contains very little information about this transform $t$. And assume you are using contrastive learning. So the contrastive learning part is basically you have the saved feature $v_I$ coming from the original image $I$ and you have the feature $v_{I^t}$ coming from the transform version and you want both of these representations to be the same. And the book paper we looked at is two different states of the art of the pretext transforms, which is the jigsaw and the rotation method discussed earlier. In some way, this is like multitask learning, but just not really trying to predict both designed rotation. You're trying to be invariant of Jigsaw rotation. 


###  Using a Large Number of Negatives

The key thing that has made contrastive learning work well in the past, taking successful attempts is using a large number of negatives. One of the good paper that introduced this was this instance discrimination paper from 2018, which introduced this concept of a memory bank. This is powered, most of the research methods which are state of the art techniques hinge on this idea for a memory bank. The memory bank is a nice way to get a large number of negatives without really increasing the sort of computing requirement. What you do is you store a feature vector per image in memory, and then you use that feature vector in your contrastive learning. 


### How it works

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig16.png"
height="75%" width="75%"/><br><b>Fig. 16</b>: How does the Memory Bank Work
</center>

Let's first talk about how you would do this entire PIRL setup without using a memory bank. So you have an image $I$ and you have an image $I^t$, and you feed-forward both of these images, you get a feature vector $f(v_I)$ from the original image $I$, you get a feature $g(v_{I^t})$ from the transform versions, the patches, in this case. What you want is the features $f$ and $g$ to be similar. And you want features from any other unrelated image to basically be dissimilar. In this case, what we can do now is if you want a lot of negatives, we would really want a lot of these negative images to be feed-forward at the same time, which really means that you need a very large batch size to be able to do this. Of course, a large batch size is not really good, if not possible, on a limited amount of GPU memory. The way to do that is to use something called a memory bank. So what this memory bank does is that it stores a feature vector for each of the images in your data set, and when you're doing contrastive learning rather than using feature vectors, say, from a different from a negative image or a different image in your batch, you can just retrieve these features from memory. You can just retrieve features of any other unrelated image from the memory and you can just substitute that to perform contrastive learning. Simply dividing the objective into two parts, there was a contrasting term to bring the feature vector from the transformed image $g(v_I)$, similar to the representation that we have in the memory so $m_I$. And similarly, we have a second contrastive convnet that tries to bring the feature $f(v_I)$ close to the feature representation that we have in memory. Essentially $g$ is being pulled close to $m_I$ and $f$ is being pulled close to $m_I$. By transitive, $f$ and $g$ are being pulled close to one another. And the reason for separating this outwards. It's stabilized training and we were not able to train without doing this. Basically, the training would not really converge. By separating this out into two forms, rather than doing direct contrastive learning between $f$ and $g$, we were able to stabilize training and actually get it working.


### PIRL Pre-training

The way to evaluate this is basically by standard pre-training evaluation set-up. For transfer learning, we can pretrain on images without labels. The standard way of doing this is to take an image net, throw away the labels and pretend as unsupervised. 


### Evaluation

And then evaluating using, say, Full fine-tuning or using our training a linear classifier. The second thing we did was also our test PIRL and it's robustness to image distribution by training it on in-the-wild images. So we just took 1 million images randomly from flickr, which is the YFCC data set. And then we basically performed pre-training on these images and then performed transplanting on different data sets.


#### Evaluating on Object Detection task

PIRL was first evaluated on object detection task (a standard task in vision) and it was able to outperform ImageNet supervised pre-trained networks on both **VOC07+12** and **VOC07** data sets. In fact, PIRL outperformed even in the stricter evaluation criteria, $AP^{all}$ and that's a good positive sign.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig17.png"
height="70%" width="70%"/><br><b>Fig. 17</b>: Object detection performance on different datasets
</center>


#### Evaluating on Semi-supervised Learning

PIRL was then evaluated on semi-supervised learning task. Again, PIRL performed fairly well. In fact, PIRL was better than even the pre-text task of Jigsaw. The only difference between the first row and the last row is that, PIRL is an invariant version, whereas Jigsaw is a covariant version.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig18.png"
height="70%" width="70%"/><br><b>Fig. 18</b>: Semi-supervised learning on ImageNet
</center>


#### Evaluating on Linear Classification

Now when evaluating on Linear Classifiers, PIRL was actually on par with the CPCv2, when it came out. It also worked well on a bunch of parameter settings and a bunch of different architectures. And of course, now you can have fairly good performance by methods like SimCLR or so. In fact, the Top-1 Accuracy for SimCLR would be around 69-70, whereas for PIRL, that'd be around 63.


<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig19.png"
height="75%" width="75%"/><br><b>Fig. 19</b>: ImageNet classification with linear models
</center>


#### Evaluating on YFCC images

PIRL was evaluated on *"In-the-wild" Flickr* images from the YFCC data set. It was able to perform better than Jigsaw, even with $100$ times smaller data set. This shows the power of taking invariance into consideration for the representation in the pre-text tasks, rather than just predicting pre-text tasks.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig20.png"
height="70%" width="70%"/><br><b>Fig. 20</b>: Pre-training on uncurated YFCC images
</center>


### Semantic Features

Now, going back to verifying the semantic features, we look at the Top-1 accuracy for PIRL and Jigsaw for different layers of representation from *conv1* to *res5*. It's interesting to note that the accuracy keeps increasing for different layers for both PIRL and Jigsaw, but drops in the 5th layer for Jigsaw. Whereas, the accuracy keeps improving for PIRL, i.e. more and more semantic.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig21.png"
height="70%" width="70%"/><br><b>Fig. 21</b>: Quality of PIRL representations per layer
</center>


### Scalability

PIRL is very good at handling problem complexity because you're never predicting the number of permutations, you're just using them as input. So, PIRL can easily scale to all 362,880 possible permutations in the 9 patches. Whereas in Jigsaw, since you're predicting that, you're limited by the size of your output space.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig22.png"
height="70%" width="70%"/><br><b>Fig. 22</b>: Effect of varying the number of patch permutations
</center>

The paper "*Misra & van der Maaten, 2019, PIRL*" also shows how PIRL could be easily extended to other tasks like Jigsaw, Rotations  and so on. Further, it could even be extended to combinations of those tasks like Jigsaw+Rotation.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/fig23.png"
height="70%" width="70%"/><br><b>Fig. 23</b>: Using PIRL with (combinations of) different pretext tasks
</center>


### Invariance vs performance

In terms of invariance property, one could, in general, assert that the invariance of PIRL is more than that of the Clustering, which in turn has more invariance than that of the pretext tasks. And similarly, the performance to is higher for PIRL than Clustering, which in turn has higher performance than pretext tasks. This suggests that taking more invariance in your method could improve performance.


### Shortcomings

1. It's not very clear as to which set of data transforms matter. Although Jigsaw works, it's not very clear why it works.
2. Saturation with model size and data size.
3. What invariances matter? (One could think about what invariances work for a particular supervised task in general as future work.)

So in general, we should try to predict more and more information and try to be as invariant as possible.


## Some important questions asked as doubts


### Contrastive learning and batch norms

1. Wouldn't the network learn only a very trivial way of separating the negatives from the positives if the contrasting network uses the batch norm layer (as the information would then pass from one sample to the other)?

**Ans**: *In PIRL, no such phenomenon was observed, so just the usual batch norm was used*

2. So is it fine to use batch norms for any contrasting networks?

**Ans**: *In general, yeah. In SimCLR, a variant of the usual batch norm is used to emulate a large batch size. So, batch norm with maybe some tweaking could be used to make the training easier*

3. Does the batch norm work in the PIRL paper only because it's implemented as a memory bank - as all the representations aren't taken at the same time? (As batch norms aren't specifically used in the MoCo paper for instance)

**Ans**: *Yeah. In PIRL, the same batch doesn't have all the representations and possibly why batch norm works here, which might not be the case for other tasks where the representations are all correlated within the batch*

4. So, other than memory bank, are there any other suggestions how to go about for n-pair loss? Should we use AlexNet or others that don't use batch norm? Or is there a way to turn off the batch norm layer? (This is for a video learning task)

**Ans**: *Generally frames are correlated in videos, and the performance of the batch norm degrades when there are correlations. Also, even the simplest implementation of AlexNet actually uses batch norm. Because, it's much more stable when trained with a batch norm. You could even use a higher learning rate and you could also use for other downstream tasks. You could use a variant of batch norm for example, group norm for video learning task, as it doesn't depend on the batch size*


### Loss functions in PIRL

1. In PIRL, why is NCE (Noise Contrastive Estimator) used for minimizing loss and not just the negative probability of the data distribution: $h(v_{I},v_{I^{t}})$?

**Ans**: *Actually, both could be used. The reason for using NCE has more to do with how the memory bank paper was set up. So, with $k+1$ negatives, it's equivalent to solving $k+1$ binary problem. Another way of doing it is using a softmax, where you apply a softmax and minimize the negative log-likelihood*


### Self-supervised learning project related tips

How do we get a simple self-supervised model working? How do we begin the implementation?

**Ans**: *There are a certain class of techniques that are useful for the initial stages. For instance, you could look at the pretext tasks. Rotation is a very easy task to implement. The number of moving pieces are in general good indicator. If you're planning to implement an existing method, then you might have to take a closer look at the details mentioned by the authors, like - the exact learning rate used, the way batch norms were used, etc. The more number of these things, the harder the implementation. Next very critical thing to consider is data augmentation. If you get something working, then add more data augmentation to it.*


### Generative models

Have you thought of combining generative models with contrasting networks?

**Ans**: *Generally, it's good idea. But, it hasn't been implemented partly because it is tricky and non-trivial to train such models. Integrative approaches are harder to implement, but perhaps the way to go in the future.*


### Distillation

Wouldn't the uncertainity of the model increase when richer targets are given by softer distributions? Also, why is it called distillation?

**Ans**: *If you train on one hot labels, your models tend to be very overconfident. Tricks like label smoothing are being used in some methods. Label smoothing is just a simple version of distillation where you are trying to predict a one hot vector. Now, rather than trying to predict the entire one-hot vector, you take some probability mass out of that, where instead of predicting a one and a bunch of zeros, you predict say $0.97$ and then you add $0.01$, $0.01$ and $0.01$ to the remaining vector (uniformly). Distillation is just a more informed way of doing this. Instead of randomly increasing the probability of an unrelated task, you have a pre-trained network to do that. In general softer distributions are very useful in pre-training methods. Models tend to be over-confident and so softer distributions are easier to train. They converge faster too. These benefits are present in distillation*

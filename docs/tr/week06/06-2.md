---
lang-ref: ch.06-2
lang: tr
lecturer: Yann LeCun
title: RNN'ler, GRU'lar, LSTM'ler, Dikkat, Diziden Diziye (Seq2Seq), ve Bellek Ağları
authors: Jiayao Liu, Jialing Xu, Zhengyang Bian, Christina Dominguez
date: 2 March 2020
translator: İrem Demirtaş
translation-date: 17 June 2020
---


<!-- ## [Deep Learning Architectures](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=2620s)

In deep learning, there are different modules to realize different functions. Expertise in deep learning involves designing architectures to complete particular tasks.  Similar to writing programs with algorithms to give instructions to a computer in earlier days, deep learning reduces a complex function into a graph of functional modules (possibly dynamic), the functions of which are finalized by learning.

As with what we saw with convolutional networks, network architecture is important. -->

## [Derin Öğrenme Mimarileri](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=2620s)

Derin öğrenmede farklı fonksiyonların gerçeklenmesi için farklı modüller bulunur. Derin öğrenmede uzmanlaşmanın gerekliliklerinden biri belirli görevleri yerine getirebilmek için mimariler tasarlamaktır. Tıpkı eskilerde bilgisayarlara yönergeler vermek için algoritmaların kullanıldığı programların yazıldığı gibi, derin öğrenme karmaşık bir fonksiyonu, fonksiyonlarının öğrenme ile belirlendiği, (yüksek ihtimalle dinamik) fonksiyonel modüllerden oluşan grafiklere indirger.

Evrişimli ağlarda gördüğümüz üzere, ağ mimarisi önemlidir.

<!-- ## Recurrent Networks

In a Convolutional Neural Network, the graph or interconnections between the modules cannot have loops. There exists at least a partial order among the modules such that the inputs are available when we compute the outputs.

As shown in Figure 1, there are loops in Recurrent Neural Networks. 

<center>
<img src="{{site.baseurl}}/images/week06/06-2/RNN_rolled.png" /><br>
Figure 1. Recurrent Neural Network with roll
</center>

 - $x(t)$ : input that varies across time
 - $\text{Enc}(x(t))$: encoder that generates a representation of input
 - $h(t)$: a representation of the input
 - $w$: trainable parameters
 - $z(t-1)$: previous hidden state, which is the output of the previous time step
 - $z(t)$: current hidden state
 - $g$: function that can be a complicated neural network; one of the inputs is $z(t-1)$ which is the output of the previous time step
 - $\text{Dec}(z(t))$: decoder that generates an output -->

## Yinelemeli Ağlar

Evrişimli sinir ağlarında grafikler ya da modüller arasındaki bağlantılar döngü içeremez. Çıktıları hesaplarken girdilerin hazır olabilmesi için en azından kısmi bir sıralama bulunur. 

Fig. 1'de gösterildiği gibi, yinelemeli ağlarda döngüler bulunur.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/RNN_rolled.png" /><br>
Fig. 1. Döngü içeren bir yinelemeli ağ
</center>

 - $x(t)$ : zamana göre değişen girdi
 - $\text{Enc}(x(t))$: girdi için bir gösterim üreten kodlayıcı
 - $h(t)$: girdinin gösterimi
 - $w$: eğitilebilir parametreler
 - $z(t-1)$: önceki gizli durum
 - $z(t)$: şimdiki gizli durum
 - $g$: karmaşık bir sinir ağı olabilecek fonksiyon; girdilerinden biri bir önceki zaman adımı olabilecek olan $z(t-1)$
 - $\text{Dec}(z(t))$: çıktı üreten kodçözücü


<!-- ## Recurrent Networks: Unroll the loop

Unroll the loop in time. The input is a sequence $x_1, x_2, \cdots, x_T$. 
<center>
 "
<img src="{{site.baseurl}}/images/week06/06-2/RNN_unrolled.png" /><br>
Figure 2. Recurrent Networks with unrolled loop
</center> 

In Figure 2, the input is $x_1, x_2, x_3$.

At time t=0, the input $x(0)$ is passed to the encoder and it generates the representation $h(x(0)) = \text{Enc}(x(0))$ and then passes it to G to generate hidden state $z(0) = G(h_0, z', w)$. At $t = 0$, $z'$ in $G$ can be initialized as $0$ or randomly initialized. $z(0)$ is passed to decoder to generate an output and also to the next time step. 

As there are no loops in this network, and we can implement backpropagation. 

Figure 2 shows a regular network with one particular characteristic: every block shares the same weights. Three encoders, decoders and G functions have same weights respectively across different time steps.

BPTT: Backprop through time.  Unfortunately, BPTT doesn't work so well in the naive form of RNN.

Problems with RNNs:

1. Vanishing gradients
   - In a long sequence, the gradients get multiplied by the weight matrix (transpose) at every time step. If there are small values in the weight matrix, the norm of gradients get smaller and smaller exponentially.
2. Exploding gradients
   - If we have a large weight matrix and the non-linearity in the recurrent layer is not saturating, the gradients will explode. The weights will diverge at the update step. We may have to use a tiny learning rate for the gradient descent to work.

One reason to use RNNs is for the advantage of remembering information in the past. However, it could fail to memorize the information long ago in a simple RNN without tricks.

An example that has vanishing gradient problem:

The input is the characters from a C Program. The system will tell whether it is a syntactically correct program. A syntactically correct program should have a valid number of braces and parentheses. Thus, the network should remember how many open parentheses and braces there are to check, and whether we have closed them all. The network has to store such information in hidden states like a counter.  However, because of vanishing gradients, it will fail to preserve such information in a long program. -->


## Yinelemeli Ağlar: Döngünün açılımı

Döngüyü zaman içinde açın. Girdisi bir sıralı dizidir $x_1, x_2, \cdots, x_T$.

<center>
 "
<img src="{{site.baseurl}}/images/week06/06-2/RNN_unrolled.png" /><br>
Fig. 2. Döngüsü açılmış yinelemeli ağlar
</center>

Fig. 2'de girdi $x_1, x_2, x_3$ şeklindedir.

t=0 zamanında, $x(0)$ girdisi kodlayıcıya iletilir ve kodlayıcı $h(x(0)) = \text{Enc}(x(0))$ gösterimini üretip gizli durumu ($z(0) = G(h_0, z', w)$) üretmesi için G'ye iletir. t=0 zamanında $G$'deki $z'$ değerlerine ilk değerleri 0 olarak veya rastgele atanır.

Bu ağda döngüler bulunmadığından, geri yayılımı gerçekleştirebiliriz.

Fig. 2 belirli bir özelliğe sahip olan bir yinelemeli ağı gösteriyor: Tüm bloklar aynı ağırlıkları paylaşıyor. Üç kodlayıcı, kodçözücü ve G fonksiyonları farklı zaman adımlarında aynı ağırlıklara sahip.

BPTT: Zaman boyunca geri yayılım (backprop through time). Ne yazık ki, BPTT RNN'lerin saf formunda çok iyi çalışmaz. 

RNN'lerin problemleri:

1. Kaybolan Gradyanlar: 
   - Uzun bir sıralı dizide, her zaman adımında gradyanlar (transpoze edilmiş) bir ağırlık matrisiyle çarpılır. Eğer ağırlık matrisinde küçük değerler varsa, gradyanların normları gittikçe, üstel bir şekilde, küçülür.
2. Patlayan Gradyanlar:
   - Eğer büyük bir ağırlık matrisimiz varsa ve yinelemeli katmandaki eğrisellik *(non-linearity)* doymuyorsa, gradyanlar patlayacaktır. Her güncelleme adımında ağırlıklar ıraksayacaktır. Gradyan inişinin çalışabilmesi için çok küçük bir öğrenme hızı kullanmamız gerekebilir. 

RNN'leri kullanmak için sebeplerden biri geçmişteki bilgiyi hatırlamının sağladığı avantajdır. Ancak bazı püf noktaları uygulanmadan RNN'ler uzun zaman önceki bilgileri hatırlayamayabilir.

Kaybolan gradyan problemine bir örnek: 

Girdi bir C programından karakterlerdir. Sistem verilen programın sentaktik açıdan doğru bir program olup olmadığını söyleyecektir. Sentaktik açıdan doğru bir program geçerli sayıda parantezlere sahip olmalıdır. Bu sebeple, ağ kaç tane açık parantez olduğunu ve bunları kapatıp kapatmadığını hatırlamalıdır. Ağ bu bilgiyi bir sayaçta tutar gibi gizli durumunda tutmalıdır. Ancak uzun bir programda, kaybolan gradyanlar yüzünden, bu bilgiyi muhafaza etmekte zorlacaktır. 

<!-- ##  RNN Tricks

- clipping gradients:  (avoid exploding gradients)
   Squash the gradients when they get too large.
- Initialization (start in right ballpark avoids exploding/vanishing)
   Initialize the weight matrices to preserve the norm to some extent. For example, orthogonal initialization initializes the weight matrix as a random orthogonal matrix. -->

##  RNN'ler için Püf Noktaları 

- Gradyanları kesmek: (patlayan gradyanları önlemek için)
   Çok büyük hale geldiklerinde gradyanları kes.
- İlk değer ataması: (doğru bir değerle başlamak kaybolan/patlayan gradyanları önler)
   Normu bir dereceye kadar korumak için ağırlık matrislerine ilk değer ata. Örneğin, ortogonal ilk değer ataması değer matrisine rastgele bir ortogonal matris olacak şekilde değerler atar.

<!-- ## Multiplicative Modules

In multiplicative modules rather than only computing a weighted sum of inputs, we compute products of inputs and then compute weighted sum of that.

Suppose $x \in {R}^{n\times1}$, $W \in {R}^{m \times n}$, $U \in {R}^{m \times n \times d}$ and $z \in {R}^{d\times1}$. Here U is a tensor.

$$
w_{ij} = u_{ij}^\top z =
\begin{pmatrix}
u_{ij1} & u_{ij2} & \cdots &u_{ijd}\\
\end{pmatrix}
\begin{pmatrix}
z_1\\
z_2\\
\vdots\\
z_d\\
\end{pmatrix} = \sum_ku_{ijk}z_k
$$

$$
s =
\begin{pmatrix}
s_1\\
s_2\\
\vdots\\
s_m\\
\end{pmatrix} = Wx =  \begin{pmatrix}
w_{11} & w_{12} & \cdots &w_{1n}\\
w_{21} & w_{22} & \cdots &w_{2n}\\
\vdots\\
w_{m1} & w_{m2} & \cdots &w_{mn}
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{pmatrix}
$$

where $s_i = w_{i}^\top x = \sum_j w_{ij}x_j$.

The output of the system is a classic weighted sum of inputs and weights. Weights themselves are also weighted sums of weights and inputs.

Hypernetwork architecture: weights are computed by another network. -->

## Çarpımsal Modüller

Çarpımsal modüllerde girdilerin ağırlıklı toplamlarını hesaplamaktansa, girdilerin çarpımlarının ağırlıklı toplamları hesaplanır.

Varsayın ki $x \in {R}^{n\times1}$, $W \in {R}^{m \times n}$, $U \in {R}^{m \times n \times d}$ ve $z \in {R}^{d\times1}$. Burada U bir tensördür.


$$
w_{ij} = u_{ij}^\top z =
\begin{pmatrix}
u_{ij1} & u_{ij2} & \cdots &u_{ijd}\\
\end{pmatrix}
\begin{pmatrix}
z_1\\
z_2\\
\vdots\\
z_d\\
\end{pmatrix} = \sum_ku_{ijk}z_k
$$

$$
s =
\begin{pmatrix}
s_1\\
s_2\\
\vdots\\
s_m\\
\end{pmatrix} = Wx =  \begin{pmatrix}
w_{11} & w_{12} & \cdots &w_{1n}\\
w_{21} & w_{22} & \cdots &w_{2n}\\
\vdots\\
w_{m1} & w_{m2} & \cdots &w_{mn}
\end{pmatrix}
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{pmatrix}
$$

, $s_i = w_{i}^\top x = \sum_j w_{ij}x_j$.

Sistemin çıktısı girdilerin ve ağırlıkların klasik ağırlıklı toplamıdır. Ağırlıklar da girdilerin ve ağırlıkların ağırlıklı toplamıdır.

Hiperağ mimarisi: Ağırlıklar başka bir ağ tarafından hesaplanır.

<!-- ## Attention

$x_1$ and $x_2$ are vectors, $w_1$ and $w_2$ are scalars after softmax where $w_1 + w_2 = 1$, and  $w_1$ and $w_2$ are between 0 and 1.

$w_1x_1 + w_2x_2$ is a weighted sum of $x_1$ and $x_2$ weighted by coefficients $w_1$ and $w_2$.

By changing the relative size of $w_1$ and $w_2$, we can switch the output of $w_1x_1 + w_2x_2$ to $x_1$ or $x_2$ or some linear combinations of $x_1$ and $x_2$.

The inputs can have multiple $x$ vectors (more than $x_1$ and $x_2$). The system will choose an appropriate combination, the choice of which is determined by another variable z. An attention mechanism allows the neural network to focus its attention on particular input(s) and ignore the others.

Attention is increasingly important in NLP systems that use transformer architectures or other types of attention.

The weights are data independent because z is data independent. -->

## Dikkat

$x_1$ ve $x_2$ vektördür, $w_1$ ve $w_2$ softmax'ten sonraki skalerlerdir ve $w_1 + w_2 = 1$ denklemini sağlar, $w_1$ ve $w_2$ aynı zamanda 0 ve 1 arasındadır.

$w_1x_1 + w_2x_2$, i $x_1$ ve $x_2$'nin  $w_1$ ve $w_2$ katsayıları tarafından ağırlıklarındırılmış ağırlıklı toplamıdır.

$w_1$ and $w_2$'nin göreceli büyüklüğünü değiştirerek, w_1x_1 + w_2x_2$'nin çıktısını $x_1$ ya da $x_2$'ye veya $x_1$ ve $x_2$'nin herhangi bir lineer kombinasyonuna dönüştürebiliriz.

Girdiler birden fazla $x$ vektörü içerebilir ($x_1$ ve $x_2$'den daha fazla). Sistem bunların uygun bir kombinasyonunu seçecektir ve bu seçim başka bir parametre olan z tarafından belirlenecektir. Bir dikkat mekanizması sinir ağının dikatinin belirli girdilere odaklanırken diğerlerini yoksaymasını sağlar. 

Dikkat NLP dönüştürücü *(transformer)* mimarileri ya da diğer dikkat tiplerini kullanan NLP (Natural Language Processing, Doğal Dil İşleme) sistemlerinde giderek daha da önemli hale gelmektedir.

Ağırlıklar veriye bağlı değildir, çünkü z veriye bağlı değildir.


<!-- ## [Gated Recurrent Units (GRU)](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=3549s)

As mentioned above, RNN suffers from vanishing/exploding gradients and can’t remember states for very long. GRU, [Cho, 2014](https://arxiv.org/abs/1406.1078), is an application of multiplicative modules that attempts to solve these problems. It's an example of recurrent net with memory (another is LSTM). The structure of A GRU unit is shown below:

<center>
<img src="{{site.baseurl}}/images/week06/06-2/GRU.png" height="300px" style="background-color:#226;"/><br>
Figure 3. Gated Recurrent Unit
</center>

$$
\begin{array}{l}
z_t = \sigma_g(W_zx_t + U_zh_{t-1} + b_z)\\
r_t = \sigma_g(W_rx_t + U_rh_{t-1} + b_r)\\
h_t = z_t\odot h_{t-1} + (1- z_t)\odot\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)
\end{array}
$$

where $\odot$ denotes element-wise multiplication(Hadamard product), $x_t$ is the input vector, $h_t$ is the output vector, $z_t$ is the update gate vector, $r_t$ is the reset gate vector, $\phi_h$ is a hyperbolic tanh, and $W$,$U$,$b$ are learnable parameters.

To be specific, $z_t$ is a gating vector that determines how much of the past information should be passed along to the future. It applies a sigmoid function to the sum of two linear layers and a bias over the input $x_t$ and the previous state $h_{t-1}$.  $z_t$ contains coefficients between 0 and 1 as a result of applying sigmoid. The final output state $h_t$ is a convex combination of $h_{t-1}$ and $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$ via $z_t$. If the coefficient is 1, the current unit output is just a copy of the previous state and ignores the input (which is the default behaviour). If it is less than one, then it takes into account some new information from the input.

The reset gate $r_t$ is used to decide how much of the past information to forget. In the new memory content $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$, if the coefficient in $r_t$ is 0, then it stores none of the information from the past. If at the same time $z_t$ is 0, then the system is completely reset since $h_t$ would only look at the input. -->

## [Gated Recurrent Units, Kapılı Yinelemeli Üniteler (GRU)](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=3549s)

Yukarıda belirtildiği üzere, RNN kaybolan/patlayan gradyanlar sorunları yaşar ve durumları uzun süre hatırlayamaz. GRU, [Cho, 2014](https://arxiv.org/abs/1406.1078), bu sorunları çözmeyi deneyen çarpımsal modellerin bir uygulamasıdır. GRU bellekli yinelemeli ağların bir örneğidir (bir diğeri LSTM'dir). Bir GRU ünitesinin yapısı aşağıda görülebilir: 

<center>
<img src="{{site.baseurl}}/images/week06/06-2/GRU.png" height="300px" style="background-color:#226;"/><br>
Fig. 3. Kapılı Yinelemeli Ünite [GRU]
</center>

$$
\begin{array}{l}
z_t = \sigma_g(W_zx_t + U_zh_{t-1} + b_z)\\
r_t = \sigma_g(W_rx_t + U_rh_{t-1} + b_r)\\
h_t = z_t\odot h_{t-1} + (1- z_t)\odot\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)
\end{array}
$$

burada $\odot$ Hadamard çarpımını, $x_t$ girdi vektörünü, gösterir, $h_t$ çıktı vektörünü, $z_t$ güncelleme kapısı vektörünü, $r_t$ sıfırlama kapısı vektörünü, $\phi_h$ hiperbolik tanh'yi, $W$,$U$,$b$ öğrenilebilir parametreleri gösterir.

Spesifik olmak gerekirse, $z_t$ geçmiş bilginin ne kadarının geleceğe aktarılması gerektiğini belirleyen bir kapı vektörüdür. İki lineer katmanın toplamına sigmoid fonksiyonunu, $x_t$ girdisine ve geçmiş $h_{t-1}$ durumuna yanlılık uygular. Sigmoidin uygulanmasının bir sonucu olarak $z_t$ 0 ve 1 arasında katsayılar içerir. Son çıktı durumu $h_t$, $h_{t-1}$ ve $z_t$ ile elde edilmiş olan $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$  ifadesinin lineer bir kombinasyonudur. Katsayı 1'se, şimdiki birimin çıktısı önceki durumun bir kopyasıdır ve (varsayılan davranış olarak) görmezlikten gelinir. 1'den daha azsa, girdiden gelen yeni bilginin bir kısmı dikkate alınır.

Sıfırlama kapısı $r_t$ geçmiş bilginin ne kadarının unutulması gerektiğini belirler. Yeni bellek içeriğinde, $\phi_h(W_hx_t + U_h(r_t\odot h_{t-1}) + b_h)$, $r_t$ katsayısı 0'sa geçmişten gelen bilgi kullanılmaz. Aynı zamanda $z_t$'de 0'sa, $h_t$ sadece girdiye bakacağından tüm sistem tamamen sıfırlanır.

<!-- ## LSTM (Long Short-Term Memory)

GRU is actually a simplified version of LSTM which came out much earlier, [Hochreiter, Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf). By building up memory cells to preserve past information, LSTMs also aim to solve long term memory loss issues in RNNs. The structure of LSTMs is shown below:

<center>
<img src="{{site.baseurl}}/images/week06/06-2/LSTM.png" height="300px"/><br>
Figure 4. LSTM
</center>

$$
\begin{array}{l}
f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f)\\
i_t = \sigma_g(W_ix_t + U_ih_{t-1} + b_i)\\
o_t = \sigma_o(W_ox_t + U_oh_{t-1} + b_o)\\
c_t = f_t\odot c_{t-1} + i_t\odot \tanh(W_cx_t + U_ch_{t-1} + b_c)\\
h_t = o_t \odot\tanh(c_t)
\end{array}
$$

where $\odot$ denotes element-wise multiplication, $x_t\in\mathbb{R}^a$ is an input vector to the LSTM unit, $f_t\in\mathbb{R}^h$ is the forget gate's activation vector, $i_t\in\mathbb{R}^h$ is the input/update gate's activation vector, $o_t\in\mathbb{R}^h$ is the output gate's activation vector, $h_t\in\mathbb{R}^h$ is the hidden state vector (also known as output), $c_t\in\mathbb{R}^h$ is the cell state vector.

An LSTM unit uses a cell state $c_t$ to convey the information through the unit. It regulates how information is preserved or removed from the cell state through structures called gates. The forget gate $f_t$ decides how much information we want to keep from the previous cell state $c_{t-1}$ by looking at the current input and previous hidden state, and produces a number between 0 and 1 as the coefficient of $c_{t-1}$.  $\tanh(W_cx_t + U_ch_{t-1} + b_c)$ computes a new candidate to update the cell state, and like the forget gate, the input gate $i_t$ decides how much of the update to be applied. Finally, the output $h_t$ will be based on the cell state $c_t$, but will be put through a $\tanh$ then filtered by the output gate $o_t$.

Though LSTMs are widely used in NLP, their popularity is decreasing. For example, speech recognition is moving towards using temporal CNN, and NLP is moving towards using transformers. -->

## LSTM (Long Short-Term Memory, Uzun Ömürlü Kısa-Dönem Belleği)

GRU aslında kendisinden çok önce çıkmış olan LSTM'in basitleştirilmiş bir halidir [Hochreiter, Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf). LSTM'ler geçmiş bilgiyi koruyan bellek hücreleri oluşturarak, aynı zamanda RNN'lerde görünen uzun süreli bellek kayıplarını da çözmeyi hedefler. LSTM'lerin yapısı aşağıda gösterilmiştir.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/LSTM.png" height="300px"/><br>
Fig. 4. LSTM
</center>

$$
\begin{array}{l}
f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f)\\
i_t = \sigma_g(W_ix_t + U_ih_{t-1} + b_i)\\
o_t = \sigma_o(W_ox_t + U_oh_{t-1} + b_o)\\
c_t = f_t\odot c_{t-1} + i_t\odot \tanh(W_cx_t + U_ch_{t-1} + b_c)\\
h_t = o_t \odot\tanh(c_t)
\end{array}
$$

burada $\odot$ Hadamard çarpımını, $x_t\in\mathbb{R}^a$ LSTM biriminin girdilerinden birini, $f_t\in\mathbb{R}^h$ unutma kapısının aktivasyon vektörünü, $i_t\in\mathbb{R}^h$ girdi/güncelleme kapısının aktivasyon vektörünü, $o_t\in\mathbb{R}^h$ çıktı kapısının aktivasyon vektörünü, $o_t\in\mathbb{R}^h$ gizli durum vektörünü (ve aynı zamanda o birimin çıktısını), $c_t\in\mathbb{R}^h$ ise hücrenin durum vektörünü gösterir.


Bir LSTM ünitesi ünite içerisinde bilgiyi iletmek için hücre durumu $c_t$'yi kullanır. Kapı isimli yapıları kullanarak bilginin hücre durumunda korunup korunmayacağını düzenler. Unutma kapısı şimdiki girdi ve önceki gizli duruma bakarak $f_t$ bir önceki hücre durumu $c_{t-1}$'den ne kadar bilgi tutmak istediğimize karar verir ve $c_{t-1}$. katsayısı ile bunu 0 ve 1 arasında bir değerle ifade eder. $\tanh(W_cx_t + U_ch_{t-1} + b_c)$ hücre durumunu güncellemek için bir aday hesaplar, ve unutma kapısında olduğu gibi, girdi kapısı $i_t$ ne kadar güncelleme uygulanacağını belirler. Son olarak, çıktı $h_t$, hücre durumu $c_t$ temel alınarak hesaplanır, ancak sonrasında $\tanh$'tan geçirilip çıktı kapısı $o_t$ tarafından filtrelenir.  

Her ne kadar LSTM'ler NLP'de sıkça kullanılıyor olsalar da, popülerlikleri azalmaktadır. Örneğin, konuşma tanıma zamansal CNN'lere, NLP ise dönüştürücülere (transformers) yönelmektedir.

<!-- ## Sequence to Sequence Model

The approach proposed by [Sutskever NIPS 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) is the first neural machine translation system to have comparable performance to classic approaches. It uses an encoder-decoder architecture where both the encoder and decoder are multi-layered LSTMs.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/Seq2Seq.png" height="300px" /><br>
Figure 5. Seq2Seq
</center>

Each cell in the figure is an LSTM. For the encoder (the part on the left), the number of time steps equals the length of the sentence to be translated. At each step, there is a stack of LSTMs (four layers in the paper) where the hidden state of the previous LSTM is fed into the next one. The last layer of the last time step outputs a vector that represents the meaning of the entire sentence, which is then fed into another multi-layer LSTM (the decoder), that produces words in the target language. In the decoder, the text is generated in a sequential fashion. Each step produces one word, which is fed as an input to the next time step.

This architecture is not satisfying in two ways: First, the entire meaning of the sentence has to be squeezed into the hidden state between the encoder and decoder. Second, LSTMs actually do not preserve information for more than about 20 words. The fix for these issues is called a Bi-LSTM, which runs two LSTMs in opposite directions.  In a Bi-LSTM the meaning is encoded in two vectors, one generated by running LSTM from left to right, and another from right to left.  This allows doubling the length of the sentence without losing too much information. -->

## Sequence to Sequence (Diziden Diziye) Modeli 

[Sutskever NIPS 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) tarafından önerilen yaklaşım klasik yöntemlerle karşılaştırılabilecek performansa sahip olan ilk sinirsel makine tercüme sistemidir. Bu yöntem bir hem kodlayıcının hem de kodçözücünün çok katmanlı LSTM'lerden oluştuğu bir kodlayıcı-kodçözücü mimarisine sahiptir.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/Seq2Seq.png" height="300px" /><br>
Fig 5. Seq2Seq
</center>

Fig. 5'teki tüm hücreler birer LSTM'dir. Kodlayıcı için (sol taraftaki kısım), zaman adımı sayısı çevirilecek cümlenin uzunluğuna eşittir. Her zaman adımında, bir önceki LSTM'in gizli durumunun bir sonrakine iletildiği LSTM yığınları bulunur (yayına göre dört katman). Son zaman adımındaki son katman tüm cümlenin anlamını temsil eden bir vektör oluşturur ve hedef dildeki kelimeleri üreten bir başka çok katmanlı LSTM'e (kodçözücü) iletilir. Kodçözücüde metin ardışık bir şekilde üretilir. Her bir zaman adımında bir kelime üretilir ve bir sonraki adıma girdi olarak verilir. 

Bu mimari iki açıdan tatmin edici değildir: İlk sebep, cümlenin tüm anlamının kodlayıcı ve kodçözücü arasındaki gizli durum vektörüne sıkıştırılmak zorunda olmasıdır. İkinci sebepse, LSTM'lerin yaklaşık olarak 20 kelimeden fazla bilgiyi tutmakta zorlanmalarıdır. Bu sorunlar ters yönlerde iki LSTM kullanan Bi-LSTM'le çözülebilir. Bi-LSTM'de anlam biri soldan sağa, diğeri sağdan sola çalışan iki LSTM tarafından oluşturulan iki vektöre kodlanabilir. Bu yöntem çok bilgi kaybetmeden cümlenin uzunluğunun iki katına çıkarılabilmesini sağlar. 

<!-- ## Seq2seq with Attention

The success of the approach above was short-lived. Another paper by [Bahdanau, Cho, Bengio](https://arxiv.org/abs/1409.0473)  suggested that instead of having a gigantic network that squeezes the meaning of the entire sentence into one vector, it would make more sense if at every time step we only focus the attention on the relevant locations in the original language with equivalent meaning, i.e. the attention mechanism.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/Seq2SeqwAttention.png" height="300px" /><br>
Figure 6. Seq2Seq with Attention
</center>

In Attention, to produce the current word at each time step, we first need to decide which hidden representations of words in the input sentence to focus on. Essentially, a network will learn to score how well each encoded input matches the current output of the decoder. These scores are normalized by a softmax, then the coefficients are used to compute a weighted sum of the hidden states in the encoder at different time steps. By adjusting the weights, the system can adjust the area of inputs to focus on. The magic of this mechanism is that the network used to compute the coefficients can be trained through backpropagation. There is no need to build them by hand!

Attention mechanisms completely transformed neural machine translation. Later, Google published a paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762), and they put forward transformer, where each layer and group of neurons is implementing attention. -->

## Dikkatli Seq2seq 

Yukarıda bahsedilen yöntemin başarısı kısa sürdü. [Bahdanau, Cho ve Bengio](https://arxiv.org/abs/1409.0473)'nun yayınında tüm cümlenin anlamını bir vektöre sıkıştıran devasa bir network yerine her adımda dikkati orijinal dildeki denk anlamlara sahip alakalı yerlere yoğunlaştırmanın daha manalı olduğu gösterildi (yani dikkat mekanizması). 

<center>
<img src="{{site.baseurl}}/images/week06/06-2/Seq2SeqwAttention.png" height="300px" /><br>
Fig 6. Dikkatli Seq2Seq
</center>

Dikkatte, her bir adımda şimdiki kelimeyi üretmek için öncelikle girdi cümlesindeki hangi kelimelerin gösterimlerine odaklanmamız gerektiğine karar vermemiz gerekir. Aslında, ağımız hangi kodlanmış girdinin, kodlayıcının şu anki çıktısıyla ne kadar eşleştiğini gösteren bir skor hesaplamayı öğrenir. Bu skorlar bir softmax ile normalize edilir ve sonrasında katsayılar kodlayıcıdaki farklı zaman adımlarının gizli durumlarının ağırlıklı toplamlarını hesaplamak için kullanılır. Sistem ağırlıkları ayarlayarak dikkat edilecek alanı ayarlamış olur. Bu mekanizmanın sihri katsayıları hesaplamak için kullanılan ağın geri yayılımla eğitilebilmesidir. Bu mekanizmaları elle oluşturmaya gerek yoktur! 

Dikkat mekanizmaları sinirsel makine tercümesini baştan aşağı değiştirdi. Sonrasında, Google bir yayınla, [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762), nöron gruplarının dikkati gerçeklediği dönüştürücüyü (transformer) öne sürdü.

<!-- ## [Memory network](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=4575s)

Memory networks stem from work at Facebook that was started by [Antoine Bordes](https://arxiv.org/abs/1410.3916) in 2014 and [Sainbayar Sukhbaatar](https://arxiv.org/abs/1503.08895) in 2015.

The idea of a memory network is that there are two important parts in your brain: one is the **cortex**, which is where you have long term memory. There is a separate chunk of neurons called the **hippocampus** which sends wires to nearly everywhere in the cortex. The hippocampus is thought to be used for short term memory, remembering things for a relatively short period of time. The prevalent theory is that when you sleep, there is a lot of information transferred from the hippocampus to the cortex to be solidified in long term memory since the hippocampus has limited capacity.

For a memory network, there is an input to the network, $x$ (think of it as an address of the memory), and compare this $x$ with vectors $k_1, k_2, k_3, \cdots$ ("keys") through a dot product. Put them through a softmax, what you get are an array of numbers which sum to one. And there are a set of other vectors $v_1, v_2, v_3, \cdots$ ("values"). Multiply these vectors by the scalers from softmax and sum these vectors up (note the resemblance to the attention mechanism) gives you the result.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork1.png" height="300px"/><br>
Figure 7. Memory Network
</center>

If one of the keys (e.g. $k_i$) exactly matches $x$, then the coefficient associated with this key will be very close to one. So the output of the system will essentially be $v_i$.

This is **addressable associative memory**. Associative memory is that if your input matches a key, you get *that* value. And this is just a soft differentiable version of it, which allows you to backpropagate and change the vectors through gradient descent.

What the authors did was tell a story to a system by giving it a sequence of sentences. The sentences are encoded into vectors by running them through a neural net that has not been pretrained. The sentences are returned to the memory of this type. When you ask a question to the system, you encode the question and put it as the input of a neural net, the neural net produces an $x$ to the memory, and the memory returns a value.

This value, together with the previous state of the network, is used to re-access the memory. And you train this entire network to produce an answer to your question. After extensive training, this model actually learns to store stories and answer questions.

$$
\alpha_i = k_i^\top x \\
c = \text{softmax}(\alpha) \\
s = \sum_i c_i v_i
$$

In memory network, there is a neural net that takes an input and then produces an address for the memory, gets the value back to the network, keeps going, and eventually produces an output. This is very much like computer since there is a CPU and an external memory to read and write.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork2.png" height="200px" />
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork3.png" height="200px" /> <br>

Figure 8. Comparision between memory network and computer (Photo by <a href='https://www.khanacademy.org/computing/ap-computer-science-principles/computers-101/computer--components/a/computer-memory'>Khan Acadamy</a>)
</center>

There are people who imagine that you can actually build **differentiable computers** out of this. One example is the [Neural Turing Machine](https://arxiv.org/abs/1410.5401) from DeepMind, which was made public three days after Facebook's paper was published on arXiv.

The idea is to compare inputs to keys, generate coefficients, and produce values - which is basically what a transformer is.  A transformer is basically a neural net in which every group of neurons is one of these networks. -->

## [Bellek Ağı](https://www.youtube.com/watch?v=ycbMGyCPzvE&t=4575s)

Bellek ağları Facebook'ta [Antoine Bordes](https://arxiv.org/abs/1410.3916) tarafından 2014'te ve [Sainbayar Sukhbaatar](https://arxiv.org/abs/1503.08895) tarafından 2015'te başlatılan çalışmalara dayanır. 

Bellek ağının arkasındaki fikir beyninizde iki önemli bölümün bulunmasıdır: biri uzun dönemli belleğin bulunduğu **korteks**tir. **Hipokampus** adında bir ayrı bir nöron yığını neredeyse korteksteki her yerle bağlantı kurar. Hipokampusun kısa süreli hafıza için kullanıldığı düşünülmektedir, buradaki şeyler görece kısa bir süre hatırlanır. Yaygın teoriye göre uyuduğunuzda büyük miktarda bilgi, hipokampusun kapasitesi sınırlı olduğundan, uzun süreli hafızada pekiştirilmek üzere hipokampustan kortekse transfer edilir.  

Bir bellek ağı için, (belleğin bir adresi olarak düşünülebilecek) bir $x$ girdisi vardır ve bu $x$ $k_1, k_2, k_3, \cdots$ ("anahtarlar") vektörleriyle skaler çarpım kullanılarak karşılaştırılır. Softmax'ten geçirildiğinde toplamları bire eşit olan bir sayı dizisi elde edilir. Bir diğer vektör kümesi de $v_1, v_2, v_3, \cdots$'dir ("değerler"). Bu vektörlerin softmax'ten gelen vektörlerle çarpılıp toplanması (dikkate olan benzerliğe dikkat edin) sonucu verir.  

<center>
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork1.png" height="300px"/><br>
Fig 7. Bellek ağı
</center>

Eğer anahtarlardan biri ((mesela $k_i$)) $x$ ile tam olarak eşleşirse, bu anahtarla ilişkili katsayı bire çok yakın olacaktır. Yani, sistemin çıktısı esasen $v_i$ olacaktır.

Bu adreslenebilir ilişkisel bellektir ("addressable associative memory"). İlişkisel bellekte eğer girdiniz bir anahtarla eşleşirse *o* değeri alırsınız. Gördüğümüz hali bunun yumuşak, türevlenebilir halidir, bu şekliye geri yayılım uygulayabilmenizi ve vektörleri gradyan inişiyle değiştirebilmenizi sağlar.

Yazarların yaptığı şey sisteme bir cümle dizisi vererek bir hikaye anlatmaktı. Cümleler önceden eğitilmemiş bir sinir ağı kullanılarak vektörlere kodlanmıştı ve bu şekilde belleğe gönderilmişti. Bu sisteme bir soru sorduğunuzda, soruyu kodlayarak sinir ağına girdi olarak verirsiniz, ağ belleğe bir $x$ üretir ve bellek bir değer döndürür.

Bu değer, ağın önceki durumuyla birlikte, belleğe tekrar erişmek için kullanılır. Tüm ağı sorunuza bir cevap üretmesi için eğitirsiniz. Kapsamlı bir eğitimden sonra model hikayeleri hafızada tutmayı ve soruları cevaplamayı öğrenir. 

$$
\alpha_i = k_i^\top x \\
c = \text{softmax}(\alpha) \\
s = \sum_i c_i v_i
$$

Bellek ağında, bir girdi alıp bellek içinde bir adres üretip değeri ağa geri getiren ve devam ederek sonunda bir çıktı üreten bir ağ bulunur. Bu bir bilgisayara çok benzer çünkü bilgisayarda da bir işlemci ve yazma ve okuma işlemlerinin gerçekleştirildiği bir harici bellek bulunur.

<center>
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork2.png" height="200px" />
<img src="{{site.baseurl}}/images/week06/06-2/MemoryNetwork3.png" height="200px" /> <br>

Fig 8. Bellek ağı ve bilgisayarın karşılaştırılması (Resim kaynağı: <a href='https://www.khanacademy.org/computing/ap-computer-science-principles/computers-101/computer--components/a/computer-memory'>Khan Academy</a>)
</center>

Bunu kullanarak **türevlenebilir bilgisayarlar** yapılabileceğini düşünen insanlar var. Bir örnek, Facebook'un yayını arXiv'da yayınlandıktan üç gün sonra tanıtılan, DeepMind'ın [Neural Turing Machine](https://arxiv.org/abs/1410.5401) 'idir.

Buradaki fikir girdileri anahtarlarla karşılaştırmak, katsayılar üretmek ve değerler oluşturmaktır - tam da dönüştürücülerin yaptığı gibi! Dönüştürücü, en basit tabirle, her nöron grubunun bu ağlardan biri olduğu bir ağdır.

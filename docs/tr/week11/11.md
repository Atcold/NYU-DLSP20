---
lang: tr
lang-ref: ch.11
title: Hafta 11
translation-date: 12 Jun 2020
translator: Murat Ekici
---


## Ders bölümü A

Bu bölümde, Pytorch'taki yaygın aktivasyon fonksiyonlarını ele alıyoruz. Özellikle dolanık ve düz aktivasyon fonksiyonlarını karşılaştıracağız. Daha sonra Pytorch'daki yaygın kayıp fonksiyonları hakkında bilgi edineceğiz.

<!--

## Lecture part A

In this section, we discussed about the common activation functions in Pytorch. In particular, we compared activations with kink(s) versus smooth activations - the former is preferred in a deep neural network as the latter might suffer with gradient vanishing problem. We then learned about the common loss functions in Pytorch.
-->

## Ders bölümü B

Bu bölümde, kayıp fonksiyonları - özellikle marj bazlı kayıplar ve bunların uygulamaları hakkında bilgi edinmeye devam edeceğiz. Daha sonra, EBM'ler için iyi bir kayıp fonksiyonunun nasıl tasarlanacağının yanı sıra iyi bilinen EBM kayıp fonksiyonlarının örneklerini inceleyeceğiz. 

<!--
## Lecture part B

In this section, we continued to learn about loss functions - in particular, margin-based losses and their applications. We then discussed how to design a good loss function for EBMs as well as examples of well-known EBM loss functions. We gave particular attention to margin-based loss function here, as well as explaining the idea of "most offending incorrect answer.
-->

## Uygulama

Bu uygulama yoğun trafikte araç kullanmak için etkili bir politika öğrenimi konu almaktadır. Farklı maliyet işlevlerini optimize ederek gerçek dünya dinamiklerinin öğrenilmiş bir modelini ortaya çıkararak birden çok politikanın eğitilmesini kapsamaktadır. Buradaki fikir, modelin eğitim verildiği durumlardan sapmasını temsil eden bir maliyet terimi kullanarak modelin tahminindeki belirsizliği en aza indirmektir.


<!--
## Practicum

This practicum proposed effective policy learning for driving in dense traffic. We trained multiple policies by unrolling a learned model of the real world dynamics by optimizing different cost functions. The idea is to minimize the uncertainty in the model's prediction by introducing a cost term that represents the model's divergence from the states it is trained on.  
-->

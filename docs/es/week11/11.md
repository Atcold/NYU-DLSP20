---
lang-ref: ch.11
lang: es
title: Semana 11
date: 6 Sep 2020
translator: juliotorrest
---

<!--## Lecture part A
-->

## Clase parte A

<!--In this section, we discussed about the common activation functions in Pytorch. In particular, we compared activations with kink(s) versus smooth activations - the former is preferred in a deep neural network as the latter might suffer with gradient vanishing problem. We then learned about the common loss functions in Pytorch.
-->

En esta sección, discutimos sobre las funciones de activación comunes en Pytorch. En particular, comparamos activaciones con punto(s) singular(es) versus activaciones suaves; en una red neuronal profunda se prefiere el primer tipo, ya que el segundo caso podría generar un problema de desvanecimiento del gradiente. Luego aprendemos sobre las funciones de pérdida comunes en Pytorch.

<!--## Lecture part B
-->

## Clase parte B

<!--In this section, we continued to learn about loss functions - in particular, margin-based losses and their applications. We then discussed how to design a good loss function for EBMs as well as examples of well-known EBM loss functions. We gave particular attention to margin-based loss function here, as well as explaining the idea of "most offending incorrect answer.
-->

En esta sección, continuamos aprendiendo sobre las funciones de pérdida, en particular, las pérdidas basadas en márgenes y sus aplicaciones. Luego discutimos cómo diseñar una buena función de pérdida para MBE, así como ejemplos de funciones de pérdida de MBE conocidas. Prestamos especial atención a la función de pérdida basada en márgenes, así como también explicamos la idea de "la respuesta incorrecta más ofensiva".

<!--## Practicum
-->

## Práctica

<!--This practicum proposed effective policy learning for driving in dense traffic. We trained multiple policies by unrolling a learned model of the real world dynamics by optimizing different cost functions. The idea is to minimize the uncertainty in the model's prediction by introducing a cost term that represents the model's divergence from the states it is trained on.
-->

Esta práctica propone el aprendizaje de políticas efectivas para conducir en tráfico pesado. Entrenamos múltiples políticas al desarrollar un modelo ya aprendido, de la dinámica del mundo real, optimizando diferentes funciones de costo. La idea es minimizar la incertidumbre en la predicción del modelo al introducir un término de costo que represente la divergencia del modelo con respecto a los estados en los que se entrena.


---
lang-ref: ch.12
lang: es
title: Semana 12
date: 14 Sep 2020
translator: je-santos
---

## Lección parte A

<!--
In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively.
-->

En esta sección presentaremos las distintas arquitecturas que se utilizan en procesamiento del lenguaje natural (NLP por sus siglas en inglés) comenzando con CNNs, RNNs, hasta llegar a los transformadores (los cuales representan las arquitecturas con mejor desempeño hasta la fecha). Consecuentemente presentaremos los distintos módulos que conforman los transformadores y las ventajas de cada uno. Para concluir, hablaremos de técnicas utilizadas para entrenar eficientemente a los transformadores.

## Lección parte B

<!--
In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (*i.e.* when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT.
-->

En esta sección presentaremos la búsqueda de haz (beam search) como un punto medio entre el decodificamiento codicioso y la búsqueda exhaustiva. Consideraremos el caso del muestreo de una distribución generativa (por ejemplo, al generar texto) e introduciremos el muestreo "top-k". Seguido, introduciremos los modelos de secuencia a secuencia (con una variante de trasformador) y traslación hacia atrás. Finalmente presentaremos técnicas entrenamiento no supervisado para aprender incrustaciones (embeddings) y discutiremos word2vec, GPT, y BERT.


## Práctica
<!--
We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures.
-->

En esta sección presentaremos los mecanismos de atención, principalmente la autoatención y la representación en las capas ocultas de los datos de entrada. Seguido de una introducción al paradigma de almacenamiento "llave-valor" y discutiremos cómo representar consultas, llaves y valores en función de rotaciones de los datos de entrada. Finalmente, utilizaremos atención para interpretar la arquitectura de los transformadores, realizando un pase hacia delante por las capas de un transformador básico, y comparándolo con las arquitecturas de encodificador-decodificador. 

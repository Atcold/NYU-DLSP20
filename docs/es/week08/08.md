---
lang-ref: ch.08
lang: es
title: Week 8
date: 8 Aug 2020
translator: LecJackS
---

<!--## Lecture part A
-->

## Clase parte A

<!--In this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence.
-->

En esta sección, nos enfocamos en la introducción de métodos contrastivos en Modelos Basados en Energía en varios aspectos. Primero, discutimos la ventaja que trae la aplicación de métodos contrastivos en el aprendizaje auto-supervisado. En segundo lugar, discutimos la arquitectura de *denoising autoencoders* y su debilidad en las tareas de reconstrucción de imágenes. También hablamos de otros métodos contrastivos, como la divergencia contrastiva y la divergencia contrastiva persistente.

<!--## Lecture part B
-->
## Clase parte B

<!--In this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved.
-->

En esta sección, discutimos en detalle las EBMs de variables latentes regularizadas, cubriendo conceptos de versiones condicionales e incondicionales de estos modelos. Luego discutimos los algoritmos de ISTA, FISTA y LISTA y miramos ejemplos de codificación dispersa y filtros aprendidos de codificadores dispersos convolucionales. Finalmente hablamos sobre Codificadores Automáticos Variacionales (*Variational Auto-Encoders*) y los conceptos subyacentes involucrados.

<!--## Practicum
-->

## Práctica

<!--In this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples.
-->

En esta sección, discutimos un tipo específico de modelo generativo llamado Autoencoders Variacionales y comparamos sus funcionalidades y ventajas sobre los Autoencoders Clásicos. Exploramos la función objetiva de VAE en detalle, entendiendo cómo impone alguna estructura en el espacio latente. Finalmente, implementamos y entrenamos un VAE en el conjunto de datos MNIST y lo usamos para generar nuevas muestras.

---
lang-ref: ch.13-3
title: Graph Convolutional Networks III
lecturer: Alfredo Canziani
authors: Go Inoue, Muhammad Osama Khan, Muhammad Shujaat Mirza, Muhammad Muneeb Afzal
date: 28 Apr 2020
translation-date: 16 Sept 2020
translator: Joaquim Castilla
---

## [Introducción a las Redes Convolucionales de Grafo(GCN)](https://www.youtube.com/watch?v=2aKXWqkbpWg&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=25&t=47s)

Las Redes Convolucionales de Grafo (GCN) son un tipo de arquitecture que utilizan la estructura de los datos.
Antes de entrar en detalles, hagamos una revisión rápida sobre la auto-atención, dado que las GCN y la auto-atención son conceptualmente relevantes.

### Revisión: Auto-atención

- En auto-atención, tenemos un conjunto de entradas $\lbrace\boldsymbol{x}\_{i}\rbrace^{t}_{i=1}$.
A diferencia de las secuencias, este conjunto no tiene un orden.
- El vector oculto $\boldsymbol{h}$ viene dado por una combinación linear de los vectores en el conjunto.
- Esto lo podemos expresar como $\boldsymbol{X}\boldsymbol{a}$ utilizando una multiplicación matriz vector, donde $\boldsymbol{a}$ contiene los coeficientes que escalan el vector de entrada $\boldsymbol{x}_{i}$.

*Para una explicación detallada, consulta las notas de la [Semana 12]({{site.baseurl}}/es/week12/12-3/).*

### Notación

<center>
<img src="{{site.baseurl}}/images/week13/13-3/figure1.png" height="400px" /><br>
<b>Fig. 1</b>: Red Convolucional de Grafo
</center>

En la Figura 1, el vértice $v$ está formado por dos vectores:
el vector de entrada $\boldsymbol{x}$ y su representación oculta $\boldsymbol{h}$.
Tenemos también múltiples vértices $v_{j}$, que están formados por $\boldsymbol{x}_{j}$ y $\boldsymbol{h}_{j}$.
En este grafo, los vértices están conectados por ejes dirigidos.

Representamos éstos ejes dirigidos con un vector de adyacencia $\boldsymbol{a}$, donde cada elemento $\alpha_{j}$ tiene asignado un $1$ si hay eje de $v_{j}$ a $v$.

$$
\alpha_{j} \stackrel{\tiny \downarrow}{=} 1 \Leftrightarrow v_{j} \rightarrow v
\tag{Ec. 1}
$$

El grado (número de ejes entrantes) $d$ se define como la norma de éste vector de adyacencia, es decir
$\Vert\boldsymbol{a}\Vert_{1} $, que es el número de unos en el vector $\boldsymbol{a}$.

$$
    d = \Vert\boldsymbol{a}\Vert_{1}
\tag{Ec. 2}
$$

El vector oculto $\boldsymbol{h}$ se determina mediante la siguiente expresión:

$$
    \boldsymbol{h}=f(\boldsymbol{U}\boldsymbol{x} + \boldsymbol{V}\boldsymbol{X}\boldsymbol{a}d^{-1})
\tag{Ec. 3}
$$

donde $f(\cdot)$ es una función no lineal como ReLU $(\cdot)^{+}$, Sigmoidal $\sigma(\cdot)$, y la tangente hiperbólica $\tanh(\cdot)$.

El término $\boldsymbol{U}\boldsymbol{x}$ tiene en cuenta al vértice $v$ en si mismo, aplicando la rotación $\boldsymbol{U}$ a la entrada $v$.

Recuerda que en auto-atención, el vector oculto $\boldsymbol{h}$ se calcula como $\boldsymbol{X}\boldsymbol{a}$, lo que significa que se hace un escalado de las columnas de $\boldsymbol{X}$ con los factores en $\boldsymbol{a}$.
En el contexto de las GCN, esto significa que si tenemos múltiples ejes entrantes, es decir, múltiples unos en el vector de adyacencia $\boldsymbol{a}$, $\boldsymbol{X}\boldsymbol{a}$ aumenta.
Por otra parte, si sólo tenemos un eje entrante, éste valor disminuye.
Para solucionar el problema de que el valor sea proporcional al número de ejes entrantes, lo dividimos por el número de ejes entrantes $d$.
Entonces aplicamos la rotación $\boldsymbol{V}$ a $\boldsymbol{X}\boldsymbol{a}d^{-1}$.

Podemos representar está representación oculta $\boldsymbol{h}$ para el conjunto entero de entradas $\boldsymbol{x}$ utilizando la siguiente notación matricial:

$$
\{\boldsymbol{x}_{i}\}^{t}_{i=1}\rightsquigarrow \boldsymbol{H}=f(\boldsymbol{UX}+ \boldsymbol{VXAD}^{-1})
\tag{Ec. 4}
$$

donde $\vect{D} = \text{diag}(d_{i})$.


## [Teoría de Residual Gated GCN con código](https://www.youtube.com/watch?v=2aKXWqkbpWg&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=25&t=992s)


# TODO Is residual gated GCN ok as it is?

Las Redes convolucionales de Grafo Residual Gated son un tipo de GCN que se pueden representar como se muestra en la Figura 2:

<center>
<img src="{{site.baseurl}}/images/week13/13-3/figure2.png" height="300px" /><br>
<b>Fig. 2</b>: Red Convolucional de Grafo Residual Gated
</center>

Al igual que con las GCN estándar, el vértice $v$ consiste en dos vectores: la entrada $\boldsymbol{x}$ y su representación oculta $\boldsymbol{h}$. Sin embargo, en este caso, los ejes también tienen una representación de características, donde $\boldsymbol{e_{j}^{x}}$ representa la representación del eje de entrada y $\boldsymbol{e_{j}^{h}}$ representa la representación del eje oculto.

La representación $\boldsymbol{h}$ del vértice $v$ se calcula mediante la siguiente ecuación:

$$
    \boldsymbol{h}=\boldsymbol{x} + \bigg(\boldsymbol{Ax} + \sum_{v_j→v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}}\bigg)^{+}
\tag{Ec. 5}
$$

donde $\boldsymbol{x}$ is la representación de la entrada, $\boldsymbol{Ax}$ representa una rotación aplicada a la entrada $\boldsymbol{x}$ y $\sum_{v_j→v}{\eta(\boldsymbol{e_{j}})\odot \boldsymbol{Bx_{j}}}$ denota el sumatorio de la multiplicación elemento a elemento de las características de entrada rotadas y un cierre $\eta(\boldsymbol{e_{j}})$. A diferencia de la antedicha GCN, donde promediamos las representaciones entrantes, el término cierre es crítico para la implementación de las Residual Gated GCN ya que nos permite modular las representaciones de entrada en base a las representaciones de los ejes.

Nótese que el sumatorio es sólo sobre los vértices ${v_j}$ que tienen ejes entrantes en el vértice ${v}$. El término residual (en las Residual Gated GCN) viene del hecho de que para calcular la representación oculta $\boldsymbol{h}$, añadimos la representación de entrada $\boldsymbol{x}$. El término cierre $\eta(\boldsymbol{e_{j}})$ se calcula como se muestra a continuación:

# TODO review this paragraph - many doubts here

$$
    \eta(\boldsymbol{e_{j}})=\sigma(\boldsymbol{e_{j}})\bigg(\sum_{v_k→v}\sigma(\boldsymbol{e_{k}})\bigg)^{-1}
\tag{Ec. 6}
$$

El término cierre $\eta(\boldsymbol{e_{j}})$ es una sigmoidal normalizada que se obtiene dividiendo la sigmoidal de la representación del eje entrante por la suma de las sigmoidales de las representaciones de todos los ejes entrantes. Nótese que para calcular el término cierre, necesitamos la representación del eje $\boldsymbol{e_{j}}$, que podemos calcular utilizando las siguientes ecuaciones:

$$
    \boldsymbol{e_{j}} = \boldsymbol{Ce_{j}^{x}} + \boldsymbol{Dx_{j}} + \boldsymbol{Ex}
\tag{Ec. 7}
$$

$$
    \boldsymbol{e_{j}^{h}}=\boldsymbol{e_{j}^{x}}+(\boldsymbol{e_{j}})^{+}
\tag{Ec. 8}
$$

La representación del eje oculto $\boldsymbol{e_{j}^{h}}$ se obtiene mediante el sumatorio de la representación del eje inicial $\boldsymbol{e_{j}^{x}}$ y $\texttt{ReLU}(\cdot)$ aplicada a $\boldsymbol{e_{j}}$, donde $\boldsymbol{e_{j}}$ por su parte viene dada por por la suma de la rotación aplicada a la representación de entrada $\boldsymbol{x_{j}}$ del vértice $v_{j}$ y una rotación aplicada a la representación de entrada $\boldsymbol{x}$ del vértice $v$.

*Nota: Para calcular las representaciones ocultas descendientes (*por ejemplo* la representación oculta de la $2^\text{ª}$ capa), podemos reemplazar las representaciones de entrada por la representación de características de la capa $1^\text{ª}$ en las ecuaciones arriba mencionadas.

# TODO: check math here






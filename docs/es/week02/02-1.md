---
lang: es
lang-ref: ch.02-1
lecturer: Yann LeCun
title: Introducción a gradiente descendente y el algoritmo de propagación hacia atrás
authors: Amartya Prasad, Dongning Fang, Yuxin Tang, Sahana Upadhya
date: 3 Feb 2020
translation-date: 30 Mar 2020
translator: juanmartinezitm
---
<!--
## [Gradient Descent optimization algorithm](https://www.youtube.com/watch?v=d9vdh3b787Y&t=29s)


### Parametrised models

$$
\bar{y} = G(x,w)
$$

Parametrised models are simply functions that depend on inputs and trainable parameters. There is no fundamental difference between the two, except that trainable parameters are shared across training samples whereas the input varies from sample to sample. In most deep learning frameworks, parameters are implicit, that is, they aren't passed when the function is called. They are 'saved inside the function', so to speak, at least in the object-oriented versions of models.

The parametrised model (function) takes in an input, has a parameter vector and produces an output. In supervised learning, this output goes into the cost function ($C(y,\bar{y}$)), which compares the true output (${y}$) with the model output ($\bar{y}$). The computation graph for this model is shown in Figure 1.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure1.jpg" alt="Figure1" style="zoom: 33%;" /></center> |
| <center>Figure 1: Computation Graph representation for a Parametrised Model </center>|

Examples of parametrised functions -

- Linear Model - Weighted Sum of Components of the Input Vector :
  $$
  \bar{y} = \sum_i w_i x_i \text{  ;  } C(y,\bar{y}) = ||y - \bar{y}||^2
  $$

- Nearest Neighbor - There is an input x and a weight matrix W with each row of the matrix indexed by k. The output is the value of k that corresponds to the row of W that is closest to x :
  $$
  \bar{y} = \text{argmin}_k ||x - w_{k,.} ||^2
  $$
  Parameterized models could also involve complicated functions.
-->

## [Algoritmo de optimización Descenso de Gradiente](https://www.youtube.com/watch?v=d9vdh3b787Y&t=29s)


### Modelos parametrizados

$$
\bar{y} = G(x,w)
$$

Los modelos parametrizados son simplemente funciones que dependen de entradas y parámetros entrenables. No hay una diferencia fundamental entre los dos, excepto que los parámetros entrenables se comparten entre las muestras de entrenamiento, mientras que la entrada varia de muestra a muestra. En la mayoría de los marcos conceptuales de aprendizaje profundo, los parámetros son implícitos, esto es, que no se indican cuando la función se llama. Se 'almacenan dentro de la función', sea dicho, por lo menos en las versiones de modelos orientadas a objetos.

El modelo parametrizado (función) toma una entrada, tiene un vector de parámetros y produce una salida. En el aprendizaje supervisado, esta salida va a la función de costo ($C(y,\bar{y}$)), que compara la verdadera salida (${y}$) con la salida del modelo ($\bar{y}$). La gráfica de cómputo para este modelo se muestra en la Figura 1.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure1.jpg" alt="Figure1" style="zoom: 33%;" /></center> |
| <center>Figure 1: Representación de la gráfica de computación para un Modelo Parametrizado</center>|

Ejemplos de funciones parametrizadas -

- Modelo Lineal - Suma ponderada de componentes del vector de entrada:

  $$
  \bar{y} = \sum_i w_i x_i, C(y,\bar{y}) = \Vert y - \bar{y}\Vert^2
  $$

- Vecino más cercano - Existe una entrada x y una matriz de ponderación W con cada fila de la matriz indexada por k. La salida es el valor de k que corresponde a la fila de W más cercana a x:

  $$
  \bar{y} = \underset{k}{\arg\min} \Vert x - w_{k,.} \Vert^2
  $$

  Los modelos parametrizados pueden también involucrar funciones complejas.
<!--
#### Block diagram notations for computation graphs

- Variables (tensor, scalar, continuous, discrete)
    - <img src="{{site.baseurl}}/images/week02/02-1/x.PNG" alt="x" style="zoom:50%;" /> is an observed input to the system
    - <img src="{{site.baseurl}}/images/week02/02-1/y.PNG" alt="y" style="zoom:50%;" /> is a computed variable which is produced by a deterministic function

- Deterministic functions

    <img src="{{site.baseurl}}/images/week02/02-1/deterministic_function.PNG" alt="deterministic_function" style="zoom:50%;" />

    - Takes in multiple inputs and can produce multiple outputs
    - It has an implicit parameter variable (${w}$)
    - The rounded side indicates the direction in which it is easy to compute. In the above diagram, it is easier to compute ${\bar{y}}$ from ${x}$ than the other way around

- Scalar-valued function

  <img src="{{site.baseurl}}/images/week02/02-1/scalar-valued.PNG" alt="scalar-valued" style="zoom:50%;" />

    - Used to represent cost functions
    - Has an implicit scalar output
    - Takes multiple inputs and outputs a single value (usually the distance between the inputs)
-->
#### Notación en diagrama de bloques para gráficas computacionales

-Variables (tensor, escalar, contínua, discreta)
    - <img src="{{site.baseurl}}/images/week02/02-1/x.PNG" alt="x" style="zoom:50%;" /> es una entrada observada al sistema
    - <img src="{{site.baseurl}}/images/week02/02-1/y.PNG" alt="y" style="zoom:50%;" /> es una variable calculada que se produce por una función determinista

- Funciones deterministas

    <img src="{{site.baseurl}}/images/week02/02-1/deterministic_function.PNG" alt="deterministic_function" style="zoom:50%;" />

    - Toman múltiples entradas y pueden producir múltiples salidas. 
    - Tienen una variable de parámetro implícita (${w}$)
    - El lado redondeado indica la dirección en la que es fácil de calcular. En el diagrama de arriba, es más fácil calcular ${\bar{y}}$ a partir de ${x}$, que al revés.

- Función escalar

  <img src="{{site.baseurl}}/images/week02/02-1/scalar-valued.PNG" alt="scalar-valued" style="zoom:50%;" />

  - Se utilizan para representar funciones de costo
	- Tienen una salida escalar implícita
	- Toma múltiples entradas y devuelve un solo valor (usualmente la distancia entre las entradas)

<!--
#### Loss functions

Loss function is a function that is minimized during training. There are two types of losses:

1) Per Sample Loss -
$$
 L(x,y,w) = C(y, G(x,w))
$$
2) Average Loss -

​	For any set of Samples $$S = \{(x[p],y[p]) \mid p=0,1...P-1 \}$$

​	Average Loss over the Set S is given by :  $$L(S,w) = \frac{1}{P} \sum_{(x,y)} L(x,y,w)$$

| <center><img src="{{site.baseurl}}/images/week02/02-1/Average_Loss.png" alt="Average_Loss" style="zoom:33%;" /></center> |
|   <center>Figure 2: Computation graph for model with Average Loss    </center>|

In the standard Supervised Learning paradigm, the loss (per sample) is simply the output of the cost function. Machine Learning is mostly about optimizing functions (usually minimizing them). It could also involve finding Nash Equilibria between two functions like with GANs. This is done using Gradient Based Methods, though not necessarily Gradient Descent.
-->
#### Funciones de pérdida

La función de pérdida es una función que se minimiza durante el entrenamiento. Existen dos tipos de pérdidas:

1) Pérdida por muestra

$$
 L(x,y,w) = C(y, G(x,w))
$$

2) Pérdida promedio -

Para cualquier conjunto de muestras

$$S = \lbrace(x[p],y[p]) \mid p \in \lbrace 0, \cdots, P-1 \rbrace \rbrace$$

La pérdida promedio sobre el conjunto $S$ se da por:

$$L(S,w) = \frac{1}{P} \sum_{(x,y)} L(x,y,w)$$

| <center><img src="{{site.baseurl}}/images/week02/02-1/Average_Loss.png" alt="Average_Loss" style="zoom:33%;" /></center> |
|   <center>Figura 2: Grafo de cómputo para un modelo con pérdida promedio    </center>|

En el paradigma estándar de aprendizaje supervisado, la pérdida (por muestra) es simplemente la salida de la función de costo. El aprendizaje automatizado consiste principalmente en optimizar funciones (usualmente minimizarlas). También, puede involucrar encontrar equilibrios de Nash entre dos funciones como es el caso de los GANs. Esto se hace utilizando Métodos Basados en Gradiente, aunque no necesariamente corresponda al Descenso de Gradiente.

<!--
### Gradient descent

A **Gradient Based Method** is a method/algorithm that finds the minima of a function, assuming that one can easily compute the gradient of that function. It assumes that the function is continuous and differentiable almost everywhere (it need not be differentiable everywhere).

**Gradient Descent Intuition** - Imagine being in a mountain in the middle of a foggy night. Since you want to go down to the village and have only limited vision, you look around your immediate vicinity to find the direction of steepest descent and take a step in that direction.

**Different methods of Gradient Descent**

- Full (batch) gradient descent update rule :
  $$
  w \leftarrow w - \eta \frac{\partial L(S,w)}{\partial w}
  $$

- For SGD (Stochastic Gradient  Descent), the update rule becomes :
  - Pick a $p$ in $\text{0,\dots,P-1}$, then update
    $$
    w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
    $$

Where ${w}$ represents the parameter to be optimized.

$\eta \text{ is a constant here but in more sophisticated algorithms, it could be a matrix}$.

If it is a positive semi-definite matrix, we'll still move downhill but not necessarily in the direction of steepest descent. In fact the direction of steepest descent may not always be the direction we want to move in.

If the function is not differentiable, i.e, it has a hole or is staircase like or flat, where the gradient doesn't give you any information, one has to resort to other methods - called 0-th Order Methods or Gradient-Free Methods. Deep Learning is all about Gradient Based Methods.

However, RL (Reinforcement Learning) involves **Gradient Estimation** without the explicit form for the gradient. An example is a robot learning to ride a bike where the robot falls every now and then. The objective function measures how long the bike stays up without falling. Unfortunately, there is no gradient for the objective function. The robot needs to try different things.

The RL cost function is not differentiable most of the time but the network that computes the output is gradient-based. This is the main difference between supervised learning and reinforcement learning. With the latter, the cost function C is not differentiable. In fact it completely unknown. It just returns an output when inputs are fed to it, like a blackbox. This makes it highly inefficient and is one of the main drawbacks of RL - particularly when the parameter vector is high dimensional (which implies a huge solution space to search in, making it hard to find where to move).

A very popular technique in RL is Actor Critic Methods. A critic method basically consists of a second C module which is a known, trainable module. One is able to train the C module, which is differentiable, to approximate the cost function/reward function. The reward is a negative cost, more like a punishment. That’s a way of making the cost function differentiable, or at least approximating it by a differentiable function so that one can backpropagate.
-->
### Descenso de gradiente

Un **Método Basado en Gradiente** es un método/algoritmo que halla el mínimo de una función, asumiendo que uno puede fácilmente calcular el gradiente de esa función. Asume que la función es contínua y diferenciable casi en cualquier parte (no necesariamente es diferenciable en cualquier parte).

**Intuición en Descenso de Gradiente** - Imagine que está en una montaña en la mitad de una noche con niebla. Dado que quiere descender a la aldea y sólo tiene visión limitada, mira alrededor de su vecindad inmediata para hallar la dirección de descenso más rápido y da un paso en esa dirección.

**Diferentes métodos de Descenso de Gradiente**

- Regla de actualización de Descenso de Gradiente de (lote o "batch") completo :

  $$
  w \leftarrow w - \eta \frac{\partial L(S,w)}{\partial w}
  $$


-Para SGD (Descenso de Gradiente Estocástico), la regla de actualización se convierte en:
  - Seleccione una $p \in \lbrace 0, \cdots, P-1 \rbrace$, entonces actualice

    $$
    w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
    $$

donde ${w}$ representa los parámetros a ser optimizados.

$\eta$ acá es una constante, pero en algoritmos más sofisticados, podría ser una matriz.

Si es una matriz semi-definida positiva, aún nos moveremos colina abajo pero no necesariamente en la dirección del descenso más rápido. De hecho, la dirección del descenso más rápido puede no ser siempre la dirección en la que nos queremos mover.

Si la función no es diferenciable, i.e, tiene un agujero, tiene forma de escalera, o es plana, donde el gradiente no da información, uno tiene que recurrir a otros métodos - llamados Métodos de orden 0 o Métodos libres de gradiente. El aprendizaje profundo se trata de Métodos Basados en Gradientes.

Sin embargo, RL (aprendizaje por refuerzo) involucra la **Estimación del Gradiente** sin la forma explícita del gradiente. Un ejemplo es un robot aprendiendo a montar en bicicleta donde el robot cae de vez en cuando. La función objetivo mide cuánto tiempo la bicicleta permanece sin caer. Desafortunadamente, no existe un gradiente para la función objetivo. El robot necesita intentar diferentes cosas.

La función de costo en RL no es diferenciable la mayor parte del tiempo pero la red que calcula la salida está basada en gradientes. Esta es la principal diferencia entre aprendizaje supervisado y aprendizaje por refuerzo. En este último, la función de costo $C$ no es diferenciable. De hecho es completamente desconocida. Solo devuelve una salida cuando se le dan entradas, como una caja negra. Esto la hace altamente ineficiente lo que es uno de los principales problemas de RL - particularmente cuando el vector de parámetros tiene muchas dimensiones (lo que implica un espacio de solución inmenso para buscar, dificultando buscar hacia donde moverse). 

Una técnica popular en RL son los Métodos de Actor Críticos. Un método crítico consiste básicamente de un segundo módulo C, entrenable, que se conoce. Uno puede entrenar el módulo C, que es diferenciable, para aproximar la función de costo/recompenza. La recompenza es un costo negativo, similar a un castigo. Esta es una forma de hacer diferenciable la función de costo, o por lo menos aproximarla con una función diferenciable de forma que se puede hacer retropropagación.

<!--
## [Advantages of SGD and backpropagation for traditional neural nets](https://www.youtube.com/watch?v=d9vdh3b787Y&t=1036s)


### Advantages of Stochastic Gradient Descent (SGD)

In practice, we use stochastic gradient to compute the gradient of the objective function w.r.t the parameters. Instead of computing the full gradient of the objective function, which is the average of all samples, stochastic gradient just takes one sample, computes the loss, $L$, and the gradient of the loss w.r.t the parameters, and then takes one step in the negative gradient direction.

$$
w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
$$

In the formula, $w$ is approached by $w$ minus the step-size, times the gradient of the per-sample loss function w.r.t the parameters for a given sample, ($x[p]$,$y[p]$).

If we do this on a single sample, we will get a very noisy trajectory as shown in Figure 3. Instead of the loss going directly downhill, it’s stochastic. Every sample will pull the loss towards a different direction. It’s just the average that pulls us to the minimum of the average. Although it looks inefficient, it’s much faster than batch gradient descent at least in the context of machine learning when the samples have some redundancy.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure2.png" alt="Figure2" style="zoom:80%;" /></center> |
| <center>Figure 3: Stochastic Gradient Descent trajectory for per sample update </center>|


In practice, we use batches instead of doing stochastic gradient descent on a single sample. We compute the average of the gradient over a batch of samples, not a single sample, and then take one step. The only reason for doing this is that we can make more efficient use of the existing hardware  (i.e. GPUs, multicore CPUs) if we use batches since it's easier to parallelize. Batching is the simplest way to parallelize.
-->
## [Ventajas de SGD y retropropagación para redes neuronales tradicionales](https://www.youtube.com/watch?v=d9vdh3b787Y&t=1036s)


### Ventajas de Descenso de Gradiente Estocástico (SGD)

En la práctica, utilizamos el gradiente estocástico para calcular el gradiente de la función objetivo con respecto a los parámetros. En vez de calcular el gradiente completo de la función objetivo, el cual se promedia entre todas las muestras, el gradiente estocástico solo toma una muestra, calcula la pérdida $L$, y el gradiente de la pérdida con respecto a los parámetros, y después da un paso en la dirección negativa del gradiente.

$$
w \leftarrow w - \eta \frac{\partial L(x[p], y[p],w)}{\partial w}
$$

En la fórmula, $w$ se aproxima como $w$ menos el tamaño de paso, multiplicado por el gradiente de la función de pérdida por muestra con respecto a los parámetros para una muestra dada, ($x[p]$,$y[p]$).

Si hacemos esto en una sola muestra, obtendremos una trayectoria bastante ruidosa, como se muestra en la Figura 3. En vez de que la pérdida se dirija directamente colina abajo, esta es estocástica. Cada muestra dirige la pérdida en una dirección diferente. Es únicamente el promedio que nos dirige al mínimo del promedio. Aunque parece ineficiente, es mucho más rápido que utilizar el gradiente descendente en lotes (batches) por lo menos en el contexto de machine learning cuando las muestras tienen alguna redundancia.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure2.png" alt="Figure2" style="zoom:80%;" /></center> |
| <center>Figura 3: Trayectoria de Descenso de Gradiente Estocástico para actualización por muestra. </center>|

En práctica, utilizamos lotes (batches) en vez de hacer un Descenso de Gradiente Estocástico en una única muestra. Calculamos el promedio del gradiente sobre las muestras del lote (batch), no en una sola muestra, y después damos el paso. La única razón para hacer esto es que podemos utilizar de forma más eficiente el hardware existente (i.e. GPUs, CPUs multinúcleo) si utilizamos lotes ya que son más fáciles de paralelizar. El utilizar lotes (Batching) es la forma más simple de paralelizar.
<!--
### Traditional neural network

Traditional Neural Nets are basically interspersed layers of linear operations and point-wise non-linear operations. For linear operations, conceptually it is just a matrix-vector multiplication. We take the (input) vector multiplied by a matrix formed by the weights. The second type of operation is to take all the components of the weighted sums vector and pass it through some simple non-linearity (i.e. $\texttt{ReLU}(\cdot)$, $\tanh(\cdot)$, …).

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure3.png" alt="Figure3" style="zoom:30%;" /></center> |
|             <center>Figure 4: Traditional Neural Network             </center>|

Figure 4 is an example of a 2-layer network, because what matters are the pairs (i.e linear+non-linear). Some people call it a 3-layer network because they count the variables. Note that if there are no non-linearities in the middle, we may as well have a single layer because the product of two linear functions is a linear function.

Figure 5 shows how the linear and non-linear functional blocks of the network stack:

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure4.png" alt="Figure4" style="zoom:30%;" /></center> |
|  <center>Figure 5: Looking inside the linear and non-linear blocks   </center>|

In the graph, $s[i]$ is the weighted sum of unit ${i}$ which is computed as:

$$
s[i]=\Sigma_{j \in UP(i)}w[i,j]\cdot z[j]
$$

where $UP(i)$ denotes the predecessors of $i$ and  $z[j]$ is the $j$th output from the previous layer.

The output $z[i]$ is computed as:

$$
z[i]=f(s[i])
$$

where $f$ is a non-linear function.
-->
### Red Neuronal Tradicional

Las redes neuronales tradicionales son básicamente capas intercaladas de operaciones lineales y operaciones no-lineales punto a punto. Para operaciones lineales, conceptualmente son sólo multiplicaciones matriz-vector. Tomamos el vector de entrada multiplicado por una matriz conformada por los pesos. El segundo tipo de operación es tomar todos los componentes de los vectores de sumas ponderadas y pasarlos a través de una no-linealidad simple (i.e. $\texttt{ReLU}(\cdot)$, $\tanh(\cdot)$, …).

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure3.png" alt="Figure3" style="zoom:30%;" /></center> |
|             <center>Figura 4: Red Neuronal Tradicional             </center>|

La Figura 4 es un ejemplo de una red de 2 capas, porque lo que importa son los pares (i.e lineal + no-lineal). Algunas personas la llaman red de 3 capas porque cuentan las variables. Note que si no hubieran no-linealidades en el medio, podríamos tener también una sola capa porque el producto de dos funciones lineales es una función lineal.

La Figura 5 muestra cómo los bloques funcionales lineales y no-lineales de la red se apilan:

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure4.png" alt="Figure4" style="zoom:30%;" /></center> |
|  <center>Figura 5: Viendo dentro de los bloques lineales y no-lineales   </center>|

En la gráfica, $s[i]$ es la suma ponderada de la unidad ${i}$ que se calcula como:

$$
s[i]=\sum_{j \in UP(i)}w[i,j]\cdot z[j]
$$

donde $UP(i)$ denota el predecesor de $i$ y $z[j]$ en la $j$th-ésima salida de la capa anterior.

La salida $z[i]$ se calcula como:

$$
z[i]=f(s[i])
$$

donde $f$ es una función no-lineal.
<!--
### Backpropagation through a non-linear function

The first way to do backpropagation is to backpropagate through a non linear function. We take a particular non-linear function $h$ from the network and leave everything else in the blackbox.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure5.png" alt="Figure5" style="zoom: 25%;" /></center> |
|    <center>Figure 6: Backpropagation through non-linear function     </center>|

We are going to use the chain rule to compute the gradients:

$$
g(h(s))' = g'(h(s))\cdot h'(s)
$$

where $h'(s)$ is the derivative of $z$ w.r.t $s$ represented by $\frac{\mathrm{d}z}{\mathrm{d}s}$.
To make the connection between derivatives clear, we rewrite the formula above as:

$$
\frac{\mathrm{d}C}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot \frac{\mathrm{d}z}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot h'(s)
$$

Hence if we have a chain of those functions in the network, we can backpropagate by multiplying by the derivatives of all the ${h}$ functions one after the other all the way back to the bottom.

It’s more intuitive to think of it in terms of perturbations. Perturbing $s$ by $\mathrm{d}s$ will perturb $z$ by:

$$\mathrm{d}z = \mathrm{d}s \cdot h'(s)$$

This would in turn perturb C by:

$$
\mathrm{d}C = \mathrm{d}z\cdot\frac{\mathrm{d}C}{\mathrm{d}z} = \mathrm{d}s\cdot h’(s)\cdot\frac{\mathrm{d}C}{\mathrm{d}z}
$$

Once again, we end up with the same formula as the one shown above.
-->

### Retropropagación a través de una función no-lineal

La primera forma de hacer retropropagación es propagar hacia atrás a través de una función no lineal. Tomamos una función no-lineal particular $h$ de la red y dejamos todo lo demás en la caja negra.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure5.png" alt="Figure5" style="zoom: 25%;" /></center> |
|    <center>Figura 6: Retropropagación a través de una función no-lineal     </center>|

Vamos a utilizar la regla de la cadena para calcular los gradientes:

$$
g(h(s))' = g'(h(s))\cdot h'(s)
$$

donde $h'(s)$ es la derivada de $z$ con respecto a $s$ representada por $\frac{\mathrm{d}z}{\mathrm{d}s}$.
Para hacer clara la conexión entre las derivadas, reescribimos la forma anterior como::

$$
\frac{\mathrm{d}C}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot \frac{\mathrm{d}z}{\mathrm{d}s} = \frac{\mathrm{d}C}{\mathrm{d}z}\cdot h'(s)
$$

Por tanto, si tenemos una cadena de esas funciones en la red, podemos retropropagar multiplicado por las derivadas de todos las funciones ${h}$ una después de la otra hasta el final.

Es más intuitivo pensar en términos de perturbaciones. Perturbar $s$ en $\mathrm{d}s$ perturbará $z$ en:

$$\mathrm{d}z = \mathrm{d}s \cdot h'(s)$$

Esto a su vez perturbará $C$ en:

$$
\mathrm{d}C = \mathrm{d}z\cdot\frac{\mathrm{d}C}{\mathrm{d}z} = \mathrm{d}s\cdot h’(s)\cdot\frac{\mathrm{d}C}{\mathrm{d}z}
$$

De nuevo, terminamos con la misma fórmula que mostramos anteriormente.

<!--
### Backpropagation through a weighted sum

For a linear module, we do backpropagation through a weighted sum. Here we view the entire network as a blackbox except for 3 connections going from a ${z}$ variable to a bunch of $s$ variables.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure6.png" alt="Figure6" style="zoom: 25%;" /></center> |
|        <center>Figure 7: Backpropagation through weighted sum        </center>|


This time the perturbation is a weighted sum. Z influences several variables. Perturbing $z$ by $\mathrm{d}z$ will perturb $s[0]$, $s[1]$ and $s[2]$ by:

$$
\mathrm{d}s[0]=w[0]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[1]=w[1]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[2]=w[2]\cdot\mathrm{d}z
$$

 This will perturb C by

$$
\mathrm{d}C = \mathrm{d}s[0]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[0]}+\mathrm{d}s[1]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[1]}+\mathrm{d}s[2]\cdot\frac{\mathrm{d}C}{\mathrm{d}s[2]}
$$

Hence C is going to vary by the sum of the 3 variations:

$$
\frac{\mathrm{d}C}{\mathrm{d}z} = \frac{\mathrm{d}C}{\mathrm{d}s[0]}\cdot w[0]+\frac{\mathrm{d}C}{\mathrm{d}s[1]}\cdot w[1]+\frac{\mathrm{d}C}{\mathrm{d}s[2]}\cdot w[2]
$$
-->
### Retropropagación a través de una suma ponderada

Para un módulo lineal, hacemos retropropagación a través de una suma ponderada. Acá vemos la red como una caja negra excepto para 3 conexiones que van desde una variable $z$ a un grupo de variables $s$.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure6.png" alt="Figure6" style="zoom: 25%;" /></center> |
|        <center>Figura 7: Retropropagación a través de una suma ponderada        </center>|

Esta vez la perturbación es una suma poderada. $Z$ influye varias variables. Perturbar a $z$ en $\mathrm{d}z$ perturbará $s[0]$, $s[1]$ y $s[2]$ en:


$$
\mathrm{d}s[0]=w[0]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[1]=w[1]\cdot \mathrm{d}z
$$

$$
\mathrm{d}s[2]=w[2]\cdot\mathrm{d}z
$$

 Esto perturbará $C$ en

$$
\mathrm{d}C = \mathrm{d}s[0]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[0]}+\mathrm{d}s[1]\cdot \frac{\mathrm{d}C}{\mathrm{d}s[1]}+\mathrm{d}s[2]\cdot\frac{\mathrm{d}C}{\mathrm{d}s[2]}
$$

Por lo tanto, $C$ variará en la suma de las 3 variaciones:

$$
\frac{\mathrm{d}C}{\mathrm{d}z} = \frac{\mathrm{d}C}{\mathrm{d}s[0]}\cdot w[0]+\frac{\mathrm{d}C}{\mathrm{d}s[1]}\cdot w[1]+\frac{\mathrm{d}C}{\mathrm{d}s[2]}\cdot w[2]
$$

<!--
## [PyTorch implementation of neural network and a generalized backprop algorithm](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2288s)


### Block diagram of a traditional neural net

- Linear blocks $s_{k+1}=w_kz_k$
- Non-linear blocks $z_k=h(s_k)$

  <center><img src="{{site.baseurl}}/images/week02/02-1/Figure 7.png" alt="Figure 7" style="zoom: 33%;" /></center>

$w_k$: matrix $z_k$: vector $h$: application of scalar ${h}$ function to every component. This is a 3-layer neural net with pairs of linear and non-linear functions, though most modern neural nets do not have such clear linear and non-linear separations and are more complex.
-->
## [Implementación en Pytorch de la red neuronal y el algoritmo de retropropagación generalizado](https://www.youtube.com/watch?v=d9vdh3b787Y&t=2288s)


### Diagrama de bloques de una red neuronal tradicional

- Bloques lineales $s_{k+1}=w_kz_k$
- Bloques no-lineales $z_k=h(s_k)$

  <center><img src="{{site.baseurl}}/images/week02/02-1/Figure 7.png" alt="Figure 7" style="zoom: 33%;" /></center>

$w_k$: matriz $z_k$: vector $h$: aplicación de la función escalar $h$ a cada componente. Esta es una red neuronal de 3 capas con pares de funciones lineales y no-lineales, aunque las redes neuronales modernas no tiene claras las separaciones entre lineal y no-lineal y son más complejas.

<!--
### PyTorch implementation

```python
import torch
from torch import nn
image = torch.randn(3, 10, 20)
d0 = image.nelement()

class mynet(nn.Module):
    def __init__(self, d0, d1, d2, d3):
        super().__init__()
        self.m0 = nn.Linear(d0, d1)
        self.m1 = nn.Linear(d1, d2)
        self.m2 = nn.Linear(d2, d3)

    def forward(self,x):
        z0 = x.view(-1)  # flatten input tensor
        s1 = self.m0(z0)
        z1 = torch.relu(s1)
        s2 = self.m1(z1)
        z2 = torch.relu(s2)
        s3 = self.m2(z2)
        return s3
model = mynet(d0, 60, 40, 10)
out = model(image)
```

- We can implement neural nets with object oriented classes in PyTorch. First we define a class for the neural net and initialize linear layers in the constructor using predefined nn.Linear class. Linear layers have to be separate objects because each of them contains a parameter vector. The nn.Linear class also adds the bias vector implicitly. Then we define a forward function on how to compute outputs with $\text{torch.relu}$ function as the nonlinear activation. We don't have to initialize separate relu functions because they don't have parameters.

- We do not need to compute the gradient ourselves since PyTorch knows how to back propagate and calculate the gradients given the forward function.
-->
### Implementación en PyTorch

```python
import torch
from torch import nn
image = torch.randn(3, 10, 20)
d0 = image.nelement()

class mynet(nn.Module):
    def __init__(self, d0, d1, d2, d3):
        super().__init__()
        self.m0 = nn.Linear(d0, d1)
        self.m1 = nn.Linear(d1, d2)
        self.m2 = nn.Linear(d2, d3)

    def forward(self,x):
        z0 = x.view(-1)  # flatten input tensor
        s1 = self.m0(z0)
        z1 = torch.relu(s1)
        s2 = self.m1(z1)
        z2 = torch.relu(s2)
        s3 = self.m2(z2)
        return s3
model = mynet(d0, 60, 40, 10)
out = model(image)
```
- Podemos implementar redes neuronales con clases orientadas a objetos en PyTorch. Primero, definimos una clase para la red neuronal e inicializamos las capas lineales en el constructor, utilizando la clase predefinida nn.Linear. Las capas lineales tienen que ser objetos separados porque cada una de ellas contiene un vector de parámetros. La clase nn.Linear también agrega el vector de sesgos (bias) implícitamente. Después defininimos una función hacia adelante (forward) de cómo calcular las salidas con la función $\text{torch.relu}$ como la activación no-lineal. No tenemos que inicializar de forma separada las funciones relu porque ellas no tienen parámetros.

- No tenemos que calcular el gradiente nosotros mismos dadro que PyTorch sabe como retropropagar y calcular los gradientes dada la función hacia adelante (forward).
<!--
### Backprop through a functional module

We now present a more generalized form of backpropagation.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure9.png" alt="Figure9" style="zoom:33%;" /></center> |
|    <center>Figure 8: Backpropagation through a functional module     </center>|


- Using chain rule for vector functions

  $$
   z_g : [d_g\times 1]
  $$

  $$
   z_f:[d_f\times 1]
  $$

  $$
  \frac{\partial c}{\partial{z_f}}=\frac{\partial c}{\partial{z_g}}\frac{\partial {z_g}}{\partial{z_f}}
  $$

  $$
  [1\times d_f]= [1\times d_g]\times[d_g\times d_f]
  $$

  This is the basic formula for $\frac{\partial c}{\partial{z_f}}$ using the chain rule. Note that the gradient of a scalar function with respect to a vector is a vector of the same size as the vector with respect to which you differentiate. In order to make the notations consistent, it is a row vector instead of a column vector.

- Jacobian matrix

  $$
  \left(\frac{\partial{z_g}}{\partial {z_f}}\right)_{ij}=\frac{(\partial {z_g})_i}{(\partial {z_f})_j}
  $$

  We need $\frac{\partial {z_g}}{\partial {z_f}}$ (Jacobian matrix entries) to compute the gradient of the cost function with respect to $z_f$ given gradient of the cost function with respect to $z_g$. Each entry $ij$ is equal to the partial derivative of the $i$th component of the output vector with respect to the $j$th component of the input vector.

  If we have a cascade of modules, we keep multiplying the Jacobian matrices of all the modules going down and we get the gradients w.r.t all the internal variables.
-->
## Retropropagación a través de un módulo funcional

Ahora presentamos una forma más generalizada de retropropagación.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure9.png" alt="Figure9" style="zoom:33%;" /></center> |
|    <center>Figura 8: Retropropagación a través de un módulo funcional     </center>|


- Utilizando la regla de la cadena para funciones vectoriales

  $$
   z_g : [d_g\times 1]
  $$

  $$
   z_f:[d_f\times 1]
  $$

  $$
  \frac{\partial c}{\partial{z_f}}=\frac{\partial c}{\partial{z_g}}\frac{\partial {z_g}}{\partial{z_f}}
  $$

  $$
  [1\times d_f]= [1\times d_g]\times[d_g\times d_f]
  $$

  Esta es la fórmula básica para $\frac{\partial c}{\partial{z_f}}$ utilizando la regla de la cadena. Note que el gradiente de una función escalar con respecto a un vector es un vector del mismo tamaño del vector con respecto al cual se está diferenciando. Con el fin de hacer las notaciones consistentes, es un vector fila en vez de un vector columna.

- Matriz Jacobiana

  $$
  \left(\frac{\partial{z_g}}{\partial {z_f}}\right)_{ij}=\frac{(\partial {z_g})_i}{(\partial {z_f})_j}
  $$

  Necesitamos $\frac{\partial {z_g}}{\partial {z_f}}$ (Entradas de la matriz jacobiana) para calcular el gradiente de la función de costo con respecto a $z_f$ dado el gradiente de la función de costo con respecto a $z_g$. Cada entrada $ij$ es igual a la derivada parcial de la $i-$ésima componente del vector de salida con respecto a la $j-$ésima componente del vector de entrada.

  Si tenemos módulos en cascada, contnuamos multiplicando por la matrices Jacobianas de todos los módulos y obtenemos los gradientes con respecto a todas las variables internas.

<!--
### Backprop through a multi-stage graph

Consider a stack of many modules in a neural network as shown in Figure 9.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure10.png" alt="Figure10" style="zoom:33%;" /></center> |
|         <center>Figure 9: Backprop through multi-stage graph         </center>|

For the backprop algorithm, we need two sets of gradients - one with respect to the states (each module of the network) and one with respect to the weights (all the parameters in a particular module). So we have two Jacobian matrices associated with each module. We can again use chain rule for backprop.

- Using chain rule for vector functions

  $$
  \frac{\partial c}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {z_k}}
  $$

  $$
  \frac{\partial c}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {w_k}}
  $$

- Two Jacobian matrices for the module
    - One with respect to $z[k]$
    - One with respect to $w[k]$

-->
### Retropropagación a través de una gráfica multi-etapa

Considere un grupo de muchos módulos en una red neuronal como se muestra en la Figura 9.

| <center><img src="{{site.baseurl}}/images/week02/02-1/Figure10.png" alt="Figure10" style="zoom:33%;" /></center> |
|         <center>Figura 9: Retropropagación a través de una gráfica multi-etapa         </center>|

Para el algoritmo de retropropagación, necesitamos dos conjuntos de gradientes - uno con respecto a los estados (cada módulo de la red) y uno con respecto a los pesos (todos los parámetros en un módulo particular). Así que tenemos dos matrices Jacobianas asociadas con cada módulo. De nuevo, podemos utilizar la regla de la cadena para hacer reptropropagación.

- Utilizando la regla de la cadena para funciones vectoriales

  $$
  \frac{\partial c}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {z_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {z_k}}
  $$

  $$
  \frac{\partial c}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial {z_{k+1}}}{\partial {w_k}}=\frac{\partial c}{\partial {z_{k+1}}}\frac{\partial f_k(z_k,w_k)}{\partial {w_k}}
  $$

- Dos matrices Jacobianas para el módulo
    - Una con respecto a $z[k]$
    - Una con respecto a $w[k]$


---
lang-ref: ch.14
title: Semana 14
lang: es
date: 15 Sep 2020
translator: lcipolina (Lucia Cipolina-Kun)
---


<!--## Lecture part A-->

<!--In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net.-->

## Clase parte A
En esta sección, discutimos sobre predicción estructurada. Primero introducimos el grafo de factores basado en energía y un metodo de inferencia efficiente. Luego damos algunos ejemplos de estos grafos con factores "llanos". Finalmente, discutimos la red the grafos Transformador.


<!--## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.-->

## Clase parte B
La segunda parte de la clase analiza en mayor profundidad la aplicación de métodos de modelos de grafos a modelos basados en energía. Después de comparar diferentes funciones de pérdida, discutimos la aplicación del algoritmo de Viterbi y el algoritmo de reenvío a redes de Transformadores gráficos. Luego pasamos a discutir la formulación lagrangiana de retropropagación y luego, la inferencia variacional para modelos basados en energía


## Practicum

<!--When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.-->

Al entrenar modelos altamente parametrizados, como redes neuronales profundas, existe el riesgo de sobreajuste de los datos de entrenamiento. Esto conduce a un mayor error de generalización. Para ayudar a reducir el sobreajuste, podemos introducir la regularización en nuestro entrenamiento, desalentando ciertas soluciones para disminuir el grado en que nuestros modelos se adaptarán al ruido.


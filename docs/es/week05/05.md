---
lang: es
lang-ref: ch.05
title: Semana 5
translation-date: 25 Mar 2020
translator: LecJackS
---

## Lección parte A

<!-- We begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on covergence.
-->
Comenzamos presentando Descenso de Gradiente. Discutimos la intuición y también hablamos sobre cómo los tamaños de los pasos juegan un papel importante para alcanzar la solución. Luego pasamos a SGD y su rendimiento en comparación con Full Batch GD. Finalmente, hablamos de las actualizaciones con Momentum, específicamente las dos reglas de actualización, la intuición detrás de momentum y su efecto en la convergencia.


## Lección parte B

<!-- We discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient.
-->
Discutimos métodos adaptativos para SGD como RMSprop y ADAM. También hablamos sobre las capas de normalización y sus efectos en el proceso de entrenamiento de las red neuronales. Finalmente, discutimos un ejemplo del mundo real de las redes neuronales siendo usadas en la industria para obtener de forma más rápida y eficiente imágenes por resonancia magnética.


## Práctica

<!-- We briefly review the matrix-multiplications and then discuss the convolutions. Key point is we use kernels by stacking and shifting. We first understand the 1D convolution by hand, and then use PyTorch to learn the dimension of kernels and output width in 1D and 2D convolutions examples. Furthermore, we use PyTorch to learn about how automatic gradient works and custom-grads.
-->
Repasamos brevemente multiplicaciones matriciales y luego discutimos convoluciones. El punto clave es que usamos kernels, apilándolos de forma desplazada. Primero entendemos la convolución en 1D hecha a mano, y luego usamos PyTorch para aprender sobre la dimensión de los kernels y el ancho de salida en ejemplos de convoluciones en 1D y 2D. Además, usamos PyTorch para aprender sobre cómo funciona el gradiente automático y los gradientes personalizados.
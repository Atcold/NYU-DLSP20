---
lang: es
lang-ref: ch.05
title: Semana 5
translation-date: 25 Mar 2020
translator: LecJackS
---

## Lección parte A

Comenzamos presentando Descenso de Gradiente. Discutimos la intuición y también hablamos sobre cómo los tamaños de los pasos juegan un papel importante para alcanzar la solución. Luego pasamos a SGD y su rendimiento en comparación con Full Batch GD. Finalmente, hablamos de las actualizaciones con Momentum, específicamente las dos reglas de actualización, la intuición detrás de momentum y su efecto en la convergencia.


## Lección parte B

Discutimos métodos adaptativos para SGD como RMSprop y ADAM. También hablamos sobre las capas de normalización y sus efectos en el proceso de entrenamiento de las red neuronales. Finalmente, discutimos un ejemplo del mundo real de las redes neuronales siendo usadas en la industria para obtener de forma más rápida y eficiente imágenes por resonancia magnética.


## Práctica

Repasamos brevemente multiplicaciones matriciales y luego discutimos convoluciones. El punto clave es que usamos kernels, apilándolos de forma desplazada. Primero entendemos la convolución en 1D hecha a mano, y luego usamos PyTorch para aprender sobre la dimensión de los kernels y el ancho de salida en ejemplos de convoluciones en 1D y 2D. Además, usamos PyTorch para aprender sobre cómo funciona el gradiente automático y los gradientes personalizados.
---
lang: es
lang-ref: ch.01
title: Semana 1
translation-date: 24 Mar 2020
translator: LecJackS
---


## Lección parte A

<!-- We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.
-->
Discutimos la motivación detrás del aprendizaje profundo. Comenzamos con la historia y la inspiración del mismo. Luego discutimos la historia del reconocimiento de patrones e introducimos el descenso de gradiente y su cálculo por retropropagación. Finalmente, discutimos la representación jerárquica de la corteza visual.


## Lección parte B

<!-- We first discuss the evolution of CNNs, from Fukushima to LeCun to AlexNet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.
-->
Primero discutimos la evolución de las CNNs, desde Fukushima a LeCun y hasta AlexNet. Luego discutimos algunas aplicaciones de CNNs, como la segmentación de imágenes, vehículos autónomos y análisis de imágenes médicas. Discutimos la naturaleza jerárquica de las redes profundas y los atributos que las hacen ventajosas. Concluimos con una discusión sobre la generación y el aprendizaje de características/representaciones.


## Práctica

<!-- We discuss the motivation for applying transformations to data points visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to understand the function and effects of these transformations. We walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by neural networks.
-->
Discutimos la motivación para aplicar transformaciones a puntos de datos visualizados en el espacio. Hablamos sobre Álgebra Lineal y la aplicación de transformaciones lineales y no lineales. Discutimos el uso de la visualización para comprender la función y los efectos de estas transformaciones. Analizamos ejemplos en Jupyter Notebook y concluimos con una discusión sobre las funciones representadas por las redes neuronales.

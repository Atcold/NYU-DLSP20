---
lang: es
lang-ref: ch.04-1
title: Álgebra lineal y convoluciones 
authors: Yuchi Ge, Anshan He, Shuting Gu, and Weiyang Wen
date: 27 Jan 2020
translation-date: 26 Mar 2020
translator: Manuel Pinar-Molina
---



## [Reseña de Álgebra Lineal](https://www.youtube.com/watch?v=OrBEon3VlQg&t=68s)

Esta parte es un resumen de álgebra lineal básica dentro del contexto de las redes neuronales. Empezamos con una capa oculta simple $\boldsymbol{h}$:

$$
\boldsymbol{h} = f(\boldsymbol{z})
$$

La salida es una función no linear $f$ aplicada a un vector $z$. Aquí $z$ es la salida de una trasnformación afín $\boldsymbol{A} \in\mathbb{R^{m\times n}}$ para el vector de entrada $\boldsymbol{x} \in\mathbb{R^n}$:

$$
\boldsymbol{z} = \boldsymbol{A} \boldsymbol{x}
$$

Para simplificar ignoraremos los sesgos. Podemos desarrollar la función linear del siguiente modo:

$$
\boldsymbol{A}\boldsymbol{x} =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix} \begin{pmatrix}
x_1 \\ \vdots \\x_n \end{pmatrix} =
\begin{pmatrix}
    \text{---} \; \boldsymbol{a}^{(1)} \; \text{---} \\
    \text{---} \; \boldsymbol{a}^{(2)} \; \text{---} \\
    \vdots \\
    \text{---} \; \boldsymbol{a}^{(m)} \; \text{---} \\
\end{pmatrix}
\begin{matrix}
    \rvert \\ \boldsymbol{x} \\ \rvert
\end{matrix} =
\begin{pmatrix}
    {\boldsymbol{a}}^{(1)} \boldsymbol{x} \\ {\boldsymbol{a}}^{(2)} \boldsymbol{x} \\ \vdots \\ {\boldsymbol{a}}^{(m)} \boldsymbol{x}
\end{pmatrix}_{m \times 1}
$$

donde $\boldsymbol{a}^{(i)}$ es la $i$-th fila de la matriz $\boldsymbol{A}$.

Para entender el significado de esta transformación, analizaremos un componente de $\boldsymbol{z}$ tal que $a^{(1)}\boldsymbol{x}$. Así  $n=2$, luego $\boldsymbol{a} = (a_1,a_2)$ y $\boldsymbol{x}  = (x_1,x_2)$. 

$\boldsymbol{a}$ y $\boldsymbol{x}$ pueden ser dibujados como vectores en los ejes de coordenadas de 2D. Ahora, si el ángulo entre $\boldsymbol{a}$ y $\hat{\boldsymbol{\imath}}$ es $\alpha$ y el ángulo entre $\boldsymbol{x}$ y $\hat{\boldsymbol{\imath}}$ es $\xi$, entonces con la formulación trigonométrica $a^\top\boldsymbol{x}$ puede ser desarrollado como:

$$
\begin {aligned}
\boldsymbol{a}^\top\boldsymbol{x} &= a_1x_1+a_2x_2\\
&=\lVert \boldsymbol{a} \rVert \cos(\alpha)\lVert \boldsymbol{x} \rVert \cos(\xi) + \lVert \boldsymbol{a} \rVert \sin(\alpha)\lVert \boldsymbol{x} \rVert \sin(\xi)\\
&=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert \big(\cos(\alpha)\cos(\xi)+\sin(\alpha)\sin(\xi)\big)\\
&=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert \cos(\xi-\alpha)
\end {aligned}
$$

La salida mide el alineado de la entrada con una fila específica de la matriz $\boldsymbol{A}$. Esto se puede entender observando el ángulo entre los dos vectores, $\xi-\alpha$. Cuando $\xi = \alpha$, los dos vectores están perfectamente alineados y se obtiene el máximo. Si $\xi - \alpha = \pi$, entonces $\boldsymbol{a}^\top\boldsymbol{x}$ alcanza su mínimo y los dos vectores están apuntando en direcciones opuestas. En esencia, la transformación lineal permitever la proyección de una entrada a varias orientacionesdefinidas por $A$. Esta afirmación es extensible a dimensiones superiores.


Otra forma de entender la transformación lineal es entendiendo que $\boldsymbol{z}$ también puede ser desarrollada como:

$$
\boldsymbol{A}\boldsymbol{x} =
\begin{pmatrix}
    \vert            & \vert            &        & \vert             \\
    \boldsymbol{a}_1 & \boldsymbol{a}_2 & \cdots & \boldsymbol{a}_n  \\
    \vert            & \vert            &        & \vert             \\
\end{pmatrix}
\begin{matrix}
    \rvert \\ \boldsymbol{x} \\ \rvert
\end{matrix} =
x_1 \begin{matrix} \rvert \\ \boldsymbol{a}_1 \\ \rvert \end{matrix} +
x_2 \begin{matrix} \rvert \\ \boldsymbol{a}_2 \\ \rvert \end{matrix} +
    \cdots +
x_n \begin{matrix} \rvert \\ \boldsymbol{a}_n \\ \rvert \end{matrix} +
$$

La salida es la suma ponderada de las columnas de la matriz $\boldsymbol{A}$. Por lo tanto, la señal no es más que una composición de la entrada.


## [Extender álgebra lineal a convoluciones](https://www.youtube.com/watch?v=OrBEon3VlQg&t=1030s)

Ahora extenderemos el álgebra lineal a convoluciones, usando un ejemplo de análisis de datos de audio. Empezamos representando una capa fully connected en forma de matriz de multiplicación: -

$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13}\\
w_{21} & w_{22} & w_{23}\\
w_{31} & w_{32} & w_{33}\\
w_{41} & w_{42} & w_{43}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

En este ejemplo, la matriz de pesos tiene un tamaño de $4 \times 3$, el vector de entrada tiene un tamaño de $3 \times 1$  y el vector de salida tiene un tamaño $4 \times 1$.

Sin embargo, para los datos de audio, los datos son mucho mayores (no 3-datos de longitud). El número de muestras en los datos de audio es igual a la duración del audio(ej. 3 segundos) por la tasa de muestreo (ej. 22.05 kHz). Como se muestra abajo, el vector de entrada $\boldsymbol{x}$ será muy grande. Así pues, la matriz de pesos se "engordará".

$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13} & w_{14} & \cdots &w_{1k}& \cdots &w_{1n}\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

La formulación de arriba será difícil de entrenar. Afortunadamente, hay maneras de simplificar esto.


### Propiedad: localidad

Debido a la localidad  (ej. no nos preocuparemos de los puntos lejanos) de los datos, $w_{1k}$ de la matriz de pesos de arriba, pueden ser sustituidos por 0 cuando $k$ es relativamente grande.Por lo tanto, la primera fila de la matriz se convierte  en kernel de dimensión 3. Verás este kernel de dimensión 3 como $\boldsymbol{a}^{(1)} = \begin{bmatrix} a_1^{(1)}  & a_2^{(1)}  & a_3^{(1)} \end{bmatrix}$.

$$
\begin{bmatrix}
a_1^{(1)}  & a_2^{(1)}  & a_3^{(1)}  & 0 & \cdots &0& \cdots &0\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$


### Propiedad: estacionalidad

Los datos de señales naturales tienen la propiedad de la estacionalidad (ej. ciertos patrones se repatirán). Esto nos ayuda a reusar el kernel $\mathbf{a}^{(1)}$  que definimos previamente. Usamos este kernel desplazándolo una posición cada vez (ej. stride = 1), resultando lo siguiente:

$$
\begin{bmatrix}
a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0 & 0 & 0 & 0&\cdots  &0\\
0 & a_1^{(1)}  & a_2^{(1)} & a_3^{(1)}  & 0&0&0&\cdots &0\\
0 & 0 & a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0&0&\cdots &0\\
0 & 0 & 0& a_1^{(1)}  & a_2^{(1)}  &a_3^{(1)} &0&\cdots &0\\
0 & 0 & 0& 0 & a_1^{(1)}  &a_2^{(1)} &a_3^{(1)} &\cdots &0\\
\vdots&&\vdots&&\vdots&&\vdots&&\vdots
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix}
$$

La parte superior derecha y la parte inferior izquierda de la matriz se sustituyen por $0$s gracias a la localidad, simplificando la matriz. El reusar ciertos kernels una y otra vez es conocido como compartir pesos.


### Multiples capas de la matriz de Teoplitz

Después de estos cambios,  el número de parámetros que nos quedan son 3 (ej. $a_1,a_2,a_3$).  En comparación con la matriz de pesos previa, que tenía 12 parámetros (ej. $w_{11},w_{12},\cdots,w_{43}$), El número actual de parámetros es demasiado restrictivo y nos gustaría ampliar el mismo.

La matriz previa puede ser considerada como una capa (ej. una capa convolucional) con el kernel $\boldsymbol{a}^{(1)}$. Entonces podemos construir multiples capas con diferentes kernels $\boldsymbol{a}^{(2)}$, $\boldsymbol{a}^{(3)}$, etc, de este modo incremenetando los parámetros.
Cada capa tiene una matriz que contiene solo un kernel que es copiado multiples veces. este tipo de matriz se llama matriz de Toeplitz. En cada matriz de Toeplitz, cada diagonal descendente de izquierda a derecha es constante. Las matrices Toeplitz que usamos aquí también son matrices reducidas.

Dado el primer kernel $\boldsymbol{a}^{(1)}$  y el vector de entrada $\boldsymbol{x}$, la primera posición de la salida dada por esta capa es, $a_1^{(1)} x_1 + a_2^{(1)} x_2 + a_3^{(1)}x_3$. Por lo tanto, el vector de salida completo tendrá la forma siguiente: -

$$
\begin{bmatrix}
\mathbf{a}^{(1)}x[1:3]\\
\mathbf{a}^{(1)}x[2:4]\\
\mathbf{a}^{(1)}x[3:5]\\
\vdots
\end{bmatrix}
$$

El mismo método de multiplicación de matriz en las siguientes capas convolucionales con otros kernels (ej. $\boldsymbol{a}^{(2)}$ y $\boldsymbol{a}^{(3)}$) para obtener resultados similares.


## [Escuchando convoluciones - Jupyter Notebook](https://www.youtube.com/watch?v=OrBEon3VlQg&t=1709s)

Puedes encontrar el Jupyter Notebook [here](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/07-listening_to_kernels.ipynb).

En este cuaderno, vamos a explorar Convolución como 'ejecutando un producto escalar'.

La librería `librosa` nos permite cargar un segmento de audio $\boldsymbol{x}$ y su frecuencia de muestreo. En este caso, hay 70641 muestras, la tasa de muestreo es 22.05kHz  y la longitud total del segmento de audio es 3.2s. La señal de audio importada es ondulada (ref. Fig 1) y podemos suponer como suena por la amplitud de eje $y$ . La señal de audio $x(t)$ es el sonido que suena cuando apagas el sistema operativo Windows (ref Fig 2).

<center>
<img src="{{site.baseurl}}/images/week04/04-1/audioSignal.png" width="500px" /><br>
<b>Fig. 1</b>: A visualization of the audio signal. <br>
</center>

<center>
<img src="{{site.baseurl}}/images/week04/04-1/notes.png" width="500px" /><br>
<b>Fig. 2</b>: Notes for the above audio signal.<br>
</center>


Necesitamos separar las notas de la forma de onda. Para conseguirlo, si usamos la transformada de Fourier (FT) todas las notas saldrían juntas y será difícil determinar el momento exacto, y la ubicación de cada tono. Por lo tanto, necesitaremos una FT localizada (también conocida como espectrograma). Como se observa en el espectrograma (ref Fig 3), diferentes tonos alcanzan su máxima frecuencia (ej. primer pico en 1600). Concatenar los cuatro tonos en sus frecuencias nos da una versión de tono de la señal original.

<center>
<img src="{{site.baseurl}}/images/week04/04-1/spectrogram.png" width="500px" /><br>
<b>Fig. 3</b>: Audio signal and its spectrogram.<br>
</center>

La convolución de la señal de entrada con todos los tonos (por ejemplo, todas las teclas del piano) puede ayudar a extraer todas las notas en la pieza de entrada (ej. las pulsaciones cuando el audio coincide con un kernel específico). Los espectrogramas de la señal original y la señal de los tonos concatenados se muestran en Fig 4 mientras que las frecuencias de la señal original y los cuatro tonos se muestran en Fig 5. La gráfica de la convolución de los cuatro kernels con la señal de entrada (señal original) se muestra en la Fig 6. La Fig 6 junto con los clips de audio de las convoluciones prueban la efectividad de las convoluciones para extraer las notas.


<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig4.png" width="500px" /><br>
<b>Fig. 4</b>: Spectrogram of original signal (left) and Sepctrogram of the concatenation of pitches (right).<br>
</center>

<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig5.png" width="500px" /><br>
<b>Fig. 5</b>: First melody's note.<br>
</center>

<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig6.png" width="500px" /><br>
<b>Fig. 6</b>: Convolution of four kernels.<br>
</center>


## Dimensionalidad de los diferentes conjuntos de datos

La última parte es una pequeña digresiónen las diferentes representaciones de la dimensionalidad y ejemplos de lo mismo. Aquí consideramos el conjunto de entrada $X$ está formado por funciones de mapeado del dominio $\Omega$ para los canales $c$.


### Ejemplos

* Datos de audio: el dominio es 1-D, señal discreta indexada en el tiempo; número de canales $c$ puede ir desde 1 (mono), 2 (stereo), 5+1 (Dolby 5.1), etc.
* Datos de imagen: el dominio es 2-D (píxeles); $c$ pueden ser desde 1(escala de grises), 3(color), 20(hiperespectral), *etc.*
* Relatividad especial: el dominio es $\mathbb{R^4} \times \mathbb{R^4}$ (space-time $\times$ four-momentum); cuando $c = 1$ es conocido como Hamiltonian.

<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig7.png" width="600px" /><br>
<b>Fig. 7</b>: Different dimensions of different types of signals.<br>
</center>

---
lang-ref: ch.06
title: Semana 6
lang: es
translation-date: 13 Apr 2020
translator: Victor Peñaloza
---

<!--## Lecture part A-->
## Lección parte A

<!--We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment.-->
Hemos discutido tres aplicaciones de las Redes Neuronales Convolucionales. Iniciamos con el reconocimiento de dígitos y su aplicación al reconocimiento de códigos ZIP de 5 dígitos. En detección de objetos, hablamos sobre cómo utilizar una arquitectura multi-escala en el escenario de detección de rostros. Finalmente, vimos como las Redes Neuronales Convolucionales son utilizadas en tareas de segmentación semántica con ejemplos concretos en un sistema de visión robótica y segmentación de objetos en un ambiente urbano.



<!--## Lecture part B-->

## Lección parte B
<!--We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq.-->
Examinamos las Redes Neuronales Recurrentes, sus problemas y las técnicas comúnmente utilizadas para mitigarlos. Luego revisamos una variedad de módulos desarrollados para resolver los problemas del modelo RNN, incluyendo Atención, GRUs (Gated Recurrent Units) (*Unidades Recurrentes con Compuertas*), LSTMs (Long Short-Term Memory) (*Gran memoria de corto plazo*) y Seq2Seq.



<!--## Practicum-->

## Práctica
<!--We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models.-->
Discutimos la arquitectura de modelos RNN convencionales y modelos LSTM, y comparamos el rendimiento entre los dos. Las redes LSTM heredan las ventajas de las RNNs y a la vez solucionan debilidades de las RNNs incluyendo una 'celda de memoria' para almacenar información en memoria por largos periodos de tiempo. Los modelos LSTM superan significativamente a los modelos RNN.
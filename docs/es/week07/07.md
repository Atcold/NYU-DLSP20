---
lang-ref: ch.07
title: Semana 7
lang: es
translation-date: 06 Sep 2020
translator: David Paredes
---

## Lección parte A

<!--We introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions.-->
Hemos introducido el concepto de los modelos basados en energía (EBM) y la intención de diferentes enfoques que no sean las redes prealimentadas. Para resolver la dificultad de la inferencia en EBM, variables latentes son usadas para proporcionar información auxiliar y permitir múltiples predicciones posibles. Finalmente, el EBM puede generalizarse al modelo probabilístico con funciones de puntuación más flexibles.


## Lección parte B

<!--We discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map.-->
Hemos discutimos el aprendizaje auto-supervisado, presentamos cómo entrenar modelos basados en energía, discutimos variables latentes de EBM, específicamente con un ejemplo explicado de K-means. También presentamos métodos de contraste, explicamos un autoencoder antirruido con un mapa topográfico, el proceso de entrenamiento y cómo se puede usar, seguido de una introducción a BERT. Finalmente, hablamos de divergencia de contraste, también explicada usando un mapa topográfico.

## Práctica
<!--We discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder.-->
Hemos discutimos algunas aplicaciones de autoencoders y hablamos sobre por qué queremos usarlas. Luego hablamos sobre diferentes arquitecturas de autoencoders (capa oculta subcompleta o sobrecompleta), cómo evitar problemas de sobreajuste y las funciones de pérdida que deberíamos usar. Finalmente implementamos un autoencoder estándar y un autoencoder antirruido

---
lang: pt
lang-ref: ch.05
title: Semana 5
translation-date: 05 Nov 2021
translator: Felipe Schiavon
---

<!--## Lecture part A
-->

## Aula parte A

<!--We begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on convergence.
-->

Começamos apresentando o Gradiente Descendente. Discutimos a intuição e também falamos sobre como os tamanhos dos passos desempenham um papel importante para se chegar à solução. Em seguida, passamos para Gradiente Descendente Estocástico (SGD) e seu desempenho em comparação com Gradiente Descendente completo (Full Batch GD). Por fim, falamos sobre as atualizações de momento, especificamente as duas regras de atualização, a intuição por trás do momento e seu efeito na convergência.

<!--
## Lecture part B
-->

## Aula parte B

<!--We discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient.
-->

Discutimos métodos adaptativos para SGD, como RMSprop e ADAM. Também falamos sobre camadas de normalização e seus efeitos no processo de treinamento das redes neurais. Finalmente, discutimos um exemplo do mundo real de redes neurais sendo usadas na indústria para tornar os exames de ressonância magnética mais rápidos e eficientes.

<!--
## Practicum
-->

## Prática

<!--We briefly review the matrix-multiplications and then discuss the convolutions. Key point is we use kernels by stacking and shifting. We first understand the 1D convolution by hand, and then use PyTorch to learn the dimension of kernels and output width in 1D and 2D convolutions examples. Furthermore, we use PyTorch to learn about how automatic gradient works and custom-grads.
-->

Revisamos brevemente as multiplicações de matrizes e, em seguida, discutimos as convoluções. O ponto principal é que usamos kernels por empilhamento e deslocamento. Primeiro entendemos a convolução de uma dimensão (1D) manualmente e, em seguida, usamos o PyTorch para aprender a dimensão dos kernels e da largura da saída em exemplos de convoluções de uma (1D) e duas dimensões (2D). Além disso, usamos o PyTorch para aprender sobre como o funciona o gradiente automático e os gradientes customizados.


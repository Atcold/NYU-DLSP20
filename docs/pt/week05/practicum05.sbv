0:00:00.000,0:00:05.339
last time we have seen that a matrix can be written basically let me draw here

0:00:05.339,0:00:12.719
the matrix so we had similar roles right and then we multiplied usually design by

0:00:12.719,0:00:18.210
one one column all right and so whenever we multiply these guys you can see these

0:00:18.210,0:00:23.340
and as two types two different equivalent types of representation it

0:00:23.340,0:00:28.980
can you see right you don't is it legible okay so you can see basically as

0:00:28.980,0:00:35.430
the output of this product has been a sequence of like the first row times

0:00:35.430,0:00:40.469
this column vector and then again I'm just okay shrinking them this should be

0:00:40.469,0:00:46.170
the same size right right because otherwise you can't multiply them so you

0:00:46.170,0:00:52.170
have this one and so on right until the last one and this is gonna be my final

0:00:52.170,0:01:00.960
vector and we have seen that each of these bodies here what are these I talk

0:01:00.960,0:01:05.339
to me please there's a scalar products right but what

0:01:05.339,0:01:08.820
do they represent what is it how can we call it what's another name for calling

0:01:08.820,0:01:13.290
a scalar product I show you last time a demonstration with some Chi government

0:01:13.290,0:01:18.119
trigonometry right what is it so this is all the projection if you

0:01:18.119,0:01:22.619
talk about geometry or you can think about this as a nun normalized cosine

0:01:22.619,0:01:29.310
value right so this one is going to be my projection basically of one kernel or

0:01:29.310,0:01:36.030
my input signal onto the kernel right so these are projections projection alright

0:01:36.030,0:01:40.619
and so then there was also a another interpretation of this like there is

0:01:40.619,0:01:45.390
another way of seeing this which was what basically we had the first column

0:01:45.390,0:01:53.579
of the matrix a multiplied by the first element of the X of these of this vector

0:01:53.579,0:01:58.260
right so back element number one then you had a second call

0:01:58.260,0:02:04.020
time's the second element of the X vector until you get to the last column

0:02:04.020,0:02:11.100
right times the last an element right suppose that this is long N and this is

0:02:11.100,0:02:16.110
M times n right so the height again is going to be the dimension towards we

0:02:16.110,0:02:19.550
should - and the width of a matrix is dimension where we're coming from

0:02:19.550,0:02:24.810
second part was the following so we said instead of using this matrix here

0:02:24.810,0:02:29.450
instead since we are doing convolutions because we'd like to exploit sparsity a

0:02:29.450,0:02:35.400
stationarity and compositionality of the data we still use the same matrix here

0:02:35.400,0:02:41.370
perhaps right we use the same guy here but then those kernels we are going to

0:02:41.370,0:02:45.510
be using them over and over again the same current across the whole signal

0:02:45.510,0:02:51.360
right so in this case the width of this matrix is no longer be it's no longer n

0:02:51.360,0:02:56.820
as it was here is going to be K which is gonna be the kernel size right so here

0:02:56.820,0:03:03.090
I'm gonna be drawing my thinner matrix and this one is gonna be K lowercase K

0:03:03.090,0:03:10.140
and the height maybe we can still call it n okay all right so let's say here I

0:03:10.140,0:03:18.230
have several kernels for example let me have my tsiyon carnal then I may have my

0:03:18.230,0:03:25.080
other non green let me change let's put pink so you have this one and

0:03:25.080,0:03:33.180
then you may have green one right and so on so how do we use these kernels right

0:03:33.180,0:03:38.280
now so we basically can use these kernels by stacking them and shifted

0:03:38.280,0:03:43.650
them a little bit right so we get the first kernel out of here and then you're

0:03:43.650,0:03:50.519
gonna get basically you get the first guy here then you shift it shift it

0:03:50.519,0:03:58.290
shift it and so on right until you get the whole matrix and we were putting a 0

0:03:58.290,0:04:02.100
here and a 0 here right this is just recap and then you have this one for the

0:04:02.100,0:04:11.379
blue color now you do magic here and just do copy copy and I you do paste

0:04:11.379,0:04:19.370
and now you can also do color see fantastic magic and we have pink one and

0:04:19.370,0:04:25.360
then you have the last one right can I do the same copy yes I can do fantastic

0:04:25.360,0:04:29.080
so you cannot do copy and paste on the paper

0:04:29.080,0:04:38.419
all right color and the last one light green okay all right so we just

0:04:38.419,0:04:44.479
duplicate how many matrices do we have now how many layers no don't count the

0:04:44.479,0:04:50.600
number like there are letters on the on the screen and K or M what is it what is

0:04:50.600,0:05:00.620
K the side usually you're just guessing you shouldn't be guessing you should

0:05:00.620,0:05:07.120
tell me the correct answer I think about this as a job interview I'm training you

0:05:07.120,0:05:14.990
so how many maps we have and right so this one here are as many as my M which

0:05:14.990,0:05:21.470
is the number of rows of this initial thing over here right all right so what

0:05:21.470,0:05:30.289
is instead the width of this little kernel here okay right okay what is the

0:05:30.289,0:05:41.349
height of this matrix what is the height of the matrix

0:05:42.340,0:05:45.480
you sure try again

0:05:49.220,0:06:04.310
I can't hear and minus k plus one okay and the final what is the output of this

0:06:04.310,0:06:08.660
thing right so the output is going to be one vector which is gonna be of height

0:06:08.660,0:06:19.430
the same right and minus k plus 1 and then it should be correct yeah but then

0:06:19.430,0:06:27.890
how many what is the thickness of this final vector M right so this stuff here

0:06:27.890,0:06:35.600
and goes as thick as M right so this is where we left last time right but then

0:06:35.600,0:06:39.770
someone asked me now then I realized so we have here as many as the different

0:06:39.770,0:06:45.170
colors right so for example in this case if I just draw to make sure we

0:06:45.170,0:06:49.730
understand what's going on you have the first thing here now you have the second

0:06:49.730,0:06:55.600
one here and I have the third one right in this case all right so last time they

0:06:59.750,0:07:03.650
asked me if someone asked me at the end of the class so how do we do convolution

0:07:03.650,0:07:09.760
when we end up in this situation over here because here we assume that my

0:07:09.760,0:07:14.990
corners are just you know whatever K long let's say three long but then they

0:07:14.990,0:07:21.380
are just one little vector right and so somebody told me no then what do you do

0:07:21.380,0:07:24.950
from here like how do we keep going because now we have a thickness before

0:07:24.950,0:07:32.510
we started with a something here this vector which had just n elements right

0:07:32.510,0:07:35.690
are you following so far I'm going faster because we already seen these

0:07:35.690,0:07:44.030
things I'm just reviewing but are you with me until now yes no yes okay

0:07:44.030,0:07:47.720
fantastic so let's see how we actually keep going so the thing is

0:07:47.720,0:07:51.680
show you right now is actually assuming that we start with that long vector

0:07:51.680,0:08:01.400
which was of height what was the height and right but in this case also this one

0:08:01.400,0:08:13.060
means that we have something that looks like this and so you have basically here

0:08:13.060,0:08:20.720
this is 1 this is also 1 so we only have a monophonic signal for example and this

0:08:20.720,0:08:26.300
was n the height right all right so let's assume now we're using a

0:08:26.300,0:08:33.950
stereophonic system so what is gonna be my domain here so you know my X can be

0:08:33.950,0:08:39.740
thought as a function that goes from the domain to the ℝ^{number of channels} so

0:08:39.740,0:08:47.840
what is this guy here yeah x is one dimension and somewhere so what is this

0:08:47.840,0:08:59.930
Ω we have seen this slide last slide of Tuesday lesson right second Ω is

0:08:59.930,0:09:11.720
not set of real numbers no someone else tries we are using computers it's time

0:09:11.720,0:09:16.520
line yes and how many samples you you have one sample number sample number two

0:09:16.520,0:09:21.710
or sample number three so you have basically a subset of the natural space

0:09:21.710,0:09:30.860
right so this one is going to be something like 0 1 2 so on set which is

0:09:30.860,0:09:36.410
gonna be subset of ℕ right so it's not ℝ. ℝ is gonna be if you have time

0:09:36.410,0:09:45.850
continuous domain what you see in this case the in the case I just showed you

0:09:45.850,0:09:55.160
so far what is seen in this case now number of input channels because this is

0:09:55.160,0:10:00.740
going to be my X right this is my input so in this case we show so far in this

0:10:00.740,0:10:07.220
case here we were just using one so it means we have a monophonic audio let's

0:10:07.220,0:10:10.880
seven now the assumption make the assumption that this guy is that it's

0:10:10.880,0:10:22.780
gonna be two such that you're gonna be talking about stereo phonic signal right

0:10:23.200,0:10:27.380
okay so let's see how this stuff changes so

0:10:27.380,0:10:38.450
in this case my let me think yeah so how do I draw I'm gonna just draw right

0:10:38.450,0:10:43.400
little complain if you don't follow are you following so far yes because if

0:10:43.400,0:10:46.550
i watch my tablet I don't see you right so you should be complaining if

0:10:46.550,0:10:50.750
something doesn't make sense right otherwise becomes boring from waiting

0:10:50.750,0:10:56.390
and watching you all the time right yes no yes okay I'm boring okay

0:10:56.390,0:11:00.080
thank you all right so we have here this signal

0:11:00.080,0:11:07.280
right and then now we have some thickness in this case what is the

0:11:07.280,0:11:14.660
thickness of this guy see right so in this case this one is going to be C and

0:11:14.660,0:11:18.589
in the case of the stereophonic signal you're gonna just have two channels left

0:11:18.589,0:11:30.170
and right and this one keeps going down right all right so our kernels if I'd

0:11:30.170,0:11:35.030
like to perform a convolution over this signal right so you have different same

0:11:35.030,0:11:44.150
pussy right and so on right if I'd like to perform a convolution one big

0:11:44.150,0:11:47.089
convolution I'm not talking about two deconvolution right because they are

0:11:47.089,0:11:52.670
still using domain which is here number one right so this is actually important

0:11:52.670,0:11:58.510
so if I ask you what type of signal this is you're gonna be basically

0:11:58.510,0:12:02.890
you have to look at this number over here right so we are talking about one

0:12:02.890,0:12:12.490
dimensional signal which is one dimensional domain right 1d domain okay

0:12:12.490,0:12:17.710
so we are still using a 1d signal but in this case it has you know you have two

0:12:17.710,0:12:25.750
values per point so what kind of kernels are we gonna be using so I'm gonna just

0:12:25.750,0:12:31.450
draw it in this case we're gonna be using something similar like this so I'm

0:12:31.450,0:12:37.990
gonna be drawing this guy let's say I have K here which is gonna be my width

0:12:37.990,0:12:42.700
of the kernel but in this case I'm gonna be also have some thickness in this case

0:12:42.700,0:12:56.230
here right so basically you apply this thing here okay and then you can go

0:12:56.230,0:13:04.060
second line and third line and so on right so you may still have like here m

0:13:04.060,0:13:11.590
kernels but in this case you also have some thickness which has to match the

0:13:11.590,0:13:17.680
other thickness right so this thickness here has to match the thickness of the

0:13:17.680,0:13:23.980
input size so let me show you how to apply the convolution so you're gonna

0:13:23.980,0:13:37.980
get one of these slices here and then you're gonna be applying this over here

0:13:39.320,0:13:46.190
okay and then you simply go down this way

0:13:46.190,0:13:53.870
alright so whenever you apply these you perform this guy here the inner product

0:13:53.870,0:14:04.410
with these over here what you get it's actually a one by one is a scalar so

0:14:04.410,0:14:09.540
whenever I use this orange thingy here on the left hand side and I do a dot

0:14:09.540,0:14:14.190
product scalar product with this one I just get a scalar so this is actually my

0:14:14.190,0:14:19.620
convolution in 1d the convolution in 1d means that it goes down this way and

0:14:19.620,0:14:27.480
only in one way that's why it's called 1d but we multiply each element of this

0:14:27.480,0:14:36.290
mask times this guy here now a second row and this guy here okay

0:14:36.290,0:14:41.090
you saw you multiply all of them you sum all of them and then you get your first

0:14:41.090,0:14:47.250
output here okay so whenever I make this multiplication I get my first output

0:14:47.250,0:14:52.050
here then I keep sliding this kernel down and then you're gonna get the

0:14:52.050,0:14:58.380
second output third out fourth and so on until you go down at the end then what

0:14:58.380,0:15:03.780
happens then happens that I'm gonna be picking up different kernel I'm gonna

0:15:03.780,0:15:07.950
back it let's say I get the third one okay let's get the second one I get a

0:15:07.950,0:15:19.050
second one and I perform the same operation you're gonna get here this one

0:15:19.050,0:15:23.240
actually let's actually make it like a matrix

0:15:26.940,0:15:33.790
you go down okay until you go with the last one which is gonna be the end right

0:15:33.790,0:15:45.450
the empty kernel which is gonna be going down this way you get the last one here

0:15:51.680,0:15:58.790
okay yes no confusing clearing so this was the question I got at the end of the

0:15:58.790,0:16:10.339
class yeah Suzy yeah because it's a dot product of all those values between so

0:16:10.339,0:16:18.259
basically do the projection of this part of the signal onto this kernel so you'd

0:16:18.259,0:16:22.879
like to see what is the contribution like what is the alignment of this part

0:16:22.879,0:16:27.350
of the signal on to this specific subspace okay this is how a convolution

0:16:27.350,0:16:31.850
works when you have multiple channels so far I'll show you just with single

0:16:31.850,0:16:35.319
channel now we have multiple channels okay so oh yeah yeah in one second one

0:16:54.259,0:16:59.509
and one one at the top one at the bottom so you actually lose the first row here

0:16:59.509,0:17:04.850
and you lose the last row here so at the end in this case the output is going to

0:17:04.850,0:17:10.490
be n minus three plus one so you lose two one on top okay in this case you

0:17:10.490,0:17:15.140
lose two at the bottom if you actually do a Center at the center the

0:17:15.140,0:17:20.390
convolution usually you lose one at the beginning one at the end every time you

0:17:20.390,0:17:24.409
perform a convolution you lose the number of the dimension of the kernel

0:17:24.409,0:17:28.789
minus one you can try if you put your hand like this you have a kernel of

0:17:28.789,0:17:34.340
three you get the first one here and it is matching then you switch one and then

0:17:34.340,0:17:39.440
you switch to right so okay with fight let's tell a parent of two right so you

0:17:39.440,0:17:44.149
have your signal of five you have your kernel with two you have one two three

0:17:44.149,0:17:49.070
and four so we started with five and you end up with four because you use a

0:17:49.070,0:17:54.500
kernel size of two if you use a kernel size of three you get one two and three

0:17:54.500,0:17:57.289
so you goes to if you use a kernel size of three okay

0:17:57.289,0:18:01.010
so you can always try to do this alright so I'm gonna show you now the

0:18:01.010,0:18:07.040
dimensions of these kernels and the outputs with PyTorch okay Yes No

0:18:07.040,0:18:18.500
all right good okay mister can you see anything

0:18:18.500,0:18:25.520
yes right I mean zoom a little bit more okay so now we can go we do

0:18:25.520,0:18:33.770
conda activate pDL, pytorch Deep Learning.

0:18:33.770,0:18:40.520
So here we can just run ipython if i press ctrl L I clear the screen and

0:18:40.520,0:18:49.820
we can do import torch then I can do from torch import nn so now we can see

0:18:49.820,0:18:54.500
for example called let's set my convolutional convolutional layer it's

0:18:54.500,0:18:59.930
going to be equal to NN conf and then I can keep going until I get

0:18:59.930,0:19:04.220
this one let's say yeah let's say I have no idea how to use this function I just

0:19:04.220,0:19:08.750
put a question mark I press ENTER and I'm gonna see here now the documentation

0:19:08.750,0:19:13.460
okay so in this case you're gonna have the first item is going to be the input

0:19:13.460,0:19:19.820
channel then I have the output channels then I have the corner sighs alright so

0:19:19.820,0:19:24.290
for example we are going to be putting here input channels we have a stereo

0:19:24.290,0:19:30.530
signal so we put two channels the number of corners we said that was M and let's

0:19:30.530,0:19:36.650
say we have 16 kernels so this is the number of kernels I'm gonna be using and

0:19:36.650,0:19:41.810
then let's have our kernel size of what the same I use here so let's have K or

0:19:41.810,0:19:47.570
the kernel size equal 3 okay in so here I'm going to define my first convolution

0:19:47.570,0:19:52.910
object so if I print this one comes you're gonna see we have a convolution a

0:19:52.910,0:19:57.580
2d combo sorry 1 deconvolution made that okay so we have a 1d convolution

0:20:02.149,0:20:08.869
which is going from two channels so a stereophonic to a sixteen channels means

0:20:08.869,0:20:16.039
I use sixteen kernels the skirmish size is 3 and then the stride is also 1 ok so

0:20:16.039,0:20:23.859
in this case I'm gonna be checking what is gonna be my convolutional weights

0:20:27.429,0:20:33.379
what is the size of the weights how many weights do we have how many how

0:20:33.379,0:20:40.069
many planes do we have for the weights 16 right so we have 16 weights what is

0:20:40.069,0:20:53.649
the length of the the day of the key of D of the kernel okay Oh what is this -

0:20:54.549,0:21:00.349
Janis right so I have 16 of these scanners which have thickness - and then

0:21:00.349,0:21:05.539
length of 3 ok makes sense right because you're gonna be applying each of these

0:21:05.539,0:21:11.629
16 across the whole signal so let's have my signal now you're gonna be is gonna

0:21:11.629,0:21:20.599
be equal toage dot R and and and oh sighs I don't know let's say 64 I also

0:21:20.599,0:21:25.129
have to say I have a batch of size 1 so I have a virtual site one so I just have

0:21:25.129,0:21:31.879
one signal and then this is gonna be 64 how many channels we said this has two

0:21:31.879,0:21:37.819
right so I have one signal one example which has two channels and has 64

0:21:37.819,0:21:46.689
samples so this is my X hold on what is the convolutional bias size

0:21:48.320,0:21:54.380
a 16 right because you have one bias / plain / / / way ok so what's gonna be in

0:21:54.380,0:22:07.539
our my convolution of X the output hello so I'm gonna still have one sample right

0:22:07.539,0:22:15.919
how many channels 16 what is gonna be the length of the signal okay that's

0:22:15.919,0:22:22.700
good 6 fix it okay fantastic all right so what if I'm gonna be using

0:22:22.700,0:22:32.240
a convolution with size of the kernel 5 what do I get now yet to shout I can't

0:22:32.240,0:22:36.320
hear you 60 okay you're following fantastic okay

0:22:36.320,0:22:44.059
so let's try now instead to use a hyper spectral image with a 2d convolution

0:22:44.059,0:22:49.100
okay so I'm going to be coding now my convolution here is going to be my in

0:22:49.100,0:22:55.490
this case is correct or is going to be a conf come to D again I don't know how to

0:22:55.490,0:22:59.059
use it so I put a question mark and then I have here input channel output channel

0:22:59.059,0:23:05.450
criticize strident padding okay so I'm going to be putting inputs tried input

0:23:05.450,0:23:10.429
channel so it's a hyper spectral image with 20 planes so what's gonna be the

0:23:10.429,0:23:16.149
input in this case 20 right because you have you start from 20 spectral bands

0:23:16.149,0:23:20.419
then we're gonna be inputting the output number of channels we let's say we're

0:23:20.419,0:23:25.330
gonna be using again 16 in this case I'm going to be inputting the kernel size

0:23:25.330,0:23:33.440
since I'm planning to use okay let's actually define let's actually define my

0:23:33.440,0:23:40.120
signal first so my X is gonna be a torch dot R and and let's say one sample with

0:23:40.120,0:23:52.820
20 channels of height for example I guess 6128 well hold on 64 and then with

0:23:52.820,0:23:58.820
128 okay so this is gonna be my my input my eople data okay

0:23:58.820,0:24:04.370
so my convolution now it can be something like this so I have 20

0:24:04.370,0:24:09.110
channels from input 16 our Mike Ernest I'm gonna be using then I'm gonna be

0:24:09.110,0:24:15.050
specifying the kernel size in this case let's use something that is like three

0:24:15.050,0:24:24.580
times five okay so what is going to be the output what are the kernel size

0:24:29.170,0:24:47.630
anyone yes no what no 20 Janice is the channels of the input data right so you

0:24:47.630,0:24:51.680
have how many kernels here 16 right there you go

0:24:51.680,0:24:56.420
we have 16 kernels which have 20 channels such that they can lay over the

0:24:56.420,0:25:03.410
input 3 by 5 right teeny like a short like yeah short but large ok so what is

0:25:03.410,0:25:08.140
gonna be my conv(x).size ? [1, 16, 62, 124]. Let's say I'd like to

0:25:16.310,0:25:22.190
actually add back the I'd like to head the sing dimensionality I can add some

0:25:22.190,0:25:25.730
padding right so here there is going to be the stride I'm gonna have a stride of

0:25:25.730,0:25:29.930
1 again if you don't remember the the syntax you can just put the question

0:25:29.930,0:25:35.120
mark can you figure out and then how much strive should I add now how much

0:25:35.120,0:25:41.870
stride in the y-direction sorry yes how much padding should I add in the

0:25:41.870,0:25:46.490
y-direction one because it's gonna be one on top one on the bottom but then

0:25:46.490,0:25:51.890
then on the x-direction okay you know you're following fantastic and so now if

0:25:51.890,0:25:57.320
I just run this one you wanna get the initial size okay so now you have both

0:25:57.320,0:26:05.500
1d and 2d the point is that what is the dimension of a convolutional kernel and

0:26:05.500,0:26:12.470
symbol for to the dimensional signal again I repeat what is the

0:26:12.470,0:26:20.049
dimensionality of the collection of careness use for two-dimensional data

0:26:20.860,0:26:27.679
again for right so four is gonna be the number of dimensions that are required

0:26:27.679,0:26:35.659
to store the collection of kernels when you perform 2d convolutions the one is

0:26:35.659,0:26:40.370
going to be the stride so if you don't know how this works you just put a

0:26:40.370,0:26:44.000
question mark and gonna tell you here so stride is gonna be telling you you

0:26:44.000,0:26:50.929
stride off you move every time the kernel by one if you are the first one

0:26:50.929,0:26:55.460
means you only is the batch size so torch expects you to always use batches

0:26:55.460,0:27:00.110
meaning how many signals you're using just one right so that our expectation

0:27:00.110,0:27:04.549
if you send an input vector which is going to be input tensor which has

0:27:04.549,0:27:12.289
dimension three is gonna be breaking and complain okay so we have still some time

0:27:12.289,0:27:18.049
to go in the second part all right second part is going to be so you've

0:27:18.049,0:27:23.779
been computing some derivatives right for the first homework right so the

0:27:23.779,0:27:31.909
following homework maybe you have to do you have to compute this one okay you're

0:27:31.909,0:27:35.510
supposed to be laughing it's a joke okay there you go

0:27:35.510,0:27:43.340
fantastic so this is what you can wrote back in the 90s for the computation of

0:27:43.340,0:27:50.029
the gradients of the of the lsdm which are gonna be covered I guess in next

0:27:50.029,0:27:54.950
next lesson so how somehow so they had to still do these things right it's kind

0:27:54.950,0:28:00.769
of crazy nevertheless we can use PyTorch to have automatic computation of these

0:28:00.769,0:28:06.500
gradients so we can go and check out how these automatic gradient works

0:28:06.500,0:28:12.159
okay all right so all right so we are going to be going

0:28:23.090,0:28:28.490
now to the notebook number three which is the yeah

0:28:28.490,0:28:33.590
invisible let me see if I can highlight it now it's even worse okay number three

0:28:33.590,0:28:41.619
Auto gratitute Oriole okay let me go fullscreen

0:28:41.619,0:28:53.029
okay so out of our tutorial was gonna be here here just create my tensor which

0:28:53.029,0:28:57.499
has as well these required gradients equal true in this case I mean asking

0:28:57.499,0:29:02.539
torch please track all the gradient computations did it got the competition

0:29:02.539,0:29:07.749
over the tensor such that we can perform computation of partial derivatives okay

0:29:07.749,0:29:13.279
in this case I'm gonna have my Y is going to be so X is simply gonna be one

0:29:13.279,0:29:20.419
two three four the Y is going to be X subtracted number two okay alright so

0:29:20.419,0:29:26.869
now we can notice that there is this grad F n grad f NN FN function here so

0:29:26.869,0:29:32.059
let's see what this stuff is we go sit there and see oh this is a sub backward

0:29:32.059,0:29:37.629
what is it meaning that the Y has been generated by a module which performs the

0:29:37.629,0:29:43.669
subtraction between X and and - right so you have X minus 2 therefore if you

0:29:43.669,0:29:51.860
check who generated Y well there's a sub a subtraction module ok so what's gonna

0:29:51.860,0:30:01.009
be now the God function of X you're supposed to answer oh okay

0:30:01.009,0:30:03.580
why is none because they should have written there

0:30:07.580,0:30:12.020
Alfredo generated that right okay all right none is fine as well

0:30:12.020,0:30:17.000
okay so let's actually put our nose inside we were here we can actually

0:30:17.000,0:30:23.770
access the first element you have the accumulation why is the accumulation I

0:30:25.090,0:30:29.830
don't know I forgot but then if you go inside there you're gonna see the

0:30:29.830,0:30:34.760
initial vector the initial tensor we are using is the one two three four okay so

0:30:34.760,0:30:41.390
inside this computational graph you can also find the original tensor okay all

0:30:41.390,0:30:46.880
right so let's now get the Z and inside is gonna be my Y square times three and

0:30:46.880,0:30:51.620
then I compute my average a it's gonna be the mean of Z right so if I compute

0:30:51.620,0:30:56.330
the square of this thing here and I multiply by three and I take the average

0:30:56.330,0:31:00.500
so this is the square part times 3 and then this is the average okay so you can

0:31:00.500,0:31:06.200
try if you don't believe me all right so let's see how this thing looks like so

0:31:06.200,0:31:10.549
I'm gonna be promoting here all these sequence of computations so we started

0:31:10.549,0:31:16.669
by from a two by two matrix what was this guy here to buy - who is this X

0:31:16.669,0:31:22.399
okay you're following it cool then we subtracted - right and then we

0:31:22.399,0:31:27.440
multiplied by Y twice right that's why you have to ro so you get the same

0:31:27.440,0:31:31.669
subtraction that is the whyatt the X minus 2 multiplied by itself then

0:31:31.669,0:31:36.649
you have another multiplication what is this okay multiply by three and then you

0:31:36.649,0:31:42.980
have the final the mean backward because this Y is green because it's mean no

0:31:42.980,0:31:51.140
okay yeah thank you for laughing okay so I compute back prop right

0:31:51.140,0:31:59.409
what does backdrop do what does this line do

0:32:00.360,0:32:08.610
I want to hear everyone you know already we compute what radians right so black

0:32:08.610,0:32:11.580
propagation is how you compute the gradients how do we train your networks

0:32:11.580,0:32:20.730
with gradients ain't right or whatever Aaron said yesterday back

0:32:20.730,0:32:27.000
propagation is that is used for computing the gradient completely

0:32:27.000,0:32:29.970
different things okay please keep them separate don't merge

0:32:29.970,0:32:34.559
them everyone after a bit that don't they don't see me those two things keep

0:32:34.559,0:32:43.740
colliding into one mushy thought don't it's painful okay she'll compute the

0:32:43.740,0:32:51.659
gradients right so guess what we are computing some gradients now okay so we

0:32:51.659,0:33:02.580
go on your page it's going to be what what was a it was the average right so

0:33:02.580,0:33:10.529
this is 1/4 right the summation of all those zᵢ

0:33:10.529,0:33:17.460
what so I goes from 1 to 4 okay so what is that I said I is going

0:33:17.460,0:33:27.539
to be equal to 3yᵢ² right yeah no questions no okay all right and then

0:33:27.539,0:33:36.840
this one is was equal to 3(x-2)² right so a what does it belong

0:33:36.840,0:33:38.899
to where does a belong to what is the ℝ

0:33:44.279,0:33:51.200
right so it's a scaler okay all right so now we can compute ∂a/∂x.

0:33:51.200,0:33:58.110
So how much is this stuff you're gonna have 1/4 comes out forum here and

0:33:58.110,0:34:03.090
then you have you know let's have this one with respect to the xᵢ element

0:34:03.090,0:34:09.179
okay so we're gonna have this one zᵢ inside is that, I have the 3yᵢ²,

0:34:09.179,0:34:15.899
and it's gonna be 3(xᵢ- 2)². Right so these three comes

0:34:15.899,0:34:22.080
out here the two comes down as well and then you multiply by (xᵢ – 2).

0:34:22.080,0:34:33.260
So far should be correct okay fantastic all right so my X was this element here

0:34:33.589,0:34:38.190
actually let me compute as well this one so this one goes away this one becomes

0:34:38.190,0:34:47.690
true this is 1.5 times xᵢ – 3. Right - 2 - 3

0:34:55.159,0:35:06.780
ok mathematics okay okay thank you all right. So what's gonna be ∂a/∂x ?

0:35:06.780,0:35:11.339
I'm actually writing the transpose directly here so for the first element

0:35:11.339,0:35:18.859
you have one you have one times 1.5 so 1.5 minus 3 you get 1 minus 1.5 right

0:35:18.859,0:35:23.670
second one is going to be 3 minus 3 you get 0 Ryan this is 3 minus 3

0:35:23.670,0:35:27.420
maybe I should write everything right so you're actually following so you have

0:35:27.420,0:35:37.589
1.5 minus 3 now you have 3 minus 3 below you have 4 point 5 minus 3 and then the

0:35:37.589,0:35:47.160
last one is going to be 6 minus 3 which is going to be equal to minus 1 point 5

0:35:47.160,0:35:59.789
0 1 point 5 and then 3 right you agree ok let me just write this on here

0:35:59.789,0:36:06.149
okay just remember so we have you be computed the backpropagation here I'm

0:36:06.149,0:36:14.609
gonna just bring it to the gradients and then the right it's the same stuff we

0:36:14.609,0:36:27.630
got here right such that I don't have to transpose it here whenever you perform

0:36:27.630,0:36:33.209
the partial derivative in PyTorch you get the same the same shape is the input

0:36:33.209,0:36:37.469
dimension so if you have a weight whatever dimension then when you compute

0:36:37.469,0:36:41.069
the partial you still have the same dimension they don't swap they don't

0:36:41.069,0:36:44.789
turn okay they just use this for practicality at the correct version I

0:36:44.789,0:36:49.919
mean the the gradient should be the transpose of that thing sorry did

0:36:49.919,0:36:54.479
Jacobian which is the transpose of the gradient right if it's a vector but this

0:36:54.479,0:37:08.130
is a tensor so whatever we just used the same same shape thing no so this one

0:37:08.130,0:37:13.639
should be a flipping I believe maybe I'm wrong but I don't think all right so

0:37:13.639,0:37:19.919
this is like basic these basic PyTorch now you can do crazy stuff because we

0:37:19.919,0:37:23.609
like crazy right I mean I do I think if you like me you

0:37:23.609,0:37:29.669
like crazy right okay so here I just create my

0:37:29.669,0:37:34.259
vector X which is going to be a three dimensional well a one-dimensional

0:37:34.259,0:37:43.769
tensor of three items I'm going to be multiplying X by two then I call this

0:37:43.769,0:37:49.859
one Y then I start my counter to zero and then until the norm of the Y is long

0:37:49.859,0:37:56.699
thousand below thousand I keep doubling Y okay and so you can get like a dynamic

0:37:56.699,0:38:01.529
graph right the graph is base is conditional to the actual random

0:38:01.529,0:38:04.979
initialization which you can't even tell because I didn't even use a seed so

0:38:04.979,0:38:08.999
everyone that is running this stuff is gonna get different numbers so these are

0:38:08.999,0:38:11.910
the final values of the why can you tell me

0:38:11.910,0:38:23.549
how many iterations we run so the mean of this stuff is actually lower than a

0:38:23.549,0:38:27.630
thousand yeah but then I'm asking whether you know how many times this

0:38:27.630,0:38:41.119
loop went through no good why it's random Rises you know it's bad question

0:38:41.119,0:38:45.539
about bad questions next time I have a something for you okay so I'm gonna be

0:38:45.539,0:38:51.569
printing this one now I'm telling you the grabbed are 2048 right

0:38:51.569,0:38:55.589
just check the central one for the moment right this is the actual gradient

0:38:55.589,0:39:04.739
so can you tell me now how many times the loop went on so someone said 11 how

0:39:04.739,0:39:14.420
many ends up for 11 okay for people just roast their hands what about the others

0:39:14.809,0:39:17.809
21 okay any other guys 11 10

0:39:25.529,0:39:30.749
okay we have actually someone that has the right solution and this loop went on

0:39:30.749,0:39:35.759
for 10 times why is that because you have the first multiplication by 2 here

0:39:35.759,0:39:40.589
and then loop goes on over and over and multiplies by 2 right so the final

0:39:40.589,0:39:45.239
number is gonna be the least number of iterations in the loop plus the

0:39:45.239,0:39:50.779
additional like addition and multiplication outside right yes no

0:39:50.779,0:39:56.670
you're sleeping maybe okay I told you not to eat before class otherwise you

0:39:56.670,0:40:05.009
get groggy okay so inference this is cool so here I'm gonna be just having

0:40:05.009,0:40:09.420
both my X & Y we are gonna just do linear regression right linear or

0:40:09.420,0:40:17.670
whatever think the add operator is just the scalar product okay so both the X

0:40:17.670,0:40:21.589
and W has have the requires gradient equal to true

0:40:21.589,0:40:27.119
being this means we are going to be keeping track of the the gradients and

0:40:27.119,0:40:31.290
the computational graph so if I execute this one you're gonna get the partial

0:40:31.290,0:40:37.710
derivatives of the inner product with respect to the Z with respect to the

0:40:37.710,0:40:43.920
input is gonna be the weights right so in the range is the input right and the

0:40:43.920,0:40:47.160
ones are the weights so partial derivative with respect to the input is

0:40:47.160,0:40:50.070
gonna be the weights partial with respect to the weights are gonna be the

0:40:50.070,0:40:56.670
input right yes no yes okay now I just you know usually it's this one is the

0:40:56.670,0:41:00.359
case I just have required gradients for my parameters because I'm gonna be using

0:41:00.359,0:41:06.030
the gradients for updating later on the the parameters of the mother is so in

0:41:06.030,0:41:12.300
this case you get none let's have in this case instead what I usually do

0:41:12.300,0:41:17.250
wanna do inference when I do inference I tell torch a torch stop tracking any

0:41:17.250,0:41:22.950
kind of operation so I say torch no God please so this one regardless of whether

0:41:22.950,0:41:28.859
your input always have the required grass true or false whatever when I say

0:41:28.859,0:41:35.060
torch no brats you do not have any computation a graph taken care of right

0:41:35.060,0:41:41.130
therefore if I try to run back propagation on a tensor which was

0:41:41.130,0:41:46.320
generated from like doesn't have actually you know graph because this one

0:41:46.320,0:41:50.940
doesn't have a graph you're gonna get an error okay so if I run this one you get

0:41:50.940,0:41:55.410
an error and you have a very angry face here because it's an error and then it

0:41:55.410,0:42:00.720
takes your element 0 of tensor does not require grads and does not have a god

0:42:00.720,0:42:07.650
function right so II which was the yeah whatever they reside here actually then

0:42:07.650,0:42:11.400
you couldn't run back problems that because there is no graph attached to

0:42:11.400,0:42:19.710
that ok questions this is so powerful you cannot do it this time with tensor

0:42:19.710,0:42:26.790
you okay tensor flow is like whatever yeah more stuff here actually more stuff

0:42:26.790,0:42:30.600
coming right now [Applause]

0:42:30.600,0:42:36.340
so we go back here we have inside the extra folder he has some nice cute

0:42:36.340,0:42:40.450
things I wanted to cover both of them just that we go just for the second I

0:42:40.450,0:42:47.290
think sorry the second one is gonna be the following so in this case we are

0:42:47.290,0:42:52.750
going to be generating our own specific modules so I like let's say I'd like to

0:42:52.750,0:42:58.030
define my own function which is super special amazing function I can decide if

0:42:58.030,0:43:02.560
I want to use it for you know training Nets I need to get the forward pass and

0:43:02.560,0:43:06.220
also have to know what is the partial derivative of the input respect to the

0:43:06.220,0:43:10.930
output such that I can use this module in any kind of you know point in my

0:43:10.930,0:43:15.670
inner code such that you know by using back prop you know chain rule you just

0:43:15.670,0:43:20.320
plug the thing. Yann went on several times as long as you know partial

0:43:20.320,0:43:23.410
derivative of the output with respect to the input you can plug these things

0:43:23.410,0:43:31.690
anywhere in your chain of operations so in this case we define my addition which

0:43:31.690,0:43:35.620
is performing the addition of the two inputs in this case but then when you

0:43:35.620,0:43:41.130
perform the back propagation if you have an addition what is the back propagation

0:43:41.130,0:43:47.020
so if you have a addition of the two things you get an output when you send

0:43:47.020,0:43:53.320
down the gradients what does it happen with the with the gradient it gets you

0:43:53.320,0:43:57.160
know copied over both sides right and that's why you get both of them are

0:43:57.160,0:44:01.390
copies or the same thing and they are sent through one side of the other you

0:44:01.390,0:44:05.170
can execute this stuff you're gonna see here you get the same gradient both ways

0:44:05.170,0:44:09.460
in this case I have a split so I come from the same thing and then I split and

0:44:09.460,0:44:13.180
I have those two things doing something else if I go down with the gradient what

0:44:13.180,0:44:20.080
do I do you add them right and that's why we have here the add install you can

0:44:20.080,0:44:23.680
execute this one you're going to see here that we had these two initial

0:44:23.680,0:44:27.910
gradients here and then when you went up or sorry when you went down the two

0:44:27.910,0:44:30.790
things the two gradients sum together and they are here okay

0:44:30.790,0:44:36.190
so again if you use pre-made things in PyTorch. They are correct this one you

0:44:36.190,0:44:41.080
can mess around you can put any kind of different in

0:44:41.080,0:44:47.950
for a function and backward function I think we ran out of time other questions

0:44:47.950,0:44:58.800
before we actually leave no all right so I see on Monday and stay warm

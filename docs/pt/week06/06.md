---
lang: pt
lang-ref: ch.06
title: Semana 6
translator: Bernardo Lago
---

<!--
## Lecture part A

We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment.
-->

## Aula parte A

Discutimos três aplicações de redes neurais convolucionais. Começamos com o reconhecimento de dígitos e a aplicação para um reconhecimento de código postal (CEP) de 5 dígitos. Na detecção de objetos, falamos sobre como usar a arquitetura multi-escala em uma configuração de detecção de faces. Por último, vimos como ConvNets são usados em tarefas de segmentação semântica com exemplos concretos em um sistema de visão robótica e segmentação de objetos em um ambiente urbano.

<!--
## Lecture part B

We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq.
-->

## Aula parte B

Examinamos redes neurais recorrentes, seus problemas e técnicas comuns para mitigar esses problemas. Em seguida, revisamos uma variedade de módulos desenvolvidos para resolver os problemas do modelo RNN, incluindo Atenção (Attention), GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory) e Seq2Seq.


<!--
## Practicum
We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models.
-->

## Prática

Discutimos a arquitetura dos modelos de RNN básica (vanilla) e LSTM e comparamos o desempenho entre os dois. O LSTM herda as vantagens do RNN, ao mesmo tempo em que melhora os pontos fracos do RNN ao incluir uma 'célula de memória' para armazenar informações na memória por longos períodos de tempo. Os modelos LSTM superam significativamente os modelos RNN.
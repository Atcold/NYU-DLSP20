---
lang-ref: ch.07
title: Semana 7
lang: pt
translation-date: 5 March 2021
translator: Catarina Carvalho
---
<!--## Lecture part A
-->
## Curso - parte A
<!--We introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions.
-->
Introduzimos o conceito dos modelos baseados em energia e o propósito de diferentes abordagens de redes para além das redes de propagação para a frente (feedforward). Para resolver a dificuldade da inferência no EBM, variáveis latentes são usadas para fornecer informações auxiliares e permitir várias previsões possíveis. Finalmente, o EBM pode generalizar para o modelo probabilístico com funções de pontuação mais flexíveis.
<!--## Lecture part B
-->
## Curso - parte B
<!--We discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map.
-->
Discutimos a aprendizagem semi-supervisionada, introduzimos como treinar um modelos baseados em energia, discutimos variáveis latentes EBM, especificamente com um exemplo explicado K-means. Também introduzimos métodos contrastivos, explicamos um autoencoder de denoising com um mapa topográfico, o seu processo de treinamento e como ele pode ser usado, seguido por uma introdução ao BERT. Finalmente, falamos sobre divergência contrastiva, também explicada usando um mapa topográfico.
<!--## Practicum
We discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder.
-->
## Prática
Discutimos algumas aplicações de autoencoders e falamos por que queremos usá-los. Em seguida, falamos sobre diferentes arquiteturas de autoencoders (sob ou mais camadas ocultas completas), como evitar problemas de memorização excessiva (overfitting) e as funções de perda que devemos usar. Finalmente, implementamos um autoencoder padrão e um denoising autoencoder.

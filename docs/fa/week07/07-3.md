---
lang-ref: ch.07-3
title: Introduction to autoencoders
lecturer: Alfredo Canziani
authors: Xinmeng Li, Atul Gandhi, Li Jiang, Xiao Li
date: 10 March 2020
---


<!--## [Application of autoencoders](https://www.youtube.com/watch?v=bggWQ14DD9M&t=55s)-->
##[کاربردهای اتوانکودر](https://www.youtube.com/watch?v=bggWQ14DD9M&t=55s)

<!--### Image generation-->
### تولید تصویر
<!--Can you tell which face is fake in Fig. 1? In fact, both of them are produced by the StyleGan2 generator. Although the facial details are very realistic, the background looks weird (left: blurriness, right: misshapen objects). This is because the neural network is trained on faces samples. The background then has a much higher variability. Here the data manifold has roughly 50 dimensions, equal to the degrees of freedom of a face image.-->
&#x202b; آیا می‌توانید بگویید کدام صورت در تصویر ۱ جعلی است؟ در واقع جفت این تصوایر توسط تولید‌کننده‌ی Style Gan2 ساخته شده اند. با وجود اینکه اطلاعات چهره بسیار واقعی هستند، پس‌زمینه عجیب به نظر می‌رسد (سمت چپ: محو، سمت راست: اشکال موهوم). این به خاطر شبکه‌ی عصبی‌ای است که روی نمونه‌ی صورت‌ها آموزش داده شده است. پس‌زمینه اما متغیر‌های بیشتری دارد. در اینجا گوناگونی داده تقریبا ۵۰ بعد دارد و برابر با درجه‌ی آزادی یک تصویر صورت است.
<center>
<img src="{{site.baseurl}}/images/week07/07-3/1_faces_gen.jpg" height="150px" /><br>
<!--<b>Fig. 1</b>: Faces generated from StyleGan2-->
&#x202b; <b>شکل. ۱</b>: صورت‌های تولید شده توسط StyleGan2
</center>


<!--### Difference of Interpolation in Pixel Space and Latent Space-->
### &#x202b; تفاوت درون یابی در فضای پیکسل و فضای نهفته
<center>
<img src="{{site.baseurl}}/images/week07/07-3/2_dog.jpg" height="120px"/><img src="{{site.baseurl}}/images/week07/07-3/2_bird.jpg" height="120px"/>
<br>
<!--<b>Fig. 2</b>: A dog and a bird-->
<b>شکل. ۲</b>: یک سگ و یک پرنده
</center>

<!--If we linearly interpolate between the dog and bird image (Fig. 2) in pixel space, we will get a fading overlay of two images in Fig. 3. From the top left to the bottom right, the weight of the dog image decreases and the weight of the bird image increases.-->
&#x202b;اگر در فضای پیکسلی بین تصویر سگ و پرنده به صورت خطی درون‌یابی کنیم، یک هم‌پوشانی محو از دو تصویر در تصویر ۳ خواهیم داشت. از گوشه‌ی بالا چپ، تا پایین راست، وزن تصویر سگ کاهش می‌یابد و وزن تصویر پرنده افزایش.
 
<center>
<img src="{{site.baseurl}}/images/week07/07-3/3_dog2bird.jpg" height="200px"/><br>
<!--<b>Fig. 3</b>: Results after interpolation-->
<b>شکل. ۳</b>: نتایج بعد از درون‌یابی
</center>

<!--If we interpolate on two latent space representation and feed them to the decoder, we will get the transformation from dog to bird in Fig. 4.-->
&#x202b; اگر بر روی نمایش دو فضای پنهان درون‌یابی کنیم و آن ها را به دیکودر بدهیم، تصویر تبدیل‌یافته‌آن از سگ به پرنده را در تصویر ۴ خواهیم داشت.
<center>
<img src="{{site.baseurl}}/images/week07/07-3/4_model_d2b.jpg" height="200px"/><br>
<!--<b>Fig. 4</b>: Results after feeding into decoder-->
<b>شکل. ۴</b>: نتایج بعد از وارد کردن به یک دکودر
</center>

<!--Obviously, latent space is better at capturing the structure of an image.-->
&#x202b; واضحا، فضای پنهان در دریافت ساختار یک عکس عملکرد بهتری دارد.

<!--### Transformation Examples-->
### مثال‌های تغییر حالت

<center>
<img src="{{site.baseurl}}/images/week07/07-3/5_zoom1.jpg
" height="120px"/><img src="{{site.baseurl}}/images/week07/07-3/5_zoom2.jpg
" height="120px"/>
<br>
<!--<b>Fig. 5</b>: Zoom-->
<b>شکل. ۵</b>: زوم
</center>

<center>
<img src="{{site.baseurl}}/images/week07/07-3/6_shift1.jpg
" height="120px"/><img src="{{site.baseurl}}/images/week07/07-3/6_shift2.jpg
" height="120px"/>
<br>
<!--<b>Fig. 6</b>: Shift-->
<b>شکل. ۶</b>: شیفت
</center>

<center>
<img src="{{site.baseurl}}/images/week07/07-3/7_bright1.jpg
" height="120px"/><img src="{{site.baseurl}}/images/week07/07-3/7_bright2.jpg" height="120px"/>
<br>
<!--<b>Fig. 7</b>: Brightness-->
<b>شکل. ۷</b>: روشنایی
</center>

<center>
<img src="{{site.baseurl}}/images/week07/07-3/8_rotation1.jpg" height="120px"/><img src="{{site.baseurl}}/images/week07/07-3/8_rotation2.jpg" height="120px"/>
<br>
<!--<b>Fig. 8</b>: Rotation (Note that the rotation could be 3D)-->
&#x202b;<b>ؤکل. ۸</b> چرخش (دقت کنید که چرخش ممکن است سه بعدی باشد)
</center>


<!--### Image Super-resolution-->
### وضوح بالای تصاویر
<!--This model aims to upscale images and reconstruct the original faces. From left to right in Fig. 9, the first column is the 16x16 input image, the second one is what you would get from a standard bicubic interpolation, the third is the output generated by the neural net, and on the right is the ground truth. (https://github.com/david-gpu/srez)-->
&#x202b; این مدل با هدف افزایش تصاویر و بازسازی صورت‌های اصلی به وجود آمده است. از چپ به راست در تصویر ۹، ستون اول ورودی تصاویر ۱۶×۱۶ است، ستون دوم آن‌چه که از یک درون‌یابی دو مکعبی خواهید گرفت می‌باشد، و ستون سوم خروجی تولید شده توسط شبکه عصبی است، و در سمت راست زمینه‌ی واقعی قرار دارد.
<center><img src="{{site.baseurl}}/images/week07/07-3/9_reconstruct.jpg" height="120px"/>
<br>
<!--<b>Fig. 9</b>: Reconstructing original faces-->
<b>شکل. ۹</b>: بازسازی صورت‌های اصلی
</center>

<!--From the output images, it is clear that there exist biases in the training data, which makes the reconstructed faces inaccurate. For example, the top left Asian man is made to look European in the output due to the imbalanced training images. The reconstructed face of the bottom left women looks weird due to the lack of images from that odd angle in the training data.-->
&#x202b; از تصاویر خروجی اینطور برمی‌اید که بایاس‌هایی در داده‌ی آموزش داده وجود دارند، که باعث می‌شوند صورت‌های بازسازی شده دقیق نباشند. برای مثال، مرد آسیایی در بالا چپ به دلیل تصاویر آموزش دیده غیر بالانس، در خروجی اروپایی به نظر می‌رسد. تصویر بازسازی شده در سمت چپ پایین، خانمی‌است که به دلیل نبود تصاویر از زوایای فرد در داده‌ی آموزش دیده، عجیب به نظر می‌رسد.

<!--### Image Inpainting-->
### &#x202b; بی‌رنگ کردن تصویر
<center>
<img src="{{site.baseurl}}/images/week07/07-3/10_facepatch.jpg" height="120px"/>
<br>
<!--<b>Fig. 10</b>: Putting grey patch on faces-->
<b>شکل. ۱۰</b>: گذاشتن نمونه‌های خاکستری روی صورت‌ها
</center>

<!--Putting a grey patch on the face like in Fig. 10 makes the image away from the training manifold. The face reconstruction in Fig. 11 is done by finding the closest sample image on the training manifold via Energy function minimization.-->
&#x202b; وجود یک تکه‌ی خاکستری در تصویر مانند تصویر ۱۰، تصویر را از گوناگونی آموز دیده دور می‌کند. بازسازی صورت در تصویر ۱۱ با استفاده از پیدا کردن نزدیک‌ترین تصویر نمونه در گوناگونی آموزش از طریق کمینه کردن تابع انرژی، انجام شده است.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/11_fixfacepatch.jpg" height="120px"/>
<br>
<!--<b>Fig. 11</b>: Reconstructed image of <b>Fig. 10</b>-->
<b>شکل. ۱۱</b>: بازسازی تصاویر از <b>شکل. ۱۰</b>
</center>


<!--### Caption to Image-->
### متن به تصویر
<center>
<img src="{{site.baseurl}}/images/week07/07-3/12_caption.jpg" height="50px"/><img src="{{site.baseurl}}/images/week07/07-3/12_capimage.jpg" height="150px"/>
<br>
<!--<b>Fig. 12</b>: Caption to Image example-->
<b>شکل. ۱۲</b> مثال متن به تصویر
</center>

<!--The translation from text description to image in Fig. 12 is achieved by extracting text features representations associated with important visual information and then decoding them to images.-->
&#x202b; ترجمه‌ توضیحات متن به تصویر در تصویر ۱۲ با استفاده از بازیابی نمایش ویژگی‌های متن که با مهم‌ترین داده‌های بصری مرتبط می‌باشد، به دست آمده و سپس به تصویر دیکد شده است.

<!--## [What are autoencoders?](https://www.youtube.com/watch?v=bggWQ14DD9M&t=879s)-->
## &#x202b; [اتوانکودر‌ها چه هستند؟](https://www.youtube.com/watch?v=bggWQ14DD9M&t=879s)
<!--Autoencoders are artificial neural networks, trained in an unsupervised manner, that aim to first learn encoded representations of our data and then generate the input data (as closely as possible) from the learned encoded representations. Thus, the output of an autoencoder is its prediction for the input.-->
&#x202b; اتوانکدر ها شبکه‌های عصبی مصنوعی هستند که به یک روش بی‌نظارتی آموزش دیده اند و هدف آن‌ها اولا یادگیری نمایش‌های انکد شده از داده‌های ما است و ثانیا داده‌ی ورودی را از نمایش‌ انکد شده‌‌ای که یاد گرفته‌اند، تولید می‌کنند. بنابراین، خروجی یک اتوانکدر پیش‌بینی آن برای ورودی‌ آن می‌باشد.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/13_ae_structure.png" width="40%"/> <br>
<!--<b>Fig. 13</b>: Architecture of a basic autoencoder<br>-->
<b>شکل. ۱۳</b>: معماری یک اتوانکودر پایه
</center>

<!--Fig. 13 shows the architecture of a basic autoencoder. As before, we start from the bottom with the input $\boldsymbol{x}$ which is subjected to an encoder (affine transformation defined by $\boldsymbol{W_h}$, followed by squashing). This results in the intermediate hidden layer $\boldsymbol{h}$. This is subjected to the decoder(another affine transformation defined by $\boldsymbol{W_x}$ followed by another squashing). This produces the output $\boldsymbol{\hat{x}}$, which is our model's prediction/reconstruction of the input. As per our convention, we say that this is a 3 layer neural network.-->
&#x202b; تصویر ۱۳ معماری یک اتوانکدر اولیه را نشان می‌دهد. مانند قبل، از پایین با ورودی .. که زیر نظر یک اتوانکدر است، شروع می‌کنیم (دگرگونی تعریف شده توسط ...، که توسط squashing دنبال شده). این در لایه‌ی مخفی میانی خود را نشان می‌دهد. این تحت نظر دیکودر است ( تبدیل تعریف شده‌ی دیگری.. که توسط یک Squashing دنبال می‌شود). این خروجی .. را تولید می‌کند، که پیش‌بینی/ بازسازی از ورودی مدلمان است. طبق قرارداد ما، می‌گوییم این یک شبکه عصبی ۳ لایه است.
<!--We can represent the above network mathematically by using the following equations:-->
&#x202b; شبکه‌ی بالا را به صورت ریاضی با استفاده از معادلات زیر نمایش می‌دهیم:

$$
\boldsymbol{h} = f(\boldsymbol{W_h}\boldsymbol{x} + \boldsymbol{b_h}) \\
\boldsymbol{\hat{x}} = g(\boldsymbol{W_x}\boldsymbol{h} + \boldsymbol{b_x})
$$

<!--We also specify the following dimensionalities:-->
&#x202b; ابعاد زیر را نیز مشخص می‌کنیم:
$$
\boldsymbol{x},\boldsymbol{\hat{x}} \in \mathbb{R}^n\\
\boldsymbol{h} \in \mathbb{R}^d\\
\boldsymbol{W_h} \in \mathbb{R}^{d \times n}\\
\boldsymbol{W_x} \in \mathbb{R}^{n \times d}\\
$$

<!--<b>Note:</b> In order to represent PCA, we can have tight weights (or tied weights) defined by $\boldsymbol{W_x}\ \dot{=}\ \boldsymbol{W_h}^\top$-->
<b>Note:</b> &#x202b; برای نمایش PCA، می‌توانیم وزن‌های ثابت (یا وزن‌های برابر) که توسط فرمول روبه رو تعریف می‌شود داشته باشیم:$\boldsymbol{W_x}\ \dot{=}\ \boldsymbol{W_h}^\top$

<!--## Why are we using autoencoders?-->
## چرا ما از اتوانکودرها استفاده میکنیم؟
<!--At this point, you may wonder what the point of predicting the input is and what are the applications of autoencoders.-->
&#x202b; در اینجا، ممکن است بخواهید بدانید که هدف از پیش‌بینی ورودی چیست و برنامه‌های خود رمزگذار چیست؟
<!--The primary applications of an autoencoder is for anomaly detection or image denoising. We know that an autoencoder's task is to be able to reconstruct data that lives on the manifold *i.e.* given a data manifold, we would want our autoencoder to be able to reconstruct only the input that exists in that manifold. Thus we constrain the model to reconstruct things that have been observed during training, and so any variation present in new inputs will be removed because the model would be insensitive to those kinds of perturbations.-->
&#x202b; کاربردهای اولیه‌ی یک اتوانکدر برای تشخیص ناهنجاری یا Denoising تصویر است. می‌دانیم که وظیفه‌ی یک اتوانکدر این است تا برای بازسازی داده‌ای که روی گوناگونی مانند یک گوناگونی داده، آمادگی داشته باشد، ما می‌خواهیم تا اتوانکدر ما، تنها بتواند ورودی‌ای که بر روی آن گوناگونی وجود دارد را بازسازی کند. بنابراین ما از مدل انتظار داریم تا آنچه را که در طول آزمایش مشاهده شده، بازسازی کند، و بنابراین هرگونه تغییر موجود در ورودی‌های جدید به علت عدم حساسیت نسبت به انواع اغتشاشات حذف خواهد شد.

<!--Another application of an autoencoder is as an image compressor. If we have an intermediate dimensionality $d$ lower than the input dimensionality $n$, then the encoder can be used as a compressor and the hidden representations (coded representations) would address all (or most) of the information in the specific input but take less space.-->
&#x202b; کاربرد دیگر یک اتوانکدر به عنوان یک متراکم‌کننده تصویر است. اگر ما یک بعد میانی D که کوچکتر از بعد ورودی n است، داشته باشیم، انکودر می‌تواند به عنوان یک متراکم کننده عمل کند و نمایش‌های مخفی (نمایش‌های کدگذاری شده) همه‌ی( یا بخشی) اطلاعات در یک ورودی مشخص در نظر می‌گیرند، اما فضای کمتری اشغال می‌کنند.

<!--## Reconstruction loss-->
##تلفات بازسازی

<!--Let us now look at the reconstruction losses that we generally use. The overall loss for the dataset is given as the average per sample loss i.e.-->
&#x202b; حال نگاهی به تلفات بازسازی که انجام دادیم بیاندازیم. تلفات کلی برای مجموعه‌دادگان در نظر گرفته شده، مساوی با میانگین تلفات هر نمونه در نظر گرفته شده‌ است.
$$
L = \frac{1}{m} \sum_{j=1}^m \ell(x^{(j)},\hat{x}^{(j)})
$$

<!--When the input is categorical, we could use the Cross-Entropy loss to calculate the per sample loss which is given by-->
&#x202b; وقتی ورودی دسته‌بندی شده است، ما می‌توانیم از تلف Cross-Entropy برای محاسبه‌ی تلف در هر نمونه که به صورت زیر داده شده است استفاده کنیم:

$$
\ell(\boldsymbol{x},\boldsymbol{\hat{x}}) = -\sum_{i=1}^n [x_i \log(\hat{x}_i) + (1-x_i)\log(1-\hat{x}_i)]
$$

<!--And when the input is real-valued, we may want to use the Mean Squared Error Loss given by-->
&#x202b; و وقتی ورودی دارای مقدار واقعی است، شاید بخواهیم تا از تلف MSE به صورت زیر استفاده کنیم:

$$
\ell(\boldsymbol{x},\boldsymbol{\hat{x}}) = \frac{1}{2} \lVert \boldsymbol{x} - \boldsymbol{\hat{x}} \rVert^2
$$


<!--## Under-/over-complete hidden layer-->
## کاملا تحت-/فوق- لایه‌ی مخفی
<!--When the dimensionality of the hidden layer $d$ is less than the dimensionality of the input $n$ then we say it is under complete hidden layer. And similarly, when $d>n$, we call it an over-complete hidden layer. Fig. 14 shows an under-complete hidden layer on the left and an over-complete hidden layer on the right.-->
&#x202b; وقتی که بعد لایه‌ی مخفی $d$ کمتر از بعد ورودی $n$ باشد، آنگا می‌گوییم که کاملا تحت (نظر) لایه‌ی مخفی هستیم. و به طور مشابه، وقتی $d$ از $n$ بزرگتر باشد، آن را فوق بر لایه‌ی مخفی (تاثیر گذار بر آن) می‌دانیم. تصویر ۱۴ یک کاملا تحت نظر لایه‌ی مخفی در سمت چپ و یک تاثیر گذار بر لایه‌ی مخفی در سمت راست را نشان می‌دهد.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/14_over_under_complete.png" width="60%"/> <br>
<!--<b>Fig. 14</b>: An under-complete *vs.* an over-complete hidden layer<br>-->
<b>شکل. ۱۴</b>: یک لایه‌ی مخفی کاملا تخت در مقابل یک لایه‌ی مخفی کاملا فوق
</center>

<!--As discussed above, an under-complete hidden layer can be used for compression as we are encoding the information from input in fewer dimensions. On the other hand, in an over-complete layer, we use an encoding with higher dimensionality than the input. This makes optimization easier.-->
&#x202b; همانطور که در بالا مطرح شد، یک بعد کاملا زیر نظر لایه‌ی مخفی می‌تواند برای متراکم‌سازی از آن‌جایی که ما در حال انکد کردن اطلاعات از ورودی در ابعاد محدودتر هستیم، مورد استفاده قرار گیرد. در طرف دیگر، در یک بعد تاثیر گذار بر لایه‌ی مخفی، از انکد کردن با ابعاد بالاتر از ورودی استفاده می‌کنیم. این بهینه‌سازی را آسان‌تر می‌کند.

<!--Since we are trying to reconstruct the input, the model is prone to copying all the input features into the hidden layer and passing it as the output thus essentially behaving as an identity function. This needs to be avoided as this would imply that our model fails to learn anything. Hence, we need to apply some additional constraints by applying an information bottleneck. We do this by constraining the possible configurations that the hidden layer can take to only those configurations seen during training. This allows for a selective reconstruction (limited to a subset of the input space) and makes the model insensitive to everything not in the manifold.-->
&#x202b; تا زمانی که برای بازسازی ورودی در تلاشیم، مدل به کپی کردن تمام ویژگی‌های ورودی به لایه‌ی مخفی و ارسال آن به عنوان خروجی تمایل دارد، بنابراین به عنوان تابع همانی رفتار می‌کند. از آن‌جایی که این به معنای آن است که مدل ما قادر به یادگیری چیزی نیست، باید از این کار جلوگیری شود. بنابراین، نیاز است تا با ایجاد یک گلوگاه اطلاعاتی محدودیت‌های مازاد ایجاد کنیم. این کار را با محدود کردن تنظیمات ممکن که لایه‌های مخفی می‌توانند بگیرند به تنظیماتی که تنها در طول آموزش می‌توان داشت، انجام می‌دهیم. این منجر به یک بازسازی انتخابی (محدود به یک بستر فضای ورودی می‌شود) می‌شود و مدل را به تمام آن‌چیزی که در گوناگونی نیست، غیر حساس می‌کند.

<!--It is to be noted that an under-complete layer cannot behave as an identity function simply because the hidden layer doesn't have enough dimensions to copy the input. Thus an under-complete hidden layer is less likely to overfit as compared to an over-complete hidden layer but it could still overfit. For example, given a powerful encoder and a decoder, the model could simply associate one number to each data point and learn the mapping. There are several methods to avoid overfitting such as regularization methods, architectural methods, etc.-->
&#x202b; شایان ذکر است که لایه‌ی تحت نظر نمی‌تواند مانند یک تابع همانی رفتار کند، زیرا که لایه‌ی مخفی فضای کافی برای کپی کردن ورودی ندارد. بنابراین در یک لایه‌ی مخفی تحت نظر در مقایسه با یک لایه‌ی تاثیر گذار، کمتر توقع اوورفیت کردن داریم، اما هنوز امکان رخ دادن اوورفیت وجود دارد. برای مثال، با داشتن یک انکودر و دیکودر قوی، مدل به سادگی می‌تواند به هر نقطه‌ی داده یک عدد نسبت دهد و نگاشت را یاد بگیرد. راه‌های متعددی برای جلوگیری از اوور فیت مانند روش‌های مناسب‌سازی، معماری و غیره وجود دارد.

<!--## Denoising autoencoder-->
&#x202b; اتوانکودر Denoising


<!--Fig.15 shows the manifold of the denoising autoencoder and the intuition of how it works.-->
&#x202b; تصویر ۱۵ گوناگونی اتوانکودر Denoising و دیدگاه پشت عملکرد آن است.
<center>
<img src="{{site.baseurl}}/images/week07/07-3/15_denoising_ae.png" width="500px" /><br>
<!--<b>Fig. 15</b>: Denoising autoencoder<br>-->
<b>شکل. ۱۵</b>: اتوانکودر Denoising
</center>

<!--In this model, we assume we are injecting the same noisy distribution we are going to observe in reality, so that we can learn how to robustly recover from it.-->
&#x202b; در این مدل، فرض می‌کنیم همان توزیع اختلالی که در واقعیت می‌خواهیم مشاهده کنیم را تزریق می‌کنیم، بنابراین می‌توانیم یاد بگیریم چگونه به خوبی آن را بازیابی کنیم. با مقایسه‌ی ورودی و خروجی، می‌توانیم بگوییم داده‌هایی که روی گوناگونی قرار دارند حرکت نکرده‌اند، و نقاطی که خارج از گوناگونی هستند خیلی حرکت کرده‌اند.

<!--By comparing the input and output, we can tell that the points that already on the manifold data did not move, and the points that far away from the manifold moved a lot.-->

<!--Fig.16 gives the relationship between the input data and output data.-->
&#x202b; شکل. ۱۶ رابطه‌ی بیت داده ورودی و داده‌ی خروجی را به ما می‌دهد

<center>
<img src="{{site.baseurl}}/images/week07/07-3/16_relation1.png" width="350px" />
<img src="{{site.baseurl}}/images/week07/07-3/16_relation2.png" width="330px" />
<br>
<!--<b>Fig. 16</b>: Input and output of denoising autoencoder<br>-->
<b>ؤکل. ۱۶</b>: ورودی و خروجی اتوانکودر Denoising
</center>

<!--We can also use different colours to represent the distance of each input point moves, Fig.17 shows the diagram.-->
&#x202b; ما همچنین برای نمایش فاصله حرکت هر یک از نقاط ورودی از رنگ‌های مختلفی می‌توانیم استفاده کنیم. شکل. ۱۷ دیاگرام را نشان می‌دهد.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/17_distance.png" width="500px" /><br>
<!--<b>Fig. 17</b>: Measuring the traveling distance of the input data<br>-->
<b>شکل. ۱۷</b>: اندازه‌گیری فاصله‌ی بین داده‌های ورودی
</center>

<!--The lighter the colour, the longer the distance a point travelled. From the diagram, we can tell that the points at the corners travelled close to 1 unit, whereas the points within the 2 branches didn't move at all since they are attracted by the top and bottom branches during the training process.-->
&#x202b; رنگ‌های روشن‌تر، نشان‌دهنده‌ی مسافت بیشتری که یک نقطه سفر کرده می‌باشند. از طریق تلگرام، می‌توانیم بگوییم که نقاط از کناره‌ها به نزدیکی ۱واحد آمده‌اند، در حالی‌که نقاط داخل دو شاخه به هیچ وجه حرکت نمی‌کنند، زیرا در طول فرآیند آموزش توسط شاخه‌های بالا و پایین جذب می‌شوند.

<!--## Contractive autoencoder-->
## اتوانکودر انقباضی

<!--Fig.18 shows the loss function of the contractive autoencoder and the manifold.-->
&#x202b; شکل. ۱۸ نشان‌دهنده‌ی تابع تلف اتوانکودر انقباضی و نمایش آن است.
<center>
<img src="{{site.baseurl}}/images/week07/07-3/18_contractive_ae.png" width="500px" /><br>
<!--<b>Fig. 18</b>: Contractive autoencoder <br>-->
<b>شکل. ۱۸</b>: اتوانکودر انقباضی
</center>

<!--The loss function contains the reconstruction term plus squared norm of the gradient of the hidden representation with respect to the input. Therefore, the overall loss will minimize the variation of the hidden layer given variation of the input. The benefit would be to make the model sensitive to reconstruction directions while insensitive to any other possible directions.-->
&#x202b;تابع تلف شامل بخش‌های بازسازی به اضافه نرم مربعات گرادیان لایه‌های مخفی با توجه به ورودی است. بنابراین، اتلاف کلی تغییرات لایه پنهان را با توجه به تنوع ورودی، کمینه می‌کند. مزیت آن در این است که این مدل نسبت به جهات بازسازی حساس باشد در حالی‌که نسبت به سایر جهات احتمالی حساس نیست.

<!--Fig.19 shows how these autoencoders work in general.-->
&#x202b; شکل. ۱۹ نشان دهندی این است که این اتوانکودرها به صورت کلی چگونه کار می‌کنند.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/19_basic_ae.png" width="500px" /><br>
<!--<b>Fig. 19</b>: Basic autoencoder <br>-->
<b>شکل. ۱۹</b>: اتوانکودر پایه<br>
</center>

<!--The training manifold is a single-dimensional object going in three dimensions. Where $\boldsymbol{x}\in \boldsymbol{X}\subseteq\mathbb{R}^{n}$, the goal for autoencoder is to stretch down the curly line in one direction, where $\boldsymbol{z}\in \boldsymbol{Z}\subseteq\mathbb{R}^{d}$. As a result, a point from the input layer will be transformed to a point in the latent layer. Now we have the correspondence between points in the input space and the points on the latent space but do not have the correspondence between regions of the input space and regions of the latent space. Afterwards, we will utilize the decoder to transform a point from the latent layer to generate a meaningful output layer.-->
&#x202b; گوناگونی آموزش یک هدف تک بعدی است که به سه بعد می رود. جایی که .... ، هدف برای اتوانکودر گسترش دادن خط پیچی در یک جهت است، در جایی که... . به عنوان یک نتیجه، یک نقطه از لایه‌ی ورودی به یک نقطه در لایه‌ی مخفی تبدیل می‌شود. اکنون ارتباط بین نقاط در فضای ورودی و نقاط در فضای پنهان را داریم، اما ارتباط بین نواحی فضای ورودی و نواحی فضای مخفی را نداریم. پس از آن، باز یک دکودر برای تبدیل یک نقطه از لایه‌ی پنهان را برای تولید یک لایه‌ی خروجی معنی‌دار، استفاده می‌کنیم

<!--## [Implement autoencoder - Notebook](https://www.youtube.com/watch?v=bggWQ14DD9M&t=2491s)-->
## &#x202b; [پیاده‌سازی اتوانکودر-Notebook](https://www.youtube.com/watch?v=bggWQ14DD9M&t=2491s)

<!--The Jupyter Notebook can be found [here](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/10-autoencoder.ipynb).-->
&#x202b; ژوپیتر نوتبوک را در اینجا می‌شود پیدا کرد [اینجا](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/10-autoencoder.ipynb)

<!--In this notebook, we are going to implement a standard autoencoder and a denoising autoencoder and then compare the outputs.-->
&#x202b; در این نوت‌بوک، می‌خواهیم یک اتوانکودر استاندارد و یک اتوانکودر Denoising را پیاده‌سازی و سپس خروجی‌های آن‌ها را مقایسه کنیم.

<!--### Define autoencoder model architecture and reconstruction loss-->
&#x202b; در این نوت‌بوک، می‌خواهیم یک اتوانکودر استاندارد و یک اتوانکودر Denoising را پیاده‌سازی و سپس خروجی‌های آن‌ها را مقایسه کنیم.

<!--Using $28 \times 28$ image, and a 30-dimensional hidden layer. The transformation routine would be going from $784\to30\to784$. By applying hyperbolic tangent function to encoder and decoder routine, we are able to limit the output range to $(-1, 1)$. Mean Squared Error (MSE) loss will be used as the loss function of this model.-->
&#x202b; با استفاده از تصویر ...، و یک لایه‌ی مخفی ۳۰ بعدی. روتین تبدیل از .. به ... تبدیل می‌شود. با اعمال تابع تانژانت هیپربولیک به روتین اتوانکودر و دیکودر،‌قادریم تا محدوده‌ی خروجی را به .. محدود کنیم. تلف MSE برای استفاده به عنوان تابع اتلاف این مدل استفاده می‌شود.
```python=
class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(n, d),
            nn.Tanh(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(d, n),
            nn.Tanh(),
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

model = Autoencoder().to(device)
criterion = nn.MSELoss()
```


<!--### Train standard autoencoder-->
### &#x202b; آموزش اتوانکودر استاندارد
<!--To train a standard autoencoder using PyTorch, you need put the following 5 methods in the training loop:-->
&#x202b; برای آموزش یک اتوانکودر استاندارد با استفاده از پای‌تورچ، نیاز است تا ۵ روش زیر را در حلقه‌ی آموزش به کار گیرید:

<!--#### Going forward:-->
### به جلو رفتن:

<!--1) Sending the input image through the model by calling `output = model(img)` . <br>-->
&#x202b; ۱) ارسال تصویر ورودی از طریق مدل با فراخوانی `output = model(img)` . <br>
  
<!--2) Compute the loss using: `criterion(output, img.data)`.-->
&#x202b; محاسبه‌ی تلفات با استفاده از: `criterion(output, img.data)`

<!--#### Going backward:-->
### به عقب رفتن:

<!--3) Clear the gradient to make sure we do not accumulate the value: `optimizer.zero_grad()`. <br>-->
&#x202b; ۳) 	 پاک‌سازی گرادیان برای اطمینان از انباشته نکردن مقدار :`optimizer.zero_grad()`. <br>

<!--4) Back propagation: `loss.backward()`<br>-->
&#x202b; ۴) Back Propagation: `loss.backward()`<br>

<!--5) Step backwards: `optimizer.step()`-->
&#x202b; قدم‌های رو به عقب: `optimizer.step()`

<!--Fig. 20 shows the output of the standard autoencoder.-->
شکل. ۲۰ خروجی اتوانکودر استاندارد را نشان می‌دهد.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/21_output_stae.png" width="500px" /><br>
<!--<b>Fig. 20</b>: Output of standard autoencoder <br>-->
<b>شکل. ۲۰</b>: خروجی اتوانکودر استاندارد
</center>
<br/>


<!--### Train denoising autoencoder-->
### &#x202b; آموزش اتوانکودر Denoising

<!--For denoising autoencoder, you need to add the following steps:<br>-->
&#x202b; برای اتوانکودر Denoising، نیاز دارید تا گام‌های زیر را اضافه کنید: <br>

<!--1) Calling `nn.Dropout()` to randomly turning off neurons. <br>-->
&#x202b; ۱) فراخوانی `nn.Dropout()` برای  خاموش کردن تصادفی نورون‌ها. <br>

<!--2) Create noise mask: `do(torch.ones(img.shape))`.<br>-->
&#x202b; ۲) نقاب نویز بسازید: `do(torch.ones(img.shape))`.<br>

<!--3) Create bad images by multiply good images to the binary masks: `img_bad = (img * noise).to(device)`.-->
&#x202b; ۳) 	با ضرب تصاویر خوب در ماسک‌های باینری تصاویر بد بسازید. `img_bad = (img * noise).to(device)`
<!--Fig. 21 shows the output of the denoising autoencoder.-->
&#x202b; شکل. ۲۱ نشان دهنده‌ی خروجی اتوانکودر denoising است.
<center>
<img src="{{site.baseurl}}/images/week07/07-3/22_out_denoising_ae.png" width="500px" /><br>
<!--<b>Fig. 21</b>: Output of denoising autoencoder <br>-->
<b> شکل. ۲۱</b>: خروجی اتوانکودر Denoising
</center>


<!--### Kernels comparison-->
### &#x202b; مقایسه‌ی کرنل‌ها

<!--It is important to note that in spite of the fact that the dimension of the input layer is $28 \times 28 = 784$, a hidden layer with a dimension of 500 is still an over-complete layer because of the number of black pixels in the image.
Below are examples of kernels used in the trained under-complete standard autoencoder. Clearly, the pixels in the region where the number exists indicate the detection of some sort of pattern, while the pixels outside of this region are basically random. This indicates that the standard autoencoder does not care about the pixels outside of the region where the number is.-->
&#x202b; توجه به این نکته که بر خلاف اینکه ابعاد لایه‌ی ورودی ... است، یک لایه‌ی مخفی به ابعاد ۵۰۰ هنوز به خاطر تعداد پیکسل‌های سیاه در تصویر، لایه‌ی تاثیرگذار شناخته می‌شود. در زیر مثال‌هایی از کرنل‌هایی که در اتوانکودر استاندارد تحت نظر آموزش‌ دیده‌اند، داریم. به طور واضح، پیکسل‌ها در این ناحیه که اعداد موجودند، نشان‌دهنده تشخیص بخش‌هایی از الگو هستند، در حالیکه پیکسل‌های خارج از این ناحیه، اساسا تصادفی می‌باشند. این نشان‌دهنده‌ی این است که اتوانکودر‌های استاندارد به پیکسل‌های خارج از نواحی که اعداد هستند، اعتنایی نمی‌کنند.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/AE_kernels.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!--<b>Figure 22:</b> Standard AE kernels.-->
<b>شکل. ۲۲</b>کرنل‌های AE استاندارد
</center>

<!--On the other hand, when the same data is fed to a denoising autoencoder where a dropout mask is applied to each image before fitting the model, something different happens. Every kernel that learns a pattern sets the pixels outside of the region where the number exists to some constant value. Because a dropout mask is applied to the images, the model now cares about the pixels outside of the number’s region.-->
&#x202b; از طرف دیگر، وقتی که همان داده به اتوانکودر Denoising در جایی که یک ماسک Dropout به هر تصویر قبل از فیت شدن به مدل داده شده‌است، چیز متفاوتی رخ می‌دهد. هر کرنل که یک الگو یاد می‌گیرد، پیکسل‌های خارج از نواحی را در جایی که در آن تعداد حدودا ثابت‌ است، تنظیم می‌کند. به خاطر ماسک Dropout ای که به تصاویر اعمال شده، مدل اکنون به پیکسل‌های خارج از محدوده‌ی اعداد نیز اهمیت می‌دهد.
<center>
<img src="{{site.baseurl}}/images/week07/07-3/denoiser_kernels.png" style="zoom: 50%; background-color:#DCDCDC;" /><br>
<!--<b>Figure 23:</b> Denoising AE kernels.-->
<b> شگل. ۲۳</b>: کرنل‌های AE denoising
</center>

<!--Compared to the state of the art, our autoencoder actually does better!! You can see the results below.-->
&#x202b; در مقایسه با State of the art، اتوانکودر ما عملکرد بهتری دارد!! نتایج آن را در زیر می‌توانید ببینید.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/AE_output.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!--<b>Figure 24:</b> Input data (MNIST digits).-->
<b> ؤکل ۲۴</b>: داده ورودی (MNIST digits)
</center>

<center>
<img src="{{site.baseurl}}/images/week07/07-3/denoiser_output.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!--<b>Figure 25:</b> Denoising AE reconstructions.-->
<b>شکل ۲۵</b>: بازسازی AE Denoising
</center>

<center>
<img src="{{site.baseurl}}/images/week07/07-3/telea_output.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!--<b>Figure 26:</b> Telea inpainting output.-->
<b>شکل ۲۶</b>: خروجی بی‌رنگ تله
</center>

<center>
<img src="{{site.baseurl}}/images/week07/07-3/navier-stokes_output.png" style="zoom: 40%; background-color:#DCDCDC;" /><br>
<!--<b>Figure 27:</b> Navier-Stokes inpainting output.-->
<b>شکل ۲۷</b>: خروجی بی رنگ Navier-Stokes
</center>

---
lang-ref: ch.07-1
lecturer: Yann LeCun
title: مدل‌های مبتنی بر انرژی
authors: Karanbir Singh Chahal, gitMeiyi He, Alexander Gao, Weicheng Zhu
date: 9 Mar 2020
translation-date : 23 Dec 2020
Lang: Fa
translator: Anis Zahedifard
---


<!--## [Overview](https://www.youtube.com/watch?v=tVwV14YkbYs&t=64s)-->
##[دید کلی](https://www.youtube.com/watch?v=tVwV14YkbYs&t=64s)
<!--We will introduce a new framework for defining models. It provides a unifying umbrella that helps define supervised, unsupervised and self-supervised models. Energy-based models observe a set of variables $x$ and output a set of variables $y$. There are 2 major problems with feed-forward nets:-->

&#x202b;ما می‌خواهیم چارچوب جدیدی برای تعریف کردن مدل‌ها، معرفی کنیم. این کار، باعث ایجاد یک چتر واحد که به تعریف مدل‌های با نظارت، بی‌نظارت، و نیمه‌نظارتی کمک می‌کند، می‌شود. مدل‌های مبتنی بر انرژی، مجموعه‌ای از متغیر‌های $x$ را مشاهده و در خروجی مجموعه‌ای از متغیر‌های $y$ را تولید می‌کنند. دو مشکل عمده با شبکه‌های فید فوروارد وجود دارد:

<!--1. What if the inference procedure is a more complex calculation than stacked layers of weighted sums?
2. What if there are multiple possible outputs for a single input? Example: Predicting future frames of video. Essentially in a classification net, we train this net to emit a score for each class. However, this is not possible to do in a continuous high dimensional domain like images. (We cannot have softmax over images!). Even if the output is discrete, it could have a large sample space. For example, the text is compositional leading to a huge number of possible combinations. Energy-based models provide a better framework to model these modalities.-->

&#x202b;۱. اگر روش استنباط یک محاسبه پیچیده‌تر از لایه های انباشته مجموع وزن‌ها باشد ، چه می‌شود؟

&#x202b; ۲. . اگر چند خروجی برای یک تک ورودی، موجود باشد چه می‌شود؟ برای مثال: پیش‌بینی قاب‌های بعدی برای یک ویدیو. اساسا در یک شبکه‌ی طبقه‌بندی، ما این شبکه را برای تخصیص دادن امتیاز به هر کلاس، آموزش می‌دهیم. به هرحال، انجام این کار برای یک دامنه پیوسته با ابعاد بالا مانند تصاویر امکان پذیر نیست. (نمی‌توانیم در تصاویر از Softmax استفاده کنیم). حتی اگر خروجی ما گسسته باشد، می‌تواند فضای نمونه‌ بزرگی داشته باشد. برای مثال، متن ترکیبی منجر به تعداد زیادی ترکیب ممکن می‌شود. مدل‌های مبتنی بر انرژی چارچوب بهتری برای مدل کردن این روش‌ها دارند.
<!--## EBM approach-->
##&#x202b; روش EBM


<!--Instead of trying to classify $x$'s to $y$'s, we would like to predict if a certain pair of ($x$, $y$) fit together or not. Or in other words, find a $y$ compatible with $x$. We can also pose the problem as finding a $y$ for which some $F(x,y)$ is low. For example:-->

&#x202b;به جای تلاش برای طبقه بندی $x’$ به $y’$، ما علاقه داریم تا پیش‌بینی کنیم آیا یک مجموعه از ($x$,$y$) مناسب همدیگر هستند با نه؟ یا به عبارت دیگر، یک $y$ مناسب برای $x$ بیابیم. ما همچنین می‌توانیم مسئله را به صورت پیدا کردن یک $y$ برای $F(x,y)$ کوچک مطرح کنیم. برای مثال:

<!-- - Is $y$ an accurate high-resolution image of $x$ ?-->
&#x202b;- آیا $y$ یک تصویر با وضوح بالای دقیق از $x$ است؟

<!-- - Is text `A` a good translation of text `B`?-->
&#x202b;- آیا `A` ترجمه‌ی خوبی از متن `B` است؟


<!-- This method of inference by minimizing a function and a large class of models work this way. By minimizing $f(x,y)$ or "energy". Hence, we perform inference by minimizing constraints where these constraints are represented by $f(x,y)$. We shall call $f(x,y)$ the "Energy function" from henceforth.
 -->


<!--### Definition-->
###تعریف


<!--We define an energy function $F: \mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{R}$ where $F(x,y)$ describes the level of dependency between $(x,y)$ pairs. (**Note** that this energy is used in inference, **not** in learning.) The inference is given by the following equation:-->
&#x202b;ما تابع انرژی را به صورت $F: \mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{R}$ تعریف می‌کنیم که $F(x,y)$ سطح وابستگی بین جفت $(x,y)$ را نشان می‌دهد. (توجه داشته باشید که این انرژی در استنباط استفاده شده، نه در یادگیری). استنباط توسط معادله زیر انجام شده است:

$$
\check{y} = \displaystyle \text{argmin}_y \left \{ F(x,y)\right \}
$$


<!--### Solution: gradient-based inference-->
### راه‌حال: استنباط مبتنی بر گرادیان کاهشی 


<!--We would like the energy function to be smooth and differentiable so that we can use it to perform the gradient-based method for inference. In order to perform inference, we search this function using gradient descent to find compatible $y$'s.
There are many alternate methods to gradient methods to obtain the minimum.-->

&#x202b;ما علاقمندیم تا تابع انرژی صاف و differentiable باشد تا از آن بتوانیم برای نمایش روش مبتنی بر شیب برای استنباط، استفاده کنیم. به منظور نمایش استنباط، ما از این تابع که از شیب نزولی برای پیدا کردن $y$’ های سازگار استفاده می‌کند، بهره می‌بریم. روش‌های جایگزین زیادی برای روش‌های گرادیان برای رسیدن به حداقل‌ها وجود دارد.
<!--**Aside**: Graphical models are a special case of Energy-Based models. The energy function decomposes as a sum of energy terms. Each energy terms take into account a subset of variables that we are dealing with. If they organize in a particular form, there are efficient inference algorithms to find the minimum of the sum of the terms with respect to the variable that we are interested in inferring.-->

&#x202b;**نکته**: مدل‌های گرافیکی موارد خاصی از مدل‌های مبتنی بر انرژی هستند. تابع انرژی به مجموع بخش‌های انرژی تجزیه می‌شود. هر بخش از انرژی زیرمجموعه‌ای از متغیرها که با‌ آن‌ها سر و کار داریم را در نظر می‌گیرد. اگر آن ‌ها به شکل مشخصی سازمان پیدا کنند، الگوریتم‌های بهینه استنباطی برای پیدا کردن کمینه‌ی مجموع شرایط با توجه به متغیر‌هایی که ما علاقمند به یافتن آن‌هاییم، وجود دارد. 
<!-- There are a type of Energy Based Models that are quite popular. These are
 -->
<!--## [EBM with latent variables](https://www.youtube.com/watch?v=tVwV14YkbYs&t=904s)-->
##&#x202b; EBM با متغیر‌های پنهان

<!--The output $y$ depends on $x$ as well as an extra variable $z$ (the latent variable) which we do not know the value of. These latent variables can provide auxiliary information. For example, a latent variable can tell you the positions of word boundaries in a chunk of text. This would be helpful to know when we want to interpret handwriting without spaces. This is also especially useful to know in speech that can have hard-to-decipher gaps. Additionally, some languages have very faint word boundaries (*e.g.* French). Hence, having this latent variable in our model will be very useful to interpret such an input.-->

&#x202b;خروجی $Y$ وابسته به $x$ و همینطور متغیر دیگری به نام $z$ (متغیر پنهان) که ما از مقدار آن آگاهی نداریم، می‌باشد. این متغیر‌های پنهان می‌توانند اطلاعات کمکی تولید کند. برای مثال، یک متغیر پنهان می‌تواند موقعیت مرز‌های جداسازی کلمات از هم در یک قسمت از متن را به شما بگوید. زمانی که می‌خواهیم نوشته‌های دست‌نویس بدون فاصله را تفسیر‌کنیم، این ویژگی خیلی مفید است. همینطور خصوصا در زمینه‌ی گفتار که شکاف‌های رمزگشایی سخت‌تری وجود دارد، استفاده از متغیر‌های پنهان کمک‌کننده‌تر خواهد بود. علاوه بر این، بسیاری از زبان‌ها، حروف و کلمات با مرز‌های نامشخص برای تفکیک از همدیگر در زبان خود دارند (مثل فرانسوی). از این رو، داشتن این متغیر‌های پنهان در مدل ما می‌تواند برای تفسیر چنین ورودی‌های مفید باشد.
<!--### Inference-->
##استنباط

<!--To do inference with latent variable EBM, we want to simultaneously minimize energy function with respect to y and z.-->

&#x202b;برای استنباط توسط متغیر‌های پنهان EBM، می‌خواهیم تا به طور همزمان تابع هزینه‌ را با توجه به y و z، کمینه کنیم.
$$\check{y}, \check{z} = \text{argmin}_{y,z} E(x,y,z)$$

<!--And this is equivalent to redefining the energy function as:-->
 و این معادل با بازتعریف تابع انرژی به صورت رو به رو می‌باشد: $$F_\infty(x,y) = \text{argmin}_{z}E(x,y,z)$$, which equals to: $$F_\beta(x,y) = -\frac{1}{\beta}\log\int_z \exp(-\beta E(x,y,z))$$.
When $\beta \rightarrow \infty$, then $\check{y} = \text{argmin}_{y}F(x,y)$.

<!--  ## Latent-Variable EBM -->
<!--Another big advantage of allowing latent variables, is that by varying the latent variable over a set, we can make the prediction output $y$ vary over the manifold of possible predictions as well (the ribbon is shown in the graph below): $F(x,y) = \text{argmin}_{z} E(x,y,z)$.-->

&#x202b;یکی دیگر از فواید بزرگ استفاده از متغیر‌‌های پنهان، گسترده بودن متغیر‌های پنهان در یک مجموعه است، ما می‌توانیم پیش‌بینی خروجی $y$ را در یک گوناگونی از پیش‌بینی‌های ممکن، پخش کنیم.
<!--This allows a machine to produce multiple outputs, not just one.-->

&#x202b;این به ماشین اجازه می‌دهد تا ماشین خروجی‌های متفاوتی تولید کند، نه فقط یک تک خروجی.
<center>
<img src="{{site.baseurl}}/images/week07/07-1/fig1.png"/><br>
<!--<b>Fig. 1</b>: Computation graph for Energy-based models-->
<B> تصویر ۱. : گراف محاسباتی برای مدل‌های مبتنی بر انرژی</B>
</center>


<!--### Examples-->
##‌مثال‌ها

<!--One example is video prediction. There are many good applications for us to use video prediction, one example is to make a video compression system. Another is to use video taken from a self-driving car and predict what other cars are going to do.-->
&#x202b;یک مثال، پیش‌بینی ویدیویی است. کاربرد‌های بسیار خوبی برای ما برای استفاده‌ی پیش‌بینی ویدیویی وجود دارد، یک کاربرد، ساخت یک سیستم فشرده‌کننده ویدیو است. کاربرد دیگر، استفاده از ویدیو‌های گرفته شده از یک اتومبیل خودران و پیش‌بینی آن که ماشین‌های دیگر قرار است چه رفتاری داشته باشند، می‌باشد.
<!--Another example is translation. Language translation has always been a difficult problem because there is no single correct translation for a piece of text from one language to another. Usually, there are a lot of different ways to express the same idea and people find it is hard to reason why they pick one over the other. So it might be nice if we have some way of parametrising all the possible translations that a system could produce to respond to a given text. Let's say if we want to translate German to English, there could be multiple translations in English that are all correct, and by varying some latent variables then you may vary the translation produced.-->

&#x202b;مثال دیگر، ترجمه است. ترجمه‌ی زبان همیشه به دلیل عدم انطباق کامل لغات بین دو زبان کار سخت و پیچیده‌ای بوده است. معمولا، راه‌های مختلفی برای بیان ایده‌ وجود دارد و مردم نمی‌توانند دلیلی برای اینکه چرا یکی از ایده‌ها را بر دیگری انتخاب می‌کنند، پیدا کنند. بنابراین، بسیار خوب می‌شود اگر راهی برای پارامتریزه کردن تمام ترجمه‌های ممکن که یک سیستم می‌تواند در جواب به یک متن ارائه کند، وجود داشته باشد. بیایید فرض کنیم از آلمانی به انگلیسی می‌خواهیم ترجمه کنیم، در این زمان چندین ترجمه‌ی ممکن درست انگلیسی وجود دارد، و با گسترش دادن متغیر‌های پنهان ممکن است ترجمه‌های بیشتر هم داشته باشیم.

<!--## [Energy-based models *v.s.* probabilistic models](https://www.youtube.com/watch?v=tVwV14YkbYs&t=1703s)-->
##مدل‌های مبتنی بر انرژی در مقابل مدل‌های احتمالاتی

<!--We can look at the energies as unnormalised negative log probabilities, and use Gibbs-Boltzmann distribution to convert from energy to probability after normalization is:-->

&#x202b;ما می‌توانیم به انرژی‌ها به عنوان احتمالات لگاریتمی منفی نرمالیزه نشده بنگریم، و از توزیع Gibbs-Boltzmann برای تبدیل از انرژی به احتمال بعد از نرمالیزه کردن استفاده کنیم:
$$P(y \mid x) = \frac{\exp (-\beta F(x,y))}{\int_{y'}\exp(-\beta F(x,y'))}$$

<!--where $\beta$ is positive constant and needs to be calibrated to fit your model. Larger $\beta$ gives a more fluctuate model while smaller $\beta$ gives a smoother model. (In physics, $\beta$ is inverse temperature: $\beta \rightarrow \infty$ means temperature goes to zero).-->

&#x202b;در جایی که بتا یک ثابت مثبت است و نیاز به کالیبراسیون برای مناسب‌سازی مدل داریم. بتا‌های بزرگتر مدل‌های نوسانی بیشتری دارند، در حالیکه بتاهای کوچکتر یک مدل یک دست و صاف‌تر ارائه می‌دهند. ( در فیزیک، بتا معکوس دماست: $\beta \rightarrow \infty$)
$$P(y,z \mid x) = \frac{\exp(-\beta F(x,y,z))}{\int_{y}\int_{z}\exp(-\beta F(x,y,z))}$$

<--!Now if marginalize over y: $P(y \mid x) = \int_z P(y,z \mid x)$, we have:-->

&#x202b;حال اگر نسبت به y آن را محدود کنیم:  $P(y \mid x) = \int_z P(y,z \mid x)$، داریم:
$$
\begin{aligned}
P(y \mid x) & = \frac{\int_z \exp(-\beta E(x,y,z))}{\int_y\int_z \exp(-\beta E(x,y,z))} \\
& = \frac{\exp \left [ -\beta \left (-\frac{1}{\beta}\log \int_z \exp(-\beta E(x,y,z))\right ) \right ] }{\int_y \exp\left [ -\beta\left (-\frac{1}{\beta}\log \int_z \exp(-\beta E(x,y,z))\right )\right ]} \\
& = \frac{\exp (-\beta F_{\beta}(x,y))}{\int_y \exp (-\beta F_{\beta} (x,y))}
\end{aligned}
$$

<!--Thus, if we have a latent variable model and want to eliminate the latent variable $z$ in a probabilistically correct way, we just need to redefine the energy function $F_\beta$ (Free Energy)-->

&#x202b;بنابراین، اگر ما یک مدل با متغیر پنهان داشته باشیم و بخواهیم متغیر پنهان $z$ را در یک روش صحیح احتمالاتی حذف کنیم، تنها به تعریف دوباره‌ی تابع انرژی نیازمندیم:
<!--### Free Energy-->
##انرژی آزاد

$$
F_{\beta}(x,y) = - \frac{1}{\beta}\log \int_z \exp (-\beta E(x,y,z))
$$

<!--Computing this can be very hard...  In fact, in most cases, it's probably intractable.  So if you have a latent variable that you want to minimize over inside of your model, or if you have a latent variable that you want to marginalize over (which you do by defining this Energy function $F$), and minimizing corresponds to the infinite $\beta$ limit of this formula, then it can be done.-->

&#x202b;محاسبه‌ی معادله‌ی بالا می‌تواند بسیار سخت باشد.. در واقع، در بیشتر موارد، ممکن است قابل محاسبه نباشد. بنابراین، اگر شما یک مقدار پنهان که می‌خواهید در داخل مدلتان کمینه کنید، دارید یا اگر شما یک منغیر پنهان دارید که می‌خواهید آن را به حاشیه ببرید (که درواقع با تعریف تابع انرژی $F$ این کار را می‌کنید)، و کمینه کردن متناظر با بی‌نهایت کردن محدوده‌ی بتا در این فرمول است، بنابراین این کار شدنی و این معادله قابل حل است.
<!--Under the definition of $F_\beta(x, y)$ above, $P(y \mid x)$ is just an application of the Gibbs-Boltzmann formula and $z$ has been marginalized implicitly inside of this.  Physicists call this "Free Energy", which is why we call it $F$.   So $e$ is the energy, and $F$ is free energy.-->

&#x202b;تحت تعریف $F_\beta(x, y)$ در بالا، $P(y \mid x)$ تنها یک کاربرد از فرمول Gibbs-Boltzmann است و $z$ در داخل این فرمول به طور مشخصی به حاشیه رانده شده است. فیزیکدان‌ها این مسئله را «انرژی آزاد» می‌نامند، که به همین خاطر ما آن را F می‌نامیم. بنابراین e انرژی است و F انرژی آزاد.
<!--***Question: Can you elaborate on the advantage that energy-based models give?  In probability-based models, you can also have latent variables, which can be marginalized over.***-->

&#x202b;***سوال: آیا می‌توانید بر روی فوایدی که مدل‌های مبتنی بر انرژی دارند بحث کنید؟ در مدل‌های مبتنی بر احتمالات، شما همچنان می‌توانید متغیر‌های پنهان داشته باشید، که می‌تواند به حاشیه رانده شود.***
<!--The difference is that in probabilistic models, you basically don't have the choice of the objective function you're going to minimize, and you have to stay true to the probabilistic framework in the sense that every object you manipulate has to be a normalized distribution (which you may approximate using variational methods, etc.). Here, we're saying that ultimately what you want to do with these models is make decisions.  If you build a system that drives a car, and the system tells you "I need to turn left with probability 0.8 or turn right with probability 0.2", you're going to turn left.  The fact that the probabilities are 0.2 and 0.8 doesn't matter -- what you want is to make the best decision, because you're forced to make a decision.  So probabilities are useless if you want to make decisions.  If you want to combine the output of an automated system with another one (for example, a human, or some other system), and these systems haven't been trained together, but rather they have been trained separately, then what you want are calibrated scores so that you can combine the scores of the two systems so that you can make a good decision.  There is only one way to calibrate scores, and that is to turn them into probabilities.  All other ways are either inferior or equivalent.  But if you're going to train a system end-to-end to make decisions, then whatever scoring function you use is fine, as long as it gives the best score to the best decision.  Energy-based models give you way more choices in how you handle the model, may more choices of how you train it, and what objective function you use. If you insist your model be probabilistic, you have to use maximum likelihood -- you basically have to train your model in such a way that the probability it gives to the data you observed is maximum.  The problem is that this can only be proven to work in the case where your model is "correct" -- and your model is never "correct".  There's a quote from a famous statistician [Goerge Box] that says ***"All models are wrong, but some are useful."***  So probabilistic models, particularly those in high-dimensional spaces, and in combinatorial spaces such as text, are all approximate models.  They're all wrong in a way, and if you try to normalize them, you make them more wrong.  So you're better off not normalizing them.-->

&#x202b;تفاوت در این است که در مدل‌های احتمالاتی، شما اساسا حق انتخاب تابع هدفی که می‌خواهید کمینه کنید را ندارید، و باید به چهارچوب احتمالاتی وفادار بمانید، یعنی هر شاخصه‌ای را که تغییر دادید، شاخصه در توزیع نرمال باقی‌ بماند و توزیع آن تغییر نکند ( ممکن است با روش‌های متنوعی آن‌ها را تقریب بزنید و غیره.). در اینجا، ما معتقدیم که در نهایت، کاری که شما می‌خواهید با این مدل‌ها انجام دهید، قابلیت تصمیم‌گیری است. اگر شما سیستمی برای راندن یک اتومبیل بسازید، و سیستم به شما بگوید: « من با احتمال ۰.۸ باید به چپ و با احتمال ۰.۲ به راست بپیچم»، شما به چپ خواهید پیچید (توسط اتومبیل). مقادیر احتمالاتی ۰.۲ و ۰.۸ برای ما اهمیت ندارند، آنچه که مهم است، داشتن بهترین تصمیم ممکن می‌باشد، زیرا که ما مجبور به گرفتن بهترین تصمیم ممکن هستیم. بنابراین احتمالات زمانی که ما نیازمند به تصمیم‌گیری هستیم، اهمیت چندانی ندارند. اگر بخواهید خروجی یک سیستم اتوماتیک شده را سیستم دیگری ادغام کنید (مثلا انسان، یا سیستم دیگری)، و این سیستم‌ها با همدیگر آموزش ندیده باشند، اما به صورت جدا آموزش دیده‌اند، آن‌گاه چیزی که شما می‌خواهید، این است که دو سیستم در یک مقیاس امتیاز بندی همسان شده باشند، تا بتوان نتیجه‌ی آن‌ها با همدیگر برای داشتن یک تصمیم مناسب، ادغام کرد. تنها یک راه برای همسان کردن نتایج و امتیازات وجود داره و آن‌هم، نمایش احتمالاتی امتیاز‌هاست. راه‌های دیگر از نظر رتبه‌بندی یا ضعیف‌تر و یا نهایتا هم‌سطح روش احتمالاتی هستند. اما اگر هدف شما آموزش یک سیستم end-to-end است، آن‌گاه استفاده از هر تابع امتیاز‌بندی، تا زمانی که بهترین تصمیمات بالاترین امتیازات را داشته باشند، بلامانع است. مدل‌های مبتنی بر انرژی انتخاب‌های بیشتری در مورد چگونگی مدیریت مدل، حتی شاید انتخاب‌های بیشتری در مورد چگونگی آمورش آن، و تابع هدفی که استفاده می‌کنید به شما بدهد. اگر تاکید دارید که مدل شما احتمالاتی باشد، شما باید از maximum Likelihood استفاده کنید—شما اساسا باید مدلتان را طوری آموزش دهید که احتمال داده‌ای که شما مشاهده کردید، بیشینه شود. اما مسئله در این‌جاست که این تنها زمانی برقرار است که مدلی که شما از آن استفاده می‌کنید، درست باشد--- و مدل شما هرگز درست نیست!! جمله‌ای از یک آمارشناس مشهور (جورج باکس) وجود دارد که می‌گوید: « همه‌ی مدل‌ها غلط هستند، اما برخی از آن‌ها مفیدند». بنابراین مدل‌‌های احتمالاتی، به صورت خاص مدل‌هایی که در فضاهای با ابعاد بالا و فضاهای ترکیبی مانند متن هستند، همه مدل‌های تقریبی‌اند. همه‌ی آن‌ا به نحوی اشتباهند و اگر تلاش کنید تا آن‌ها را نرمالیزه کنید، آن‌ها را بیشتر به خطا می‌اندازید. بنابراین بهتر است تا آن‌ها را نرمالیزه نکنید.
<center>
<img src="{{site.baseurl}}/images/week07/07-1/fig3.png" height="75%" width="75%"/><br>
<!--<b>Fig. 2</b>: Visualization of the energy function that captures dependency between x and y-->

&#x202b;<b>تصویر ۲. : به نمایش درآوردن تابع انرژی که وابستگی بین x و y را دریافت می‌کند</b>
</center>

<!--This is an energy function that's meant to capture the dependency between *x* and *y*.  It's like a mountain range if you will.  The valleys are where the black dots are (these are data points), and there are mountains all around.  Now, if you train a probabilistic model with this, imagine that the points are actually on an infinitely thin manifold.  So the data distribution for the black dots is actually just a line, and there are three of them.  They don't actually have any width.  So if you train a probabilistic model on this, your density model should tell you when you are on this manifold.  On this manifold, the density is infinite, and just $\varepsilon$ outside of it should be zero.  That would be the correct model of this distribution.  Not only should the density be infinite, but the integral over [x and y] should be 1.  This is very difficult to implement on the computer!  Not only that, but it's also basically impossible.  Let's say you want to compute this function through some sort of neural net -- your neural net will have to have infinite weights, and they would need to be calibrated in such a way that the integral of the output of that system over the entire domain is 1.  That's basically impossible.  The accurate, correct probabilistic model for this particular data example is impossible.  This is what maximum likelihood will want you to produce, and there's no computer in the world that can compute this.  So in fact, it's not even interesting.  Imagine that you had the perfect density model for this example, which is a thin plate in that (x, y) space -- you couldn't do inference!  If I give you a value of x, and ask you "what's the best value of y?"  You wouldn't be able to find it because all values of y except a set of zero-probability have a probability of zero, and there are just a few values that are possible.  For these values of x for example:-->
این یک تابع انرژی است که رابطه‌ و وابستگی بین *x* و *y* را در نظر می‌گیرد. به نوعی شبیه رشته‌ی کوه است. دره‌ها جاهایی‌اند که نقاط سیاه وجود دارد ( این‌ها نقاط داده‌اند)، و کوه‌ها دور آن‌ها را گرفته‌اند. حال، اگر مدلی احتمالاتی را با این آموزش بدهید، تصور کنید که نقاط در یک سطح گوناگونی بی‌نهایت قرار دارند. بنابراین توزیع داده برای نقاط سیاه تنها یک خط است، و سه تا از آن‌ها وجود دارد. قطعا هیچ پهنایی ندارند، بنابراین، اگر یک مدل احتمالاتی روی این آموزش دهید، مپل چگالی شما به شما خواهد گفت که چه زمان بر روی این گوناگونی قرار گرفته‌اید. بر روی این گوناگونی، چگالی بی‌نهایت است، و فقط – بیرون آن باید صفر باشد. این مدل درست توزیع خواهد بود. نه تنها چگالی باید بی‌نهایت باشد، بلکه انتگرال بر روی دو محور *x* و *y* باید مساوی با ۱ باشد. پیاده‌سازی این بر روی کامپیوتر بسیار سخت است. در واقع سخت نیس، غیر ممکن است! بیایید این تابع را از طریق مدل‌های شبکه‌ی عصبی محاسبه کنید—شبکه‌ی عصبی شما باید وزن‌های بی‌نهای داشته باشد و نیازمند کالیبراسیون به گونه‌ای است که انتگرال خروجی سیستم در کل دامنه‌ی آن مساوی صفر باشد. این مسئله اساسا ناممکن است. وجود مدل دقیق و درست احتمالاتی برای این داده‌ی مشخص در مثال غیر ممکن است. این چیزی‌است که maximum likelihood از شما انتظار دارد تا تولید کنید، و هیچ کامپیوتری در دنیا وجود ندارد که بتواند این را محسابه کند. بنابراین این موضوع حتی جالب هم نیست. تصور کنید که شما مدل چگالی برای این مثال را در اختیار داشتید، که یک صفحه‌ی نازک در فضای (x,y) است—شما نمی‌توانستید برداشت مشخصی از آن داشته باشید!! اگر من به شما مقداری از x را بدهم و از شما بپرسم، بهترین مقدار y کدام است؟ شما قادر به پیدا کردن آن نخواهید بود، زیرا که همه‌ی مقادیر Y به جز مجموعه‌ای از احتمال رخ‌داد صفر، احتمال صفر دارند، و فقط مقادیر کمی احتمال رخ‌داد برای آن وجود دارد. برای این مقادیر x به طور مثال:

<center>
<img src="{{site.baseurl}}/images/week07/07-1/fig4.png" height="75%" width="75%"/><br>
<!--<b>Fig. 3</b>: Example for multiple prediction of EBM as an implicit function-->
<b>تصویر ۳. : مثالی از پیش‌بینی‌های متعددی از EBM به عنوان یک تابع ضمنی</b>
&#x202b;<b>تصویر ۳. : مثالی از پیش‌بینی‌های متعددی از EBM به عنوان یک تابع ضمنی</b>
</center>

<!--There are 3 values of y that are possible, and they are infinitely narrow.  So you wouldn't be able to find them.  There's no inference algorithm that will allow you to find them.  The only way you can find them is if you make your contrast function smooth and differentiable, and then you can start from any point and by gradient descent you can find a good value for y for any value of x.  But this is not going to be a good probabilistic model of the distribution if the distribution is of the type I mentioned.  So here is a case where insisting to have a good probabilistic model is actually bad.  Maximum likelihood sucks [in this case]!-->

&#x202b;سه مقدار ممکن از y وجود دارد و به بی‌نهایت باریک هستنند. بنابراین شما قادر به پیدا کردن آن‌ها نخواهید بود. هیچ الگوی استنباطی وجود ندارد که به شما اجازه دهد تا آن‌ها را بیابید. تنها راهی که شما می‌توانید آن مقادیر را بیابید، این است که تابع کنتراست خود را صاف و قابل تشخیص کنید، و سپس از هر نقطه‌ای می‌توانید شروع کنید و با استفاده از گرادیان نزولی مقادیر مناسبی ازy را برای هر مقدار x بیابید. اما اگر توزیع مدل، از جمله‌ی توزیع‌هایی باشد که در قبل اشاره شد، این مدل، مدل احتمالاتی خوبی نخواهد بود. بنابراین در اینجا، مدل احتمالاتی‌ای که تاکید داشتیم خوب باشد، مدل واقعا بدی از آب در‌آمد. Maximum likelihood در این‌جا به کار ما نیامد!

<!--So if you are a true Bayesian, you say "oh but you can correct this by having a strong prior where the prior says your density function has to be smooth".  You could think of this as a prior.  But, everything you do in Bayesian terms -- take the logarithm thereof, forget about normalization -- you get energy-based models.  Energy-based models that have a regulariser, which is additive to your energy function, are completely equivalent to Bayesian models where the likelihood is exponential of the energy, and now you get $\exp(\text{energy}) \exp(\text{regulariser})$, and so it's equal to $\exp(\text{energy} + \text{regulariser})$.  And if you remove the exponential you have an energy-based model with an additive regulariser.-->

&#x202b;بنابراین اگر شما یک Bayesian واقعی هستی، احتمالا خواهید گفت: « این مشکل را می‌توانید با داشتن یک پریور قوی در جایی که پریور معتقد است تابع چگالی شما باید صاف باشد، می‌توانید حل کنید». این مسئله را می‌توانید به عنوان یک پریور ببینید. اما، همه‌ی کارهایی که شما انجام می‌دهید در شرایط بیزی رخ می‌دهد—از این رو لگاریتم آن را بگیرید، و نرمالیزه کردن را فراموش کنید—حال شما مدل‌های مبتنی بر انرژی دارید. مدل‌های مبتنی بر انرژی که يک تنظیم‌کننده دارند، که باعث بهبود عملکرد تابع انرژی شما می‌شود، به صورت کامل معادل با مدل‌های بیزی که likelihood تابع نمایی انرژی است، می‌باشند و حالا شما – را دارید. و اگر تابع نمایی را حذف کنید، یک مدل مبتنی بر انرژی به همراه یک تنظیم‌کننده اضافه دارید.
<!--So there is a correspondence between probabilistic and Bayesian methods there, but insisting that you do maximum likelihood is sometimes bad for you, particularly in high-dimensional spaces or combinatorial spaces where your probabilistic model is very wrong. It's not very wrong in discrete distributions (it's okay) but in continuous cases, it can be really wrong.  And all the models are wrong.-->

&#x202b;بنابراین در اینجا یک ارتباط بین روش‌های بیزی و احتمالاتی وجود دارد، اما توجه داشته باشید که استفاده از maximum likelihood گاهی برای شما مخرب است، خصوصا زمانی که در فضای با ابعاد بالا یا ترکیبی که مدل‌های احتمالاتی خیلی اشتباهند، هستید. در توزیع‌های گسسته خیلی اشتباه نیست، اما در موقعیت‌های پیوسته، واقعا می‌تواند اشتباه باشید. و همه‌ی مدل‌ها اشتباه هستند.
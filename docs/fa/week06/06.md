---
lang-ref: ch.06
lang: fa
title: هفته ۶
translation-date: 21 Sep 2020
translator: Alireza Moradi
---

<!--
## Lecture part A

We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment.
-->

## بخش اول درس

ما در مورد سه کاربرد شبکه‌های عصبی کانولوشنال بحث کردیم. با تشخیص ارقام و کاربرد آن در تشخیص کد پستی ۵ رقمی شروع کردیم. در تشخیص اشیاء، در مورد چگونگی استفاده از معماری چند مقیاسی در تنظیمات سیستم تشخیص چهره صحبت کردیم. در پایان، دیدیم که چگونه از شبکه‌های کانولوشنال در وظایف تقسیم‌بندی معنایی (semantic segmentation tasks)، با مثال‌های عینی در سیستم بینایی ربات و تشخیص شیء در یک محیط شهری، استفاده می‌شود.  

<!--
## Lecture part B

We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq.
-->

## بخش دوم درس

شبکه‌های عصبی بازگشتی (RNN)، مشکلات آن‌ها، و تکنیک‌های مرسوم برای کاهش این مشکلات را بررسی کردیم. سپس مدل‌های مختلفی، شامل توجه (Attention)، دستگاه بازگشتی درگاهی (Gated Recurrent Unit - GRU)، حافظه‌ی کوتاه مدت بلند (Long Short-Term Memory - LSTM) و Seq2Seq، که توسعه داده شده‌اند تا مشکلات مدل RNN را حل کنند را مرور کردیم. 

<!--
## Practicum
We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models.
-->

## تمرین عملی
ما در مورد RNN سنتی و مدل‌های LSTM صحبت کردیم و کارایی این دو را مقایسه کردیم. LSTM فواید RNN را به ارث می‌برد و در عین حال ضعف‌های RNN را با استفاده از یک سلول حافظه (memory cell)، برای ذخیره‌ی اطلاعات در حافظه برای مدت طولانی، بهبود می‌دهد. مدل‌های LSTM به طور قابل ملاحظه‌ای از مدل‌های RNN بهتر عمل می‌کنند.

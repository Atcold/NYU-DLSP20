---
lang-ref: ch.12
lang: fa
title: هفته ۱۲
translation-date: 29 Sep 2020
translator: Ali Abedi
---


## بخش اول ارائه

<!--In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively. -->
در این بخش ما می‌خواهیم در مورد معماری‌های مختلف شبکه‌ها در بخش پردازش زبان‌های طبیعی صحبت کنیم.این ساختارها از شبکه‌های کانولوشنی شروع می‌شود با شبکه‌های بازگشتی ادامه پیدا می‌کند و در نهایت به ساختارهای مدرن انتقالی می‌رسیم. در ادامه در مورد ماژول‌های مختلف که انتقال‌ها را تشکیل می‌دهند و چگونه ویژگی‌های مثبت انتقال‌ها را به پردازش زبان‌های طبیعی می‌دهند بحث می‌کنیم و در نهایت روش‌هایی را برای آموزش بهینه شبکه‌ها بیان می‌کنیم. 


## بخش دوم ارائه
<!-- In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (*i.e.* when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT. -->
در این بخش ما جستجوی پرتوی محلی را به عنوان موضوعی بین دیکود کردن حریصانه و جستجوی جامع معرفی می‌کنیم. مورد نمونه‌گیری از توزیع مولد (مثلا در زمان ایحاد متن) را مورد بررسی قرار می‌دهیم و نمونه گیری top-k را معرفی می‌کنیم. متعاقبامدل‌های توالی به توالی (با یک مدل انتقال) و بازگردانی ترجمه را بررسی می‌کنیم. در نهایت روش‌های غیر نظارت شده برای یادگیری مدل‌های تعبیه شده، کلمه به بردار، GPT و BERT را معرفی می‌کنیم.

## عملی

<!-- We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures. -->
ما مکانیسم مراقبت را با تمرکز بر خود مرافبتی و نمود لایه‌های مخفی آن نسبت به ورودی معرفی می‌کنیم. سپس یک الگوی ذخیره سازی کلید-مقداری را معرفی می‌کنیم و در مورد نحوه نمایش کوئری‌ها، کلید‌ها و مقدارها بحث می‌کنیم. در نهایت نیز ما از مکانسیم‌های مراقبتی برای تحلیل ساختار‌های انتقالی و برداشتن یک قدم رو به جلو (forward pass) در یک ساختار انتقالی ابتدایی و مقایسه الگوی کدکننده-کدگشا برای ساختارهای ترتیبی استفاده می‌کنیم.

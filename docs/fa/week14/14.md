---
lang-ref: ch.14
lang: fa
title: هفته ۱۴
translation-date: 26 Sep 2020
translator: Nikan Doosti Lakhani
---

<!--
## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net.
-->

## بخش اول درس
در این قسمت، ما پیش بینی ساخت یافته را مورد بحث قرار داده ایم. ابتدا گراف فاکتور مبتنی بر انرژی و استنتاج بهینه برای آن را معرفی کرده ایم. سپس، تعدادی مثال برای گراف فاکتور های مبتنی بر انرژی ساده با فاکتورهای "سطحی" را ارایه داده ایم. در پایان، شبکه مبدل گراف را مورد بحث قرار داده ایم.

<!--
## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models.
-->

## بخش دوم درس
در بخش دوم این درس کاربردهای روش های مدل گرافی را برای مدل های مبتنی بر انرژی مورد بحث قرار می دهد. بعد از صرف مقداری زمان برای مقایسه توابع خطای مختلف، به بررسی کابردهای الگوریتم Viterbi و الگوریتم جلورونده در شبکه های مبدل گرافی می پردازیم. سپس فرموله بندی لاگرانژ برای پس انتشار و همچنین استنتاج تغییر پذیر برای مدل های مبتنی بر انرژی مورد بحث قرار می گیرد.

<!--
## Practicum

When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise.
-->

## بخش عملی
در زمان آموزش مدل های بسیار پارامتری شده مانند شبکه های عصبی عمیق، ریسک بیش برازش بر داده های آموزش وجود دارد. این موضوع موجب خطای عمومی سازی بیشتر خواهد شد. برای کمک به کاهش بیش برازش، ما می توانیم قانونمندسازی را برای آموزش معرفی کنیم تا با اینکار مانع راه حل های خاصی شویم تا شدتی که مدل های ما بر نویز برازش پیدا می کنند را کاهش دهیم.


---
lang-ref: ch.02
lang: ja
title: 第2週
translation-date: 6 Dec 2020
translator: Shiro Takagi
---


<!-- ## Lecture part A

We start by understanding what parametrised models are and then discuss what a loss function is. We then look at Gradient-based methods and how it's used in the backpropagation algorithm in a traditional neural network. We conclude this section by learning how to implement a neural network in PyTorch followed by a discussion on a more generalized form of backpropagation.


## Lecture part B

We begin with a concrete example of backpropagation and discuss the dimensions of Jacobian matrices. We then look at various basic neural net modules and compute their gradients, followed by a brief discussion on softmax and logsoftmax. The other topic of discussion in this part is Practical Tricks for backpropagation.


## Practicum

We give a brief introduction to supervised learning using artificial neural networks. We expound on the problem formulation and conventions of data used to train these networks. We also discuss how to train a neural network for multi class classification, and how to perform inference once the network is trained. -->

## レクチャーパートA

パラメトリックモデルとは何かを理解することから始め、損失関数とは何かを議論します。次に、伝統的なニューラルネットワークにおいて、勾配に基づく方法が誤差逆伝播アルゴリズムでどのように使用されているかを見ていきます。最後に、PyTorchでニューラルネットワークを実装する方法を学び、誤差逆伝播法のより一般的な形について議論して、このセクションを締めくくります。


## レクチャーパートB

誤差逆伝播法の具体例から始め、ヤコビ行列の次元について議論します。次に、様々な基本的なニューラルネットモジュールを見て、その勾配を計算し、softmaxとlogsoftmaxについて簡単に議論します。このパートのもう一つのトピックは、誤差逆伝播法のための実践的なコツです。


## 演習

人工ニューラルネットワークを用いた教師あり学習について簡単に紹介します。これらのネットワークを訓練するために使用される問題の定式化とデータの慣例について説明します。また、マルチクラス分類のためのニューラルネットワークの訓練方法や、ネットワークが訓練された後に推論を行う方法についても議論します。

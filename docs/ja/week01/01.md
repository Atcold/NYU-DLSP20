---
lang-ref: ch.01
lang: ja
title: 第1週
translation-date: 5 Oct 2020
translator: Shohei Ogawa
---

<!-- ## Lecture part A -->
## レクチャーパートA

<!-- We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.  -->

この章ではディープラーニングを学ぶモチベーションについて扱います。ディープラーニングの歴史とそのきっかけに始まり、パターン認識の歴史について説明して、さらに勾配降下法とその計算方法である誤差逆伝播法を導入します。最後に視覚野の階層的表現について述べます。


<!-- ## Lecture part B -->
## レクチャーパートB

<!-- We first discuss the evolution of CNNs, from Fukushima to LeCun to AlexNet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.  -->

まず初めに福島やLeCunの仕事からAlexNetまでのCNNの進展について扱い、画像のセグメンテーション、自動運転、医療画像の分析でのCNNの利用例を議論します。そのあと、ディープネットワークの階層構造とその利点となる性質について説明します。最後に特徴量や表現の生成と学習について述べて、この章のまとめとします。


<!-- ## Practicum -->
## 演習

<!-- We discuss the motivation for applying transformations to data points visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to understand the function and effects of these transformations. We walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by neural networks.  -->

まずどうして空間上に可視化されたデータ点を変換すると良いのかについて議論し、線形代数と線形変換、非線形変換の仕方について扱います。さらに変換の作用と影響をよく理解するために可視化を行います。 Jupyter Notebookを使って例を一通り扱い、ニューラルネットワークによって表現される関数について述べてまとめとします。

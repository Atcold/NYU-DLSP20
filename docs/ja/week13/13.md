---
lang-ref: ch.13
title: Week 13
lang: ja
translation-date: 6 Dec 2020
translator: Shiro Takagi
---


<!-- ## Lecture part A -->
## レクチャーパートA

<!-- In this section, we discuss the architecture and convolution of traditional convolutional neural networks. Then we extend to the graph domain. We understand the characteristics of graph and define the graph convolution. Finally, we introduce spectral graph convolutional neural networks and discuss how to perform spectral convolution. -->
このセクションでは、従来の畳み込みニューラルネットワークのアーキテクチャと畳み込みについて議論します。次に、これをグラフ領域に拡張します。グラフの特性を理解し、グラフ畳み込みを定義します。最後に、スペクトルグラフ畳み込みニューラルネットワークを紹介し、スペクトル畳み込みの実行方法について議論します。


<!-- ## Lecture part B -->
## レクチャーパートB

<!-- This section covers the complete spectrum of Graph Convolutional Networks (GCNs), starting with the implementation of Spectral Convolution through Spectral Networks. It then provides insights on applicability of the other convolutional definition of Template Matching to graphs, leading to Spatial networks. Various architectures employing the two approaches are detailed out with their corresponding pros & cons, experiments, benchmarks and applications. -->
このセクションでは、スペクトルネットワークによるスペクトル畳み込みの実装から始まり、グラフ畳み込みネットワーク（GCN）の全範囲をカバーしています。次に、他の畳み込みの定義であるテンプレートマッチングのグラフへの適用可能性についての洞察を提供し、空間的ネットワークを紹介します。2つのアプローチを採用した様々なアーキテクチャが、対応する長所と短所、実験、ベンチマーク、アプリケーションとともに詳細に説明されます。


<!-- ## Practicum -->
## 演習

<!-- In this section, we introduce Graph Convolutional Network (GCN) which is one type of architecture that utilizes the structure of data.  Actually, the concept of GCNs is closely related to self-attention. After understanding the general notation, representation and equations of GCN, we delve into the theory and code of a specific type of GCN known as Residual Gated GCN. -->
ここでは、データの構造を利用したアーキテクチャの一つであるグラフ畳み込みネットワーク(Graph Convolutional Network: GCN)について紹介します。 実は、GCNの概念はself-attentionと密接に関係しています。GCNの一般的な表記法、表現法、方程式を理解した後、Residual Gated GCNと呼ばれる特定のタイプのGCNの理論とコードについて掘り下げていきます。

---
lang-ref: ch.05
title: Week 5
authors: Vaibhav Gupta, Himani Shah, Gowri Addepalli, Lakshmi Addepalli
date: 24 Feb 2020
lang: ja
translation-date: 21 Sep 2020
translator: Jesmer Wong
---

<!-- ## Lecture part A -->
##講義　第一部分

<!-- We begin by introducing Gradient Descent. We discuss the intuition and also talk about how step sizes play an important role in reaching the solution. Then we move on to SGD and its performance in comparison to Full Batch GD. Finally we talk about Momentum Updates, specifically the two update rules, the intuition behind momentum and its effect on convergence. -->

まず、勾配降下法を紹介します。これをパットの考えについて説明し、ステップサイズでどうやって回答が求められるのか、その重要な役割を果たす方法についても説明します。次に、確率的勾配降下法について説明し、フルバッチ勾配降下法と比べてそのパフォーマンスに説明します。最後に、動量アップデート、特に2つの更新ルール、この更新ルールの直感の考えと収束への影響について説明します。


<!-- ## Lecture part B -->
##講義　第二部分

<!-- We discuss adaptive methods for SGD such as RMSprop and ADAM. We also talk about normalization layers and their effects on the neural network training process. Finally, we discuss a real-world example of neural nets being used in industry to make MRI scans faster and more efficient. -->

RMSpropやADAMなどの確率的勾配降下法の最適化方法について説明します。 また、正規化レイヤーとそれらがニューラルネットワークのトレーニングプロセスに与える影響についても説明します。 最後に、業界の例としてMRIスキャンをより高速かつ効率的にするために使用されたニューラルネットについて説明します。



<~-- ## Practicum -->
実習

<!-- We briefly review the matrix-multiplications and then discuss the convolutions. Key point is we use kernels by stacking and shifting. We first understand the 1D convolution by hand, and then use PyTorch to learn the dimension of kernels and output width in 1D and 2D convolutions examples. Furthermore, we use PyTorch to learn about how automatic gradient works and custom-grads. -->

マトリクスの乗算を簡単に確認してから、畳み込みについて説明します。
ポイントは、たたみとシフトとのカーネルを使用することです。 

まずは1D畳み込みを手作業で理解し、次にPyTorchで使って、カーネルの次元と1Dおよび2D畳み込みの例の出力幅を学習して、。 さらに、PyTorchを使用して、自動勾配降下法の原理とカスタマイズした勾配について学習します。


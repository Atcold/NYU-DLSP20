---
lang-ref: ch.06
title: Week 6
lang: ja
translation-date: 6 Dec 2020
translator: Shiro Takagi
---

<!-- ## Lecture part A -->
## レクチャーパートA

<!-- We discussed three applications of convolutional neural networks. We started with digit recognition and the application to a 5-digit zip code recognition. In object detection, we talk about how to use multi-scale architecture in a face detection setting. Lastly, we saw how ConvNets are used in semantic segmentation tasks with concrete examples in a robotic vision system and object segmentation in an urban environment. -->
畳み込みニューラルネットワークの3つの応用について議論しました。まず、数字認識から始まり、5桁の郵便番号認識への応用についてお話しました。物体検出では、顔検出の設定でマルチスケールアーキテクチャをどのように利用するかを話しました。最後に、ロボット視覚システムと都市環境における物体セグメンテーションの具体例を用いて、セマンティックセグメンテーションタスクに畳み込みニューラルネットがどのように利用されているかを見ました。


<!-- ## Lecture part B -->
## レクチャーパートB

<!-- We examine Recurrent Neural Networks, their problems, and common techniques for mitigating these issues.  We then review a variety of modules developed to resolve RNN model issues including Attention, GRUs (Gated Recurrent Unit), LSTMs (Long Short-Term Memory), and Seq2Seq. -->
リカレントニューラルネットワークとその問題点、およびこれらの問題点を緩和するための一般的な技術を検討します。 次に、Attention、GRU（Gated Recurrent Unit）、LSTM（Long Short-Term Memory）、Seq2Seqなど、RNNモデルの問題を解決するために開発された様々なモジュールをおさらいします。


<!-- ## Practicum -->
## 演習

<!-- We discussed architecture of Vanilla RNN and LSTM models and compared the performance between the two. LSTM inherits advantages of RNN, while improving RNN's weaknesses by including a 'memory cell' to store information in memory for long periods of time. LSTM models significantly outperforms RNN models. -->
通常のRNNモデルとLSTMモデルのアーキテクチャを議論し、両者の性能を比較しました。LSTMはRNNの長所を継承しつつ、RNNの弱点を改善し、情報を長時間メモリに格納する「メモリセル」を含むことで、RNNモデルの性能を大幅に向上させています。LSTMモデルはRNNモデルを大きく凌駕しています。

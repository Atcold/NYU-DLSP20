---
lang-ref: ch.09
title: Week 9
lang: ja
translation-date: 6 Dec 2020
translator: Shiro Takagi
---

<!-- ## Lecture part A -->
## レクチャーパートA

<!-- We discussed discriminative recurrent sparse auto-encoders and group sparsity. The main idea was how to combine sparse coding with discriminative training. We went through how to structure a network with a recurrent autoencoder similar to LISTA and a decoder. Then we discussed how to use group sparsity to extract invariant features. -->
識別リカレントスパースオートエンコーダーとグループスパース性について議論しました。スパース符号化と識別タスクの訓練をどのように組み合わせるかが主な考え方でした。LISTAに似たリカレントオートエンコーダとデコーダを用いてネットワークを構成する方法を説明しました。次に、グループスパース性を用いて不変な特徴を抽出する方法について議論しました。

<!-- ## Lecture part B -->
## レクチャーパートB

<!-- In this section, we talked about the World Models for autonomous control including the neural network architecture and training schema. Then, we discussed the difference between World Models and Reinforcement Learning (RL). Finally, we studied Generative Adversarial Networks (GANs) in terms of energy-based model with the contrastive method. -->
ここでは、自律制御のための世界モデルについて、ニューラルネットワークのアーキテクチャや学習スキーマを含めて説明しました。次に、世界モデルと強化学習(RL)の違いについて述べました。最後に、エネルギーベースモデルの観点からGAN（Generative Adversarial Networks）について対照学習を用いて検討しました。

<!-- ## Practicum -->
## 演習

<!-- During this week's practicum, we explored Generative Adversarial Networks (GANs) and how they can produce realistic generative models. We then compared GANs with VAEs from week 8 to highlight key differences between two networks. Next, we discussed several model limitations of GANs. Finally, we looked at the source code for the PyTorch example Deep Convolutional Generative Adversarial Networks (DCGAN). -->
今週の演習では、GAN（Generative Adversarial Networks）と、それがどのようにして現実的な生成モデルを生成できるかを探りました。次に、2つのネットワークの主な違いを強調するために、GANと第8週で紹介したVAEを比較しました。次に、GANのモデルの限界について議論しました。最後に、PyTorchの例であるDeep Convolutional Generative Adversarial Networks (DCGAN)のソースコードを見ました。

---
lang: ja
lang-ref: ch.09-2
lecturer: Yann LeCun
title: 世界モデル(World Model)と敵対的生成ネットワーク(Generative Adversarial Network)
authors: Bofei Zhang, Andrew Hopen, Maxwell Goldstein, Zeping Zhan
date: 30 Mar 2020
translation-date: 9 Aug 2020
translator: Zeng Xiao
---


## [自動制御のための世界モデル](https://www.youtube.com/watch?v=Pgct8PKV7iw&t=2526s)

自己教師あり学習の最も重要な用途の一つは、制御のための世界モデル(World Model)を学習することである。人間がタスクを実行するとき、周りがどのように動くのかの内部モデルを心の中に持っている。例えば、9ヶ月目の赤ちゃんは物理学の直感を、ほとんど観察によって得ている。ある意味では、これは自己教師あり学習に似ている。何が起こるかを予測する学習によって、抽象的な原理が得られる。更に、内部モデルによって、私たちは世界に対して行動出来るようになった。例えば、物理学の直感と筋肉の使い方で、落ちてくるペンをどうやって掴められるのを予測し、そして実行することができる。

### 世界モデルとは？

4つの大きいモジュールで構成される自律知能システムである（図1）。まず、認識モジュールは世界を観測し、世界の状態の表現を計算する。この表現は完全ではなく、その理由は1)エージェントが全宇宙を観測していないこと、2)観測の精度が限られていることである。フィードフォワード・モデルでは、認識モジュールは最初のタイムステップだけに存在することに注意を。続いて、Actorモジュール(方策モジュールとも呼ばれる)は、世界の表現に基づいて、何らかの行動をとることを想像する。第三に、モデルモジュールが(時には潜在的な特徴を含んだ)世界の表現で与えられた行動の結果を予測する。この予測は、次の世界の状態の推測として次のタイムステップに渡され、最初の時間ステップの認識モジュールと同じ役割を担う。図2は、このフィードフォワードの過程の詳細を示しています。最後に、批評家モジュールがこの予測を、提案されたアクションを実行するためのコストに変換する。例えば、ペンが落ちている速度の推定がこうで、筋肉をこう動かしたら、どれくらいペンが外れるのか？

<center>
<img src="{{site.baseurl}}/images/week09/09-2/week9_world_models_arch.png" height="400px" /><br>
<b>図1</b>：自律知能システムの世界モデルの構造。
</center>

<center>
<img src="{{site.baseurl}}/images/week09/09-2/week9_world_models.png" height="400px" /><br>
<b>図2</b>：モデル部の構造。
</center>


## 古典的な仕組み

古典的な最適制御では、Actor(方策)モジュールのかわりにアクション変数だけが存在する。この定式化はモデル予測制御と呼ばれる、古典的な方法で最適化されており、1960年代にNASAがロケットの軌道を計算するのに使われた。人間のコンピュータ（ほとんどが黒人女性の数学者）から電子コンピュータへの切り替えを際に。このシステムのイメージとしては、非展開RNN、潜在変数としての行動、そして誤差逆伝播と勾配法(離散的行動に動的計画法など他の手法も可能)で、タイムステップごとのコストの合計を最小化する行動系列を推論していると考えられる。

ちなみに、ここで潜在変数を「推論」し、パラメータを「学習」するという言葉遣いになっているが、これらを最適化する過程は大体似ているものである。一つ重要な違いは、潜在変数がサンプル毎に値をとるのに対し、パラメータはサンプル間で共有されることである。

## 少しの改良

行動を計画する毎に複雑な誤差逆伝播を行うのは避けたいので、変分自己符号化器のスパースコーディングを改善するために使った同じトリックを使う。つまり、世界の表現から直接に、最適な行動系列を予測する符号化器を学習する。このために使われる符号化器は、方策ネットワークと呼ぶ。

<center>
<img src="{{site.baseurl}}/images/week09/09-2/week9_policy_network.png" height="400px" /><br>
<b>図3</b>: 方策ネットワーク
</center>

学習済みの方策ネットワークを使って、認識の直後に最善の行動系列を予測できる。

## [強化学習 (RL)](https://www.youtube.com/watch?v=Pgct8PKV7iw&t=3993s)

RLとこれまで学んできたものとの主な違いは2つあります。
1. 強化学習の環境のコスト関数はブラックボックスである。言い換えれば、エージェントは報酬の仕組みを理解していない。
2. RLの設定では、環境を進めるために世界のモデルを使うのではなく、実世界に行動して、何が起こるかを観測することで学習する。実世界では、完璧に状態を観測することができないため、次の出来事を予測できない場合もあり得る。

強化学習の主の問題は、コスト関数が微分できないことである。結果的に、試行錯誤によって学習するしかない。そうなると、いかに効率的に状態空間を探索するかが問題になる。これを解決したら次の課題は、探索と利用という基本の問題である。つまり、最大限に学べるために行動をとるか、学んだ知識を利用して報酬が最大化出来る行動を優先するかの二択である。

Actor-Critic系は、ActorとCriticを両方学習させる、人気のある強化学習のアルゴリズムだ。強化学習の手法はだいたい同じく、コスト関数のモデル(critic)を学習させている。Actor-Critic系でのcriticの役割は、価値関数の期待値を学習することである。Criticがニューラルネットだったので、モジュール越した勾配逆伝播が可能になる。Actor(「役者」)が環境の中で取るべき行動を提案し、Critic(「批評家」)がコスト関数のモデルを学習する。ActorとCriticの連携で、Criticを使わないよりもっと効率的な学習が可能になる。世界の良いモデルを持っていなければ、学習するのは難しくなる。例えば、崖の隣の車は、崖から落ちるのが知らないならとてもまずいことになるだろう。人間や動物はとても良い世界モデルを頭の中に持っているから、RLのエージェントより遥かに早く学習することができた。

「偶然的不確定性」と「認識論的不確定性」といった不確定性のせいで、そもそも未来を予測できない場合もある。偶然的不確定性の原因は、環境の中の制御・観測不可能な要素が存在すること。そして認識論的不確定性は、モデルに学習データがまだ足りないから、未来を予測できないことを指す。

世界モデルが予測するものは以下である。

$$\hat s_{t+1} = g(s_t, a_t, z_t)$$

ここで$z$は値がわからない潜在変数である。$z$で表すのは、わからないけど予測に影響を与える要素(すなわち偶然的不確定性)である。$z$を正則化するために、疎分散、ノイズ、または符号化器を使うことができる。世界モデルでプランニングを学習できる。このシステムは、復号化器に状態の表現と不確定性$z$を復号化させることで動く。$\hat s_{t+1}$と実際に観測された$s_{t+1}$の差を最小化するような$z$を最尤として定義される。

## [敵対的生成ネットワーク](https://www.youtube.com/watch?v=Pgct8PKV7iw&t=5430s)

敵対的生成ネットワーク(Generative Adversarial Network、GAN)には多くのバリエーションがあるが、ここでは対照法を用いたエネルギーに基づくモデルの一種として考える。GANは訓練用のサンプルのエネルギーを押し下げると同時に、対照サンプルのエネルギーを押し上げる。基本的なGANは2つの部分から構成され、対照サンプルを知能的に生成する生成器と、識別器（Criticとも）というコスト関数として使われるエネルギーモデルをもつ。生成器と識別器は両方ともニューラルネットである。

GANへ訓練と対照の2種類のサンプルが入力される。訓練サンプルは識別器に通し，そのエネルギーを下げる。対照サンプルは、何らかの分布から潜在変数をサンプリングし、それをジェネレーターに通して、訓練標本に似たものを生成させ、識別器に通し，そのエネルギーを上げる。識別器のロス関数は次のようである。

$$\sum_i L_d(F(y), F(\bar{y}))$$

ここでの$L_d$は、$F(y)$を減らしつつ$F(\bar{y})$を増やせられるものであれば、マージンベースの損失関数(例えば、$F(y) + [m - F(\bar{y})]^+$ or $\log(1 + \exp[F(y)]) + \log(1 + \exp[-F(\bar{y})])$)でも問題はない。ここの$y$がラベルで、$\bar{y}$は、$y$自身を除いて最もエネルギーを低くする変数である。

生成器のロス関数は少し違う。

$$L_g(F(\bar{y}))  = L_g(F(G(z)))$$

ここで$z$は潜在変数で、$G$が生成器のニューラルネットである。生成器には、識別器を騙せるようなエネルギーが低い$\bar{y}$を生成するように重みを調整する。

このタイプのモデルが敵対的生成ネットワークと呼ばれる理由は、2つのお互いに敵対している目的関数持ち、それらを同時に最小化していることである。これらの2つの関数間のナッシュ均衡を見つけようとしているが、勾配降下法はデフォルトではこれができないので、この問題は勾配降下の問題ではない。

真の多様体に近いサンプルを持っている場合は問題がある。無限に薄い多様体があるとする。そしたら識別器は、多様体の外側では $0$ の確率を、多様体上では無限大の確率を出力する必要がある。これは非常に難しいので、GANはシグモイドを使って、多様体の外側では$0$を出力し、多様体上では$1$を出力させる。しかし、識別器を多様体の外側で$0$を生成するように学習させてしまえば、そのエネルギー関数が全く使えなくなってしまう。エネルギー関数が滑らかさを失い、多様体の外側のエネルギーがすべて無限大になり、内側のエネルギーが$0$になるから。エネルギー値が小さなステップで $0$ からすぐ無限大になることは望ましくない。これを解決するため、エネルギー関数を正規化する手法が多く提案されてある。改良されたGANの良い例は、識別器の重みの大きさを制限するWasserstein GANがある。

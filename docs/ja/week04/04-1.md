---
lang-ref: ch.04-1
lecturer: Alfredo Canziani
title: 線形代数と畳み込み
authors: Yuchi Ge, Anshan He, Shuting Gu, and Weiyang Wen
date: 18 Feb 2020
lang: ja
translation-date: 10 Sep 2020
translator: Jesmer Wong
---


## [線形代数の復習](https://www.youtube.com/watch?v=OrBEon3VlQg&t=68s)

この部分は、ニューラルネットワークのコンテキストでの基本的な線形代数の要約で説明します。 隠れ層からスタートすると $\boldsymbol{h}$:

$$
\boldsymbol{h} = f(\boldsymbol{z})
$$

この輸出は　ベクトル$z$　非線形関数　$f$　に適用された結果です。
ここの $z$ は　入力されたベクトル　$\boldsymbol{x} \in\mathbb{R^n}$　をアフィン変換した　$\boldsymbol{A} \in\mathbb{R^{m\times n}}$　の結果です:

$$
\boldsymbol{z} = \boldsymbol{A} \boldsymbol{x}
$$

簡単にするために、バイアスは無視されます。上記の線形方程式は次のように展開できます。

$$
\boldsymbol{A}\boldsymbol{x} =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix} \begin{pmatrix}
x_1 \\ \vdots \\x_n \end{pmatrix} =
\begin{pmatrix}
    \text{---} \; \boldsymbol{a}^{(1)} \; \text{---} \\
    \text{---} \; \boldsymbol{a}^{(2)} \; \text{---} \\
    \vdots \\
    \text{---} \; \boldsymbol{a}^{(m)} \; \text{---} \\
\end{pmatrix}
\begin{matrix}
    \rvert \\ \boldsymbol{x} \\ \rvert
\end{matrix} =
\begin{pmatrix}
    {\boldsymbol{a}}^{(1)} \boldsymbol{x} \\ {\boldsymbol{a}}^{(2)} \boldsymbol{x} \\ \vdots \\ {\boldsymbol{a}}^{(m)} \boldsymbol{x}
\end{pmatrix}_{m \times 1}
$$

上記の $\boldsymbol{a}^{(i)}$ はマトリクス $\boldsymbol{A}$　のなかの第$i$の行です。

この変換の意味を理解するために、$ a ^ {（1）} \ boldsymbol {x} $などの$ \ boldsymbol {z} $の1つのコンポーネントを分析してみましょう。例えば $ n = 2 $にすると、$ \ boldsymbol {a} =（a_1、a_2）$および$ \ boldsymbol {x} =（x_1、x_2）$とします。

$ \ boldsymbol {a} $および$ \ boldsymbol {x} $は、2D座標軸のベクトルとして描画できます。 ここで、$ \ boldsymbol {a} $と$ \ hat {\ boldsymbol {\ imath}} $の間の角度が$ \ alpha $で、$ \ boldsymbol {x} $と$ \ hat {\ boldsymbol {の間の角度が \ imath}} $は$ \ xi $であり、三角式を使用すると、$ a ^ \ top \ boldsymbol {x} $は次のように展開できます。

$$
\begin {aligned}
\boldsymbol{a}^\top\boldsymbol{x} &= a_1x_1+a_2x_2\\
&=\lVert \boldsymbol{a} \rVert \cos(\alpha)\lVert \boldsymbol{x} \rVert \cos(\xi) + \lVert \boldsymbol{a} \rVert \sin(\alpha)\lVert \boldsymbol{x} \rVert \sin(\xi)\\
&=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert \big(\cos(\alpha)\cos(\xi)+\sin(\alpha)\sin(\xi)\big)\\
&=\lVert \boldsymbol{a} \rVert \lVert \boldsymbol{x} \rVert \cos(\xi-\alpha)
\end {aligned}
$$

この輸出は入力されたベクトルがマトリクス$\boldsymbol{A}$にどのぐらいに配置された値です。 
これは、2つのベクトルの間の角度　$ \ xi- \ alpha $でみると同じく理解できるでしょう。
$ \ xi = \ alpha $の場合、2つのベクトルは完全に合わせされ、最大値が達成されます。
ただし　$ \ xi-\ alpha = \ pi $の場合、2つのベクトルはお互いに反対方向を指するため　$ \ boldsymbol {a} ^ \ top \ boldsymbol {x} $最小値に達します。
要するに、この線形変換により、入力されたベクトルを$ A $で定義されたさまざまな方向へ投影された値を見ることができます。
このようなやりかたも、より高い次元にも適用できるでしょう。

線形変換を理解するもう1つの方法は、$ \ boldsymbol {z} $も次のように展開すれば：


$$
\boldsymbol{A}\boldsymbol{x} =
\begin{pmatrix}
    \vert            & \vert            &        & \vert             \\
    \boldsymbol{a}_1 & \boldsymbol{a}_2 & \cdots & \boldsymbol{a}_n  \\
    \vert            & \vert            &        & \vert             \\
\end{pmatrix}
\begin{matrix}
    \rvert \\ \boldsymbol{x} \\ \rvert
\end{matrix} =
x_1 \begin{matrix} \rvert \\ \boldsymbol{a}_1 \\ \rvert \end{matrix} +
x_2 \begin{matrix} \rvert \\ \boldsymbol{a}_2 \\ \rvert \end{matrix} +
    \cdots +
x_n \begin{matrix} \rvert \\ \boldsymbol{a}_n \\ \rvert \end{matrix}
$$

輸出は、マトリクス$ \ boldsymbol {A} $の列で重み付きさせた合計です。
よって、信号は入力の構成にすぎません。


## [線形代数の概念を畳み込みへ展開](https://www.youtube.com/watch?v=OrBEon3VlQg&t=1030s)

次に、オーディオデータ分析の例を使用して、線形代数の概念を畳み込みに展開します。 
まずは、全結合層をマトリクス乗算の形式で表すと：-

$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13}\\
w_{21} & w_{22} & w_{23}\\
w_{31} & w_{32} & w_{33}\\
w_{41} & w_{42} & w_{43}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

この例では、重みがのせたマトリクスのサイズは$ 4 \ times 3 $、入力ベクトルのサイズは$ 3 \ times 1 $なので、出力ベクトルのサイズは$ 4 \ times 1 $です。

ただし、オーディオデータの場合、データサイズは非常に長くなります（サイズはもう3ではありません）。
オーディオデータのサンプル数は、オーディオの時間（*例：3秒）　かける　サンプリングレート（例：* 22.05 kHz）です。 下記のように、入力ベクトル$ \ boldsymbol {x} $は非常に長くなります。 こうしては、重みマトリクスは「厖大」だと言えるでしょう。


$$
\begin{bmatrix}
w_{11} & w_{12} & w_{13} & w_{14} & \cdots &w_{1k}& \cdots &w_{1n}\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$

このような公式はトレーニングするとが困難になります。 幸いに、同じことが単純にできる方法があります。



### 局所性プロパティ

データの局所性（*つまり、遠くにあるデータポイントは対象外）により、
上記の重みマトリクスの$ w_ {1k} $は、$ k $の値が大きい場合に0で埋めることができます。
よって、マトリクスの最初の行のカーネルは3のサイズはになります。
このサイズ3のカーネルを$ \ boldsymbol {a} ^ {（1）} = \ begin {bmatrix} a_1 ^ {（1）}＆a_2として示しましょう ^ {（1）}およびa_3 ^ {（1）} \ end {bmatrix} $。

$$
\begin{bmatrix}
a_1^{(1)}  & a_2^{(1)}  & a_3^{(1)}  & 0 & \cdots &0& \cdots &0\\
w_{21} & w_{22} & w_{23}& w_{24} & \cdots & w_{2k}&\cdots &w_{2n}\\
w_{31} & w_{32} & w_{33}& w_{34} & \cdots & w_{3k}&\cdots &w_{3n}\\
w_{41} & w_{42} & w_{43}& w_{44} & \cdots & w_{4k}&\cdots &w_{4n}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix} = \begin{bmatrix}
y_1\\
y_2\\
y_3\\
y_4
\end{bmatrix}
$$


### 定常性プロパティ

自然なデータ信号には定常性の特性があります（*特定のパターン/モチーフが繰り返されます*）。
だから、先ほど定義したカーネル$ \ mathbf {a} ^ {（1）} $を再利用できます。
よって、毎回に１ステップ飛び跳ね（*ストライドは1）でこのカーネルを使用すること、下記のようになります：


$$
\begin{bmatrix}
a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0 & 0 & 0 & 0&\cdots  &0\\
0 & a_1^{(1)}  & a_2^{(1)} & a_3^{(1)}  & 0&0&0&\cdots &0\\
0 & 0 & a_1^{(1)} & a_2^{(1)}  & a_3^{(1)}  & 0&0&\cdots &0\\
0 & 0 & 0& a_1^{(1)}  & a_2^{(1)}  &a_3^{(1)} &0&\cdots &0\\
0 & 0 & 0& 0 & a_1^{(1)}  &a_2^{(1)} &a_3^{(1)} &\cdots &0\\
\vdots&&\vdots&&\vdots&&\vdots&&\vdots
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots\\
x_k\\
\vdots\\
x_n
\end{bmatrix}
$$

局所性の特性のおかげで、マトリックスの右上部分と左下部分の両方も$ 0 $ sで埋めることができ、マトリクスのスパース性になります。
特定のカーネルを何度も再利用することを重み共有と呼ばれます。


### テプリッツ行列の多重の層

上記の特性で使って変更すると、残ったパラメーターの数は3だけです（* i.e. * $ a_1、a_2、a_3 $）。 
これを前回の12個のパラメーターの重みマトリクス（* ie * $ w_ {11}、w_ {12}、\ cdots、w_ {43} $）と比較すると、今のパラメーター数が多く制限されたため、展開したいのです。

前のマトリクスは、カーネル$ \ boldsymbol {a} ^ {（1）} $付きのあるレイヤー（**畳み込みレイヤー）と見なすことができます。
次に、異なったカーネル$ \ boldsymbol {a} ^ {（2）} $、$ \ boldsymbol {a} ^ {（3）} $などで複数のレイヤーを作成でき、パラメーター数量を増やすことができます。


各層のには、多数複製された1つのカーネルをマトリクスに含まれます。
このタイプのマトリクスは、テプリッツマトリクスと呼ばれます。
このようなのテプリッツマトリクスでは、
対角数字は一定とされます。
ここで使用するテプリッツマトリクスは、スパース特性もあります。

最初のカーネル$ \ boldsymbol {a} ^ {（1）} $と入力ベクトル$ \ boldsymbol {x} $なので、この層で出力の最初のエントリは$ a_1 ^ {（1）} x_1です + a_2 ^ {（1）} x_2 + a_3 ^ {（1）} x_3 $。
したがって、出力ベクトル全体は次のようになります。-

$$
\begin{bmatrix}
\mathbf{a}^{(1)}x[1:3]\\
\mathbf{a}^{(1)}x[2:4]\\
\mathbf{a}^{(1)}x[3:5]\\
\vdots
\end{bmatrix}
$$


次の畳み込み層と他のカーネル（* eg * $ \ boldsymbol {a} ^ {（2）} $と$ \ boldsymbol {a} ^ {（3）} $）で同じマトリクス乗算法もと適用され、結果もふさわしくようになります。

## [Listening to convolutions - Jupyter Notebook](https://www.youtube.com/watch?v=OrBEon3VlQg&t=1709s)

Jupyter Notebookは下記のリンクにあります [here](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/07-listening_to_kernels.ipynb).

このノートブックでは、「スカラーでの計算」としての畳み込みのやりかたを理解していきます。

ライブラリ `librosa`を使用すると、オーディオクリップ$ \ boldsymbol {x} $とそのサンプリングレートを読み込むことができます。 
この例に、サンプルは70641個があり、サンプリングレートは22.05kHz、クリップの全長は3.2秒です。
読み込まれたオーディオ信号はばらつきがあり（図1を参照）、$ y $軸の振幅から見るとどのような声がするのかを推測できます。
オーディオ信号$ x（t）$は、実際にはWindowsシステムをオフにしたときに再生されるサウンドです（図2を参照）。

<center>
<img src="{{site.baseurl}}/images/week04/04-1/audioSignal.png" width="500px" /><br>
<b>Fig. 1</b>: A visualization of the audio signal. <br>
</center>

<center>
<img src="{{site.baseurl}}/images/week04/04-1/notes.png" width="500px" /><br>
<b>Fig. 2</b>: Notes for the above audio signal.<br>
</center>

波形から音符を分離する必要があります。 これをフーリエ変換（FT）で使用すると、すべての音が一緒に出て、各ピッチの正確な時間と場所を把握するのが難しくなります。
よって、ローカライズされたFT変換が必要です（スペクトログラムとも呼ばれます）。
スペクトログラムからみますと（図3を参照）、異なったピッチと異なった周波数でピークが見えます（*例：* 1600で最初のピッチピーク）。 それらの周波数で4つのピッチを連結すると、元の信号のピッチバージョンが得られます。


<center>
<img src="{{site.baseurl}}/images/week04/04-1/spectrogram.png" width="500px" /><br>
<b>Fig. 3</b>: Audio signal and its spectrogram.<br>
</center>


入力信号とすべてのピッチ（たとえば、ピアノのすべての音階）を畳み込むすると、入力信号のすべてのノートを抽出できます（*特定のカーネルにオーディオと一致したときの的中して抽出可能）。 
元の信号といろんなピッチを連結された信号のスペクトログラムは図4に示し、
元の信号と4つのピッチの周波数を図5に示します。
4つのカーネルを入力信号（元の信号）と畳み込みされた波形を図6に示します。
図6には、畳み込みされた信号のオーディオクリップで、ノートの抽出における畳み込みの有効性を証明します。

<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig4.png" width="500px" /><br>
<b>Fig. 4</b>: Spectrogram of original signal (left) and Sepctrogram of the concatenation of pitches (right).<br>
</center>

<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig5.png" width="500px" /><br>
<b>Fig. 5</b>: First note of the melody.<br>
</center>

<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig6.png" width="500px" /><br>
<b>Fig. 6</b>: Convolution of four kernels.<br>
</center>


## 異なるデータセットの次元数

最後に、次元数のさまざまな表現とその例の短い余談です。
ここで、仮に入力セット$ X $がドメイン$ \ Omega $からチャネル$ c $にマッピングされたことを考えます。


### Examples

* Audio data: domain is 1-D, discrete signal indexed by time; number of channels $c$ can range from 1 (mono), 2 (stereo), 5+1 (Dolby 5.1), *etc.*
* Image data: domain is 2-D (pixels); $c$ can range from 1(greyscale), 3(colour), 20(hyperspectral), *etc.*
* Special relativity: domain is $\mathbb{R^4} \times \mathbb{R^4}$ (space-time $\times$ four-momentum); when $c = 1$ it is called Hamiltonian.

<center>
<img src="{{site.baseurl}}/images/week04/04-1/fig7.png" width="600px" /><br>
<b>Fig. 7</b>: Different dimensions of different types of signals.<br>
</center>

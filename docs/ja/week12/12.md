---
lang-ref: ch.12
title: Week 12
lang: ja
translation-date: 6 Dec 2020
translator: Shiro Takagi
---


<!-- ## Lecture part A

In this section we discuss the various architectures used in NLP applications, beginning with CNNs, RNNs, and eventually covering the state of-the art architecture, transformers. We then discuss the various modules that comprise transformers and how they make transformers advantageous for NLP tasks. Finally, we discuss tricks that allow transformers to be trained effectively.  -->

## レクチャーパートA

このセクションでは、CNN、RNNから始まり、最終的には最新のアーキテクチャであるtransformerまで、NLPの応用で使用される様々なアーキテクチャについて説明します。次に、transformerを構成する様々なモジュールと、それがどのようにtransformerをNLPタスクにおいて有利なものにしているかを議論します。最後に、transformerを効果的に学習させるためのコツについて議論します。


<!-- ## Lecture part B

In this section we introduce beam search as a middle ground betwen greedy decoding and exhaustive search. We consider the case of wanting to sample from the generative distribution (i.e. when generating text) and introduce "top-k" sampling. Subsequently, we introduce sequence to sequence models (with a transformer variant) and backtranslation. We then introduce unsupervised learning approaches for learning embeddings and discuss word2vec, GPT, and BERT. -->

## レクチャーパートB

本節では、貪欲なデコーディングと全状態探索の中間地点としてのビーム探索を紹介します。ここでは、生成分布からサンプリングしたい場合（テキストを生成する場合）を考え、トップk個のサンプリングを導入します。その後、sequence-tosequenceモデル（transformerを含む）と逆翻訳を紹介します。その後、教師なし学習による埋め込み学習のアプローチを紹介し、word2vec、GPT、BERTについて議論します。

<!-- ## Practicum

We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture, taking a forward pass through a basic transformer, and comparing the encoder-decoder paradigm to sequential architectures. -->

## 演習

ここでは、self-attentionとその入力の隠れ表現に焦点を当てて、attentionを紹介します。次に、キーバリューストアパラダイムを紹介し、入力の回転としてクエリ、キー、バリューを表現する方法を議論します。最後に、transformerアーキテクチャを解釈するためにattentionを使用し、基本的なtransformerの順伝播を見ることで、エンコーダ/デコーダパラダイムをシークエンシャルなアーキテクチャと比較します。

---
lang-ref: ch.12-3
title: Attention and the Transformer
lecturer: Alfredo Canziani
authors: Francesca Guiso, Annika Brundyn, Noah Kasmanoff, and Luke Martin
date: 21 Apr 2020
lang: ja
translation-date: 6 Dec 2020
translator: Shiro Takagi
---


## [Attention](https://www.youtube.com/watch?v=f01J0Dri-6k&t=69s)

<!-- We introduce the concept of attention before talking about the Transformer architecture. There are two main types of attention: self attention *vs.* cross attention, within those categories, we can have hard *vs.* soft attention.

As we will later see, transformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs. -->

Transformerのアーキテクチャの話をする前に、attentionの概念を紹介します。attentionには主に2つのタイプがあります：self attention と cross attentionで、これらのカテゴリ内では、ハードなattention と ソフトなattentionがあります。

後述しますが、Transformerはattentionモジュールで構成されています。これはシーケンスではなく、集合間の写像です。


### Self Attention (I)

<!-- Consider a set of $t$ input $\boldsymbol{x}$'s: -->
$t$個の入力$\boldsymbol{x}$のなす集合を考えましょう。

$$
\lbrace\boldsymbol{x}_i\rbrace_{i=1}^t = \lbrace\boldsymbol{x}_1,\cdots,\boldsymbol{x}_t\rbrace
$$

ただし各$\boldsymbol{x}_i$は$n$次元のベクトルです。その集合は$t$個の要素を持っているので、各要素は$\mathbb{R}^n$に属しており、その集合を行列として表現することができます$\boldsymbol{X}\in\mathbb{R}^{n \times t}$。

<!-- With self-attention, the hidden representation $h$ is a linear combination of the inputs: -->
Self-attentionでは、隠れ表現 $h$ は入力の線形結合です。

$$
\boldsymbol{h} = \alpha_1 \boldsymbol{x}_1 + \alpha_2 \boldsymbol{x}_2 + \cdots +  \alpha_t \boldsymbol{x}_t
$$

<!-- Using the matrix representation described above, we can write the hidden layer as the matrix product: -->
上述した行列表現を用いて、隠れ層を行列積の形で書くことができます。

$$
\boldsymbol{h} = \boldsymbol{X} \boldsymbol{a}
$$

<!-- where $\boldsymbol{a} \in \mathbb{R}^t$ is a column vector with components $\alpha_i$. -->
ただし$\boldsymbol{a} \in \mathbb{R}^t$は要素が$\alpha_i$の列ベクトルです。

<!-- Note that this differs from the hidden representation we have seen so far, where the inputs are multiplied by a matrix of weights.

Depending on the constraints we impose on the vector $\vect{a}$, we can achieve hard or soft attention. -->

これは、これまで見てきた隠れた表現とは異なることに注意してください。

ベクトル $\vect{a}$ に課す制約に応じて、ハードattentionとソフトattentionが得られます。


<!-- #### Hard Attention -->
#### ハード Attention

<!-- With hard-attention, we impose the following constraint on the alphas: $\Vert\vect{a}\Vert_0 = 1$. This means $\vect{a}$ is a one-hot vector. Therefore, all but one of the coefficients in the linear combination of the inputs equals zero, and the hidden representation reduces to the input $\boldsymbol{x}_i$ corresponding to the element $\alpha_i=1$. -->
ハードattentionでは、$\alpha$に次のような制約を課します：$\Vert\vect{a}\Vert_0 = 1$。これは、$\vect{a}$がone-hotベクトルであることを意味しています。したがって、入力の線形結合の係数の1つ以外はすべてゼロに等しく、隠れ表現は、要素$\alpha_i=1$に対応する入力$\boldsymbol{x}_i$に帰着されます。


<!-- #### Soft Attention -->
#### ソフト Attention

<!-- With soft attention, we impose that $\Vert\vect{a}\Vert_1 = 1$. The hidden representations is a linear combination of the inputs where the coefficients sum up to 1. -->
ソフトattentionでは、 $\Vert\vect{a}\Vert_1 = 1$ とします。隠れ表現は、係数の和が1になるように入力を線形結合したものです。


### Self Attention (II)

<!-- Where do the $\alpha_i$ come from?

We obtain the vector $\vect{a} \in \mathbb{R}^t$ in the following way: -->

$\alpha_i$はどこから来るのでしょうか。

ベクトル$\vect{a} \in \mathbb{R}^t$ は次のようにして得られます：

$$
\vect{a} = \text{[soft](arg)max}_{\beta} (\boldsymbol{X}^{\top}\boldsymbol{x})
$$


<!-- Where $\beta$ represents the inverse temperature parameter of the $\text{soft(arg)max}(\cdot)$. $\boldsymbol{X}^{\top}\in\mathbb{R}^{t \times n}$ is the transposed matrix representation of the set $\lbrace\boldsymbol{x}_i \rbrace\_{i=1}^t$, and $\boldsymbol{x}$ represents a generic $\boldsymbol{x}_i$ from the set. Note that the $j$-th row of $X^{\top}$ corresponds to an element $\boldsymbol{x}_j\in\mathbb{R}^n$, so the $j$-th row of $\boldsymbol{X}^{\top}\boldsymbol{x}$ is the scalar product of $\boldsymbol{x}_j$ with each $\boldsymbol{x}_i$ in $\lbrace \boldsymbol{x}_i \rbrace\_{i=1}^t$. -->

ここで、$\beta$は、$\text{soft(arg)max}(\cdot)$の逆温度パラメータを表します。$\boldsymbol{X}^{\top}\in\mathbb{R}^{t \times n}$ は、集合$\lbrace\boldsymbol{x}_i \rbrace_{i=1}^t$の転置行列表現であり、$\boldsymbol{x}$ は、集合の中の一般的な $\boldsymbol{x}_i$ を表しています。$X^{\top}$の$j$番目の行は、要素$\boldsymbol{x}_j\in\mathbb{R}^n$に対応しています。なので、$\boldsymbol{X}^{\top}\boldsymbol{x}$の$j$番目の行は、$\boldsymbol{x}_j$と$\lbrace \boldsymbol{x}_i \rbrace_{i=1}^t$の中の各$\boldsymbol{x}_i$とのスカラー積となります。

<!-- The components of the vector $\vect{a}$ are also called "scores" because the scalar product between two vectors tells us how aligned or similar two vectors are. Therefore, the elements of $\vect{a}$ provide information about the similarity of the overall set to a particular $\boldsymbol{x}_i$. -->
ベクトル$\vect{a}$の構成要素は、「スコア」とも呼ばれます。なぜなら、二つのベクトルのスカラー積は、二つのベクトルがどれだけ似ているかを教えてくれるからです。したがって、$\vect{a}$の要素は、全体の集合がある特定の$\boldsymbol{x}_i$にどれくらい類似しているかについての情報を与えます。

<!-- The square brackets represent an optional argument. Note that if $\arg\max(\cdot)$ is used, we get a one-hot vector of alphas, resulting in hard attention. On the other hand, $\text{soft(arg)max}(\cdot)$ leads to soft attention. In each case, the components of the resulting vector $\vect{a}$ sum to 1. -->
角括弧は任意の引数です。なお、$\arg\max(\cdot)$を使うと、$\alpha$のone-hotベクトルを得てしまうことに注意してください。一方、$\text{soft(arg)max}(\cdot)$を使うと、ソフトattentionになります。いずれの場合も、結果として得られるベクトルの成分の和は１となります。

<!-- Generating $\vect{a}$ this way gives a set of them, one for each $\boldsymbol{x}_i$. Moreover, each $\vect{a}_i \in \mathbb{R}^t$ so we can stack the alphas in a matrix $\boldsymbol{A}\in \mathbb{R}^{t \times t}$. -->
このようにして$\vect{a}$を生成すると、$\boldsymbol{x}_i$ごとに1つの集合が得られます。さらに、各$\vect{a}_i \in \mathbb{R}^t$を積み上げて、行列$\boldsymbol{A} \in \mathbb{R}^{t \times t}$を得ることができます。

<!-- Since each hidden state is a linear combination of the inputs $\boldsymbol{X}$ and a vector $\vect{a}$, we obtain a set of $t$ hidden states, which we can stack into a matrix $\boldsymbol{H}\in \mathbb{R}^{n \times t}$. -->
それぞれの隠れ状態は、入力$\boldsymbol{X}$とベクトル$\vect{a}$の線形結合なので、$t$の隠れ状態の集合が得られ、これを行列$\boldsymbol{H}\in \mathbb{R}^{n \times t}$として積み上げることができます。

$$
\boldsymbol{H}=\boldsymbol{XA}
$$


<!-- ## [Key-value store](https://www.youtube.com/watch?v=f01J0Dri-6k&t=1056s) -->
## [キーバリューストア](https://www.youtube.com/watch?v=f01J0Dri-6k&t=1056s)

<!-- A key-value store is a paradigm designed for storing (saving), retrieving (querying) and managing associative arrays (dictionaries / hash tables).

For example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for "lasagne" - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.

Basically, the query is the question. Given one query, we check this query against every key and retrieve all matching content. -->

キーバリューストアは、保存 (ストア)、検索 (クエリ)、連想配列 (辞書 / ハッシュテーブル) の管理のために設計されたパラダイムです。

例えば、ラザニアを作るためのレシピを見つけたいとします。レシピブックを持っていて、「ラザニア」と検索すると、これがクエリになります。このクエリは、データセット内のすべての可能性のあるキーに対してチェックされます。クエリが各タイトルとどの程度一致しているかをチェックして、クエリとすべてのキーとの間で最も一致する最大のスコアを見つけます。もし出力がargmax関数であれば、最も高いスコアを持つ単一のレシピを取得します。ソフトなargmax関数を使えば、確率分布を得ることができます。これによって、クエリにマッチするレシピの中から、最も類似した内容から類似してないものへと順番に検索することができます。

基本的には、クエリは質問です。1つのクエリが与えられると、このクエリをすべてのキーに対してチェックして、一致するすべてのコンテンツを検索します。


<!-- ### Queries, keys and values -->
### クエリ、キー、バリュー

$$
\begin{aligned}
\vect{q} &= \vect{W_q x} \\
\vect{k} &= \vect{W_k x} \\
\vect{v} &= \vect{W_v x}
\end{aligned}
$$

<!-- Each of the vectors $\vect{q}, \vect{k}, \vect{v}$ can simply be viewed as rotations of the specific input $\vect{x}$. Where $\vect{q}$ is just $\vect{x}$ rotated by $\vect{W_q}$, $\vect{k}$ is just $\vect{x}$ rotated by $\vect{W_k}$ and similarly for $\vect{v}$. Note that this is the first time we are introducing "learnable" parameters. We also do not include any non-linearities since attention is completely based on orientation. -->
それぞれのベクトル$\vect{q}, \vect{k}, \vect{v}$ は、簡単に言えば、特定の入力$\vect{x}$の回転と見ることができます。ここで、$\vect{q}$は、$\vect{W_q}$で回転させただけの$\vect{x}$であり、$\vect{k}$は、$\vect{W_k}$で回転させただけの$\vect{x}$であり、$\vect{v}$についても同様です。学習可能なパラメータを導入するのは今回が初めてであることに注意してください。また、attentionは完全に方向性にのみ基づいているので、非線形性も含めていません。

<!-- In order to compare the query against all possible keys, $\vect{q}$ and $\vect{k}$ must have the same dimensionality, i.e. $\vect{q}, \vect{k} \in \mathbb{R}^{d'}$. -->
全ての可能なキーと比較するためには、$\vect{q}$ と $\vect{k}$ は同じ次元でなければなりません。例えば、$\vect{q}, \vect{k} \in \mathbb{R}^{d'}$。

<!-- However, $\vect{v}$ can be of any dimension. If we continue with our lasagne recipe example - we need the query to have the dimension as the keys, i.e. the titles of the different recipes that we're searching through. The dimension of the corresponding recipe retrieved, $\vect{v}$, can be arbitrarily long though. So we have that $\vect{v} \in \mathbb{R}^{d''}$. -->
しかし、$\vect{v}$ は任意の次元である可能性があります。ラザニアのレシピの例を続けるならば、クエリは次元をキー、すなわち、検索している異なるレシピのタイトルとして持つ必要があります。検索された対応するレシピの次元、$\vect{v}$は、任意の長さにすることができます。だから、$\vect{v} \in \mathbb{R}^{d''}$です。

<!-- For simplicity, here we will make the assumption that everything has dimension $d$, i.e. -->
ここでは簡単のために、すべてが $d$ 次元であると仮定します。例えば

$$
d' = d'' = d
$$

<!-- So now we have a set of $\vect{x}$'s, a set of queries, a set of keys and a set of values. We can stack these sets into matrices each with $t$ columns since we stacked $t$ vectors; each vector has height $d$. -->
これで、$\vect{x}$の集合、つまりクエリの集合、キーの集合、バリューの集合ができました。$t$本のベクトルを積み重ねたので、これらの集合を、それぞれ $t$ 列を持つ行列として積み重ねることができます。このとき、各ベクトルの高さは $d$ です。

$$
\{ \vect{x}_i \}_{i=1}^t \rightsquigarrow \{ \vect{q}_i \}_{i=1}^t, \, \{ \vect{k}_i \}_{i=1}^t, \, \, \{ \vect{v}_i \}_{i=1}^t \rightsquigarrow \vect{Q}, \vect{K}, \vect{V} \in \mathbb{R}^{d \times t}
$$

<!-- We compare one query $\vect{q}$ against the matrix of all keys $\vect{K}$: -->
一つのクエリ$\vect{q}$ を全てのキー$\vect{K}$からなる行列と比較します。

$$
\vect{a} = \text{[soft](arg)max}_{\beta} (\vect{K}^{\top} \vect{q}) \in \mathbb{R}^t
$$

<!-- Then the hidden layer is going to be the linear combination of the columns of $\vect{V}$ weighted by the coefficients in $\vect{a}$: -->
すると隠れ層は、$\vect{a}$に入っている係数によって重み付られた$\vect{V}$の列の線形結合になります。

$$
\vect{h} = \vect{V} \vect{a} \in \mathbb{R}^d
$$

<!-- Since we have $t$ queries, we'll get $t$ corresponding $\vect{a}$ weights and therefore a matrix $\vect{A}$ of dimension $t \times t$. -->
$t$のクエリを持っていますので、それに対応する$t$本の$\vect{a}$重み、したがって$t \times t$次元の行列$\vect{A}$を持っていることになります。

$$
\{ \vect{q}_i \}_{i=1}^t \rightsquigarrow \{ \vect{a}_i \}_{i=1}^t, \rightsquigarrow \vect{A} \in \mathbb{R}^{t \times t}
$$

<!-- Therefore in matrix notation we have: -->
したがって、行列表記では次のようになります。

$$
\vect{H} = \vect{VA} \in \mathbb{R}^{d \times t}
$$

<!-- As an aside, we typically set $\beta$ to: -->
余談ですが、通常は$\beta$を次のように設定します。

$$
\beta = \frac{1}{\sqrt{d}}
$$

<!-- This is done to keep the temperature constant across different choices of dimension $d$ and so we divide by the square root of the number of dimensions $d$. (Think what is the length of the vector $\vect{1} \in \R^d$.) -->
これは、次元$d$の選択の違いによって決まる温度を一定に保つために行われるので、次元$d$の平方根で割っています(ベクトル $\vect{1} \in \R^d$ の長さは何か考えてみてください)。

<!-- For implementation, we can speed up computation by stacking all the $\vect{W}$'s into one tall $\vect{W}$ and then calculate $\vect{q}, \vect{k}, \vect{v}$ in one go: -->
実装する上では、全ての$\vect{W}$を一つの$\vect{W}$にスタックしてから$\vect{q}、\vect{k}、\vect{v}$を計算することで、計算の高速化を行えます。

$$
\begin{bmatrix}
\vect{q} \\
\vect{k} \\
\vect{v}
\end{bmatrix} =
\begin{bmatrix}
\vect{W_q} \\
\vect{W_k} \\
\vect{W_v}
\end{bmatrix} \vect{x} \in \mathbb{R}^{3d}
$$

<!-- There is also the concept of "heads". Above we have seen an example with one head but we could have multiple heads. For example, say we have $h$ heads, then we have $h$ $\vect{q}$'s, $h$ $\vect{k}$'s and $h$ $\vect{v}$'s and we end up with a vector in $\mathbb{R}^{3hd}$: -->
また、「ヘッド」という概念もあります。上では、ヘッドが一つの例を見ましたが、複数のヘッドを持つこともできます。例えば、$h$個のヘッドがあるとすると、$h$個の $\vect{q}$、 $h$個の $\vect{k}$及び $h$個の $\vect{v}$があり、最終的には、$\mathbb{R}^{3hd}$のベクトルになります。

$$
\begin{bmatrix}
\vect{q}^1 \\
\vect{q}^2 \\
\vdots \\
\vect{q}^h \\
\vect{k}^1 \\
\vect{k}^2 \\
\vdots \\
\vect{k}^h \\
\vect{v}^1 \\
\vect{v}^2 \\
\vdots \\
\vect{v}^h
\end{bmatrix} =
\begin{bmatrix}
\vect{W_q}^1 \\
\vect{W_q}^2 \\
\vdots \\
\vect{W_q}^h \\
\vect{W_k}^1 \\
\vect{W_k}^2 \\
\vdots \\
\vect{W_k}^h \\
\vect{W_v}^1 \\
\vect{W_v}^2 \\
\vdots \\
\vect{W_v}^h
\end{bmatrix} \vect{x} \in \R^{3hd}
$$

<!-- However, we can still transform the multi-headed values to have the original dimension $\R^d$ by using a $\vect{W_h} \in \mathbb{R}^{d \times hd}$. This is just one possible way to implement the key-value store. -->
しかし、元の次元$\R^d$を得るために、$\vect{W_h} \in \mathbb{R}^{d \times hd}$ を使用することでmulti-headの値を変換することができます。これは、キーバリューストアを実装するための一つの方法に過ぎません。


<!-- ## [The Transformer](https://www.youtube.com/watch?v=f01J0Dri-6k&t=2114s) -->
## [Transformer](https://www.youtube.com/watch?v=f01J0Dri-6k&t=2114s)

<!-- Expanding on our knowledge of attention in particular, we now interpret the fundamental building blocks of the transformer. In particular, we will take a forward pass through a basic transformer, and see how attention is used in the standard encoder-decoder paradigm and compares to the sequential architectures of RNNs. -->
Attentionの知識を拡張して、Transformerの基本的な構成要素を解釈します。特に、基本的なTransformerの順伝播を見ることで、標準的なエンコーダ／デコーダパラダイムでattentionがどのように使用されているのかをみて、RNNのようなシークエンシャルなアーキテクチャと比較します。

<!-- ### Encoder-Decoder Architecture -->
### エンコーダ・デコーダアーキテクチャ

<!-- We should be familiar with this terminology. It is shown most prominently during autoencoder demonstrations, and is prerequisite understanding up to this point. To summarize, an input is fed through an encoder and decoder which impose some sort of bottleneck on the data, forcing only the most important information through. This information is stored in the output of the encoder block, and can be used for a variety of unrelated tasks. -->
この用語には親しまなければなりません。これはオートエンコーダーのデモで最もよく示されており、ここまでの理解が前提となっています。要約すると、入力はエンコーダとデコーダを通して供給され、データにある種のボトルネックを課して、最も重要な情報だけを強制的に通過させます。この情報はエンコーダブロックの出力に格納され、関連性のない様々なタスクに使用されます。

<!-- <center>
<img src="{{site.baseurl}}/images/week12/12-3/figure1.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>Figure 1:</b> Two example diagrams of an autoencoder. The model on the left shows how an autoencoder can be design with two affine transformations + activations, where the image on the right replaces this single "layer" with an arbitrary module of operations.
</center> -->
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure1.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>図1:</b> オートエンコーダーの2つの例の図。左のモデルは、2つのアフィン変換＋活性化関数でオートエンコーダーを設計する方法を示しており、右の図は、この単一の「層」を任意の演算モジュールに置き換えています。
</center>

<!-- Our "attention" is drawn to the autoencoder layout as shown in the model on the right and will now take a look inside, in the context of transformers. -->
私たちの「attention」は、右のモデルに示されているように、オートエンコーダーのレイアウトに引き寄せられていますが、ここで、Transformerの文脈で、内部を見てみましょう。


<!-- ### Encoder Module -->
### エンコーダーモジュール

<!-- <center>
<img src="{{site.baseurl}}/images/week12/12-3/figure2.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>Figure 2:</b> The transformer encoder, which accepts at set of inputs $\vect{x}$, and outputs a set of hidden representations $\vect{h}^\text{Enc}$.
</center> -->
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure2.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>図2:</b> transfomerのエンコーダは、入力集合である $\vect{x}$ の入力を受け、隠れ表現の集合である $\vect{h}^\text{Enc}$ を出力します。
</center>

<!-- The encoder module accepts a set of inputs, which are simultaneously fed through the self attention block and bypasses it to reach the `Add, Norm` block. At which point, they are again simultaneously passed through the 1D-Convolution and another `Add, Norm` block, and consequently outputted as the set of hidden representation. This set of hidden representation is then either sent through an arbitrary number of encoder modules (i.e.: more layers), or to the decoder. We shall now discuss these blocks in more detail. -->
エンコーダモジュールへの一連の入力は、同時にself attentionブロックを通過して `Add, Norm` ブロックに到達するために順次処理されます。この時点で、それらは再び 1次元畳み込み と `Add, Norm` ブロックを同時に通過し、結果として隠れ表現の集合として出力されます。そして、この隠れ表現の集合は、任意の数のエンコーダモジュール（つまり、より多くの層）を介して送られるか、デコーダに送られます。ここでは、これらのブロックについて詳しく説明します。


### Self-attention

<!-- The self-attention model is a normal attention model. The query, key, and value are generated from the same item of the sequential input. In tasks that try to model sequential data, positional encodings are added prior to this input. The output of this block is the attention-weighted values. The self-attention block accepts a set of inputs, from $1, \cdots , t$, and outputs $1, \cdots, t$ attention weighted values which are fed through the rest of the encoder. -->
Self-attentionモデルは、通常のattentionモデルです。クエリ、キー、バリューは逐次入力の同じ項目から生成されます。系列データをモデル化しようとするタスクでは、この入力の前にpositional encodingが追加されます。このブロックの出力は、attentionで重み付られた値です。Self-attentionブロックは、$1, \cdots , t$からの入力を受け入れ、$1, \cdots, t$のattentionで重み付られた値を出力し、それが残りのエンコーダーに入力されます。

<!-- <center>
<img src="{{site.baseurl}}/images/week12/12-3/figure3.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>Figure 3:</b> The self-attention block. The sequence of inputs is shown as a set along the 3rd dimension, and concatenated.
</center> -->
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure3.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>Figure 3:</b> Self-attentionブロック。 入力系列は、3次元に沿った集合として示され、連結されています。
</center>


#### Add, Norm

<!-- The add norm block has two components. First is the add block, which is a residual connection, and layer normalization. -->
Add normブロックは２つの構成要素を持っています。residual connectionであるaddブロックと、layer normalizationからなるnormブロックです。


<!-- #### 1D-convolution -->
#### 一次元畳み込み

<!-- Following this step, a 1D-convolution (aka a position-wise feed forward network) is applied. This block consists of two dense layers. Depending on what values are set, this block allows you to adjust the dimensions of the output $\vect{h}^\text{Enc}$. -->
このステップに続いて、1次元畳み込み（別名、位置情報フィードフォワードネットワーク）が適用されます。このブロックは全結合層で構成されています。このブロックでは、どのような値を設定するかによって、出力される $\vect{h}^\text{Enc}$ の次元を調整することができます。


<!-- ### Decoder Module -->
### デコーダーモジュール

<!-- The transformer decoder follows a similar procedure as the encoder. However, there is one additional sub-block to take into account. Additionally, the inputs to this module are different. -->
Transformer のデコーダは、エンコーダと同様の手順に従います。しかし、考慮しなければならない追加のサブブロックが1つあります。さらに、このモジュールへの入力が異なります。

<!-- <center>
<img src="{{site.baseurl}}/images/week12/12-3/figure5.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>Figure 4:</b> A friendlier explanation of the decoder.
</center> -->
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure5.png" style="zoom: 60%; background-color:#DCDCDC;" /><br>
<b>図4:</b> デコーダーのより親切な説明
</center>


#### Cross-attention

<!-- The cross attention follows the query, key, and value setup used for the self-attention blocks.  However, the inputs are a little more complicated. The input to the decoder is a data point $\vect{y}\_i$, which is then passed through the self attention and add norm blocks, and finally ends up at the cross-attention block. This serves as the query for cross-attention, where the key and value pairs are the output $\vect{h}^\text{Enc}$, where this output was calculated with all past inputs $\vect{x}\_1, \cdots, \vect{x}\_{t}$. -->
Cross attentionは、self-attentionブロックで使用されるクエリ、キー、バリューの設定に従います。 しかし、入力は少し複雑です。デコーダへの入力は、データ点 $\vect{y}_i$ であり、これは、self-attentionブロックとadd normブロックを通過し、最終的にcross attentionブロックで終了します。これはcross-attentionへのクエリとして機能します。そこでは、キーバリューのペアは$\vect{h}^\text{Enc}$の出力です。そしてこの出力は過去のすべての$\vect{x}_1, \cdots, \vect{x}_{t}$を使って計算されます。


<!-- ## Summary -->
## まとめ

<!-- A set, $\vect{x}\_1$ to $\vect{x}\_{t}$ is fed through the encoder. Using self-attention and some more blocks, an output representation, $\lbrace\vect{h}^\text{Enc}\rbrace_{i=1}^t$ is obtained, which is fed to the decoder. After applying self-attention to it, cross attention is applied. In this block, the query corresponds to a representation of a symbol in the target language $\vect{y}\_i$, and the key and values are from the source language sentence ($\vect{x}\_1$ to $\vect{x}\_{t}$). Intuitively, cross attention finds which values in the input sequence are most relevant to constructing $\vect{y}\_t$, and therefore deserve the highest attention coefficients. The output of this cross attention is then fed through another 1D-convolution sub-block, and we have $\vect{h}^\text{Dec}$. For the specified target language, it is straightforward from here to see how training will commence, by comparing $\lbrace\vect{h}^\text{Dec}\rbrace_{i=1}^t$ to some target data. -->
$\vect{x}_1$から$\vect{x}_{t}$の集合がエンコーダを介して送られてきます。Self-attentionといくつかのブロックを使って、出力の表現 $\lbrace\vect{h}^\text{Enc}\rbrace_{i=1}^t$ を得て、それをデコーダに送ります。これにself-attentionをかけた後、cross-attentionをかけます。このブロックでは、クエリは、対象言語におけるシンボルの表現に対応し、キーとバリューは、ソース言語の文（$\vect{x}_1$ to $\vect{x}_{t}$）のものです。直感的には、cross-attentionは、入力文のどの値が、$\vect{y}_t$を構築するのに最も関連性があり、したがって、最も高いattention係数に値するかを見つけます。このcross-attentionの出力は、別の１次元畳み込みサブブロックを介して与えられ、 $\vect{h}^\text{Dec}$ が得られます。指定された対象言語については、ここから、$\lbrace\vect{h}^\text{Dec}\rbrace_{i=1}^t$ をいくつかの対象データと比較することで、どのように訓練が起こるのかを簡単に見ることができます。


<!-- ### Word Language Models -->
### 単語言語モデル

<!-- There are a few important facts we left out before to explain the most important modules of a transformer, but will need to discuss them now to understand how transformers can achieve state-of-the-art results in language tasks. -->
Transformerの最も重要なモジュールを説明する前に、説明せずに残していたいくつかの重要な事実がありますが、Transformerがどのように言語タスクで最先端の結果を達成することができるかを理解するために、今すぐにそれらを議論する必要があります。


<!-- #### Positional encoding

Attention mechanisms allow us to parallelize the operations and greatly accelerate a model's training time,  but loses sequential information. The positional encoding feature enables allows us to capture this context. -->

#### Positional encoding

Attention機構を利用することで並列処理が可能になり、モデルの学習時間を大幅に短縮することができますが、逐次的な情報は失われてしまいます。positional encoding機能を用いることで、この文脈を捉えることが可能となります。


<!-- #### Semantic Representations

Throughout the training of a transformer, many hidden representations are generated. To create an embedding space similar to the one used by the word-language model example in PyTorch, the output of the cross-attention, will provide a semantic representation of the word $x_i$, at which point further experimentation can be performed over this dataset. -->

#### 意味表現

トランスフォーマーの訓練を通して、多くの隠れ表現が生成されます。PyTorch の単語モデルの例で使用されているものと同様の埋め込み空間を作成するために、cross-attentionの出力は $x_i$ という単語の意味表現を提供します。これによってこのデータセット上でのさらなる実験が可能となります。



<!-- ### Code Summary

We will now see the blocks of transformers discussed above in a far more understandable format, code!

The first module we will look at the multi-headed attention block. Depenending on query, key, and values entered into this block, it can either be used for self or cross attention. -->

### コードの概要

ここからは、上で説明したtransformerのブロックを、はるかにわかりやすい、コードという形で見ていきましょう!

最初のモジュールでは、multi-head attentionブロックを見ていきます。このブロックに入力されたクエリー、キー、バリューに応じて、self-attentionにもcross-attentionにも使用できます。


```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, p, d_input=None):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        if d_input is None:
            d_xq = d_xk = d_xv = d_model
        else:
            d_xq, d_xk, d_xv = d_input
        # Embedding dimension of model is a multiple of number of heads
        assert d_model % self.num_heads == 0
        self.d_k = d_model // self.num_heads
        # These are still of dimension d_model. To split into number of heads
        self.W_q = nn.Linear(d_xq, d_model, bias=False)
        self.W_k = nn.Linear(d_xk, d_model, bias=False)
        self.W_v = nn.Linear(d_xv, d_model, bias=False)
        # Outputs of all sub-layers need to be of dimension d_model
        self.W_h = nn.Linear(d_model, d_model)
```


<!-- Initialization of multi-headed attention class. If a `d_input` is provided, this becomes cross attention. Otherwise, self-attention. The query, key, value setup is constructed as a linear transformation of the input `d_model`. -->
Multi-head attentionクラスの初期化では、`d_input` が与えられた場合はcross-attentionとなります。それ以外の場合はself-attentionとなります。クエリ、キー、バリューの設定は入力 `d_model` の線形変換として構築されます。


```python
def scaled_dot_product_attention(self, Q, K, V):
    batch_size = Q.size(0)
    k_length = K.size(-2)

    # Scaling by d_k so that the soft(arg)max doesnt saturate
    Q = Q / np.sqrt(self.d_k)  # (bs, n_heads, q_length, dim_per_head)
    scores = torch.matmul(Q, K.transpose(2,3))  # (bs, n_heads, q_length, k_length)

    A = nn_Softargmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length)

    # Get the weighted average of the values
    H = torch.matmul(A, V)  # (bs, n_heads, q_length, dim_per_head)

    return H, A
```

<!-- Return hidden layer corresponding to encodings of values after scaled by the attention vector. For book-keeping purposes (which values in the sequence were masked out by attention?) A is also returned. -->
Attentionベクトルでスケーリングされた後の値のpositional enncodingに対応する隠れ層を返します。シーケンス中のどの値がattentionによってマスクされたかを見るために A も返されます。

```python
def split_heads(self, x, batch_size):
    return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
```

<!-- Split the last dimension into (`heads` × `depth`). Return after transpose to put in shape (`batch_size` × `num_heads` × `seq_length` × `d_k`) -->
最後の次元を (`heads` × `depth`) に分割します。(`batch_size` × `num_heads` × `seq_length` × `d_k`) の形にするために転置した後に出力を返します。

```python
def group_heads(self, x, batch_size):
    return x.transpose(1, 2).contiguous().
        view(batch_size, -1, self.num_heads * self.d_k)
```

<!-- Combines the attention heads together, to get correct shape consistent with batch size and sequence length. -->
Attention headを組み合わせて、バッチサイズとシーケンスの長さに合った正しいシェイプにします。

```python
def forward(self, X_q, X_k, X_v):
    batch_size, seq_length, dim = X_q.size()
    # After transforming, split into num_heads
    Q = self.split_heads(self.W_q(X_q), batch_size)
    K = self.split_heads(self.W_k(X_k), batch_size)
    V = self.split_heads(self.W_v(X_v), batch_size)
    # Calculate the attention weights for each of the heads
    H_cat, A = self.scaled_dot_product_attention(Q, K, V)
    # Put all the heads back together by concat
    H_cat = self.group_heads(H_cat, batch_size)  # (bs, q_length, dim)
    # Final linear layer
    H = self.W_h(H_cat)  # (bs, q_length, dim)
    return H, A
```

<!-- The forward pass of multi headed attention.

Given an input is split into q, k, and v, at which point these values are fed through a scaled dot product attention mechanism, concatenated and fed through a final linear layer. The last output of the attention block is the attention found, and the hidden representation that is passed through the remaining blocks.

Although the next block shown in the transformer/encoder's is the Add,Norm, which is a function already built into PyTorch. As such, it is an extremely simple implementation, and does not need it's own class. Next is the 1-D convolution block. Please refer to previous sections for more details.

Now that we have all of our main classes built (or built for us), we now turn to an encoder module. -->

Multi-head attentionの順伝播についてです。

入力が q, k, v に分割され、その時点で、これらの値は、スケーリングされたドット積attention機構を介して入力され、連結され、最終的な線形層へ入力されます。Attentionブロックの最後の出力は、見つかったattentionと、残りのブロックに渡される隠れ表現です。

Transformer/エンコーダーの次のブロックはAdd,Normですが、これはPyTorchにすでに組み込まれている関数です。このように、これは非常にシンプルな実装であり、独自のクラスを必要としません。次は1次元畳み込みブロックです。詳細は前のセクションを参照してください。

これで、すべてのメインクラスがビルドされました（またはビルドしてもらった）ので、次にエンコーダモジュールに移ります。

```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, conv_hidden_dim, p=0.1):
        self.mha = MultiHeadAttention(d_model, num_heads, p)
        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)
        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)

    def forward(self, x):
        attn_output, _ = self.mha(x, x, x)
        out1 = self.layernorm1(x + attn_output)
        cnn_output = self.cnn(out1)
        out2 = self.layernorm2(out1 + cnn_output)
        return out2
```

<!-- In the most powerful transformers, an arbitarily large number of these encoders are stacked on top of one another.

Recall that self attention by itself does not have any recurrence or convolutions, but that's what allows it to run so quickly. To make it sensitive to position we provide positional encodings. These are calculated as follows: -->

最も強力なTransformerでは、任意の数のこれらのエンコーダが互いに積み重ねられています。

Self-attentionはそれ自体では再帰や畳み込みを持たないことを思い出してください。位置に敏感になるように、我々はpositional encodingを提供します。これらは次のように計算されます。


$$
\begin{aligned}
E(p, 2)    &= \sin(p / 10000^{2i / d}) \\
E(p, 2i+1) &= \cos(p / 10000^{2i / d})
\end{aligned}
$$


<!-- As to not take up too much room on the finer details, we will point you to https://github.com/Atcold/pytorch-Deep-Learning/blob/master/15-transformer.ipynb for the full code used here.


An entire encoder, with N stacked encoder layers, as well as position embeddings, is written out as -->

あまりスペースを取らないように、細かい詳細については、ここで使用されている完全なコードについては https://github.com/Atcold/pytorch-Deep-Learning/blob/master/15-transformer.ipynb を参照してください。


Positional encodingと同様に、N個のスタックされたエンコーダ層を持つエンコーダ全体は、以下のように書き出されます

```python
class Encoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim,
            input_vocab_size, maximum_position_encoding, p=0.1):
        self.embedding = Embeddings(d_model, input_vocab_size,
                                    maximum_position_encoding, p)
        self.enc_layers = nn.ModuleList()
        for _ in range(num_layers):
            self.enc_layers.append(EncoderLayer(d_model, num_heads,
                                                ff_hidden_dim, p))
    def forward(self, x):
        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)
        for i in range(self.num_layers):
            x = self.enc_layers[i](x)
        return x  # (batch_size, input_seq_len, d_model)
```


<!-- ## Example Use -->
<!-- 
There is a lot of tasks you can use just an Encoder for. In the accompanying notebook, we see how an encoder can be used for sentiment analysis.

Using the imdb review dataset, we can output from the encoder a latent representation of the a sequence of text, and train this encoding process with binary cross entropy, corresponding to a positive or negative movie review.

Again we leave out the nuts and bolts, and direct you to the notebook, but here is the most important architectural components used in the transformer: -->

## 使用例

エンコーダだけを使ってもできるタスクはたくさんあります。付属のノートブックでは、エンコーダがどのように感情分析に使用できるかを見ています。

imdbのレビューデータセットを使って、エンコーダーからテキスト系列の潜在表現を出力し、このエンコーディング過程をバイナリークロスエントロピーで訓練して、映画のレビューが肯定的か否定的かに対応させます。

ここでも詳細の説明は省略して、ノートを参照していただきたいのですが、ここではTransoformerで使われている最も重要なアーキテクチャコンポーネントを紹介します。


```python
class TransformerClassifier(nn.Module):
    def forward(self, x):
        x = Encoder()(x)
        x = nn.Linear(d_model, num_answers)(x)
        return torch.max(x, dim=1)

model = TransformerClassifier(num_layers=1, d_model=32, num_heads=2,
                         conv_hidden_dim=128, input_vocab_size=50002, num_answers=2)
```
<!-- Where this model is trained in typical fashion. -->
これはモデルが典型的な方法で訓練されるところです。
<!-- TODO: Dont get what "Where this model is trained in typical fashion. " means in this context -->

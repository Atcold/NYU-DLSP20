---
lang-ref: ch.03
lang: ja
title: 第3週
translation-date: 16 Aug 2020
translator: Takashi Shinozaki
---

<!-- ## Lecture part A -->
## レクチャー　パート　A

<!-- We first see a visualization of a 6-layer neural network. Next we begin with the topic of Convolutions and Convolution Neural Networks (CNN). We review several types of parameter transformations in the context of CNNs and introduce the idea of a kernel, which is used to learn features in a hierarchical manner. Thereby allowing us to classify our input data which is the basic idea motivating the use of CNNs. -->
まず最初に6層のニューラルネットワークの可視化の結果について概観し、その後、畳み込みと畳み込みニューラルネットワーク(CNN)について見てゆきます。CNNを理解するために必要ないくつかのタイプのパラメータ変換について解説し、階層的な方法で特徴を学習するためにカーネルを用いる手法ついて紹介します。これによって様々な入力データを分類することができるようになりますが、これがCNNを用いる動機となる基本的な考え方となります。


<!-- ## Lecture part B -->
## レクチャー　パート　B

<!-- We give an introduction on how CNNs have evolved over time. We discuss in detail different CNN architectures, including a modern implementation of LeNet5 to exemplify the task of digit recognition on the MNIST dataset. Based on its design principles, we expand on the advantages of CNNs which allows us to exploit the compositionality, stationarity, and locality features of natural images. -->
CNNがどのように進化してきたかについて紹介します。MNISTデータセットの数字画像を認識するLeNet5の最新の実装を含めて、さまざまなCNNアーキテクチャについて詳細に議論します。その設計原理に基づいて、自然画像の構成性、定常性、局所性の特徴を利用できるCNNの利点を拡張します。


<!-- ## Practicum -->
## 演習
<!-- Properties of natural signals that are most relevant to CNNs are discussed in more detail, namely: Locality, Stationarity, and Compositionality. We explore precisely how a kernel exploits these features through sparsity, weight sharing and the stacking of layers, as well as motivate the concepts of padding and pooling. Finally, a performance comparison between FCN and CNN was done for different data modalities. -->
CNNに最も関連する自然信号の特性である局所性、定常性、構成性、について詳細に議論します。カーネルがどのようにしてこれらの特徴を利用しているのかを、スパース性、重み共有、レイヤーの多層化を通して精細に確認し、パディングとプーリングの概念について理解します。最後に、FCNとCNNの性能比較を異なるタイプのデータに対して行います。

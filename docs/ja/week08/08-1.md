---
lang-ref: ch.08-1
lecturer: Yann LeCun
title: Contrastive Methods in Energy-Based Models
authors: Vishwaesh Rajiv, Wenjun Qu, Xulai Jiang, Shuya Zhao
date: 23 Mar 2020
lang: ja
translation-date: 28 Nov 2020
translator: Jesmer Wong
---


<!-- ## [Recap](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=5s) -->
## [要約](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=5s)

<!-- Dr. LeCun spent the first ~15 min giving a review of energy-based models. Please refer back to last week (Week 7 notes) for this information, especially the concept of contrastive learning methods.　-->

LeCun博士は、最初の約15分間で、エネルギーベースのモデル（EBM）のレビューを行いました。この明細、特に対照学習法の概念については、先週（第7週のメモ）を参照してください。

<!-- As we have learned from the last lecture, there are two main classes of learning methods: -->
前回の講義から学んだように、学習方法には主に2つのクラスがあります。

<!-- 1. Contrastive Methods that push down the energy of training data points, $F(x_i, y_i)$, while pushing up energy on everywhere else, $F(x_i, y’)$.
2. Architectural Methods that build energy function $F$ which has minimized/limited low energy regions by applying regularization. -->

1.コントラスト方法はトレーニング・データポイントのエネルギー$ F（x_i、y_i）$を押し下げ、他のすべてのエネルギー$ F（x_i、y ’）$を押し上げること。
2.アーキテクチャ手法は正則化を適用することによって低エネルギー領域を最小化/制限したエネルギー関数$ F $を構築すること。

<!-- To distinguish the characteristics of different training methods, Dr. Yann LeCun has further summarized 7 strategies of training from the two classes mention before. One of which is methods that are similar to Maximum Likelihood method, which push down the energy of data points and push up everywhere else. -->

さまざまなトレーニング方法の特徴を区別するために、LeCun博士は、前述の2つのクラスから7つのトレーニング戦略をさらに分けるようにします。

その1つは、データポイントのエネルギーを押し下げ、他のすべてのところを押し上げる、つまり最尤法に似た方法です。

<!-- Maximum Likelihood method probabilistically pushes down energies at training data points and pushes everywhere else for every other value of $y’\neq y_i$. Maximum Likelihood doesn't “care” about the absolute values of energies but only “cares” about the difference between energy. Because the probability distribution is always normalized to sum/integrate to 1, comparing the ratio between any two given data points is more useful than simply comparing absolute values.　-->

最尤法は、トレーニング・データポイントでエネルギーを確率的に押し下げ、$ y ’\neq y_i $の他のすべての値を他のすべての場所に押し下げます。
最尤法は、エネルギーの絶対値を「ケア」するのではなく、エネルギー間の差を「ケア」するだけです。
確率分布は常に合計で1になるように正規化されているため、任意の2つのデータポイント間の比率を比較する方が、単に絶対値を比較するよりも便利です。

<!-- ## [Contrastive methods in self-supervised learning](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=613s) -->
## [自己教師あり学習のコントラスト的な方法](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=613s)

<!-- In contrastive methods, we push down on the energy of observed training data points ($x_i$, $y_i$), while pushing up on the energy of points outside of the training data manifold. -->
コントラストな方法では、観測されたトレーニング・データポイント（$ x_i $、$ y_i $）のエネルギーを押し下げ、トレーニングデータ多様体以外のポイントのエネルギーを押し上げます。

<!-- In self-supervised learning, we use one part of the input to predict the other parts. We hope that our model can produce good features for computer vision that rival those from supervised tasks. -->
自己教師あり学習では、入力の一部を使用して他の部分を予測します。このモデルが教わるタスクのモデルに合わせるコンピュータービジョンの優れた機能を生みだされることだろう。

<!-- Researchers have found empirically that applying contrastive _embedding_ methods to self-supervised learning models can indeed have good performances which rival that of supervised models. We will explore some of these methods and their results below.　-->
研究者によりますと、コントラストな埋め込み方法を自己教師あり学習モデルに適用すると、教師ありモデルに匹敵する優れたパフォーマンスが得られることを実証的に発見しました。これらの方法のとその結果を以下で説明します。

<!-- ### Contrastive embedding  -->
### コントラスト的な埋め込み

<!-- Consider a pair ($x$, $y$), such that $x$ is an image and $y$ is a transformation of $x$ that preserves its content (rotation, magnification, cropping, *etc.*). We call this a **positive** pair.　-->
$ x $が画像で、$ y $がそのコンテンツを変換（回転、倍率、トリミングなど）しても$ x $を保持できるようなペア（$ x $、$ y $）について考えてみましょう。これは**ポジティブペア**と呼びます。

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig1.png" width="50%"/><br>
<!-- <b>Fig. 1</b>: Positive Pair -->
<b>Fig. 1</b>: ポジティブペア
</center>

<!-- Conceptually, contrastive embedding methods take a convolutional network, and feed $x$ and $y$ through this network to obtain two feature vectors: $h$ and $h'$. Because $x$ and $y$ have the same content (*i.e.* a positive pair), we want their feature vectors to be as similar as possible.
As a result, we choose a similarity metric (such as cosine similarity) and a loss function that maximizes the similarity between $h$ and $h'$. By doing this, we lower the energy for images on the training data manifold.　-->

概念的には、コントラスト的な埋め込み方法は畳み込みネットワークを使用し、このネットワークを介して$ x $と$ y $を入力して、$ h $と$ h'$の2つの特徴ベクトルを取得します。 $ x $と$ y $の内容は同じであるため（*つまり*ポジティブペア）、それらの特徴ベクトルをできるだけ類似させたいと考えています。
その結果、類似性メトリック（コサイン類似性など）と損失関数を選択し、$ h $と$ h'$の間の類似性を最大化するようにします。これで、トレーニングデータ多様体上の画像のエネルギーが押し下げられます。

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig2.png" width="50%"/><br>
<b>Fig. 2</b>: Negative Pair
</center>

<!-- However, we also have to push up on the energy of points outside this manifold. So we also generate **negative** samples ($x_{\text{neg}}$, $y_{\text{neg}}$), images with different content (different class labels, for example). We feed these to our network above, obtain feature vectors $h$ and $h'$, and now try to minimize the similarity between them.　-->

ただし、この多様体以外の点のエネルギーも押し上げる必要があります。そのため、**ネガティブサンプル**（$ x _ {\text {neg}} $、$ y _ {\text {neg}} $）、異なったコンテンツ（異なるクラスラベルなど）の画像も生成します。これらを上記のネットワークに入力し、特徴ベクトル$ h $と$ h '$を取得して、それらの間の類似性を最小限に抑えようとします。

<!-- This method allows us to push down on the energy of similar pairs while pushing up on the energy of dissimilar pairs. -->

この方法では、類似したペアのエネルギーを押し下げながら、異なるペアのエネルギーを押し上げることができます。

<!-- Recent results (on ImageNet) have shown that this method can produce features that are good for object recognition that can rival the features learned through supervised methods. -->

最近の結果（ImageNet）は、この方法が、教師あり方法で学習した機能に匹敵する、オブジェクト認識に適した機能を生成できることを示しています。


<!-- ### Self-Supervised Results (MoCo, PIRL, SimCLR) -->
### 自己監視結果（MoCo、PIRL、SimCLR）

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig3.png" height="75%" width="75%"/><br>
<b>Fig. 3</b>: PIRL and MoCo on ImageNet
</center>

<!-- As seen in the figure above, MoCo and PIRL achieve SOTA results (especially for lower-capacity models, with a small number of parameters). PIRL is starting to approach the top-1 linear accuracy of supervised baselines (~75%). -->
上の図に示されているように、MoCoとPIRLはSOTAの結果を達成します（特に、パラメーターの数が少なくスケールが小さいモデルで使う場合）。 PIRLは、学習タスクありベースラインの上位1の線形精度（〜75％）に近づき始めています。

<!-- We can understand PIRL more by looking at its objective function: NCE (Noise Contrastive Estimator) as follows. -->
PIRLは、その目的関数であるNCE（ノイズ・コントラスティブ・推定量）を次のように理解していきましょう。


$$
h(v_I,v_{I^t})=\frac{\exp\big[\frac{1}{\tau}s(v_I,v_{I^t})\big]}{\exp\big[\frac{1}{\tau}s(v_I,v_{I^t})\big]+\sum_{I'\in D_{N}}\exp\big[\frac{1}{\tau}s(v_{I^t},v_{I'})\big]}
$$

$$
L_{\text{NCE}}(I,I^t)=-\log\Big[h\Big(f(v_I),g(v_{I^t})\Big)\Big]-\sum_{I'\in D_N}\log\Big[1-h\Big(g(v_{I^t}),f(v_{I'})\Big)\Big]
$$

<!-- Here we define the similarity metric between two feature maps/vectors as the cosine similarity.　-->
ここでは、コサイン類似性で2つの特徴マップ/ベクトル間の類似性メトリックを定義します。


<!-- What PIRL does differently is that it doesn't use the direct output of the convolutional feature extractor. It instead defines different *heads* $f$ and $g$, which can be thought of as independent layers on top of the base convolutional feature extractor. -->

PIRLの違いは、畳み込み特徴を抽出することで直接出力を使用しないことです。代わりに、異なる *heads* $ f $と$ g $を定義します。これらは、基本畳み込み特徴抽出器の上にある独立層としてと考えるでしょう。



<!-- Putting everything together, PIRL's NCE objective function works as follows. In a mini-batch, we will have one positive (similar) pair and many negative (dissimilar) pairs. We then compute the similarity between the transformed image's feature vector ($I^t$) and the rest of the feature vectors in the minibatch (one positive, the rest negative). We then compute the score of a softmax-like function on the positive pair. Maximizing a softmax score means minimizing the rest of the scores, which is exactly what we want for an energy-based model. The final loss function, therefore, allows us to build a model that pushes the energy down on similar pairs while pushing it up on dissimilar pairs.　-->

すべてをまとめると、PIRLのNCE目的関数は次のように示します。ミニバッチでは、1つのポジティブ（類似した）ペアと多くのネガティブ（異なる）ペアがあります。次に、ミニバッチ内のなかに変換された画像の特徴ベクトル（$ I ^ t $）と残りの特徴ベクトル（1つは正、残りは負）の間の類似性を計算します。次に、ポジティブペアのソフトマックスのような関数のスコアを計算します。ソフトマックススコアを最大化することは、残りのスコアを最小化することと同じであり、これはエネルギーベースのモデルに必要なことです。したがって、最終な損失関数を使用すると、類似したペアでエネルギーを押し下げ、異なるペアでエネルギーを押し上げるモデルを構築できます。

<!-- Dr. LeCun mentions that to make this work, it requires a large number of negative samples. In SGD, it can be difficult to consistently maintain a large number of these negative samples from mini-batches. Therefore, PIRL also uses a cached memory bank. -->

LeCun博士は、これを機能させるには、多数のネガティブサンプルが必要であると述べています。 SGDでは、ミニバッチからのこれらのネガティブサンプルを一貫して維持することが難しい場合があります。したがって、PIRLはキャッシュされたメモリバンクも使用します。

<!-- **Question**: Why do we use cosine similarity instead of L2 Norm?
Answer: With an L2 norm, it's very easy to make two vectors similar by making them "short" (close to centre) or make two vectors dissimilar by making them very "long" (away from the centre). This is because the L2 norm is just a sum of squared partial differences between the vectors. Thus, using cosine similarity forces the system to find a good solution without "cheating" by making vectors short or long.　-->

**質問**： なぜL2-ノルムよりコサイン類似度を使用するのですか？
回答：L2ノルムでは、ベクトル間の部分差の2乗の合計にすぎないため、
2つのベクトルを「短く」（中心に近い）にすることで類似させることや、2つのベクトルを非常に「長く」（中心から遠ざける）することで非類似にすることは非常に簡単です。
したがって、コサイン類似度を使用すると、システム上、ベクトルを短くしたり長くしたりすることで、「カーニング」をせずに適切な解を見つけることができます。


### SimCLR

<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig5.png" height="75%" width="75%"/><br>
<b>Fig. 4</b>: SimCLR Results on ImageNet
</center>

<!-- SimCLR shows better results than previous methods. In fact, it reaches the performance of supervised methods on ImageNet, with top-1 linear accuracy on ImageNet.　The technique uses a sophisticated data augmentation method to generate similar pairs, and they train for a massive amount of time (with very, very large batch sizes) on TPUs.

Dr. LeCun believes that SimCLR, to a certain extend, shows the limit of contrastive methods. There are many, many regions in a high-dimensional space where you need to push up the energy to make sure it's actually higher than on the data manifold. As you increase the dimension of the representation, you need more and more negative samples to make sure the energy is higher in those places not on the manifold. -->

SimCLRは、上記方法よりも優れた結果が達成できます。これが、ImageNetの教師あり学習方法のパフォーマンスに達し、ImageNetの線形精度はトップ1になります。この手法では、高度なデータ拡張方法を使用して類似したペアを生成し、TPUで長時間（非常に大きなバッチサイズで）トレーニングを行います。

SimCLRは、ある程度、コントラスト的な方法の限界を示していることはLeCun博士がそれを信じています。高次元空間には、データ多様体よりも実際に高いことを確認するためにエネルギーを押し上げる必要がある多くの領域があります。また、表現の次元を増やせば増やすほど、マニフォールド上にない場所でエネルギーが高くなるようにするため、多くのネガティブサンプルが必要になります。


<!-- ## [Denoising autoencoder](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=1384s) -->
## [オートエンコーダのノイズ除去](https://www.youtube.com/watch?v=ZaVP2SY23nc&t=1384s)

<!-- In [week 7's practicum](https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-3/), we discussed denoising autoencoder. The model tends to learn the representation of the data by reconstructing corrupted input to the original input.
More specifically, we train the system to produce an energy function that grows quadratically as the corrupted data move away from the data manifold. -->

先週の講義により、オートエンコーダのノイズ除去について説明しました。このモデルは、破損した入力を元の入力に再構築することにより、データの表現を学習する傾向があります。特に、破損したデータがデータ多様体から離れるにつれて二次で成長したエネルギー関数を生成するようにそのシステムをトレーニングします。


<center>
<img src="{{site.baseurl}}/images/week08/08-1/fig6.png" height="75%" width="75%"/><br>
<b>Fig. 5</b>: Architecture of denoising autoencoder
</center>


<!-- ### Issues
However, there are several problems with denoising autoencoders. One problem is that in a high dimensional continuous space, there are uncountable ways to corrupt a piece of data. So there is no guarantee that we can shape the energy function by simply pushing up on lots of different locations.

Another problem with the model is that it performs poorly when dealing with images due to the lack of latent variables. Since there are many ways to reconstruct the images, the system produces various predictions and doesn't learn particularly good features.

Besides, corrupted points in the middle of the manifold could be reconstructed to both sides. This will create flat spots in the energy function and affect the overall performance. -->

### 問題

ただし、オートエンコーダのノイズ除去にはいくつかの問題があります。 1つの問題は、高次元の連続空間では、データの一部を破損する無数の方法があることです。したがって、さまざまな場所を押し上げるだけでエネルギー関数を形成できるという保証はありません。
モデルのもう1つの問題は、潜在変数がない画像を処理するときにパフォーマンスが低下することです。画像を再構成する方法はたくさんあるので、システムはさまざまな予測を生成し、特に優れた特徴を学習しません。さらに、マニフォールドの中央にある破損したポイントは、両側に再構築できます。これにより、エネルギー関数にフラットスポットが作成され、全体的なパフォーマンスに影響します。


<!-- ## Other Contrastive Methods

There are other contrastive methods such as contrastive divergence, Ratio Matching, Noise Contrastive Estimation, and Minimum Probability Flow. We will briefly discuss the basic idea of contrastive divergence.　-->

## 他のコントラスト的な方法

コントラスト的な発散、比率マッチング、ノイズのコントラスティブな推定、最小確率フローなど、他のコントラスト的な方法があります。こちらにコントラスト的な発散の基本的な考え方について簡単に説明します。

<!-- ### Contrastive Divergence

Contrastive divergence (CD) is another model that learns the representation by smartly corrupting the input sample.　In a continuous space, we first pick a training sample $y$ and lower its energy. For that sample, we use some sort of gradient-based process to move down on the energy surface with noise. If the input space is discrete, we can instead perturb the training sample randomly to modify the energy. If the energy we get is lower, we keep it. Otherwise, we discard it with some probability.

Keep doing so will eventually lower the energy of $y$. We can then update the parameter of our energy function by comparing $y$ and the contrasted sample $\bar y$ with some loss function.　-->

### コントラスト的な発散

対照的発散（CD）は、入力サンプルをスマートに破損することによって表現を学習する別のモデルです。連続空間では、最初にトレーニングサンプル$ y $を選択し、そのエネルギーを下げます。そのサンプルでは、​​ある種の勾配ベースのプロセスを使用して、ノイズとともにエネルギーを押し下げます。入力空間が離散的である場合、代わりにトレーニングサンプルをランダムに摂動させてエネルギーを変更できます。
得るエネルギーが低ければ維持して、そうではないと、ある程度の確率で破棄します。これをやり続けると、最終的に$ y $のエネルギーが低下できます。次に、$ y $と対照的なサンプル$ \bar y $を損失関数と比較することにより、エネルギー関数のパラメーターを更新できます。

<!-- ### Persistent Contrastive Divergence

One of the refinements of contrastive divergence is persistent contrastive divergence. The system uses a bunch of "particles" and remembers their positions. These particles are moved down on the energy surface just like what we did in the regular CD.

Eventually, they will find low energy places in our energy surface and will cause them to be pushed up. However, the system does not scale well as the dimensionality increases. -->

### 持続的なコントラストな発散

コントラスト的な発散（CD）からの改良したものは、持続的なコントラスト的な発散です。システムは一連の「粒子」を使用し、それらの位置を記憶します。これらの粒子は、通常のCDで行ったのと同じように、エネルギー的に下に移動します。最終的に、この粒子らはエネルギー面で低エネルギーの場所を見つけ、それらを押し上げさせます。ただし、次元が増加するにつれて、システムは適切にスケーリングされません。

---
lang-ref: ch.08
title: Week 8
authors: Vishwaesh Rajiv, Wenjun Qu, Xulai Jiang, Shuya Zhao
date: 23 Mar 2020
lang: ja
translation-date: 28 Nov 2020
translator: Jesmer Wong
---

<!-- ## Lecture part A -->
##講義　第一部分

<!-- In this section, we focused on the introduction of contrastive methods in Energy-Based Models in several aspects. First, we discuss the advantage brought by applying contrastive methods in self-supervised learning. Second, we discussed the architecture of denoising autoencoders and their weakness in image reconstruction tasks. We also talked about other contrastive methods, like contrastive divergence and persistent contrastive divergence. -->

このセクションでは、いくつかの角度でエネルギー・ベース・モデル（EBM）にコントラスト的な方法を中心として紹介していきましょう。
まずは、自己教師あり学習にコントラスト的な方法を適用するその利点について説明します。次に、ノイズ除去オートエンコーダのアーキテクチャと、画像再構成タスクにおけるそれらの弱点について説明します。また、コントラスト的な発散や持続的なコントラスト発散のような他のコントラスト的な方法についても説明しました。


<!-- ## Lecture part B -->
##講義　第二部分

<!-- In this section, we discussed regularized latent variable EBMs in detail covering concepts of conditional and unconditional versions of these models. We then discussed the algorithms of ISTA, FISTA and LISTA and look at examples of sparse coding and filters learned from convolutional sparse encoders. Finally we talked about Variational Auto-Encoders and the underlying concepts involved. -->

このセクションでは、正則化された潜在変数つきのEBMについて、とくに条件付きバージョンと無条件バージョンのモデル概念を詳しく説明します。次に、ISTA、FISTA、およびLISTAのアルゴリズムについて説明し、畳み込みスパースエンコーダーから学習したスパースコーディングとフィルターの例を見ていきます。最後に、変分付きオートエンコーダーとそれに関連する基本的な概念について説明していきます。


<!-- ## Practicum -->
実習

<!-- In this section, we discussed a specific type of generative model called Variational Autoencoders and compared their functionalities and advantages over Classic Autoencoders. We explored the objective function of VAE in detail, understanding how it enforced some structure in the latent space. 
Finally, we implemented and trained a VAE on the MNIST dataset and used it to generate new samples. -->

このセクションでは、変分付きオートエンコーダー（VAE）とばれるある特定のタイプの生成モデルについて説明し、この機能と従来のオートエンコーダーに対する利点を比較してみました。VAEの目的関数を詳細に理解し、どうやって潜在スペースに構造させて適用する方法を理解しました。最後に、MNISTデータセットにVAEでトレーニングし、それを使用して新しいサンプルを生成しました。


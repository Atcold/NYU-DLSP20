---
lang-ref: ch.14
title: Week 14
lang: ja
translation-date: 6 Dec 2020
translator: Shiro Takagi
---


<!-- ## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net. -->

## レクチャーパートA

本節では、構造化予測について考察しました。まず、エネルギーベース因子グラフとその効率的な推論について紹介しました。次に、"浅い"因子を持つ単純なエネルギーベース因子グラフの例を示しました。最後に、グラフ変換ネットについて述べました。


<!-- ## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models. -->

## レクチャーパートB

講義の第2項では、エネルギーベースのモデルへのグラフィカルモデルの手法の適用についてさらに議論します。異なる損失関数を比較した後、ビタビアルゴリズムとフォワードアルゴリズムのgraphical transformerネットワークへの適用について議論します。その後、誤差逆伝播のラグランジュ方程式の議論に移り、エネルギーベースのモデルのための変分推論に移ります。

<!-- ## Practicum

When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise. -->

## 演習

ディープニューラルネットワークのような高度にパラメトライズされたモデルを学習する場合、学習データに過学習するリスクがあります。これは、より大きな汎化誤差につながります。過学習を減らすために、訓練に正則化を導入して、モデルがノイズに適合するような解を見つけることを抑制することができます。

---
lang-ref: ch.14
title: Week 14
lang: ja
translation-date: 6 Dec 2020
translator: Shiro Takagi
---


<!-- ## Lecture part A

In this section, we discussed the structured prediction. We first introduced the Energy-Based factor graph and efficient inference for it. Then we gave some examples for simple Energy-Based factor graphs with “shallow” factors. Finally, we discussed the Graph Transformer Net. -->

## レクチャーパートA

本節では、構造化予測について考察しました。まず、エネルギーベース因子グラフとその効率的な推論について紹介しました。次に、「浅い」因子を持つ単純なエネルギーベース因子グラフの例を示しました。最後に、Graph Transformer Netについて述べました。


<!-- ## Lecture part B

The second leg of the lecture further discusses the application of graphical model methods to energy-based models. After spending some time comparing different loss functions, we discuss the application of the Viterbi algorithm and forward algorithm to graphical transformer networks. We then transition to discussing the Lagrangian formulation of backpropagation and then variational inference for energy-based models. -->

## レクチャーパートB

この講義では、エネルギーベースモデルへのグラフィカルモデルの手法の適用についてさらに議論します。異なる損失関数を比較した後、ビタビアルゴリズムとフォワードアルゴリズムのgraphical transformerネットワークへの適用について議論します。その後、誤差逆伝播のラグランジュの未定乗数法による定式化の議論に移り、そしてエネルギーベースのモデルのための変分推論について議論します。

<!-- ## Practicum

When training highly parametrised models such as deep neural networks there is a risk of overfitting to the training data. This leads to greater generalization error. To help reduce overfitting we can introduce regularization into our training, discouraging certain solutions to decrease the extent to which our models will fit to noise. -->

## 演習

ディープニューラルネットワークのような高度にパラメトライズされたモデルを学習する場合、学習データにオーバーフィットするリスクがあります。これは、より大きな汎化誤差につながってしまいます。過学習を減らすために、訓練に正則化を導入して、モデルがノイズに適合するような解を見つけることを抑制することができます。

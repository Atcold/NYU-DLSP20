---
lang-ref: ch.07
lang: ja
title: 第7週
translation-date: 12 Aug 2020
translator: Shiro Takagi
---

<!-- ## Lecture part A -->
## レクチャー　パート　A

<!-- We introduced the concept of the energy-based models and the intention for different approaches other than feed-forward networks. To solve the difficulty of the inference in EBM, latent variables are used to provide auxiliary information and enable multiple possible predictions. Finally, the EBM can generalize to probabilistic model with more flexible scoring functions. -->

エネルギーベースモデルという考え方と、フィーフォフォワードネットワークとは異なるアプローチの意図を紹介しました。EBMにおける推論の難しさを解決するために、潜在変数を用いることで補助情報を提供することで、複数のありうる予測を可能にしました。最後に、EBMはより柔軟なスコア関数を持つ確率モデルへと一般化することができました。

<!-- ## Lecture part B -->
## レクチャー　パート　B

<!-- We discussed self-supervised learning, introduced how to train an Energy-based models, discussed Latent Variable EBM, specifically with an explained K-means example. We also introduced Contrastive Methods, explained a denoising autoencoder with a topographic map, the training process, and how it can be used, followed by an introduction to BERT. Finally, we talked about Contrastive Divergence, also explained using a topographic map. -->

自己教師あり学習について議論し、エネルギーベースのモデルの学習方法を紹介し、潜在変数EBMについて議論しました。具体的にはK平均法の例を挙げて説明しました。また、コントラスティブ法についても紹介し、地形図を用いてデノイジングオートエンコーダー、学習プロセス、その使用方法について説明し、続いてBERTの紹介をしました。最後に、コントラスティブダイバージェンスについて、同じく地形図を用いて説明しました。

<!-- ## Practicum -->
## 演習
<!-- We discussed some applications of Autoencoders and talked about why we want to use them. Then we talked about different architectures of Autoencoders (under or over complete hidden layer), how to avoid overfitting issues and the loss functions we should use. Finally we implemented a standard Autoencoder and a denoising Autoencoder. -->
オートエンコーダーの応用について議論し、なぜオートエンコーダーを使いたいのかを話しました。次に、オートエンコーダーの異なるアーキテクチャ（アンダーコンプリートな隠れ層とオーバーコンプリートな隠れ層）、過学習の問題を回避する方法、使用すべき損失関数について話しました。最後に、標準的なオートエンコーダーとデノイジングオートエンコーダーを実装しました。
